
@misc{mcinnes_umap_2020,
	title = {{UMAP}: {Uniform} {Manifold} {Approximation} and {Projection} for {Dimension} {Reduction}},
	shorttitle = {{UMAP}},
	url = {http://arxiv.org/abs/1802.03426},
	doi = {10.48550/arXiv.1802.03426},
	abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
	urldate = {2023-04-09},
	publisher = {arXiv},
	author = {McInnes, Leland and Healy, John and Melville, James},
	month = sep,
	year = {2020},
	note = {arXiv:1802.03426 [cs, stat]},
	keywords = {Computer Science - Computational Geometry, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{chollet_deep_2021,
	title = {Deep {Learning} with {Python}, {Second} {Edition}},
	isbn = {978-1-63835-009-5},
	abstract = {Unlock the groundbreaking advances of deep learning with this extensively revised edition of the bestselling original. Learn directly from the creator of Keras and master practical Python deep learning techniques that are easy to apply in the real world.In Deep Learning with Python, Second Edition you will learn:  Deep learning from first principles Image classification \& image segmentation Timeseries forecasting Text classification and machine translation Text generation, neural style transfer, and image generation  Deep Learning with Python has taught thousands of readers how to put the full capabilities of deep learning into action. This extensively revised second edition introduces deep learning using Python and Keras, and is loaded with insights for both novice and experienced ML practitioners. You’ll learn practical techniques that are easy to apply in the real world, and important theory for perfecting neural networks.  Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.  About the technology Recent innovations in deep learning unlock exciting new software capabilities like automated language translation, image recognition, and more. Deep learning is becoming essential knowledge for every software developer, and modern tools like Keras and TensorFlow put it within your reach, even if you have no background in mathematics or data science.   About the book Deep Learning with Python, Second Edition introduces the field of deep learning using Python and the powerful Keras library. In this new edition, Keras creator François Chollet offers insights for both novice and experienced machine learning practitioners. As you move through this book, you’ll build your understanding through intuitive explanations, crisp illustrations, and clear examples. You’ll pick up the skills to start developing deep-learning applications.  What's inside  Deep learning from first principles Image classification and image segmentation Time series forecasting Text classification and machine translation Text generation, neural style transfer, and image generation  About the reader For readers with intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required.  About the author François Chollet is a software engineer at Google and creator of the Keras deep-learning library.  Table of Contents 1 What is deep learning? 2 The mathematical building blocks of neural networks 3 Introduction to Keras and TensorFlow 4 Getting started with neural networks: Classification and regression 5 Fundamentals of machine learning 6 The universal workflow of machine learning 7 Working with Keras: A deep dive 8 Introduction to deep learning for computer vision 9 Advanced deep learning for computer vision 10 Deep learning for timeseries 11 Deep learning for text 12 Generative deep learning 13 Best practices for the real world 14 Conclusions},
	language = {en},
	publisher = {Simon and Schuster},
	author = {Chollet, Francois},
	month = dec,
	year = {2021},
	note = {Google-Books-ID: mjVKEAAAQBAJ},
	keywords = {Computers / Data Science / Machine Learning, Computers / Data Science / Neural Networks, Computers / Languages / Python},
}

@misc{szegedy_going_2014,
	title = {Going {Deeper} with {Convolutions}},
	url = {http://arxiv.org/abs/1409.4842},
	doi = {10.48550/arXiv.1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	urldate = {2023-04-09},
	publisher = {arXiv},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = sep,
	year = {2014},
	note = {arXiv:1409.4842 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{ravikumar_hyperspectral_2022,
	title = {Hyperspectral {Image} {Classification} {Using} {Deep} {Matrix} {Capsules}},
	volume = {01},
	doi = {10.1109/ICDSAAI55433.2022.10028853},
	abstract = {Hyperspectral image (HSI) classification is used in multiple domains like precision agriculture, mineral exploration, remote sensing, and others. Conventionally, couvolutional neural networks (CNNs) were used in HSI classification, however they have limitations in exploiting spectral-spatial relationships, which is a key factor in understanding HSI. Even though deeper CNN architectures and use of 3-D-CNNs mitigate the above problem to a certain extent, they have increased computational complexity, which inhibits their use in resource-limited devices like IoT and edge computing devices. In this paper, we propose a novel method based on the concept of matrix capsules with Expectation-Maximization (EM) routing algorithm which is specifically designed to accommodate the nuances in the HSI data to efficiently tackle the aforementioned problems. The capsule units enable effective identification of spectral siguatures and part-whole relationships in the data while EM routing ensures viewpoint-invariance. Three representative HSI data sets are used to verify the effectiveness of the proposed method. The empirical results demonstrate that the proposed method is better than the current state-of-the-art methods in terms of accuracy while having 25 times fewer model parameters and requiring over 65 times less storage space. The source code can be found at https://github.com/DeepMatrixCapsules/DeepMatrixCapsules.},
	booktitle = {2022 {International} {Conference} on {Data} {Science}, {Agents} \& {Artificial} {Intelligence} ({ICDSAAI})},
	author = {Ravikumar, Anirudh and Rohit, P N and Nair, Mydhili K and Bhatia, Vimal},
	month = dec,
	year = {2022},
	keywords = {Benchmark testing, Capsule network, Computer architecture, Feature extraction, Neural networks, Routing, Sensors, Source coding, classification, convolutional neural network (CNN), deep learning, expectation-maximization (EM) routing, hyperspectral image (HSI) classification, matrix capsule, remote sensing, spectral-spatial, viewpoint-invariance},
	pages = {1--7},
}

@misc{chollet_xception_2017,
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	url = {http://arxiv.org/abs/1610.02357},
	doi = {10.48550/arXiv.1610.02357},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	urldate = {2023-04-09},
	publisher = {arXiv},
	author = {Chollet, François},
	month = apr,
	year = {2017},
	note = {arXiv:1610.02357 [cs]
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{han_state_2020,
	title = {State of the art in digital surface modelling from multi-view high resolution satellite images},
	volume = {V-2-2020},
	url = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/V-2-2020/351/2020/},
	doi = {10.5194/isprs-annals-V-2-2020-351-2020},
	abstract = {{\textbackslash}textlessp{\textbackslash}textgreater{\textbackslash}textlessstrong class="journal-contentHeaderColor"{\textbackslash}textgreaterAbstract.{\textbackslash}textless/strong{\textbackslash}textgreater Data from the optical satellite imaging sensors running 24/7, is collecting in embarrassing abundance nowadays. Besides more suitable for large-scale mapping, multi-view high-resolution satellite images (HRSI) are cheaper when comparing to Light Detection And Ranging (LiDAR) data and aerial remotely sensed images, which are more accessible sources for digital surface modelling and updating. Digital Surface Model (DSM) generation is one of the most critical steps for mapping, 3D modelling, and semantic interpretation. Computing DSM from this dataset is relatively new, and several solutions exist in the market, both commercial and open-source solutions, the performances of these solutions have not yet been comprehensively analyzed. Although some works and challenges have focused on the DSM generation pipeline and the geometric accuracy of the generated DSM, the evaluations, however, do not consider the latest solutions as the fast development in this domain. In this work, we discussed the pipeline of the considered both commercial and opensource solutions, assessed the accuracy of the multi-view satellite image-based DSMs generation methods with LiDAR-derived DSM as the ground truth. Three solutions, including Satellite Stereo Pipeline (S2P), PCI Geomatica, and Agisoft Metashape, are evaluated on a WorldView-3 multi-view satellite dataset both quantitatively and qualitatively with the LiDAR ground truth. Our comparison and findings are presented in the experimental section.{\textbackslash}textless/p{\textbackslash}textgreater},
	language = {English},
	urldate = {2021-12-16},
	booktitle = {{ISPRS} {Annals} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Han, Y. and Wang, S. and Gong, D. and Wang, Y. and Wang, Y. and Ma, X.},
	month = aug,
	year = {2020},
	pages = {351--356},
}

@article{kumar_new_2021,
	title = {New effective spectral matching measures for hyperspectral data analysis},
	volume = {42},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431161.2021.1890265},
	doi = {10.1080/01431161.2021.1890265},
	abstract = {The successful implementation of Spectral Matching Measures (SMMs) often plays a crucial role in material discrimination and classification using hyperspectral dataset. The commonly exploited SMMs, such as Spectral Angle Mapper (SAM), Spectral Information Divergence (SID), and their hybrid, i.e., SIDSAMtan, show limited discrimination ability while discriminating spectrally similar materials. This study presents three new effective SMMs named Dice Spectral Similarity Coefficient (DSSC), Kumar–Johnson Spectral Similarity Coefficient (KJSSC), and a hybrid of DSSC and KJSSC, i.e., KJDSSCtan, for accurate discrimination of spectrally similar materials. A wide range of hyperspectral datasets of minerals and vegetation acquired under laboratory and real atmospheric conditions were used to compare and evaluate the performance of newly proposed and existing SMMs using Relative Spectral Discrimination Power (RSDPW) statistics. We also assessed the discrimination ability of the proposed and existing SMMs using spectra of selected minerals and vegetation species with an added component of random noise and linearly synthesized mixed spectra. An in-depth comparison and evaluation of different SMMs demonstrated that the discrimination power of the proposed SMMs is significantly higher than existing SMMs. The proposed SMMs also outperform existing SMMs when discriminating noisy and linearly synthesized mixed counterparts. The KJSSC and DSSC show similar efficacy in discriminating spectra of minerals and vegetation, whereas their hybrid measure, i.e., KJDSSCtan shows significantly higher spectral discrimination ability. Therefore, the newly proposed hybrid measure, i.e., KJDSSCtan is recommended over existing SMMs for successful material discrimination and classification using hyperspectral data.},
	number = {11},
	urldate = {2023-02-24},
	journal = {International Journal of Remote Sensing},
	author = {Kumar, Chandan and Chatterjee, Snehamoy and Oommen, Thomas and Guha, Arindam},
	month = jun,
	year = {2021},
	pages = {4126--4156},
}

@phdthesis{poux_smart_2019,
	address = {Liège, Belgique},
	type = {{PhD} {Thesis}},
	title = {The {Smart} {Point} {Cloud}: {Structuring} {3D} intelligent point data},
	shorttitle = {The {Smart} {Point} {Cloud}},
	url = {https://orbi.uliege.be/handle/2268/235520},
	abstract = {Discrete spatial datasets known as point clouds often lay the groundwork for decision-making applications. E.g., we can use such data as a reference for autonomous cars and robot’s navigation, as a layer for floor-plan’s creation and building’s construction, as a digital asset for environment modelling and incident prediction... Applications are numerous, and potentially increasing if we consider point clouds as digital reality assets. Yet, this expansion faces technical limitations mainly from the lack of semantic information within point ensembles. Connecting knowledge sources is still a very manual and time-consuming process suffering from error-prone human interpretation. This highlights a strong need for domain-related data analysis to create a coherent and structured information. The thesis clearly tries to solve automation problematics in point cloud processing to create intelligent environments, i.e. virtual copies that can be used/integrated in fully autonomous reasoning services. We tackle point cloud questions associated with knowledge extraction – particularly segmentation and classification – structuration, visualisation and interaction with cognitive decision systems. We propose to connect both point cloud properties and formalized knowledge to rapidly extract pertinent information using domain-centered graphs. The dissertation delivers the concept of a Smart Point Cloud (SPC) Infrastructure which serves as an interoperable and modular architecture for a unified processing. It permits an easy integration to existing workflows and a multi-domain specialization through device knowledge, analytic knowledge or domain knowledge. Concepts, algorithms, code and materials are given to replicate findings and extend current applications.},
	language = {en},
	urldate = {2021-12-26},
	school = {Université de Liège, Liège, Belgique},
	author = {Poux, Florent},
	month = jun,
	year = {2019},
}

@article{modrego-fernandez_propuesta_2022,
	title = {Propuesta metodológica para análisis arqueológicos de altitud: aplicación al {Castillo} de {Arenas} ({Campillo} de {Arenas}, {Jaén})},
	copyright = {Derechos de autor 2022 Cuadernos de Arqueología de la Universidad de Navarra},
	issn = {2387-1814},
	shorttitle = {Propuesta metodológica para análisis arqueológicos de altitud},
	url = {https://revistas.unav.edu/index.php/cuadernos-de-arqueologia/article/view/42145},
	doi = {10.15581/012.30.2.012},
	abstract = {This article tries to establish a methodology on altitude analysis in Landscape Archaeology. The interest of this variable for studies of historical settlement has been relegated by the prominence for the protagonism of other such as the visibility or the capture of nearby resources. In this way, two methods are exposed, with two different work scales, in order to create this non-existent corpus. As a case study, the Castillo de Arenas (Campillo de Arenas, Jaén) is presented, a fortress that operated on the Castilian-Nasrid border between the 13th-15th centuries.},
	language = {es},
	urldate = {2023-04-05},
	journal = {Cuadernos de Arqueología de la Universidad de Navarra},
	author = {Modrego-Fernández, Roque and Martín-Civantos, José-María},
	month = dec,
	year = {2022},
	keywords = {Altitud, Arqueología del Paisaje, Estadística Espacial, SIG, fortalezas islámicas},
	pages = {273--300},
}

@article{fernandez_hervas_castillo_1986,
	title = {El castillo de {Arenas}, fortaleza nazarita del {Reino} de {Granada}},
	issn = {0008-7505},
	url = {https://dialnet.unirioja.es/servlet/articulo?codigo=7032847},
	language = {spa},
	number = {91},
	urldate = {2023-04-05},
	journal = {Castillos de España: publicación de la Asociación Española de Amigos de los Castillos},
	author = {Fernández Hervás, Enrique},
	year = {1986},
	note = {Publisher: Asociación Española de Amigos de los Castillos
Section: Castillos de España: publicación de la Asociación Española de Amigos de los Castillos},
	pages = {41--44},
}

@article{buslaev_albumentations_2020,
	title = {Albumentations: {Fast} and {Flexible} {Image} {Augmentations}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	shorttitle = {Albumentations},
	url = {https://www.mdpi.com/2078-2489/11/2/125},
	doi = {10.3390/info11020125},
	abstract = {Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve corresponding output labels. In computer vision, image augmentations have become a common implicit regularization technique to combat overfitting in deep learning models and are ubiquitously used to improve performance. While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations of flipping, rotating, scaling, and cropping. Moreover, image processing speed varies in existing image augmentation libraries. We present Albumentations, a fast and flexible open source library for image augmentation with many various image transform operations available that is also an easy-to-use wrapper around other augmentation libraries. We discuss the design principles that drove the implementation of Albumentations and give an overview of the key features and distinct capabilities. Finally, we provide examples of image augmentations for different computer vision tasks and demonstrate that Albumentations is faster than other commonly used image augmentation tools on most image transform operations.},
	language = {en},
	number = {2},
	urldate = {2023-03-31},
	journal = {Information},
	author = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
	month = feb,
	year = {2020},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {computer vision, data augmentation, deep learning},
	pages = {125},
}

@inproceedings{li_multi-branch_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multi-branch {Semantic} {GAN} for {Infrared} {Image} {Generation} from {Optical} {Image}},
	isbn = {978-3-030-36189-1},
	doi = {10.1007/978-3-030-36189-1_40},
	abstract = {Infrared remote sensing images capture the information of ground objects by their thermal radiation differences. However, the facility required for infrared imaging is not only priced high but also demands strict testing conditions. Thus it becomes an important topic to seek a way to convert easily-obtained optical remote sensing images into infrared remote sensing images. The conventional approaches cannot generate satisfactory infrared images due to the challenge of this task and many unknown parameters to be determined. In this paper, we proposed a novel multi-branch semantic GAN (MBS-GAN) for infrared image generation from the optical image. In the proposed model, we draw on the idea from Ensemble Learning and propose to use more than one generator to synthesize the infrared images with different semantic information. Specially, we integrate scene classification into image transformation to train models with scene information, which assists learned generation models to capture more semantic characteristics. The generated images are evaluated by PSNR, SSIM and cosine similarity. The experimental results prove that this proposed method is able to generate images retaining the infrared radiation characteristics of ground objects and performs well in converting optical images to infrared images.},
	language = {en},
	booktitle = {Intelligence {Science} and {Big} {Data} {Engineering}. {Visual} {Data} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Li, Lei and Li, Pengfei and Yang, Meng and Gao, Shibo},
	editor = {Cui, Zhen and Pan, Jinshan and Zhang, Shanshan and Xiao, Liang and Yang, Jian},
	year = {2019},
	keywords = {Generative adversarial networks, Infrared image generation, Residual neural network},
	pages = {484--494},
}

@article{zhang_synthetic_2019,
	title = {Synthetic {Data} {Generation} for {End}-to-{End} {Thermal} {Infrared} {Tracking}},
	volume = {28},
	issn = {1941-0042},
	doi = {10.1109/TIP.2018.2879249},
	abstract = {The usage of both off-the-shelf and end-to-end trained deep networks have significantly improved the performance of visual tracking on RGB videos. However, the lack of large labeled datasets hampers the usage of convolutional neural networks for tracking in thermal infrared (TIR) images. Therefore, most state-of-the-art methods on tracking for TIR data are still based on handcrafted features. To address this problem, we propose to use image-to-image translation models. These models allow us to translate the abundantly available labeled RGB data to synthetic TIR data. We explore both the usage of paired and unpaired image translation models for this purpose. These methods provide us with a large labeled dataset of synthetic TIR sequences, on which we can train end-to-end optimal features for tracking. To the best of our knowledge, we are the first to train end-to-end features for TIR tracking. We perform extensive experiments on the VOT-TIR2017 dataset. We show that a network trained on a large dataset of synthetic TIR data obtains better performance than one trained on the available real TIR data. Combining both data sources leads to further improvement. In addition, when we combine the network with motion features, we outperform the state of the art with a relative gain of over 10\%, clearly showing the efficiency of using synthetic data to train end-to-end TIR trackers.},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Zhang, Lichao and Gonzalez-Garcia, Abel and van de Weijer, Joost and Danelljan, Martin and Khan, Fahad Shahbaz},
	month = apr,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Correlation, Feature extraction, Target tracking, Videos, Visual tracking, Visualization, deep learning, generative networks, thermal infrared},
	pages = {1837--1850},
}

@article{li_i-gans_2021,
	title = {I-{GANs} for {Infrared} {Image} {Generation}},
	volume = {2021},
	issn = {1076-2787},
	url = {https://www.hindawi.com/journals/complexity/2021/6635242/},
	doi = {10.1155/2021/6635242},
	abstract = {The making of infrared templates is of great significance for improving the accuracy and precision of infrared imaging guidance. However, collecting infrared images from fields is difficult, of high cost, and time-consuming. In order to address this problem, an infrared image generation method, infrared generative adversarial networks (I-GANs), based on conditional generative adversarial networks (CGAN) architecture is proposed. In I-GANs, visible images instead of random noise are used as the inputs, and the D-LinkNet network is also utilized to build the generative model, enabling improved learning of rich image textures and identification of dependencies between images. Moreover, the PatchGAN architecture is employed to build a discriminant model to process the high-frequency components of the images effectively and reduce the amount of calculation required. In addition, batch normalization is used to optimize the training process, and thereby, the instability and mode collapse of the generated adversarial network training can be alleviated. Finally, experimental verification is conducted on the produced infrared/visible light dataset (IVFG). The experimental results reveal that high-quality and reliable infrared data are generated by the proposed I-GANs.},
	language = {en},
	urldate = {2023-03-17},
	journal = {Complexity},
	author = {Li, Bing and Xian, Yong and Su, Juan and Zhang, Da Q. and Guo, Wei L.},
	month = mar,
	year = {2021},
	note = {Publisher: Hindawi},
	pages = {e6635242},
}

@article{yi_cycle_2023,
	title = {Cycle {Generative} {Adversarial} {Network} {Based} on {Gradient} {Normalization} for {Infrared} {Image} {Generation}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/13/1/635},
	doi = {10.3390/app13010635},
	abstract = {Image generation technology is currently one of the popular directions in computer vision research, especially regarding infrared imaging, bearing critical applications in the military field. Existing algorithms for generating infrared images from visible images are usually weak in perceiving the salient regions of images and cannot effectively highlight the ability to generate texture details in infrared images, resulting in less texture details and poorer generated image quality. In this study, a cycle generative adversarial network method based on gradient normalization was proposed to address the current problems of poor infrared image generation, lack of texture detail and unstable models. First, to address the problem of limited feature extraction capability of the UNet generator network that makes the generated IR images blurred and of low quality, the use of the residual network with better feature extraction capability in the generator was employed to make the generated infrared images highly defined. Secondly, in order to solve issues concerning severe lack of detailed information in the generated infrared images, channel attention and spatial attention mechanisms were introduced into the ResNet with the attention mechanism used to weight the generated infrared image features in order to enhance feature perception of the prominent regions of the image, helping to generate image details. Finally, to tackle the problem where the current training models of adversarial generator networks are insufficiently stable, which leads to easy collapse of the model, a gradient normalization module was introduced in the discriminator network to stabilize the model and render it less prone to collapse during the training process. The experimental results on several datasets showed that the proposed method obtained satisfactory data in terms of objective evaluation metrics. Compared with the cycle generative adversarial network method, the proposed method in this work exhibited significant improvement in data validity on multiple datasets.},
	language = {en},
	number = {1},
	urldate = {2023-03-17},
	journal = {Applied Sciences},
	author = {Yi, Xing and Pan, Hao and Zhao, Huaici and Liu, Pengfei and Zhang, Canyu and Wang, Junpeng and Wang, Hao},
	month = jan,
	year = {2023},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {channel attention, cycle generative adversarial networks, gradient normalization, residual networks, spatial attention},
	pages = {635},
}

@inproceedings{kniaz_thermalgan_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{ThermalGAN}: {Multimodal} {Color}-to-{Thermal} {Image} {Translation} for {Person} {Re}-identification in {Multispectral} {Dataset}},
	isbn = {978-3-030-11024-6},
	shorttitle = {{ThermalGAN}},
	doi = {10.1007/978-3-030-11024-6_46},
	abstract = {We propose a ThermalGAN framework for cross-modality color-thermal person re-identification (ReID). We use a stack of generative adversarial networks (GAN) to translate a single color probe image to a multimodal thermal probe set. We use thermal histograms and feature descriptors as a thermal signature. We collected a large-scale multispectral ThermalWorld dataset for extensive training of our GAN model. In total the dataset includes 20216 color-thermal image pairs, 516 person ID, and ground truth pixel-level object annotations. We made the dataset freely available (http://www.zefirus.org/ThermalGAN/). We evaluate our framework on the ThermalWorld dataset to show that it delivers robust matching that competes and surpasses the state-of-the-art in cross-modality color-thermal ReID.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Kniaz, Vladimir V. and Knyaz, Vladimir A. and Hladůvka, Jiří and Kropatsch, Walter G. and Mizginov, Vladimir},
	editor = {Leal-Taixé, Laura and Roth, Stefan},
	year = {2019},
	keywords = {Conditional GAN, Person re-identification, Thermal images},
	pages = {606--624},
}

@misc{isola_image--image_2018,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	doi = {10.48550/arXiv.1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	urldate = {2023-03-17},
	publisher = {arXiv},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv:1611.07004 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ozkanoglu_infragan_2022,
	title = {{InfraGAN}: {A} {GAN} architecture to transfer visible images to infrared domain},
	volume = {155},
	issn = {0167-8655},
	shorttitle = {{InfraGAN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865522000332},
	doi = {10.1016/j.patrec.2022.01.026},
	abstract = {Utilizing both visible and infrared (IR) images in various deep learning based computer vision tasks has been a recent trend. Consequently, datasets having both visible and IR image pairs are desired in many applications. However, while large image datasets taken at the visible spectrum can be found in many domains, large IR-based datasets are not easily available in many domains. The lack of IR counterparts of the available visible image datasets limits existing deep algorithms to perform on IR images effectively. In this paper, to overcome with that challenge, we introduce a generative adversarial network (GAN) based solution and generate the IR equivalent of a given visible image by training our deep network to learn the relation between visible and IR modalities. In our proposed GAN architecture (InfraGAN), we introduce using structural similarity as an additional loss function. Furthermore, in our discriminator, we do not only consider the entire image being fake or real but also each pixel being fake or real. We evaluate our comparative results on three different datasets and report the state of the art results over five metrics when compared to Pix2Pix and ThermalGAN architectures from the literature. We report up to +16\% better performance in Structural Similarity Index Measure (SSIM) over Pix2Pix and +8\% better performance over ThermalGAN for VEDAI dataset. Further gains on different metrics and on different datasets are also reported in our experiments section.},
	language = {en},
	urldate = {2023-03-17},
	journal = {Pattern Recognition Letters},
	author = {Özkanoğlu, Mehmet Akif and Ozer, Sedat},
	month = mar,
	year = {2022},
	keywords = {Domain transfer, GANs, Infrared image generation},
	pages = {69--76},
}

@article{shanmugam_spectral_2014,
	title = {Spectral matching approaches in hyperspectral image processing},
	volume = {35},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431161.2014.980922},
	doi = {10.1080/01431161.2014.980922},
	abstract = {Many spectral matching algorithms, ranging from the traditional clustering techniques to the recent automated matching models, have evolved. This paper provides a review and up-to-date information on the past and current role of the spectral matching approaches adopted in hyperspectral satellite image processing. The need for spectral matching has been deliberated and a list of spectral matching algorithms has been compared and described. A review of the conventional spectral angle measures and the advanced automated spectral matching tools indicates that, for better performance of target detection, there is a need for combining two or more spectral matching techniques. From the studies of several authors, it is inferred that continuous improvement in the matching techniques over the past few years is due to the need to handle and analyse hyperspectral image data for various applications. The need to develop a well-built and specialized spectral library to accommodate the resources from enormous spectral data is suggested. This may improve accuracy in mineral and soil mapping, vegetation species identification and health monitoring, and target detection. The future role of cloud computing in accessing globally distributed spectral libraries and performing spectral matching is highlighted. Rather than inferring that a particular matching algorithm is the best, this paper points out the requirements of an ideal algorithm. With increasing usage of hyperspectral data for resources mapping, the review presented in this paper will certainly benefit the large and emerging community of hyperspectral image users.},
	number = {24},
	urldate = {2023-02-27},
	journal = {International Journal of Remote Sensing},
	author = {Shanmugam, S. and SrinivasaPerumal, P.},
	month = dec,
	year = {2014},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01431161.2014.980922},
	pages = {8217--8251},
}

@article{choi_kaist_2018,
	title = {{KAIST} {Multi}-{Spectral} {Day}/{Night} {Data} {Set} for {Autonomous} and {Assisted} {Driving}},
	volume = {19},
	issn = {1558-0016},
	doi = {10.1109/TITS.2018.2791533},
	abstract = {We introduce the KAIST multi-spectral data set, which covers a great range of drivable regions, from urban to residential, for autonomous systems. Our data set provides the different perspectives of the world captured in coarse time slots (day and night), in addition to fine time slots (sunrise, morning, afternoon, sunset, night, and dawn). For all-day perception of autonomous systems, we propose the use of a different spectral sensor, i.e., a thermal imaging camera. Toward this goal, we develop a multi-sensor platform, which supports the use of a co-aligned RGB/Thermal camera, RGB stereo, 3-D LiDAR, and inertial sensors (GPS/IMU) and a related calibration technique. We design a wide range of visual perception tasks including the object detection, drivable region detection, localization, image enhancement, depth estimation, and colorization using a single/multi-spectral approach. In this paper, we provide a description of our benchmark with the recording platform, data format, development toolkits, and lessons about the progress of capturing data sets.},
	number = {3},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Choi, Yukyung and Kim, Namil and Hwang, Soonmin and Park, Kibaek and Yoon, Jae Shin and An, Kyounghwan and Kweon, In So},
	month = mar,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {Calibration, Cameras, Dataset, KAIST multi-sepctral, Laser radar, Optical distortion, Optical imaging, Optical sensors, Thermal sensors, advanced driver assistance system, autonomous driving, benchmarks, multi-spectral dataset in day and night, multi-spectral vehicle system},
	pages = {934--948},
}

@misc{varney_dales_2020,
	title = {{DALES}: {A} {Large}-scale {Aerial} {LiDAR} {Data} {Set} for {Semantic} {Segmentation}},
	shorttitle = {{DALES}},
	url = {http://arxiv.org/abs/2004.11985},
	doi = {10.48550/arXiv.2004.11985},
	abstract = {We present the Dayton Annotated LiDAR Earth Scan (DALES) data set, a new large-scale aerial LiDAR data set with over a half-billion hand-labeled points spanning 10 square kilometers of area and eight object categories. Large annotated point cloud data sets have become the standard for evaluating deep learning methods. However, most of the existing data sets focus on data collected from a mobile or terrestrial scanner with few focusing on aerial data. Point cloud data collected from an Aerial Laser Scanner (ALS) presents a new set of challenges and applications in areas such as 3D urban modeling and large-scale surveillance. DALES is the most extensive publicly available ALS data set with over 400 times the number of points and six times the resolution of other currently available annotated aerial point cloud data sets. This data set gives a critical number of expert verified hand-labeled points for the evaluation of new 3D deep learning algorithms, helping to expand the focus of current algorithms to aerial data. We describe the nature of our data, annotation workflow, and provide a benchmark of current state-of-the-art algorithm performance on the DALES data set.},
	urldate = {2023-03-15},
	publisher = {arXiv},
	author = {Varney, Nina and Asari, Vijayan K. and Graehling, Quinn},
	month = apr,
	year = {2020},
	note = {arXiv:2004.11985 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{baltsavias_comparison_1999,
	title = {A comparison between photogrammetry and laser scanning},
	volume = {54},
	doi = {10.1016/s0924-2716(99)00014-3},
	number = {2-3},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Baltsavias, Emmanuel P.},
	month = jul,
	year = {1999},
	pages = {83--94},
}

@inproceedings{mei_fast_2007,
	title = {Fast {Hydraulic} {Erosion} {Simulation} and {Visualization} on {GPU}},
	doi = {10.1109/pg.2007.15},
	booktitle = {15th {Pacific} {Conference} on {Computer} {Graphics} and {Applications} ({PG}'07)},
	publisher = {IEEE},
	author = {Mei, Xing and Decaudin, Philippe and Hu, Bao-Gang},
	month = oct,
	year = {2007},
	pages = {47--56},
}

@article{zohdi_rapid_2020,
	title = {Rapid simulation-based uncertainty quantification of flash-type time-of-flight and {Lidar}-based body-scanning processes},
	volume = {359},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782519301951},
	doi = {10.1016/j.cma.2019.03.056},
	abstract = {Lidar and other time-of-flight technologies have recently received significant attention in both academic and industrial biomechanics communities, driven by technological advances in human body scanners. This paper develops an efficient and rapid computational method to simulate fast flash-type single-pulse Lidar, based on decomposition of a Lidar pulse into a group of rays, which are then tracked and processed. This allows one to quickly quantify the uncertainty in the response by identifying regions where the return signal will be erroneous due to multiple reflections.},
	language = {en},
	urldate = {2021-05-06},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Zohdi, T. I.},
	month = feb,
	year = {2020},
	keywords = {Flash-type, Lidar, Simulation, Uncertainty quantification},
	pages = {112386},
}

@article{behley_towards_2021,
	title = {Towards {3D} {LiDAR}-based semantic scene understanding of {3D} point cloud sequences: {The} {SemanticKITTI} {Dataset}},
	volume = {40},
	doi = {10.1177/02783649211006735},
	number = {8-9},
	journal = {The International Journal on Robotics Research},
	author = {Behley, J. and Garbade, M. and Milioto, A. and Quenzel, J. and Behnke, S. and Gall, J. and Stachniss, C.},
	year = {2021},
	pages = {959--967},
}

@article{pandzic_error_2017,
	title = {Error model of direct georeferencing procedure of terrestrial laser scanning},
	volume = {78},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580517300183},
	doi = {10.1016/j.autcon.2017.01.003},
	abstract = {Processing of raw point cloud data obtained as a result of terrestrial laser scanning (TLS) sometimes involves georeferencing, i.e. transformation of point cloud data to an external coordinate system. This paper focuses on defining the error model of point positions obtained through a “station-orientation” procedure of direct georeferencing. The original error model presented by the authors relevant in this field is partly altered. All modifications are explained in detail within the paper and the reported model is statistically verified based on the carefully conducted experiment using Leica ScanStation P20 scanner. The obtained values of the uncertainty measures of direct georeferencing which are of a few millimetre magnitude prove that this procedure can be efficiently used for planning and carrying out even more demanding surveying tasks, including those during monitoring and maintenance of constructed facilities. Additionally, traversing capabilities of terrestrial laser scanners tightly connected with direct georeferencing should contribute to mass introduction of laser scanning into the construction industry thanks to its similarities to the highly automated procedures of polar surveying and traversing which are traditionally employed among surveyors.},
	language = {en},
	urldate = {2021-12-26},
	journal = {Automation in Construction},
	author = {Pandžić, Jelena and Pejić, Marko and Božić, Branko and Erić, Verica},
	month = jun,
	year = {2017},
	keywords = {3D point cloud, Construction industry, Direct georeferencing, Error model, Terrestrial laser scanning, Uncertainty},
	pages = {13--23},
}

@article{chen_storage_2022,
	title = {Storage method of multi-channel lidar data based on tree structure},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-13138-9},
	doi = {10.1038/s41598-022-13138-9},
	abstract = {The multi-channel lidar has fast acquisition speed, large data volume, high dimension, and vital real-time storage, which makes it challenging to be met using the traditional lidar data storage methods. This paper presents a novel approach to storing the multi-channel lidar data based on the principle of the tree structure, the adjacency linked list, the binary data storage. In the proposed system, a tree structure is constructed by the four-dimensional structure of the multi-channel lidar data, and a data retrieval method of the multi-channel lidar data file is given. The results show that the proposed tree structure approach can save the storage capacity and improve the retrieval speed, which can meet the needs for efficient storage and retrieval of multi-channel lidar data, and improve the data storage utilization and the practicality of multi-channel lidar system.},
	language = {en},
	number = {1},
	urldate = {2022-11-17},
	journal = {Scientific Reports},
	author = {Chen, Hao and Gao, Fei and Zhu, Qingsong and Yan, Qing and Hua, Dengxin and Stanič, Samo},
	month = may,
	year = {2022},
	keywords = {Information technology, Mechanical engineering},
	pages = {9075},
}

@article{robles-ortega_new_2013,
	title = {A new approach to create textured urban models through genetic algorithms},
	volume = {243},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025513002697},
	doi = {10.1016/j.ins.2013.03.053},
	abstract = {The automatic process for texturing 3D urban environments realistically is a challenging topic in Computer Graphics, especially when the model is generated from 2D GIS data from real sites. In order to achieve realistic scenes, buildings, urban furniture and street roadways should be modeled automatically also considering the slope of steep streets. A texturing process by hand which takes into account all the urban elements of a city is an impracticable task, especially if quality results are required. In this paper we propose an automatic method for building texturization in entire urban models using a small set of images. The different features of lower and upper parts of buildings are considered by implementing two different genetic algorithms. Our method can be used in cities with both horizontal and steep streets because the street slope is processed by the algorithm.},
	language = {en},
	urldate = {2022-01-21},
	journal = {Information Sciences},
	author = {Robles-Ortega, María Dolores and Ortega, Lidia and Feito, Francisco R.},
	month = sep,
	year = {2013},
	keywords = {2 GIS, 3 city modeling, Client–server architecture, Genetic algorithm, Georeferenced database, Texture mapping},
	pages = {1--19},
}

@article{voegtle_influences_2008,
	title = {Influences of different materials on the measurement of a {Terrestrial} {Laser} {Scanner} ({TLS})},
	volume = {37},
	journal = {Proc. of the XXI Congress, the International Society for Photogrammetry and Remote Sensing, ISPRS2008},
	author = {Voegtle, Thomas and Schwab, I and Landes, Tania},
	year = {2008},
}

@book{ebert_texturing_2002,
	address = {San Francisco, CA, USA},
	edition = {3rd},
	title = {Texturing and {Modeling}: {A} {Procedural} {Approach}},
	isbn = {978-1-55860-848-1},
	shorttitle = {Texturing and {Modeling}},
	abstract = {From the Publisher: This the classic text/tutorial/reference introducing and defining the procedural approach to texturing and modeling and computer graphics in general. This Second Edition is completely revised and updated to the current computer graphics marketplace. It contains a toolbox of procedures upon which programmers can build a library of textures and objects, and includes extensive explanations of how these functions work and how to design new functions. New chapters include: Cellular Texture Generation, Volumetric Cloud Modeling with Implicit Functions, Interacting with Virtual Actors, and Rendering with Adaptive Level of Detail.},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Ebert, David S. and Musgrave, F. Kenton and Peachey, Darwyn and Perlin, Ken and Worley, Steven},
	year = {2002},
}

@article{meister_survey_2021,
	title = {A {Survey} on {Bounding} {Volume} {Hierarchies} for {Ray} {Tracing}},
	volume = {40},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.142662},
	doi = {10.1111/cgf.142662},
	abstract = {Ray tracing is an inherent part of photorealistic image synthesis algorithms. The problem of ray tracing is to find the nearest intersection with a given ray and scene. Although this geometric operation is relatively simple, in practice, we have to evaluate billions of such operations as the scene consists of millions of primitives, and the image synthesis algorithms require a high number of samples to provide a plausible result. Thus, scene primitives are commonly arranged in spatial data structures to accelerate the search. In the last two decades, the bounding volume hierarchy (BVH) has become the de facto standard acceleration data structure for ray tracing-based rendering algorithms in offline and recently also in real-time applications. In this report, we review the basic principles of bounding volume hierarchies as well as advanced state of the art methods with a focus on the construction and traversal. Furthermore, we discuss industrial frameworks, specialized hardware architectures, other applications of bounding volume hierarchies, best practices, and related open problems.},
	language = {en},
	number = {2},
	urldate = {2022-01-21},
	journal = {Computer Graphics Forum},
	author = {Meister, Daniel and Ogaki, Shinji and Benthin, Carsten and Doyle, Michael J. and Guthe, Michael and Bittner, Jiří},
	year = {2021},
	keywords = {CCS Concepts, Massively parallel algorithms, Sorting and searching, Visibility, • Computing methodologies → Ray tracing, • Theory of computation → Computational geometry},
	pages = {683--712},
}

@article{bolkas_effect_2018,
	title = {Effect of target color and scanning geometry on terrestrial {LiDAR} point-cloud noise and plane fitting},
	volume = {12},
	doi = {10.1515/jag-2017-0034},
	number = {1},
	journal = {Journal of Applied Geodesy},
	author = {Bolkas, Dimitrios and Martinez, Aaron},
	month = jan,
	year = {2018},
	pages = {109--127},
}

@article{vanegas_inverse_2012,
	title = {Inverse design of urban procedural models},
	volume = {31},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2366145.2366187},
	doi = {10.1145/2366145.2366187},
	abstract = {We propose a framework that enables adding intuitive high level control to an existing urban procedural model. In particular, we provide a mechanism to interactively edit urban models, a task which is important to stakeholders in gaming, urban planning, mapping, and navigation services. Procedural modeling allows a quick creation of large complex 3D models, but controlling the output is a well-known open problem. Thus, while forward procedural modeling has thrived, in this paper we add to the arsenal an inverse modeling tool. Users, unaware of the rules of the underlying urban procedural model, can alternatively specify arbitrary target indicators to control the modeling process. The system itself will discover how to alter the parameters of the urban procedural model so as to produce the desired 3D output. We label this process inverse design.},
	number = {6},
	urldate = {2022-01-21},
	journal = {ACM Transactions on Graphics},
	author = {Vanegas, Carlos A. and Garcia-Dorado, Ignacio and Aliaga, Daniel G. and Benes, Bedrich and Waddell, Paul},
	month = nov,
	year = {2012},
	keywords = {interactive, inverse procedural modeling, procedural modeling, urban models},
	pages = {168:1--168:11},
}

@article{hofle_correction_2007,
	title = {Correction of laser scanning intensity data: {Data} and model-driven approaches},
	volume = {62},
	doi = {10.1016/j.isprsjprs.2007.05.008},
	number = {6},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Höfle, Bernhard and Pfeifer, Norbert},
	month = dec,
	year = {2007},
	pages = {415--433},
}

@article{schaer_accuracy_2012,
	title = {Accuracy {Estimation} for {Laser} {Point} {Cloud} {Including} {Scanning} {Geometry}},
	volume = {36},
	journal = {International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Schaer, Philipp and Skaloud, Jan and Landtwing, S. and Legat, Klaus},
	month = may,
	year = {2012},
}

@article{risbol_lidar_2018,
	title = {{LiDAR} from drones employed for mapping archaeology - {Potential}, benefits and challenges},
	volume = {25},
	doi = {10.1002/arp.1712},
	number = {4},
	journal = {Archaeological Prospection},
	author = {Risbøl, Ole and Gustavsen, Lars},
	year = {2018},
	pages = {329--338},
}

@article{lohani_airborne_2017,
	title = {Airborne {LiDAR} {Technology}: {A} {Review} of {Data} {Collection} and {Processing} {Systems}},
	volume = {87},
	issn = {0369-8203, 2250-1762},
	shorttitle = {Airborne {LiDAR} {Technology}},
	url = {http://link.springer.com/10.1007/s40010-017-0435-9},
	doi = {10.1007/s40010-017-0435-9},
	language = {en},
	number = {4},
	urldate = {2021-01-05},
	journal = {Proceedings of the National Academy of Sciences, India Section A: Physical Sciences},
	author = {Lohani, Bharat and Ghosh, Suddhasheel},
	month = dec,
	year = {2017},
	pages = {567--579},
}

@inproceedings{ullrich_advances_2019,
	title = {Advances in lidar point cloud processing},
	volume = {11005},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11005/110050K/Advances-in-lidar-point-cloud-processing/10.1117/12.2518856.full},
	doi = {10.1117/12.2518856},
	abstract = {LIDAR sensors and LIDAR systems utilized for precise surveying in various fields of application are operated from significantly distinct platforms ranging from static platforms during a single 3D scan acquisition in terrestrial or static laser scanning to a multitude of different platforms in kinematic laser scanning like mobile laser scanning, UAV-based laser scanning or airborne laser scanning. The related fields of application impose substantially different requirements with respect to accuracy, measurement rate, and data density. The results have to serve various data consumer communities and impose vastly dissimilar requirements on the LIDAR equipment, e.g., size, weight, cost and performance. Still, there are some general issues one has to address in data processing and delivery. In some cases, the emphasis lies specifically on rapid point cloud processing and delivery – although delivery time requirements may range from seconds up to weeks, depending on the application at hand. Processing requirements are demanding as in this paper we assume final point clouds to be clean – i.e. virtually noise free –, georeferenced, and consistent. We discuss general challenges in the data processing chain applicable to all types of LIDAR, regardless of the underlying technology, i.e. waveform LIDAR, discrete LIDAR, single-photon LIDAR, or Geiger-mode LIDAR. Applications include, e.g., rapid generation of data previews for the operator in kinematic LIDAR and the automated registration of all acquired point clouds in stop-and-go acquisition with static LIDAR.},
	urldate = {2023-03-15},
	booktitle = {Laser {Radar} {Technology} and {Applications} {XXIV}},
	publisher = {SPIE},
	author = {Ullrich, Andreas and Pfennigbauer, Martin},
	month = may,
	year = {2019},
	pages = {157--166},
}

@article{kaijaluoto_precise_2015,
	title = {{PRECISE} {INDOOR} {LOCALIZATION} {FOR} {MOBILE} {LASER} {SCANNER}},
	volume = {XL-4/W5},
	issn = {2194-9034},
	url = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-4-W5/1/2015/},
	doi = {10.5194/isprsarchives-XL-4-W5-1-2015},
	abstract = {Accurate 3D data is of high importance for indoor modeling for various applications in construction, engineering and cultural heritage documentation. For the lack of GNSS signals hampers use of kinematic platforms indoors, TLS is currently the most accurate and precise method for collecting such a data. Due to its static single view point data collection, excessive time and data redundancy are needed for integrity and coverage of data. However, localization methods with affordable scanners are used for solving mobile platform pose problem. The aim of this study was to investigate what level of trajectory accuracies can be achieved with high quality sensors and freely available state of the art planar SLAM algorithms, and how well this trajectory translates to a point cloud collected with a secondary scanner. {\textbackslash}textlessbr{\textbackslash}textgreater{\textbackslash}textlessbr{\textbackslash}textgreater In this study high precision laser scanners were used with a novel way to combine the strengths of two SLAM algorithms into functional method for precise localization. We collected five datasets using Slammer platform with two laser scanners, and processed them with altogether 20 different parameter sets. The results were validated against TLS reference. The results show increasing scan frequency improves the trajectory, reaching 20 mm RMSE levels for the best performing parameter sets. Further analysis of the 3D point cloud showed good agreement with TLS reference with 17 mm positional RMSE. With precision scanners the obtained point cloud allows for high level of detail data for indoor modeling with accuracies close to TLS at best with vastly improved data collection efficiency.},
	language = {en},
	urldate = {2021-01-01},
	journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Kaijaluoto, R. and Hyyppä, A.},
	month = may,
	year = {2015},
	pages = {1--6},
}

@techreport{amer_accuracy_2018,
	title = {Accuracy {Assessment} of {Laser} {Scanner} under {Different} {Projections} {Angles}},
	author = {Amer, Huda A and Shaker, Ibrahim F and Abdel-Gawad, Ahmed K and Ragab, Ayman and Mogahed, Yasser},
	year = {2018},
}

@inproceedings{soudarissanane_error_2007,
	title = {Error budget of terrestrial laser scanning: influence of the incidence angle on the scan quality},
	doi = {10.13140/RG.2.1.1877.6404},
	language = {en},
	booktitle = {Proceedings of {3D}-{NordOst}},
	author = {Soudarissanane, Sylvie and Ree, Jane Van and Bucksch, Alexander and Lindenbergh, Roderik},
	year = {2007},
	pages = {1--8},
}

@misc{siemens_simcenter_2021,
	title = {Simcenter},
	urldate = {2021-07-01},
	author = {{Siemens}},
	year = {2021},
}

@article{liu_deep_2019,
	title = {Deep {Learning} on {Point} {Clouds} and {Its} {Application}: {A} {Survey}},
	volume = {19},
	issn = {1424-8220},
	shorttitle = {Deep {Learning} on {Point} {Clouds} and {Its} {Application}},
	url = {https://www.mdpi.com/1424-8220/19/19/4188},
	doi = {10.3390/s19194188},
	abstract = {Point cloud is a widely used 3D data form, which can be produced by depth sensors, such as Light Detection and Ranging (LIDAR) and RGB-D cameras. Being unordered and irregular, many researchers focused on the feature engineering of the point cloud. Being able to learn complex hierarchical structures, deep learning has achieved great success with images from cameras. Recently, many researchers have adapted it into the applications of the point cloud. In this paper, the recent existing point cloud feature learning methods are classified as point-based and tree-based. The former directly takes the raw point cloud as the input for deep learning. The latter first employs a k-dimensional tree (Kd-tree) structure to represent the point cloud with a regular representation and then feeds these representations into deep learning models. Their advantages and disadvantages are analyzed. The applications related to point cloud feature learning, including 3D object classification, semantic segmentation, and 3D object detection, are introduced, and the datasets and evaluation metrics are also collected. Finally, the future research trend is predicted.},
	language = {en},
	number = {19},
	urldate = {2020-12-23},
	journal = {Sensors},
	author = {Liu, Weiping and Sun, Jia and Li, Wanyi and Hu, Ting and Wang, Peng},
	month = sep,
	year = {2019},
	pages = {4188},
}

@inproceedings{tan_toronto-3d_2020,
	title = {Toronto-{3D}: {A} large-scale mobile lidar dataset for semantic segmentation of urban roadways},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Tan, Weikai and Qin, Nannan and Ma, Lingfei and Li, Ying and Du, Jing and Cai, Guorong and Yang, Ke and Li, Jonathan},
	year = {2020},
	pages = {202--203},
}

@misc{lg_electronics_rd_lab_lgsvl_2021,
	title = {{LGSVL} {Simulator}},
	urldate = {2021-07-01},
	author = {{LG Electronics R\&D Lab}},
	year = {2021},
}

@article{cheung_lidar_2018,
	title = {{LiDAR} remote sensing and applications},
	doi = {10.1080/19475683.2018.1471522},
	journal = {Annals of GIS},
	author = {Cheung, Wing},
	month = may,
	year = {2018},
}

@book{dong_lidar_2018,
	title = {{LiDAR} {Remote} {Sensing} and {Applications}},
	isbn = {978-1-138-74724-1},
	publisher = {CRC Press},
	author = {Dong, Pinliang and Chen, Qi},
	month = jan,
	year = {2018},
	doi = {10.4324/9781351233354},
}

@inproceedings{stava_interactive_2008,
	series = {{SCA} '08},
	title = {Interactive {Terrain} {Modeling} {Using} {Hydraulic} {Erosion}},
	isbn = {978-3-905674-10-1},
	booktitle = {Proceedings of the 2008 {ACM} {SIGGRAPH}/{Eurographics} {Symposium} on {Computer} {Animation}},
	publisher = {Eurographics Association},
	author = {Št'ava, Ondřej and Beneš, Bedřich and Brisbin, Matthew and Křivánek, Jaroslav},
	year = {2008},
	pages = {201--210},
}

@article{mohan_robust_2019,
	title = {Robust {Optimal} {Sensor} {Planning} for {Occlusion} {Handling} in {Dynamic} {Robotic} {Environments}},
	volume = {19},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2019.2899929},
	abstract = {Optimal sensor planning for workspace detection in robotic environments is hindered due to sensor occlusions. These occlusions are often dynamic. Probabilistic optimization frameworks, which generally deal with the uncertain nature of these occlusions, suffer from unreliability and/or unavailability of probability distribution functions. This paper proposes and analyzes a robust optimization approach (minimax), which generates sensor configurations based on occlusion scenarios that cause maximum obstruction of the robotic workspace. The optimal solution is independent of probability distribution functions and provides a guaranteed level of workspace visibility regardless of occluder positions, thus accounting for random occlusions. The method also allows the user to determine the impact of the worst case occlusion scenarios leading to a broader perspective on sensor planning. Evaluation of the approach for a mobile medical X-ray robotic system in a simulation healthcare environment shows the effectiveness of the proposed method.},
	number = {11},
	journal = {IEEE Sensors Journal},
	author = {Mohan, Rishi and de Jager, Bram},
	month = jun,
	year = {2019},
	keywords = {Optimal sensor placement, Optimization, Planning, Probabilistic logic, Probability distribution, Robot sensing systems, dynamic occlusions, minimax optimization, workspace detection},
	pages = {4259--4270},
}

@article{majercik_ray-box_2018,
	title = {A {Ray}-{Box} {Intersection} {Algorithm} and {Efficient} {Dynamic} {Voxel} {Rendering}},
	volume = {7},
	issn = {2331-7418},
	number = {3},
	journal = {Journal of Computer Graphics Techniques (JCGT)},
	author = {Majercik, Alexander and Crassin, Cyril and Shirley, Peter and McGuire, Morgan},
	month = sep,
	year = {2018},
	pages = {66--81},
}

@misc{pan_semanticposs_2020,
	title = {{SemanticPOSS}: {A} {Point} {Cloud} {Dataset} with {Large} {Quantity} of {Dynamic} {Instances}},
	author = {Pan, Yancheng and Gao, Biao and Mei, Jilin and Geng, Sibo and Li, Chengkun and Zhao, Huijing},
	year = {2020},
}

@misc{dji_zenmuse_2020,
	title = {Zenmuse {L1} {Specs}},
	urldate = {2021-12-06},
	author = {{DJI}},
	year = {2020},
}

@article{soudarissanane_incidence_2009,
	title = {Incidence angle influence on the quality of terrestrial laser scanning points},
	volume = {38},
	journal = {Proceedings ISPRS Workshop Laserscanning 2009, 1-2 Sept 2009, Paris, France},
	author = {Soudarissanane, Sylvie and Lindenbergh, Roderik and Menenti, Massimo and Teunissen, P.},
	year = {2009},
}

@article{roberge_fast_2018,
	title = {Fast {Genetic} {Algorithm} {Path} {Planner} for {Fixed}-{Wing} {Military} {UAV} {Using} {GPU}},
	volume = {54},
	issn = {1557-9603},
	doi = {10.1109/TAES.2018.2807558},
	abstract = {Military unmanned aerial vehicles (UAVs) are employed in highly dynamic environments and must often adjust their trajectories based on the evolving situation. To operate autonomously and safely, a UAV must be equipped with a path planning module capable of quickly recalculating a feasible and quasi-optimal path in flight while in the event a new obstacle or threat has been detected or simply if the destination point is changed during the mission. To allow for a fast path planning, this paper proposes a parallel implementation of the genetic algorithm on graphics processing unit (GPU). The trajectories are built as series of line segments connected by circular arcs resulting in smooth paths suitable for fixed-wing UAVs. The fitness function we defined takes into account the dynamic constraints of the UAVs and aims to minimize fuel consumption and average flying altitude in order to improve range and avoid detection by enemy radars. This fitness function is also implemented on the GPU and different parallelization strategies were developed and tested for each step of the fitness evaluation. By exploiting the massively parallel architecture of GPUs, the execution time of the proposed path planner was reduced by a factor of 290x compared to a sequential execution on CPU. The path planning module developed was tested using 18 scenarios on six realistic three-dimensional terrains with multiple no-fly zones. We found that the proposed GPU-based path planner was able to find quasi-optimal solutions in a timely fashion allowing in-flight planning.},
	number = {5},
	journal = {IEEE Transactions on Aerospace and Electronic Systems},
	author = {Roberge, Vincent and Tarbouchi, Mohammed and Labonté, Gilles},
	month = oct,
	year = {2018},
	keywords = {Genetic algorithms, Graphics processing units, Optimization, Planning, Trajectory, UAV, Unmanned aerial vehicles, genetic algorithm, graphics processing units, parallel computing, path planning},
	pages = {2105--2117},
}

@inproceedings{brown_time-gated_2005,
	address = {Orlando, Florida, USA},
	title = {Time-gated topographic {LIDAR} scene simulation},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.604326},
	doi = {10.1117/12.604326},
	urldate = {2021-01-06},
	author = {Brown, Scott D. and Blevins, Daniel D. and Schott, John R.},
	editor = {Kamerman, Gary W.},
	month = may,
	year = {2005},
	pages = {342},
}

@article{dimitrov_segmentation_2015,
	title = {Segmentation of building point cloud models including detailed architectural/structural features and {MEP} systems},
	volume = {51},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0926580514002593},
	doi = {10.1016/j.autcon.2014.12.015},
	language = {en},
	urldate = {2021-01-15},
	journal = {Automation in Construction},
	author = {Dimitrov, Andrey and Golparvar-Fard, Mani},
	month = mar,
	year = {2015},
	pages = {32--45},
}

@phdthesis{soudarissanane_geometry_2016,
	type = {{PhD} {Thesis}},
	title = {The geometry of terrestrial laser scanning; identification of errors, modeling and mitigation of scanning geometry},
	url = {http://resolver.tudelft.nl/uuid:b7ae0bd3-23b8-4a8a-9b7d-5e494ebb54e5},
	urldate = {2021-01-09},
	school = {Delft University of Technology},
	author = {Soudarissanane, S.S.},
	year = {2016},
	doi = {10.4233/UUID:B7AE0BD3-23B8-4A8A-9B7D-5E494EBB54E5},
	doi = {10.4233/UUID:B7AE0BD3-23B8-4A8A-9B7D-5E494EBB54E5},
	doi = {10.4233/UUID:B7AE0BD3-23B8-4A8A-9B7D-5E494EBB54E5},
}

@techreport{boehler_investigating_2018,
	title = {Investigating {LASER} {Scanner} {Accuracy}},
	institution = {i3mainz, Institute for Spatial Information and Surveying Technology, FH Mainz, Holzstrasse 36, 55116 Mainz, Germany},
	author = {Boehler, W and Marbs, A., M. Bordas Vicent},
	year = {2018},
}

@article{chen_ole_2020,
	title = {{OLE}: {A} {Novel} {Oceanic} {Lidar} {Emulator}},
	issn = {0196-2892, 1558-0644},
	shorttitle = {{OLE}},
	url = {https://ieeexplore.ieee.org/document/9258961/},
	doi = {10.1109/TGRS.2020.3035381},
	urldate = {2020-12-23},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Chen, Peng and Jamet, Cedric and Mao, Zhihua and Pan, Delu},
	year = {2020},
	pages = {1--15},
}

@article{deems_lidar_2013,
	title = {Lidar measurement of snow depth: a review},
	volume = {59},
	doi = {10.3189/2013jog12j154},
	number = {215},
	journal = {Journal of Glaciology},
	author = {Deems, Jeffrey S. and Painter, Thomas H. and Finnegan, David C.},
	year = {2013},
	pages = {467--479},
}

@article{niese_procedural_2020,
	title = {Procedural {Urban} {Forestry}},
	url = {http://arxiv.org/abs/2008.05567},
	abstract = {The placement of vegetation plays a central role in the realism of virtual scenes. We introduce procedural placement models (PPMs) for vegetation in urban layouts. PPMs are environmentally sensitive to city geometry and allow identifying plausible plant positions based on structural and functional zones in an urban layout. PPMs can either be directly used by defining their parameters or can be learned from satellite images and land register data. Together with approaches for generating buildings and trees, this allows us to populate urban landscapes with complex 3D vegetation. The effectiveness of our framework is shown through examples of large-scale city scenes and close-ups of individually grown tree models; we also validate it by a perceptual user study.},
	urldate = {2022-01-21},
	journal = {arXiv:2008.05567 [cs]},
	author = {Niese, Till and Pirk, Sören and Albrecht, Matthias and Benes, Bedrich and Deussen, Oliver},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
}

@article{haider_development_2022,
	title = {Development of {High}-{Fidelity} {Automotive} {LiDAR} {Sensor} {Model} with {Standardized} {Interfaces}},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/19/7556},
	doi = {10.3390/s22197556},
	abstract = {This work introduces a process to develop a tool-independent, high-fidelity, ray tracing-based light detection and ranging (LiDAR) model. This virtual LiDAR sensor includes accurate modeling of the scan pattern and a complete signal processing toolchain of a LiDAR sensor. It is developed as a functional mock-up unit (FMU) by using the standardized open simulation interface (OSI) 3.0.2, and functional mock-up interface (FMI) 2.0. Subsequently, it was integrated into two commercial software virtual environment frameworks to demonstrate its exchangeability. Furthermore, the accuracy of the LiDAR sensor model is validated by comparing the simulation and real measurement data on the time domain and on the point cloud level. The validation results show that the mean absolute percentage error (MAPE) of simulated and measured time domain signal amplitude is 1.7\%. In addition, the MAPE of the number of points Npoints and mean intensity Imean values received from the virtual and real targets are 8.5\% and 9.3\%, respectively. To the author’s knowledge, these are the smallest errors reported for the number of received points Npoints and mean intensity Imean values up until now. Moreover, the distance error derror is below the range accuracy of the actual LiDAR sensor, which is 2 cm for this use case. In addition, the proving ground measurement results are compared with the state-of-the-art LiDAR model provided by commercial software and the proposed LiDAR model to measure the presented model fidelity. The results show that the complete signal processing steps and imperfections of real LiDAR sensors need to be considered in the virtual LiDAR to obtain simulation results close to the actual sensor. Such considerable imperfections are optical losses, inherent detector effects, effects generated by the electrical amplification, and noise produced by the sunlight.},
	language = {en},
	number = {19},
	urldate = {2022-11-17},
	journal = {Sensors},
	author = {Haider, Arsalan and Pigniczki, Marcell and Köhler, Michael H. and Fink, Maximilian and Schardt, Michael and Cichy, Yannik and Zeh, Thomas and Haas, Lukas and Poguntke, Tim and Jakobi, Martin and Koch, Alexander W.},
	month = jan,
	year = {2022},
	keywords = {CarMaker, advanced driver-assistance systems, automotive LiDAR sensor, co-simulation environment, functional mock-up interface, functional mock-up unit, open simulation interface, open standard, point clouds, proving ground, silicon photomultipliers detector, standardized interfaces, time domain signal},
	pages = {7556},
}

@book{mcmanamon_lidar_2019,
	title = {{LiDAR} {Technologies} and {Systems}},
	isbn = {978-1-5106-2539-6},
	url = {https://www.spiedigitallibrary.org/ebooks/PM/LiDAR-Technologies-and-Systems/eISBN-9781510625402/10.1117/3.2518254},
	urldate = {2021-11-09},
	publisher = {SPIE},
	author = {McManamon, Paul F.},
	month = jul,
	year = {2019},
	doi = {10.1117/3.2518254},
}

@article{soudarissanane_scanning_2011,
	title = {Scanning geometry: {Influencing} factor on the quality of terrestrial laser scanning points},
	volume = {66},
	issn = {09242716},
	shorttitle = {Scanning geometry},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271611000098},
	doi = {10.1016/j.isprsjprs.2011.01.005},
	language = {en},
	number = {4},
	urldate = {2021-01-05},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Soudarissanane, Sylvie and Lindenbergh, Roderik and Menenti, Massimo and Teunissen, Peter},
	month = jul,
	year = {2011},
	pages = {389--399},
}

@article{caesar_nuscenes_2020,
	title = {{nuScenes}: {A} multimodal dataset for autonomous driving},
	shorttitle = {{nuScenes}},
	url = {http://arxiv.org/abs/1903.11027},
	abstract = {Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.},
	urldate = {2021-12-06},
	journal = {arXiv:1903.11027 [cs, stat]},
	author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
	month = may,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@article{li_3d_2022,
	title = {{3D} model-based scan planning for space frame structures considering site conditions},
	volume = {140},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580522002369},
	doi = {10.1016/j.autcon.2022.104363},
	abstract = {The acquisition of point cloud data from a space frame using terrestrial laser scanning is usually affected by many occluding components and site conditions and therefore needs to achieve optimal priori planning, which is handled as the planning for scanning (P4S) problem. This paper describes a three-dimensional model-based P4S approach for space frame structures, where a space modeling solution is employed to simulate the scanning target and environment. The P4S problem modeling is used to define the visibility analysis and constraints. Lastly, a two-phase optimization is proposed to solve the P4S problem and compared with a weighted greedy algorithm. Experiments were conducted on a full-scale space frame to validate the proposed approach.},
	language = {en},
	urldate = {2022-09-07},
	journal = {Automation in Construction},
	author = {Li, Dongsheng and Liu, Jiepeng and Zeng, Yan and Cheng, Guozhong and Dong, Biqin and Chen, Y. Frank},
	month = aug,
	year = {2022},
	keywords = {Genetic algorithm, Greedy algorithm, Planning for scanning, Space frame, Terrestrial laser scanning},
	pages = {104363},
}

@article{hendrich_ray_2019,
	title = {Ray {Classification} for {Accelerated} {BVH} {Traversal}},
	volume = {38},
	doi = {10.1111/cgf.13769},
	number = {4},
	journal = {Computer Graphics Forum},
	author = {Hendrich, J. and Pospíšil, A. and Meister, D. and Bittner, J.},
	month = jul,
	year = {2019},
	pages = {49--56},
}

@article{cordonnier_authoring_2017,
	title = {Authoring landscapes by combining ecosystem and terrain erosion simulation},
	volume = {36},
	doi = {10.1145/3072959.3073667},
	number = {4},
	journal = {ACM Transactions on Graphics},
	author = {Cordonnier, Guillaume and Galin, Eric and Gain, James and Benes, Bedrich and Guérin, Eric and Peytavie, Adrien and Cani, Marie-Paule},
	month = jul,
	year = {2017},
	pages = {1--12},
}

@article{yin_simulation_2016,
	title = {Simulation of satellite, airborne and terrestrial {LiDAR} with {DART} ({II}): {ALS} and {TLS} multi-pulse acquisitions, photon counting, and solar noise},
	volume = {184},
	issn = {00344257},
	shorttitle = {Simulation of satellite, airborne and terrestrial {LiDAR} with {DART} ({II})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0034425716302668},
	doi = {10.1016/j.rse.2016.07.009},
	language = {en},
	urldate = {2021-01-05},
	journal = {Remote Sensing of Environment},
	author = {Yin, Tiangang and Lauret, Nicolas and Gastellu-Etchegorry, Jean-Philippe},
	month = oct,
	year = {2016},
	pages = {454--468},
}

@article{fang_augmented_2020,
	title = {Augmented {LiDAR} {Simulator} for {Autonomous} {Driving}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/8972449/},
	doi = {10.1109/LRA.2020.2969927},
	number = {2},
	urldate = {2021-01-16},
	journal = {IEEE Robotics and Automation Letters},
	author = {Fang, Jin and Zhou, Dingfu and Yan, Feilong and Zhao, Tongtong and Zhang, Feihu and Ma, Yu and Wang, Liang and Yang, Ruigang},
	month = apr,
	year = {2020},
	pages = {1931--1938},
}

@article{weiser_opaque_2021,
	title = {Opaque voxel-based tree models for virtual laser scanning in forestry applications},
	volume = {265},
	issn = {0034-4257, 1879-0704},
	url = {https://publikationen.bibliothek.kit.edu/1000137077},
	doi = {10.1016/j.rse.2021.112641},
	abstract = {Virtual laser scanning (VLS), the simulation of laser scanning in a computer environment, is a useful tool for field campaign planning, acquisition optimisation, and development and sensitivity analyses of algorithms in various disciplines including forestry research. One key to meaningful VLS is a suitable 3D representation of the objects of interest. For VLS of forests, the way trees are constructed influences both the performance and the realism of the simulations. In this contribution, we analyse how well VLS can reproduce scans of individual trees in a forest. Specifically, we examine how different voxel sizes used to create a virtual forest affect point cloud metrics (e.g., height percentiles) and tree metrics (e.g., tree height and crown base height) derived from simulated point clouds. The level of detail in the voxelisation is dependent on the voxel size, which influences the number of voxel cells of the model. A smaller voxel size (i.e., more voxels) increases the computational cost of laser scanning simulations but allows for more detail in the object representation. We present a method that decouples voxel grid resolution from final voxel cube size by scaling voxels to smaller cubes, whose surface area is proportional to estimated normalised local plant area density. Voxel models are created from terrestrial laser scanning point clouds and then virtually scanned in one airborne and one UAV-borne simulation scenario. Using a comprehensive dataset of spatially overlapping terrestrial, UAV-borne and airborne laser scanning field data, we compare metrics derived from simulated point clouds and from real reference point clouds. Compared to voxel cubes of fixed size with the same base grid size, using scaled voxels greatly improves the agreement of simulated and real point cloud metrics and tree metrics. This can be largely attributed to reduced artificial occlusion effects. The scaled voxels better represent gaps in the canopy, allowing for higher and more realistic crown penetration. Similarly high accuracy in the derived metrics can be achieved using regular fixed-sized voxel models with notably finer resolution, e.g., 0.02 m. But this can pose a computational limitation for running simulations over large forest plots due to the ca. 50 times higher number of filled voxels. We conclude that opaque scaled voxel models enable realistic laser scanning simulations in forests and avoid the high computational cost of small fixed-sized voxels.},
	language = {de},
	urldate = {2022-01-07},
	journal = {Remote Sensing of Environment},
	author = {Weiser, Hannah and Winiwarter, Lukas and Anders, Katharina and Fassnacht, Fabian Ewald and Höfle, Bernhard},
	year = {2021},
	pages = {112641},
}

@article{zhou_street-view_2022,
	title = {Street-view imagery guided street furniture inventory from mobile laser scanning point clouds},
	volume = {189},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271622001265},
	doi = {10.1016/j.isprsjprs.2022.04.023},
	abstract = {Outdated or sketchy inventory of street furniture may misguide the planners on the renovation and upgrade of transportation infrastructures, thus posing potential threats to traffic safety. Previous studies have taken their steps using point clouds or street-view imagery (SVI) for street furniture inventory, but there remains a gap to balance semantic richness, localization accuracy and working efficiency. Therefore, this paper proposes an effective pipeline that combines SVI and point clouds for the inventory of street furniture. The proposed pipeline encompasses three steps: (1) Off-the-shelf street furniture detection models are applied on SVI for generating two-dimensional (2D) proposals and then three-dimensional (3D) point cloud frustums are accordingly cropped; (2) The instance mask and the instance 3D bounding box are predicted for each frustum using a multi-task neural network; (3) Frustums from adjacent perspectives are associated and fused via multi-object tracking, after which the object-centric instance segmentation outputs the final street furniture with 3D locations and semantic labels. This pipeline was validated on datasets collected in Shanghai and Wuhan, producing component-level street furniture inventory of nine classes. The instance-level mean recall and precision reach 86.4\%, 80.9\% and 83.2\%, 87.8\% respectively in Shanghai and Wuhan, and the point-level mean recall, precision, weighted coverage all exceed 73.7\%.},
	language = {en},
	urldate = {2022-11-18},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Zhou, Yuzhou and Han, Xu and Peng, Mingjun and Li, Haiting and Yang, Bo and Dong, Zhen and Yang, Bisheng},
	month = jul,
	year = {2022},
	keywords = {Instance segmentation, Mobile laser scanning, Neural network, Point clouds, Street furniture, Street-view imagery},
	pages = {63--77},
}

@misc{speedtree_speedtree_2021,
	title = {{SpeedTree}},
	author = {{SpeedTree}},
	year = {2021},
}

@article{westling_simtreels_2020,
	title = {{SimTreeLS}: {Simulating} aerial and terrestrial laser scans of trees},
	shorttitle = {{SimTreeLS}},
	url = {http://arxiv.org/abs/2011.11954},
	abstract = {There are numerous emerging applications for digitizing trees using terrestrial and aerial laser scanning, particularly in the fields of agriculture and forestry. Interpretation of LiDAR point clouds is increasingly relying on data-driven methods (such as supervised machine learning) that rely on large quantities of hand-labelled data. As this data is potentially expensive to capture, and difficult to clearly visualise and label manually, a means of supplementing real LiDAR scans with simulated data is becoming a necessary step in realising the potential of these methods. We present an open source tool, SimTreeLS (Simulated Tree Laser Scans), for generating point clouds which simulate scanning with user-defined sensor, trajectory, tree shape and layout parameters. Upon simulation, material classification is kept in a pointwise fashion so leaf and woody matter are perfectly known, and unique identifiers separate individual trees, foregoing post-simulation labelling. This allows for an endless supply of procedurally generated data with similar characteristics to real LiDAR captures, which can then be used for development of data processing techniques or training of machine learning algorithms. To validate our method, we compare the characteristics of a simulated scan with a real scan using similar trees and the same sensor and trajectory parameters. Results suggest the simulated data is significantly more similar to real data than a sample-based control. We also demonstrate application of SimTreeLS on contexts beyond the real data available, simulating scans of new tree shapes, new trajectories and new layouts, with results presenting well. SimTreeLS is available as an open source resource built on publicly available libraries.},
	urldate = {2021-01-05},
	journal = {arXiv:2011.11954 [cs, eess]},
	author = {Westling, Fredrik and Bryson, Mitch and Underwood, James},
	month = nov,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{coveney_terrestrial_2011,
	title = {Terrestrial laser scan error in the presence of dense ground vegetation: {Terrestrial} laser scan error},
	volume = {26},
	issn = {0031868X},
	shorttitle = {Terrestrial laser scan error in the presence of dense ground vegetation},
	url = {http://doi.wiley.com/10.1111/j.1477-9730.2011.00647.x},
	doi = {10.1111/j.1477-9730.2011.00647.x},
	language = {en},
	number = {135},
	urldate = {2021-01-05},
	journal = {The Photogrammetric Record},
	author = {Coveney, Seamus and Stewart Fotheringham, A.},
	month = sep,
	year = {2011},
	pages = {307--324},
}

@article{huising_errors_1998,
	title = {Errors and accuracy estimates of laser data acquired by various laser scanning systems for topographic applications},
	volume = {53},
	issn = {09242716},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271698000136},
	doi = {10.1016/S0924-2716(98)00013-6},
	language = {en},
	number = {5},
	urldate = {2021-01-06},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Huising, E.J. and Gomes Pereira, L.M.},
	month = oct,
	year = {1998},
	pages = {245--261},
}

@article{chen_automatic_2022,
	title = {Automatic {Labeling} to {Generate} {Training} {Data} for {Online} {LiDAR}-{Based} {Moving} {Object} {Segmentation}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3166544},
	abstract = {Understanding the scene is key for autonomously navigating vehicles, and the ability to segment the surroundings online into moving and non-moving objects is a central ingredient of this task. Often, deep learning-based methods are used to perform moving object segmentation (MOS). The performance of these networks, however, strongly depends on the diversity and amount of labeled training data—information that may be costly to obtain. In this letter, we propose an automatic data labeling pipeline for 3D LiDAR data to save the extensive manual labeling effort and to improve the performance of existing learning-based MOS systems by automatically annotation training data. Our proposed approach achieves this by processing the data offline in batches, i.e., it is not designed to run online on a vehicle. It labels the actually moving objects such as driving cars and pedestrians as moving. In contrast, the non-moving objects, e.g., parked cars, lamps, roads, or buildings, are labeled as static. We show that this approach allows us to label LiDAR data highly effectively and compare our results to those of other label generation methods. We also train a deep neural network with our automatically generated labels and achieve comparable performance to the one trained with manual labels on the same data—and an even better performance when using additional datasets with labels generated by our approach. Furthermore, we evaluate our method on multiple datasets using different sensors, and our experiments indicate that our method can generate labels in different outdoor environments.},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Chen, Xieyuanli and Mersch, Benedikt and Nunes, Lucas and Marcuzzi, Rodrigo and Vizzo, Ignacio and Behley, Jens and Stachniss, Cyrill},
	month = jul,
	year = {2022},
	keywords = {Automobiles, Deep learning methods, Laser radar, Object segmentation, Point cloud compression, Semantics, Sensors, Simultaneous localization and mapping, object detection, segmentation and categorization, semantic scene understanding},
	pages = {6107--6114},
}

@article{ogayar-anguita_gpu-based_2020,
	title = {A {GPU}-{Based} {Framework} for {Generating} {Implicit} {Datasets} of {Voxelized} {Polygonal} {Models} for the {Training} of {3D} {Convolutional} {Neural} {Networks}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2965624},
	abstract = {In this paper we present an efficient GPU-based framework to dynamically perform the voxelization of polygonal models for training 3D convolutional neural networks. It is designed to manage the dataset augmentation by using efficient geometric transformations and random vertex displacements in GPU. With the proposed system, every voxelization is carried out on-the-fly for directly feeding the network. The computing performance with this approach is much better than with the standard method, that carries out every voxelization of each model separately and has much higher data processing overhead. The core voxelization algorithm works by using the standard rendering pipeline of the GPU. It generates binary voxels for both the interior and the surface of the models. Moreover, it is simple, concise and very compatible with commodity hardware, as it neither uses complex data structures nor needs vendor-specific hardware or additional dependencies. This framework dramatically reduces the input/output operations, and completely eliminates the storage footprint of the voxelization dataset, managing it as an implicit dataset.},
	journal = {IEEE Access},
	author = {Ogayar-Anguita, Carlos J. and Rueda-Ruiz, Antonio J. and Segura-Sánchez, Rafael J. and Díaz-Medina, Miguel and García-Fernández, Ángel L.},
	year = {2020},
	keywords = {3D-CNN, B-rep, Computational modeling, Graphics processing units, Object recognition, Rendering (computer graphics), Solid modeling, Three-dimensional displays, Training, Voxelization, boundary representation, convolutional neural network, geometric deep learning, polygonal meshes},
	pages = {12675--12687},
}

@book{akenine-moller_real-time_2018,
	address = {Boca Raton, FL, USA},
	title = {Real-{Time} {Rendering} 4th {Edition}},
	isbn = {978-1-138-62700-0},
	publisher = {A K Peters/CRC Press},
	author = {Akenine-Möller, Tomas and Haines, Eric and Hoffman, Naty and Pesce, Angelo and Iwanicki, Micha{\textbackslash}l and Hillaire, Sébastien},
	year = {2018},
}

@article{isheil_systematic_2011,
	title = {Systematic error correction of a {3D} laser scanning measurement device},
	volume = {49},
	issn = {01438166},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0143816610001971},
	doi = {10.1016/j.optlaseng.2010.09.006},
	language = {en},
	number = {1},
	urldate = {2021-01-05},
	journal = {Optics and Lasers in Engineering},
	author = {Isheil, A. and Gonnet, J.-P. and Joannic, D. and Fontaine, J.-F.},
	month = jan,
	year = {2011},
	pages = {16--24},
}

@inproceedings{riordan_lidar_2021,
	title = {{LiDAR} simulation for performance evaluation of {UAS} detect and avoid},
	doi = {10.1109/ICUAS51884.2021.9476817},
	language = {eng},
	urldate = {2022-01-07},
	booktitle = {2021 {International} {Conference} on {Unmanned} {Aircraft} {Systems} ({ICUAS})},
	author = {Riordan, James and Manduhu, Manduhu and Black, Julie and Dow, Alexander and Dooly, Gerard and Matalonga, Santiago},
	year = {2021},
	pages = {1355--1363},
}

@incollection{braunl_lidar_2020,
	address = {Cham},
	title = {Lidar {Sensors}},
	isbn = {978-3-030-38896-6 978-3-030-38897-3},
	url = {http://link.springer.com/10.1007/978-3-030-38897-3_4},
	language = {en},
	urldate = {2020-12-23},
	booktitle = {Robot {Adventures} in {Python} and {C}},
	publisher = {Springer International Publishing},
	author = {Bräunl, Thomas},
	year = {2020},
	doi = {10.1007/978-3-030-38897-3_4},
	pages = {47--51},
}

@inproceedings{narayanan_classification_2009,
	title = {Classification of {SHOALS} 3000 bathymetric {LiDAR} signals using decision tree and ensemble techniques},
	doi = {10.1109/tic-sth.2009.5444456},
	booktitle = {2009 {IEEE} {Toronto} {International} {Conference} {Science} and {Technology} for {Humanity} ({TIC}-{STH})},
	publisher = {IEEE},
	author = {Narayanan, Ramu and Kim, Heungsik Brian and Sohn, Gunho},
	month = sep,
	year = {2009},
	pages = {462--467},
}

@article{meister_parallel_2018,
	title = {Parallel {Locally}-{Ordered} {Clustering} for {Bounding} {Volume} {Hierarchy} {Construction}},
	volume = {24},
	doi = {10.1109/tvcg.2017.2669983},
	number = {3},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Meister, Daniel and Bittner, Jiri},
	month = mar,
	year = {2018},
	keywords = {Approximation algorithms, Buffer storage, Clustering algorithms, GPU ray tracing, Graphics processing units, Morton curve, Nearest neighbor searches, Ray tracing, Sorting, approximate nearest neighbors, bounding volume hierarchy construction, cluster pairs, computational geometry, locally-ordered agglomerative clustering, massively parallel construction algorithm, object hierarchies, parallel algorithms, parallel locally-ordered clustering, pattern clustering, ray tracing, three-dimensional graphics and realism},
	pages = {1345--1353},
}

@article{gastellu-etchegorry_simulation_2016,
	title = {Simulation of satellite, airborne and terrestrial {LiDAR} with {DART} ({I}): {Waveform} simulation with quasi-{Monte} {Carlo} ray tracing},
	volume = {184},
	issn = {0034-4257},
	shorttitle = {Simulation of satellite, airborne and terrestrial {LiDAR} with {DART} ({I})},
	url = {https://www.sciencedirect.com/science/article/pii/S003442571630267X},
	doi = {10.1016/j.rse.2016.07.010},
	abstract = {LIght Detection And Ranging (LiDAR) provides unique data on the 3-D structure of atmosphere constituents and the Earth's surface. Simulating LiDAR returns for different laser technologies and Earth scenes is fundamental for evaluating and interpreting signal and noise in LiDAR data. Different types of models are capable of simulating LiDAR waveforms of Earth surfaces. Semi-empirical and geometric models can be imprecise because they rely on simplified simulations of Earth surfaces and light interaction mechanisms. On the other hand, Monte Carlo ray tracing (MCRT) models are potentially accurate but require long computational time. Here, we present a new LiDAR waveform simulation tool that is based on the introduction of a quasi-Monte Carlo ray tracing approach in the Discrete Anisotropic Radiative Transfer (DART) model. Two new approaches, the so-called “box method” and “Ray Carlo method”, are implemented to provide robust and accurate simulations of LiDAR waveforms for any landscape, atmosphere and LiDAR sensor configuration (view direction, footprint size, pulse characteristics, etc.). The box method accelerates the selection of the scattering direction of a photon in the presence of scatterers with non-invertible phase function. The Ray Carlo method brings traditional ray-tracking into MCRT simulation, which makes computational time independent of LiDAR field of view (FOV) and reception solid angle. Both methods are fast enough for simulating multi-pulse acquisition. Sensitivity studies with various landscapes and atmosphere constituents are presented, and the simulated LiDAR signals compare favorably with their associated reflectance images and Laser Vegetation Imaging Sensor (LVIS) waveforms. The LiDAR module is fully integrated into DART, enabling more detailed simulations of LiDAR sensitivity to specific scene elements (e.g., atmospheric aerosols, leaf area, branches, or topography) and sensor configuration for airborne or satellite LiDAR sensors.},
	language = {en},
	urldate = {2021-05-06},
	journal = {Remote Sensing of Environment},
	author = {Gastellu-Etchegorry, Jean-Philippe and Yin, Tiangang and Lauret, Nicolas and Grau, Eloi and Rubio, Jeremy and Cook, Bruce D. and Morton, Douglas C. and Sun, Guoqing},
	month = oct,
	year = {2016},
	keywords = {Box method, DART, LVIS, LiDAR, Monte Carlo ray tracing, Multiple scattering, Radiative transfer model, Ray Carlo},
	pages = {418--435},
}

@article{moller_fast_1997,
	title = {Fast, {Minimum} {Storage} {Ray}-{Triangle} {Intersection}},
	volume = {2},
	doi = {10.1080/10867651.1997.10487468},
	number = {1},
	journal = {Journal of Graphics Tools},
	author = {Möller, Tomas and Trumbore, Ben},
	month = jan,
	year = {1997},
	pages = {21--28},
}

@misc{mcguire_computer_2017,
	title = {Computer {Graphics} {Archive}},
	author = {McGuire, Morgan},
	month = jul,
	year = {2017},
}

@incollection{catmull_class_1974,
	title = {A {Class} {Of} {Local} {Interpolating} {Splines}},
	booktitle = {Computer {Aided} {Geometric} {Design}},
	publisher = {Elsevier},
	author = {Catmull, Edwin and Rom, Raphael},
	year = {1974},
	doi = {10.1016/b978-0-12-079050-0.50020-5},
	pages = {317--326},
}

@inproceedings{chen_analysis_2022,
	title = {Analysis of {Real}-{Time} {LiDAR} {Sensor} {Simulation} for {Testing} {Automated} {Driving} {Functions} on a {Vehicle}-in-the-{Loop} {Testbench}},
	doi = {10.1109/IV51971.2022.9827048},
	abstract = {A vehicle-in-the-loop (ViL) testbench offers the possibility to test complex scenarios with ready-to-drive vehicles. For this purpose, the environmental sensors are simulated or stimulated. Essential component as a LiDAR is for automated driving systems (AD), its realistic behavior is hard to stimulate on the testbench. We propose a physics-based LiDAR model, which is real-time capable and shows many realistic features. This model simulates the important effects of laser propagation and reflection, mirror reflection motion distortion, reflection detectability and beam divergence. Besides that, we measured the reflectance of materials of interest to determine the reflection model parameters. Experiments proved that the simulation is real-time capable and the results showed a good match with measured data.},
	booktitle = {2022 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Chen, Haopeng and Müller, Steffen},
	month = jun,
	year = {2022},
	keywords = {Laser noise, Laser radar, Laser theory, Measurement by laser beam, Real-time systems, Reflection, Reflectivity},
	pages = {1605--1614},
}

@article{fischer_autobiomes_2020,
	title = {{AutoBiomes}: procedural generation of multi-biome landscapes},
	volume = {36},
	doi = {10.1007/s00371-020-01920-7},
	number = {10-12},
	journal = {The Visual Computer},
	author = {Fischer, Roland and Dittmann, Philipp and Weller, René and Zachmann, Gabriel},
	month = jul,
	year = {2020},
	pages = {2263--2272},
}

@article{chen_denoising_2018,
	title = {Denoising of point cloud data for computer-aided design, engineering, and manufacturing},
	volume = {34},
	issn = {0177-0667, 1435-5663},
	url = {http://link.springer.com/10.1007/s00366-017-0556-4},
	doi = {10.1007/s00366-017-0556-4},
	language = {en},
	number = {3},
	urldate = {2021-01-05},
	journal = {Engineering with Computers},
	author = {Chen, Hao and Shen, Jie},
	month = jul,
	year = {2018},
	pages = {523--541},
}

@article{guillard_learning_2022,
	title = {Learning to {Simulate} {Realistic} {LiDARs}},
	url = {https://arxiv.org/abs/2209.10986v1},
	doi = {10.48550/arXiv.2209.10986},
	abstract = {Simulating realistic sensors is a challenging part in data generation for autonomous systems, often involving carefully handcrafted sensor design, scene properties, and physics modeling. To alleviate this, we introduce a pipeline for data-driven simulation of a realistic LiDAR sensor. We propose a model that learns a mapping between RGB images and corresponding LiDAR features such as raydrop or per-point intensities directly from real datasets. We show that our model can learn to encode realistic effects such as dropped points on transparent surfaces or high intensity returns on reflective materials. When applied to naively raycasted point clouds provided by off-the-shelf simulator software, our model enhances the data by predicting intensities and removing points based on the scene's appearance to match a real LiDAR sensor. We use our technique to learn models of two distinct LiDAR sensors and use them to improve simulated LiDAR data accordingly. Through a sample task of vehicle segmentation, we show that enhancing simulated point clouds with our technique improves downstream task performance.},
	language = {en},
	urldate = {2022-11-17},
	author = {Guillard, Benoit and Vemprala, Sai and Gupta, Jayesh K. and Miksik, Ondrej and Vineet, Vibhav and Fua, Pascal and Kapoor, Ashish},
	month = sep,
	year = {2022},
}

@inproceedings{yue_lidar_2018,
	title = {A lidar point cloud generator: from a virtual world to autonomous driving},
	booktitle = {{ICMR}},
	publisher = {ACM},
	author = {Yue, Xiangyu and Wu, Bichen and Seshia, Sanjit A and Keutzer, Kurt and Sangiovanni-Vincentelli, Alberto L},
	year = {2018},
	pages = {458--464},
}

@article{fan_error_2015,
	title = {Error in target-based georeferencing and registration in terrestrial laser scanning},
	volume = {83},
	issn = {00983004},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0098300415300066},
	doi = {10.1016/j.cageo.2015.06.021},
	language = {en},
	urldate = {2021-01-05},
	journal = {Computers \& Geosciences},
	author = {Fan, Lei and Smethurst, Joel A. and Atkinson, Peter M. and Powrie, William},
	month = oct,
	year = {2015},
	keywords = {Accuracy, Error, Georeferencing, Point clouds, Registration, Terrestrial laser scanning (TLS)},
	pages = {54--64},
}

@article{masini_medieval_2018,
	title = {Medieval {Archaeology} {Under} the {Canopy} with {LiDAR}. {The} ({Re}){Discovery} of a {Medieval} {Fortified} {Settlement} in {Southern} {Italy}},
	volume = {10},
	doi = {10.3390/rs10101598},
	number = {10},
	journal = {Remote Sensing},
	author = {Masini, Nicola and Gizzi, Fabrizio and Biscione, Marilisa and Fundone, Vincenzo and Sedile, Michele and Sileo, Maria and Pecci, Antonio and Lacovara, Biagio and Lasaponara, Rosa},
	month = oct,
	year = {2018},
	pages = {1598},
}

@article{hovi_real_2014,
	title = {Real and simulated waveform-recording {LiDAR} data in juvenile boreal forest vegetation},
	volume = {140},
	issn = {00344257},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0034425713003714},
	doi = {10.1016/j.rse.2013.10.003},
	language = {en},
	urldate = {2021-01-05},
	journal = {Remote Sensing of Environment},
	author = {Hovi, A. and Korpela, I.},
	month = jan,
	year = {2014},
	pages = {665--678},
}

@article{mendez_lidar_2013,
	title = {{LiDAR} simulation in modelled orchards to optimise the use of terrestrial laser scanners and derived vegetative measures},
	volume = {115},
	issn = {15375110},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1537511013000226},
	doi = {10.1016/j.biosystemseng.2013.02.003},
	language = {en},
	number = {1},
	urldate = {2021-01-05},
	journal = {Biosystems Engineering},
	author = {Méndez, Valeriano and Catalán, Heliodoro and Rosell-Polo, Joan R. and Arnó, Jaume and Sanz, Ricardo},
	month = may,
	year = {2013},
	pages = {7--19},
}

@article{yun_simulation_2019,
	title = {Simulation of multi-platform {LiDAR} for assessing total leaf area in tree crowns},
	volume = {276-277},
	issn = {01681923},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168192319302187},
	doi = {10.1016/j.agrformet.2019.06.009},
	language = {en},
	urldate = {2021-01-05},
	journal = {Agricultural and Forest Meteorology},
	author = {Yun, Ting and Cao, Lin and An, Feng and Chen, Bangqian and Xue, Lianfeng and Li, Weizheng and Pincebourde, Sylvain and Smith, Martin J. and Eichhorn, Markus P.},
	month = oct,
	year = {2019},
	pages = {107610},
}

@inproceedings{revelles_efficient_2000,
	title = {An {Efficient} {Parametric} {Algorithm} for {Octree} {Traversal}},
	booktitle = {Journal of {WSCG}},
	author = {Revelles, J. and Ureña, C. and Lastra, M.},
	year = {2000},
	pages = {212--219},
}

@article{feng_analysis_2001,
	title = {Analysis of digitizing errors of a laser scanning system},
	volume = {25},
	issn = {01416359},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0141635900000714},
	doi = {10.1016/S0141-6359(00)00071-4},
	language = {en},
	number = {3},
	urldate = {2021-01-05},
	journal = {Precision Engineering},
	author = {Feng, Hsi-Yung and Liu, Yixin and Xi, Fengfeng},
	month = jul,
	year = {2001},
	pages = {185--191},
}

@article{fusaro_pushing_2022,
	title = {Pushing the {Limits} of {Learning}-based {Traversability} {Analysis} for {Autonomous} {Driving} on {CPU}},
	url = {https://arxiv.org/abs/2206.03083v1},
	doi = {10.48550/arXiv.2206.03083},
	abstract = {Self-driving vehicles and autonomous ground robots require a reliable and accurate method to analyze the traversability of the surrounding environment for safe navigation. This paper proposes and evaluates a real-time machine learning-based Traversability Analysis method that combines geometric features with appearance-based features in a hybrid approach based on a SVM classifier. In particular, we show that integrating a new set of geometric and visual features and focusing on important implementation details enables a noticeable boost in performance and reliability. The proposed approach has been compared with state-of-the-art Deep Learning approaches on a public dataset of outdoor driving scenarios. It reaches an accuracy of 89.2\% in scenarios of varying complexity, demonstrating its effectiveness and robustness. The method runs fully on CPU and reaches comparable results with respect to the other methods, operates faster, and requires fewer hardware resources.},
	language = {en},
	urldate = {2022-11-17},
	author = {Fusaro, Daniel and Olivastri, Emilio and Evangelista, Daniele and Imperoli, Marco and Menegatti, Emanuele and Pretto, Alberto},
	month = jun,
	year = {2022},
}

@article{mallet_full-waveform_2009,
	title = {Full-waveform topographic lidar: {State}-of-the-art},
	volume = {64},
	issn = {09242716},
	shorttitle = {Full-waveform topographic lidar},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271608000993},
	doi = {10.1016/j.isprsjprs.2008.09.007},
	language = {en},
	number = {1},
	urldate = {2021-01-04},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Mallet, Clément and Bretar, Frédéric},
	month = jan,
	year = {2009},
	pages = {1--16},
}

@article{piggott_understanding_2020,
	title = {Understanding the physics of coherent {LiDAR}},
	url = {http://arxiv.org/abs/2011.05313},
	abstract = {Coherent LiDAR (Light Detecting And Ranging) is a promising 3D imaging technology that provides significant advantages over more traditional LiDAR systems. In addition to being immune to ambient light, it directly measures the velocity of moving objects by sensing Doppler shift of light, and can achieve exceptional depth accuracies. The goal of this manuscript is to explain the basic physics of coherent LiDAR with rigorous derivations from first principles. We first discuss the sensitivity of coherent detection, and derive the number of photons needed to robustly detect a LiDAR return. We then turn our attention to the collection efficiency of coherent LiDAR, and show that signal strength is strongly dependent upon how well the laser beams are focused.},
	language = {en},
	urldate = {2021-01-03},
	journal = {arXiv:2011.05313 [physics]},
	author = {Piggott, Alexander Y.},
	month = nov,
	year = {2020},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Physics - Applied Physics},
}

@article{kashani_review_2015,
	title = {A {Review} of {LIDAR} {Radiometric} {Processing}: {From} {Ad} {Hoc} {Intensity} {Correction} to {Rigorous} {Radiometric} {Calibration}},
	volume = {15},
	doi = {10.3390/s151128099},
	number = {11},
	journal = {Sensors},
	author = {Kashani, Alireza and Olsen, Michael and Parrish, Christopher and Wilson, Nicholas},
	month = nov,
	year = {2015},
	pages = {28099--28128},
}

@article{hodgson_accuracy_2004,
	title = {Accuracy of {Airborne} {Lidar}-{Derived} {Elevation}},
	volume = {70},
	doi = {10.14358/pers.70.3.331},
	number = {3},
	journal = {Photogrammetric Engineering \& Remote Sensing},
	author = {Hodgson, Michael E. and Bresnahan, Patrick},
	month = mar,
	year = {2004},
	pages = {331--339},
}

@inproceedings{lopez_gpu-accelerated_2021,
	title = {A {GPU}-accelerated {LiDAR} {Sensor} for {Generating} {Labelled} {Datasets}},
	isbn = {978-3-03868-160-1},
	doi = {10.2312/ceig.20211360},
	booktitle = {Spanish {Computer} {Graphics} {Conference} ({CEIG})},
	publisher = {The Eurographics Association},
	author = {López, Alfonso and Ogayar Anguita, Carlos Javier and Feito Higueruela, Francisco Ramón},
	editor = {Ortega, Lidia M. and Chica, Antonio},
	year = {2021},
}

@article{makowski_synthetic_2019,
	title = {Synthetic silviculture},
	volume = {38},
	doi = {10.1145/3306346.3323039},
	number = {4},
	journal = {ACM Transactions on Graphics},
	author = {Makowski, Mi{\textbackslash}textbackslashlosz and Hädrich, Torsten and Scheffczyk, Jan and Michels, Dominik L. and Pirk, Sören and Pa{\textbackslash}textbackslashlubicki, Wojtek},
	month = jul,
	year = {2019},
	pages = {1--14},
}

@article{nishida_interactive_2016,
	title = {Interactive sketching of urban procedural models},
	volume = {35},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2897824.2925951},
	doi = {10.1145/2897824.2925951},
	abstract = {3D modeling remains a notoriously difficult task for novices despite significant research effort to provide intuitive and automated systems. We tackle this problem by combining the strengths of two popular domains: sketch-based modeling and procedural modeling. On the one hand, sketch-based modeling exploits our ability to draw but requires detailed, unambiguous drawings to achieve complex models. On the other hand, procedural modeling automates the creation of precise and detailed geometry but requires the tedious definition and parameterization of procedural models. Our system uses a collection of simple procedural grammars, called snippets, as building blocks to turn sketches into realistic 3D models. We use a machine learning approach to solve the inverse problem of finding the procedural model that best explains a user sketch. We use non-photorealistic rendering to generate artificial data for training convolutional neural networks capable of quickly recognizing the procedural rule intended by a sketch and estimating its parameters. We integrate our algorithm in a coarse-to-fine urban modeling system that allows users to create rich buildings by successively sketching the building mass, roof, facades, windows, and ornaments. A user study shows that by using our approach non-expert users can generate complex buildings in just a few minutes.},
	number = {4},
	urldate = {2022-01-21},
	journal = {ACM Transactions on Graphics},
	author = {Nishida, Gen and Garcia-Dorado, Ignacio and Aliaga, Daniel G. and Benes, Bedrich and Bousseau, Adrien},
	month = jul,
	year = {2016},
	keywords = {inverse procedural modeling, machine learning, sketching},
	pages = {130:1--130:11},
}

@article{douglas_algorithms_1973,
	title = {Algorithms for the reduction of the number of points required to represent a digitized line or its caricature},
	volume = {10},
	issn = {0317-7173},
	url = {https://utpjournals.press/doi/abs/10.3138/FM57-6770-U75U-7727},
	doi = {10.3138/FM57-6770-U75U-7727},
	abstract = {All digitizing methods, as a general rule, record lines with far more data than is necessary for accurate graphic reproduction or for computer analysis. Two algorithms to reduce the number of points required to represent the line and, if desired, produce caricatures, are presented and compared with the most promising methods so far suggested. Line reduction will form a major part of automated generalization.},
	number = {2},
	urldate = {2022-12-15},
	journal = {Cartographica: The International Journal for Geographic Information and Geovisualization},
	author = {Douglas, David H and Peucker, Thomas K},
	month = dec,
	year = {1973},
	pages = {112--122},
}

@article{taher_feasibility_2022,
	title = {Feasibility of {Hyperspectral} {Single} {Photon} {Lidar} for {Robust} {Autonomous} {Vehicle} {Perception}},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/15/5759},
	doi = {10.3390/s22155759},
	abstract = {Autonomous vehicle perception systems typically rely on single-wavelength lidar sensors to obtain three-dimensional information about the road environment. In contrast to cameras, lidars are unaffected by challenging illumination conditions, such as low light during night-time and various bidirectional effects changing the return reflectance. However, as many commercial lidars operate on a monochromatic basis, the ability to distinguish objects based on material spectral properties is limited. In this work, we describe the prototype hardware for a hyperspectral single photon lidar and demonstrate the feasibility of its use in an autonomous-driving-related object classification task. We also introduce a simple statistical model for estimating the reflectance measurement accuracy of single photon sensitive lidar devices. The single photon receiver frame was used to receive 30 12.3 nm spectral channels in the spectral band 1200–1570 nm, with a maximum channel-wise intensity of 32 photons. A varying number of frames were used to accumulate the signal photon count. Multiple objects covering 10 different categories of road environment, such as car, dry asphalt, gravel road, snowy asphalt, wet asphalt, wall, granite, grass, moss, and spruce tree, were included in the experiments. We test the influence of the number of spectral channels and the number of frames on the classification accuracy with random forest classifier and find that the spectral information increases the classification accuracy in the high-photon flux regime from 50\% to 94\% with 2 channels and 30 channels, respectively. In the low-photon flux regime, the classification accuracy increases from 30\% to 38\% with 2 channels and 6 channels, respectively. Additionally, we visualize the data with the t-SNE algorithm and show that the photon shot noise in the single photon sensitive hyperspectral data contributes the most to the separability of material specific spectral signatures. The results of this study provide support for the use of hyperspectral single photon lidar data on more advanced object detection and classification methods, and motivates the development of advanced single photon sensitive hyperspectral lidar devices for use in autonomous vehicles and in robotics.},
	language = {en},
	number = {15},
	urldate = {2022-11-17},
	journal = {Sensors},
	author = {Taher, Josef and Hakala, Teemu and Jaakkola, Anttoni and Hyyti, Heikki and Kukko, Antero and Manninen, Petri and Maanpää, Jyri and Hyyppä, Juha},
	month = jan,
	year = {2022},
	keywords = {SPAD, autonomous driving, classification, hyperspectral LIDAR, multispectral, object detection, photon shot noise, remote sensing, single photon, spectral signature},
	pages = {5759},
}

@article{serrano_intuitive_2016,
	title = {An intuitive control space for material appearance},
	volume = {35},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2980179.2980242},
	doi = {10.1145/2980179.2980242},
	abstract = {Many different techniques for measuring material appearance have been proposed in the last few years. These have produced large public datasets, which have been used for accurate, data-driven appearance modeling. However, although these datasets have allowed us to reach an unprecedented level of realism in visual appearance, editing the captured data remains a challenge. In this paper, we present an intuitive control space for predictable editing of captured BRDF data, which allows for artistic creation of plausible novel material appearances, bypassing the difficulty of acquiring novel samples. We first synthesize novel materials, extending the existing MERL dataset up to 400 mathematically valid BRDFs. We then design a large-scale experiment, gathering 56,000 subjective ratings on the high-level perceptual attributes that best describe our extended dataset of materials. Using these ratings, we build and train networks of radial basis functions to act as functionals mapping the perceptual attributes to an underlying PCA-based representation of BRDFs. We show that our functionals are excellent predictors of the perceived attributes of appearance. Our control space enables many applications, including intuitive material editing of a wide range of visual properties, guidance for gamut mapping, analysis of the correlation between perceptual attributes, or novel appearance similarity metrics. Moreover, our methodology can be used to derive functionals applicable to classic analytic BRDF representations. We release our code and dataset publicly, in order to support and encourage further research in this direction.},
	number = {6},
	urldate = {2022-11-17},
	journal = {ACM Transactions on Graphics},
	author = {Serrano, Ana and Gutierrez, Diego and Myszkowski, Karol and Seidel, Hans-Peter and Masia, Belen},
	month = dec,
	year = {2016},
	keywords = {BRDF editing, appearance editing, light reflection models, visual perception},
	pages = {186:1--186:12},
}

@misc{hesai_pandaset_2021,
	title = {{PandaSet}},
	urldate = {2021-12-14},
	author = {{Hesai}},
	year = {2021},
}

@article{xie_linking_2020,
	title = {Linking {Points} {With} {Labels} in {3D}: {A} {Review} of {Point} {Cloud} {Semantic} {Segmentation}},
	volume = {8},
	issn = {2168-6831, 2473-2397, 2373-7468},
	shorttitle = {Linking {Points} {With} {Labels} in {3D}},
	url = {http://arxiv.org/abs/1908.08854},
	doi = {10.1109/MGRS.2019.2937630},
	abstract = {3D Point Cloud Semantic Segmentation (PCSS) is attracting increasing interest, due to its applicability in remote sensing, computer vision and robotics, and due to the new possibilities offered by deep learning techniques. In order to provide a needed up-to-date review of recent developments in PCSS, this article summarizes existing studies on this topic. Firstly, we outline the acquisition and evolution of the 3D point cloud from the perspective of remote sensing and computer vision, as well as the published benchmarks for PCSS studies. Then, traditional and advanced techniques used for Point Cloud Segmentation (PCS) and PCSS are reviewed and compared. Finally, important issues and open questions in PCSS studies are discussed.},
	number = {4},
	urldate = {2020-12-23},
	journal = {IEEE Geoscience and Remote Sensing Magazine},
	author = {Xie, Yuxing and Tian, Jiaojiao and Zhu, Xiao Xiang},
	month = dec,
	year = {2020},
	keywords = {Cameras, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Image segmentation, Laser radar, Semantics, Sensors, Synthetic aperture radar, Three-dimensional displays},
	pages = {38--59},
}

@inproceedings{manivasagam_lidarsim_2020,
	address = {Seattle, WA, USA},
	title = {{LiDARsim}: {Realistic} {LiDAR} {Simulation} by {Leveraging} the {Real} {World}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{LiDARsim}},
	url = {https://ieeexplore.ieee.org/document/9157601/},
	doi = {10.1109/CVPR42600.2020.01118},
	urldate = {2021-01-05},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Manivasagam, Sivabalan and Wang, Shenlong and Wong, Kelvin and Zeng, Wenyuan and Sazanovich, Mikita and Tan, Shuhan and Yang, Bin and Ma, Wei-Chiu and Urtasun, Raquel},
	month = jun,
	year = {2020},
	keywords = {Data models, Laser radar, Physics, Robot sensing systems, Solid modeling, Three-dimensional displays, Vehicle dynamics},
	pages = {11164--11173},
}

@inproceedings{hackel_semantic3d_2017,
	title = {{SEMANTIC3D}: {A} new large-scale point cloud classification benchmark},
	volume = {IV-1-W1},
	booktitle = {{ISPRS} {Annals} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	author = {Hackel, Timo and Savinov, N. and Ladicky, L. and Wegner, Jan D. and Schindler, K. and Pollefeys, M.},
	year = {2017},
	pages = {91--98},
}

@article{su_simulation_2019,
	title = {A {Simulation} {Method} for {LIDAR} of {Autonomous} {Cars}},
	volume = {234},
	doi = {10.1088/1755-1315/234/1/012055},
	journal = {IOP Conference Series: Earth and Environmental Science},
	author = {Su, Hu and Wang, Rui and Chen, Kaixin and Chen, Yizhan},
	month = mar,
	year = {2019},
	pages = {012055},
}

@article{isa_design_2017,
	title = {Design and analysis of a {3D} laser scanner},
	volume = {111},
	issn = {02632241},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0263224117304633},
	doi = {10.1016/j.measurement.2017.07.028},
	language = {en},
	urldate = {2021-01-05},
	journal = {Measurement},
	author = {Isa, Mohammed A. and Lazoglu, Ismail},
	month = dec,
	year = {2017},
	pages = {122--133},
}

@misc{montes_soldado_overview_2012,
	title = {An {Overview} of {BRDF} {Models}},
	author = {Montes Soldado, Rosana and Ureña Almagro, Carlos},
	month = mar,
	year = {2012},
}

@article{melero_fast_2019,
	title = {Fast collision detection between high resolution polygonal models},
	volume = {83},
	doi = {10.1016/j.cag.2019.07.006},
	journal = {Computers \& Graphics},
	author = {Melero, Francisco Javier and Aguilera, Ángel and Feito, Francisco Ramón},
	month = oct,
	year = {2019},
	pages = {97--106},
}

@misc{hable_uncharted_2010,
	title = {Uncharted 2: {HDR} {Lighting}},
	author = {Hable, John},
	month = mar,
	year = {2010},
}

@inproceedings{gschwandtner_blensor_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{BlenSor}: {Blender} {Sensor} {Simulation} {Toolbox}},
	isbn = {978-3-642-24031-7},
	shorttitle = {{BlenSor}},
	doi = {10.1007/978-3-642-24031-7_20},
	abstract = {This paper introduces a novel software package for the simulation of various types of range scanners. The goal is to provide researchers in the fields of obstacle detection, range data segmentation, obstacle tracking or surface reconstruction with a versatile and powerful software package that is easy to use and allows to focus on algorithmic improvements rather than on building the software framework around it. The simulation environment and the actual simulations can be efficiently distributed with a single compact file. Our proposed approach facilitates easy regeneration of published results, hereby highlighting the value of reproducible research.},
	language = {en},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer},
	author = {Gschwandtner, Michael and Kwitt, Roland and Uhl, Andreas and Pree, Wolfgang},
	editor = {Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Wang, Song and Kyungnam, Kim and Benes, Bedrich and Moreland, Kenneth and Borst, Christoph and DiVerdi, Stephen and Yi-Jen, Chiang and Ming, Jiang},
	year = {2011},
	keywords = {Complex Scene, Game Engine, Lidar Sensor, Obstacle Detection, Pitch Angle},
	pages = {199--208},
}

@inproceedings{wu_squeezesegv2_2019,
	title = {{SqueezeSegV2}: {Improved} {Model} {Structure} and {Unsupervised} {Domain} {Adaptation} for {Road}-{Object} {Segmentation} from a {LiDAR} {Point} {Cloud}},
	shorttitle = {{SqueezeSegV2}},
	doi = {10.1109/ICRA.2019.8793495},
	abstract = {Earlier work demonstrates the promise of deep-learning-based approaches for point cloud segmentation; however, these approaches need to be improved to be practically useful. To this end, we introduce a new model SqueezeSegV2. With an improved model structure, SqueezeSetV2 is more robust against dropout noises in LiDAR point cloud and therefore achieves significant accuracy improvement. Training models for point cloud segmentation requires large amounts of labeled data, which is expensive to obtain. To sidestep the cost of data collection and annotation, simulators such as GTA-V can be used to create unlimited amounts of labeled, synthetic data. However, due to domain shift, models trained on synthetic data often do not generalize well to the real world. Existing domain-adaptation methods mainly focus on images and most of them cannot be directly applied to point clouds. We address this problem with a domain-adaptation training pipeline consisting of three major components: 1) learned intensity rendering, 2) geodesic correlation alignment, and 3) progressive domain calibration. When trained on real data, our new model exhibits segmentation accuracy improvements of 6.0-8.6\% over the original SqueezeSeg. When training our new model on synthetic data using the proposed domain adaptation pipeline, we nearly double test accuracy on real-world data, from 29.0\% to 57.4\%. Our source code and synthetic dataset are open sourced. https://github.com/xuanyuzhou98/SqueezeSegV2.},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Wu, Bichen and Zhou, Xuanyu and Zhao, Sicheng and Yue, Xiangyu and Keutzer, Kurt},
	month = may,
	year = {2019},
	keywords = {Adaptation models, Data models, Laser radar, Pipelines, Sensors, Three-dimensional displays, Training},
	pages = {4376--4382},
}

@misc{the_american_society_for_photogrammetry__remote_sensing_specification_2013,
	title = {{LAS} {Specification}, {Version} 1.4},
	author = {{The American Society for Photogrammetry \& Remote Sensing}},
	year = {2013},
}

@article{xiao_synlidar_2021,
	title = {{SynLiDAR}: {Learning} {From} {Synthetic} {LiDAR} {Sequential} {Point} {Cloud} for {Semantic} {Segmentation}},
	journal = {arXiv preprint arXiv:2107.05399},
	author = {Xiao, Aoran and Huang, Jiaxing and Guan, Dayan and Zhan, Fangneng and Lu, Shijian},
	year = {2021},
}

@article{iqbal_simulation_2020,
	title = {Simulation of an {Autonomous} {Mobile} {Robot} for {LiDAR}-{Based} {In}-{Field} {Phenotyping} and {Navigation}},
	volume = {9},
	issn = {2218-6581},
	url = {https://www.mdpi.com/2218-6581/9/2/46},
	doi = {10.3390/robotics9020046},
	abstract = {The agriculture industry is in need of substantially increasing crop yield to meet growing global demand. Selective breeding programs can accelerate crop improvement but collecting phenotyping data is time- and labor-intensive because of the size of the research fields and the frequency of the work required. Automation could be a promising tool to address this phenotyping bottleneck. This paper presents a Robotic Operating System (ROS)-based mobile field robot that simultaneously navigates through occluded crop rows and performs various phenotyping tasks, such as measuring plant volume and canopy height using a 2D LiDAR in a nodding configuration. The efficacy of the proposed 2D LiDAR configuration for phenotyping is assessed in a high-fidelity simulated agricultural environment in the Gazebo simulator with an ROS-based control framework and compared with standard LiDAR configurations used in agriculture. Using the proposed nodding LiDAR configuration, a strategy for navigation through occluded crop rows is presented. The proposed LiDAR configuration achieved an estimation error of 6.6\% and 4\% for plot volume and canopy height, respectively, which was comparable to the commonly used LiDAR configurations. The hybrid strategy with GPS waypoint following and LiDAR-based navigation was used to navigate the robot through an agricultural crop field successfully with an root mean squared error of 0.0778 m which was 0.2\% of the total traveled distance. The presented robot simulation framework in ROS and optimized LiDAR configuration helped to expedite the development of the agricultural robots, which ultimately will aid in overcoming the phenotyping bottleneck.},
	language = {en},
	number = {2},
	urldate = {2021-01-05},
	journal = {Robotics},
	author = {Iqbal, Jawad and Xu, Rui and Sun, Shangpeng and Li, Changying},
	month = jun,
	year = {2020},
	keywords = {Gazebo, LiDAR, ROS, agriculture, autonomous, navigation, phenotyping, robotics, simulation},
	pages = {46},
}

@article{li_deep_2020,
	title = {Deep {Learning} for {LiDAR} {Point} {Clouds} in {Autonomous} {Driving}: {A} {Review}},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Deep {Learning} for {LiDAR} {Point} {Clouds} in {Autonomous} {Driving}},
	url = {https://ieeexplore.ieee.org/document/9173706/},
	doi = {10.1109/TNNLS.2020.3015992},
	urldate = {2021-01-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Li, Ying and Ma, Lingfei and Zhong, Zilong and Liu, Fei and Chapman, Michael A. and Cao, Dongpu and Li, Jonathan},
	year = {2020},
	pages = {1--21},
}

@article{lee_validation_2020,
	title = {{VALIDATION} {OF} {LIDAR} {CALIBRATION} {USING} {A} {LIDAR} {SIMULATOR}},
	volume = {XLIII-B1-2020},
	issn = {2194-9034},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLIII-B1-2020/39/2020/},
	doi = {10.5194/isprs-archives-XLIII-B1-2020-39-2020},
	abstract = {Abstract. LIDAR is being widely used for mapping and modelling because it accurately scans and acquires 3D geometric information of the surrounding environment. In order to improve the accuracy of the LIDAR measurement, it is necessary to precisely estimate the intrinsic parameters as well as extrinsic parameters and eliminate the systematic errors. Many studies are conducted to eliminate these errors caused by the intrinsic parameters of LIDAR. However, when the result of intrinsic calibration is verified using actual LIDAR data, there is a problem that other error factors cannot be excluded. Therefore, in this study, the LIDAR intrinsic calibration is verified by using a LIDAR simulator that simulates the mechanism of the actual LIDAR. When constructing a LIDAR simulator, the systematic error is inserted according to the intrinsic parameter model of LIDAR. And according to the method of scanning with LIDAR, it is divided into upright scanning and tilted scanning, and the error included LIDAR simulation data is generated. After that, the intrinsic parameters are estimated by applying the plane-based intrinsic calibration. Since values of the intrinsic parameters are known, they are compared with the estimated parameters, and the results of estimate are analyzed according to the scanning method.},
	language = {en},
	urldate = {2020-12-23},
	journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Lee, G. and Cheon, J. and Lee, I.},
	month = aug,
	year = {2020},
	pages = {39--44},
}

@article{mathew_urban_2019,
	title = {Urban {Walkability} {Design} {Using} {Virtual} {Population} {Simulation}},
	volume = {38},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13585},
	doi = {10.1111/cgf.13585},
	abstract = {We present a system to generate a procedural environment that produces a desired crowd behaviour. Instead of altering the behavioural parameters of the crowd itself, we automatically alter the environment to yield such desired crowd behaviour. This novel inverse approach is useful both to crowd simulation in virtual environments and to urban crowd planning applications. Our approach tightly integrates and extends a space discretization crowd simulator with inverse procedural modelling. We extend crowd simulation by goal exploration (i.e. agents are initially unaware of the goal locations), variable-appealing sign usage and several acceleration schemes. We use Markov chain Monte Carlo to quickly explore the solution space and yield interactive design. We have applied our method to a variety of virtual and real-world locations, yielding one order of magnitude faster crowd simulation performance over related methods and several fold improvement of crowd indicators.},
	language = {en},
	number = {1},
	urldate = {2022-01-21},
	journal = {Computer Graphics Forum},
	author = {Mathew, C. D. Tharindu and Knob, Paulo R. and Musse, Soraia Raupp and Aliaga, Daniel G.},
	year = {2019},
	keywords = {Computing methodologies → Interactive simulation, Procedural animation, animation, human simulation},
	pages = {455--469},
}

@inproceedings{peinecke_lidar_2008,
	address = {St. Paul, MN, USA},
	title = {Lidar simulation using graphics hardware acceleration},
	isbn = {978-1-4244-2207-4},
	url = {http://ieeexplore.ieee.org/document/4702838/},
	doi = {10.1109/DASC.2008.4702838},
	urldate = {2021-01-05},
	booktitle = {2008 {IEEE}/{AIAA} 27th {Digital} {Avionics} {Systems} {Conference}},
	publisher = {IEEE},
	author = {Peinecke, Niklas and Lueken, Thomas and Korn, Bernd R.},
	month = oct,
	year = {2008},
	keywords = {Acceleration, Computational modeling, Computer graphics, Hardware, Infrared sensors, Laser radar, Machine vision, Millimeter wave radar, Millimeter wave technology, Sensor systems, enhanced vision systems, graphics hardware acceleration, lidar simulation, multi sensor fusion, multi-sensor data, optical radar, sensor fusion, synthetic vision systems},
	pages = {4.D.4--1--4.D.4--8},
}

@article{vacek_learning_2022,
	title = {Learning to {Predict} {Lidar} {Intensities}},
	volume = {23},
	issn = {1558-0016},
	doi = {10.1109/TITS.2020.3037980},
	abstract = {We propose a data-driven method for simulating lidar sensors. The method reads computer-generated data, and (i) extracts geometrically simulated lidar point clouds and (ii) predicts the strength of the lidar response – lidar intensities. Qualitative evaluation of the proposed pipeline demonstrates the ability to predict systematic failures such as no/low responses on polished parts of car bodyworks and windows, or strong responses on reflective surfaces such as traffic signs and license/registration plates. We also experimentally show that enhancing the training set by such simulated data improves the segmentation accuracy on the real dataset with limited access to real data. Implementation of the resulting lidar simulator for the GTA V game, as well as the accompanying large dataset, is made publicly available.},
	number = {4},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Vacek, Patrik and Jašek, Otakar and Zimmermann, Karel and Svoboda, Tomáš},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {Automobiles, Cameras, Computational modeling, Games, Laser radar, Robotics, Sensors, Three-dimensional displays, intelligent transportation systems, machine learning, neural network applications, sensor development, simulation},
	pages = {3556--3564},
}

@article{dupuy_adaptive_2018,
	title = {An adaptive parameterization for efficient material acquisition and rendering},
	volume = {37},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3272127.3275059},
	doi = {10.1145/3272127.3275059},
	abstract = {One of the key ingredients of any physically based rendering system is a detailed specification characterizing the interaction of light and matter of all materials present in a scene, typically via the Bidirectional Reflectance Distribution Function (BRDF). Despite their utility, access to real-world BRDF datasets remains limited: this is because measurements involve scanning a four-dimensional domain at sufficient resolution, a tedious and often infeasibly time-consuming process. We propose a new parameterization that automatically adapts to the behavior of a material, warping the underlying 4D domain so that most of the volume maps to regions where the BRDF takes on non-negligible values, while irrelevant regions are strongly compressed. This adaptation only requires a brief 1D or 2D measurement of the material's retro-reflective properties. Our parameterization is unified in the sense that it combines several steps that previously required intermediate data conversions: the same mapping can simultaneously be used for BRDF acquisition, storage, and it supports efficient Monte Carlo sample generation. We observe that the above desiderata are satisfied by a core operation present in modern rendering systems, which maps uniform variates to direction samples that are proportional to an analytic BRDF. Based on this insight, we define our adaptive parameterization as an invertible, retro-reflectively driven mapping between the parametric and directional domains. We are able to create noise-free renderings of existing BRDF datasets after conversion into our representation with the added benefit that the warped data is significantly more compact, requiring 16KiB and 544KiB per spectral channel for isotropic and anisotropic specimens, respectively. Finally, we show how to modify an existing gonio-photometer to provide the needed retro-reflection measurements. Acquisition then proceeds within a 4D space that is warped by our parameterization. We demonstrate the efficacy of this scheme by acquiring the first set of spectral BRDFs of surfaces exhibiting arbitrary roughness, including anisotropy.},
	number = {6},
	urldate = {2022-05-06},
	journal = {ACM Transactions on Graphics},
	author = {Dupuy, Jonathan and Jakob, Wenzel},
	month = dec,
	year = {2018},
	keywords = {microfacet theory, photorealistic rendering, spectral BRDF},
	pages = {274:1--274:14},
}

@article{tachella_real-time_2019,
	title = {Real-time {3D} reconstruction from single-photon lidar data using plug-and-play point cloud denoisers},
	volume = {10},
	copyright = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-12943-7},
	doi = {10.1038/s41467-019-12943-7},
	abstract = {Single-photon lidar has emerged as a prime candidate technology for depth imaging through challenging environments. Until now, a major limitation has been the significant amount of time required for the analysis of the recorded data. Here we show a new computational framework for real-time three-dimensional (3D) scene reconstruction from single-photon data. By combining statistical models with highly scalable computational tools from the computer graphics community, we demonstrate 3D reconstruction of complex outdoor scenes with processing times of the order of 20 ms, where the lidar data was acquired in broad daylight from distances up to 320 metres. The proposed method can handle an unknown number of surfaces in each pixel, allowing for target detection and imaging through cluttered scenes. This enables robust, real-time target reconstruction of complex moving scenes, paving the way for single-photon lidar at video rates for practical 3D imaging applications.},
	language = {en},
	number = {1},
	urldate = {2022-12-15},
	journal = {Nature Communications},
	author = {Tachella, Julián and Altmann, Yoann and Mellado, Nicolas and McCarthy, Aongus and Tobin, Rachael and Buller, Gerald S. and Tourneret, Jean-Yves and McLaughlin, Stephen},
	month = nov,
	year = {2019},
	keywords = {Applied optics, Computational science, Electrical and electronic engineering, Imaging techniques},
	pages = {4984},
}

@misc{mikhail_n_polyanskiy_refractive_nodate,
	title = {Refractive index database},
	url = {https://refractiveindex.info},
	urldate = {2022-12-15},
	author = {{Mikhail N. Polyanskiy}},
	note = {Publication Title: Refractive index database},
}

@article{vogt_comparison_2021,
	title = {Comparison of {iPad} {Pro}®’s {LiDAR} and {TrueDepth} {Capabilities} with an {Industrial} {3D} {Scanning} {Solution}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7080},
	url = {https://www.mdpi.com/2227-7080/9/2/25},
	doi = {10.3390/technologies9020025},
	abstract = {Today’s smart devices come equipped with powerful hard- and software-enabling professional use cases. The latest hardware by Apple utilizes LiDAR and TrueDepth, which offer the capability of 3D scanning. Devices equipped with these camera systems allow manufacturers to obtain 3D data from their customers at low costs, which potentially enables time-efficient mass customization and product differentiation strategies. However, the utilization is limited by the scanning accuracy. To determine the potential application of LiDAR and TrueDepth as a 3D scanning solution, in this paper an evaluation was performed. For this purpose, different Lego bricks were scanned with the technologies and an industrial 3D scanner. The results were compared according to shape and position tolerances. Even though the industrial 3D scanner consistently delivered more accurate results, the accuracy of the smart device technologies may already be sufficient, depending on the application.},
	language = {en},
	number = {2},
	urldate = {2022-11-29},
	journal = {Technologies},
	author = {Vogt, Maximilian and Rips, Adrian and Emmelmann, Claus},
	month = jun,
	year = {2021},
	keywords = {3D scanning, LiDAR, TrueDepth Camera, iPad Pro, reverse engineering},
	pages = {25},
}

@inproceedings{juttula_determination_2012,
	title = {Determination of refractive index of softwood using immersion liquid method},
	doi = {10.1109/I2MTC.2012.6229360},
	abstract = {Immersion liquid method is used to determine refractive indices of wood material. An integrating sphere setup is used to measure optical transmission of liquid containing small wood particles. Sweeping the refractive index of the liquid affects its transmission properties through light scattering caused by refractive index mismatch between solid wood particles and the liquid. The refractive index of the solid material is obtained by finding the refractive index value of the liquid which provides the lowest scattering value (maximum transmission). The refractive indices of softwood samples were determined within a wavelength range from 500 nm to 800 nm. The refractive index of sapwood and heartwood were found to be 1.56 and 1.57 at sodium D-line (589.3 nm), respectively. Measured refractive index values will be used when building models of wood for optical simulations.},
	booktitle = {2012 {IEEE} {International} {Instrumentation} and {Measurement} {Technology} {Conference} {Proceedings}},
	author = {Juttula, Harri and Mäkynen, Anssi J},
	month = may,
	year = {2012},
	keywords = {Liquids, Reflectivity, Refractive index, Temperature measurement, Wavelength measurement, immersion method, refractive index, wood},
	pages = {1231--1234},
}

@article{cai_survey_2022,
	title = {A {Survey} on {Data}-{Driven} {Scenario} {Generation} for {Automated} {Vehicle} {Testing}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-1702},
	url = {https://www.mdpi.com/2075-1702/10/11/1101},
	doi = {10.3390/machines10111101},
	abstract = {Automated driving is a promising tool for reducing traffic accidents. While some companies claim that many cutting-edge automated driving functions have been developed, how to evaluate the safety of automated vehicles remains an open question, which has become a crucial bottleneck. Scenario-based testing has been introduced to test automated vehicles, and much progress has been achieved. While data-driven and knowledge-based approaches are hot research topics, this survey is mainly about Data-Driven Scenario Generation (DDSG) for automated vehicle testing. Rather than describe the contributions of every study respectively, in this survey, methodologies from various studies are anatomized as solutions for several significant problems and compared with each other. This way, scholars and engineers can quickly find state-of-the-art approaches to the issues they might encounter. Furthermore, several critical challenges that might hinder DDSG are described, and responding solutions are presented at the end of this survey.},
	language = {en},
	number = {11},
	urldate = {2022-11-25},
	journal = {Machines},
	author = {Cai, Jinkang and Deng, Weiwen and Guang, Haoran and Wang, Ying and Li, Jiangkun and Ding, Juan},
	month = nov,
	year = {2022},
	keywords = {Traffic Safety, automated vehicles, scenario-based testing},
	pages = {1101},
}

@article{alsadik_ideal_2020,
	title = {Ideal {Angular} {Orientation} of {Selected} 64-{Channel} {Multi} {Beam} {Lidars} for {Mobile} {Mapping} {Systems}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/3/510},
	doi = {10.3390/rs12030510},
	abstract = {Lidar technology is thriving nowadays for different applications mainly for autonomous navigation, mapping, and smart city technology. Lidars vary in different aspects and can be: multi beam, single beam, spinning, solid state, full 360 field of view FOV, single or multi pulse returns, and many other geometric and radiometric aspects. Users and developers in the mapping industry are continuously looking for new released Lidars having high properties of output density, coverage, and accuracy while keeping a lower cost. Accordingly, every Lidar type should be well evaluated for the final intended mapping aim. This evaluation is not easy to implement in practice because of the need to have all the investigated Lidars available in hand and integrated into a ready to use mapping system. Furthermore, to have a fair comparison; it is necessary to ensure the test applied in the same environment at the same travelling path among other conditions. In this paper, we are evaluating two state-of-the-art multi beam Lidar types: Ouster OS-1-64 and Hesai Pandar64 for mapping applications. The evaluation of the Lidar types is applied in a simulation environment which approximates reality. The paper shows the determination of the ideal orientation angle for the two Lidars by assessing the density, coverage, and accuracy and presenting clear performance quantifications and conclusions.},
	language = {en},
	number = {3},
	urldate = {2022-11-24},
	journal = {Remote Sensing},
	author = {Alsadik, Bashar},
	month = jan,
	year = {2020},
	keywords = {3D simulation, Lidar, Ouster OS-1-64, Pandar64, mobile mapping systems, point cloud coverage, point cloud density},
	pages = {510},
}

@techreport{li_lidar_2016,
	address = {Warrendale, PA},
	type = {{SAE} {Technical} {Paper}},
	title = {{LiDAR} {Sensor} {Modeling} for {ADAS} {Applications} under a {Virtual} {Driving} {Environment}},
	url = {https://www.sae.org/publications/technical-papers/content/2016-01-1907/},
	abstract = {LiDAR sensors have played more and more important role on Intelligent and Connected Vehicles (ICV) and Advanced Driver Assistance Systems (ADAS) .However, the development and testing of LiDAR sensors under real driving environment for ADAS applications are greatly limited by various factors, and oft},
	language = {English},
	number = {2016-01-1907},
	urldate = {2022-11-21},
	institution = {SAE International},
	author = {Li, Yaxin and Wang, Ying and Deng, Weiwen and Li, Xin and Liu, Zhenyi and Jiang, Lijun},
	month = sep,
	year = {2016},
	doi = {10.4271/2016-01-1907},
}

@article{zhao_method_2021,
	title = {Method and {Applications} of {Lidar} {Modeling} for {Virtual} {Testing} of {Intelligent} {Vehicles}},
	volume = {22},
	issn = {1558-0016},
	doi = {10.1109/TITS.2020.2978438},
	abstract = {With the common use of lidar sensors on intelligent vehicles, the simulation of lidar in autonomous driving is necessary. This study proposes an innovative lidar modeling method, and introduces solutions for the simulation of the lidar detection function and its physical mechanism. The model consists of geometric and physical models, and can simulate both point clouds and targets. The geometric model addresses the spatial relationship between lidar and the environment. The physical model describes how the lidar detection mechanism may influence detection results. Signal attenuation and unwanted raw data caused by raindrops are the main consideration in the physical model. Characteristics of lidar signal attenuation in different weather conditions are modeled, and a simplified lidar equation is derived for use by lidar users, as opposed to designers. Unwanted raw data are simulated in a stochastic model employing the Monte Carlo method, where raindrop size and distance are sampled. The model is calibrated and validated with real lidar data. The application of the proposed lidar model for an autonomous emergency braking system is introduced.},
	number = {5},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Zhao, Jian and Li, Yaxin and Zhu, Bing and Deng, Weiwen and Sun, Bohua},
	month = may,
	year = {2021},
	keywords = {Atmospheric modeling, Attenuation, Automotive engineering, Data models, Intelligent vehicles, Laser radar, Meteorology, Three-dimensional displays, lidar modeling, signal attenuation, simulation test, unwanted raw data},
	pages = {2990--3000},
}

@inproceedings{hanke_generic_2015,
	title = {Generic architecture for simulation of {ADAS} sensors},
	doi = {10.1109/IRS.2015.7226306},
	abstract = {With rising complexity of advanced driver assistance systems (ADAS), development quality and speed are of increasing importance. Virtualization contributes towards this goal by providing a controlled environment even before hardware prototypes are available. One crucial component is a realistic description of the vehicle's sensors by means of sensor models. We propose a generic, modular architecture for the design of such models. The car's surroundings are represented by a list of objects that is modified during the virtualized sensing process. The result can then provide stimulation of ADAS function algorithms. Using the proposed architecture as a framework, we construct an exemplary model employing a stochastic variation of target object positions.},
	booktitle = {2015 16th {International} {Radar} {Symposium} ({IRS})},
	author = {Hanke, T. and Hirsenkorn, N. and Dehlink, B. and Rauch, A. and Rasshofer, R. and Biebl, E.},
	month = jun,
	year = {2015},
	keywords = {Data models, Position measurement, Sensors, Signal processing, Stochastic processes, Vehicles},
	pages = {125--130},
}

@inproceedings{roth_analysis_2011,
	title = {Analysis and {Validation} of {Perception} {Sensor} {Models} in an {Integrated} {Vehicle} and {Environment} {Simulation}},
	author = {Roth, Erwin and Dirndorfer, Tobias and Knoll, Alois and Neumann-Cosel, Kilian von and Ganslmeier, Thomas and Kern, Andreas and Fischer, Marc},
	year = {2011},
}

@article{ahn_real-time_2020,
	title = {Real-time {Simulation} of {Physical} {Multi}-sensor {Setups}},
	volume = {15},
	issn = {2524-8804},
	url = {https://doi.org/10.1007/s38314-020-0207-1},
	doi = {10.1007/s38314-020-0207-1},
	language = {en},
	number = {6},
	urldate = {2022-11-21},
	journal = {ATZelectronics worldwide},
	author = {Ahn, Natalya and Höfer, Andreas and Herrmann, Martin and Donn, Christian},
	month = jun,
	year = {2020},
	pages = {8--11},
}

@inproceedings{dosovitskiy_carla_2017,
	title = {{CARLA}: {An} {Open} {Urban} {Driving} {Simulator}},
	shorttitle = {{CARLA}},
	url = {https://proceedings.mlr.press/v78/dosovitskiy17a.html},
	abstract = {We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform’s utility for autonomous driving research.},
	language = {en},
	urldate = {2022-11-21},
	booktitle = {Proceedings of the 1st {Annual} {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
	month = oct,
	year = {2017},
	pages = {1--16},
}

@inproceedings{hanke_generation_2017,
	title = {Generation and validation of virtual point cloud data for automated driving systems},
	doi = {10.1109/ITSC.2017.8317864},
	abstract = {The performance of an automated driving system is crucially affected by its environmental perception. The vehicle's perception of its environment provides the foundation for the automated responses computed by the system's logic algorithms. As perception relies on the vehicle's sensors, simulating sensor behavior in a virtual world constitutes virtual environmental perception. This is the task performed by sensor models. In this work, we introduce a real-time capable model of the measurement process for an automotive lidar sensor employing a ray tracing approach. The output of the model is point cloud data based on the geometry and material properties of the virtual scene. With this low level sensor data as input, a vehicle internal representation of the environment is constructed by means of an occupancy grid mapping algorithm. By using a virtual environment that has been constructed from high-fidelity measurements of a real world scenario, we are able to establish a direct link between real and virtual world sensor data. Directly comparing the resulting sensor output and environment representations from both cases, we are able to quantitatively explore the validity and fidelity of the proposed sensor measurement model.},
	booktitle = {2017 {IEEE} 20th {International} {Conference} on {Intelligent} {Transportation} {Systems} ({ITSC})},
	author = {Hanke, Timo and Schaermann, Alexander and Geiger, Matthias and Weiler, Konstantin and Hirsenkorn, Nils and Rauch, Andreas and Schneider, Stefan-Alexander and Biebl, Erwin},
	month = oct,
	year = {2017},
	keywords = {Data models, Laser radar, Ray tracing, Sensor phenomena and characterization, Sensor systems, Three-dimensional displays},
	pages = {1--6},
}

@article{bechtold_helios_2016,
	title = {Helios: a {Multi}-{Purpose} {LIDAR} {Simulation} {Framework} for {Research}, {Planning} and {Training} of {Laser} {Scanning} {Operations} with {Airborne}, {Ground}-{Based} {Mobile} and {Stationary} {Platforms}},
	volume = {III3},
	issn = {2194-9050 ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	shorttitle = {Helios},
	url = {https://ui.adsabs.harvard.edu/abs/2016ISPAnIII3..161B},
	doi = {10.5194/isprs-annals-III-3-161-2016},
	abstract = {In many technical domains of modern society, there is a growing demand for fast, precise and automatic acquisition of digital 3D models of a wide variety of physical objects and environments. Laser scanning is a popular and widely used technology to cover this demand, but it is also expensive and complex to use to its full potential. However, there might exist scenarios where the operation of a real laser scanner could be replaced by a computer simulation, in order to save time and costs. This includes scenarios like teaching and training of laser scanning, development of new scanner hardware and scanning methods, or generation of artificial scan data sets to support the development of point cloud processing and analysis algorithms. To test the feasibility of this idea, we have developed a highly flexible laser scanning simulation framework named Heidelberg LiDAR Operations Simulator (HELIOS). HELIOS is implemented as a Java library and split up into a core component and multiple extension modules. Extensible Markup Language (XML) is used to define scanner, platform and scene models and to configure the behaviour of modules. Modules were developed and implemented for (1) loading of simulation assets and configuration (i.e. 3D scene models, scanner definitions, survey descriptions etc.), (2) playback of XML survey descriptions, (3) TLS survey planning (i.e. automatic computation of recommended scanning positions) and (4) interactive real-time 3D visualization of simulated surveys. As a proof of concept, we show the results of two experiments: First, a survey planning test in a scene that was specifically created to evaluate the quality of the survey planning algorithm. Second, a simulated TLS scan of a crop field in a precision farming scenario. The results show that HELIOS fulfills its design goals.},
	urldate = {2022-11-21},
	journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Bechtold, S. and Höfle, B.},
	month = jun,
	year = {2016},
	pages = {161--168},
}

@inproceedings{goodin_sensor_2009,
	title = {Sensor modeling for the {Virtual} {Autonomous} {Navigation} {Environment}},
	doi = {10.1109/ICSENS.2009.5398491},
	abstract = {The Virtual Autonomous Navigation Environment (VANE) is a high fidelity, physics-based simulation process that produces realistic simulated sensor output for use in the development and testing of Autonomous Mobility Systems (AMS). The VANE produces simulated sensor output for ranging and camera sensors that are characterized by a few easily determined input parameters. This flexibility allows for the efficient characterization of a sensor interaction with a particular AMS. This paper presents the development of these models and some initial results.},
	booktitle = {2009 {IEEE} {SENSORS}},
	author = {Goodin, Chris and Kala, Raju and Carrrillo, Alex and Liu, Linda Y.},
	month = oct,
	year = {2009},
	keywords = {Blades, Charge coupled devices, Charge-coupled image sensors, Computational modeling, Filters, Geometrical optics, Navigation, Optical sensors, Sensor phenomena and characterization, Sensor systems},
	pages = {1588--1592},
}

@article{rosenberger_sequential_2020,
	title = {Sequential lidar sensor system simulation: a modular approach for simulation-based safety validation of automated driving},
	volume = {5},
	issn = {2365-5135},
	shorttitle = {Sequential lidar sensor system simulation},
	url = {https://doi.org/10.1007/s41104-020-00066-x},
	doi = {10.1007/s41104-020-00066-x},
	abstract = {Validating safety is an unsolved challenge before autonomous driving on public roads is possible. Since only the use of simulation-based test procedures can lead to an economically viable solution for safety validation, computationally efficient simulation models with validated fidelity are demanded. A central part of the overall simulation tool chain is the simulation of the perception components. In this work, a sequential modular approach for simulation of active perception sensor systems is presented on the example of lidar. It enables the required level of fidelity of synthetic object list data for safety validation using beforehand simulated point clouds. The elaborated framework around the sequential modules provides standardized interfaces packaging for co-simulation such as Open Simulation Interface (OSI) and Functional Mockup Interface (FMI), while providing a new level of modularity, testability, interchangeability, and distributability. The fidelity of the sequential approach is demonstrated on an everyday scenario at an intersection that is performed in reality at first and reproduced in simulation afterwards. The synthetic point cloud is generated by a sensor model with high fidelity and processed by a tracking model afterwards, which, therefore, outputs bounding boxes and trajectories that are close to reality.},
	language = {en},
	number = {3},
	urldate = {2022-11-21},
	journal = {Automotive and Engine Technology},
	author = {Rosenberger, Philipp and Holder, Martin Friedrich and Cianciaruso, Nicodemo and Aust, Philip and Tamm-Morschel, Jonas Franz and Linnhoff, Clemens and Winner, Hermann},
	month = dec,
	year = {2020},
	keywords = {Automated driving, Lidar sensor simulation, Safety validation},
	pages = {187--197},
}

@inproceedings{hirsenkorn_non-parametric_2015,
	title = {A non-parametric approach for modeling sensor behavior},
	doi = {10.1109/IRS.2015.7226346},
	abstract = {Realistic sensor models contribute to the progress of advanced driver assistance systems; off-line development is enabled and rare critical scenarios can be tested. In this paper a non-parametric (i.e., data driven) statistical framework is developed to reproduce sensor behavior. A detailed probability density function is constructed via kernel density estimation by exploiting measurements of an automotive radar system and a high-precision reference system. The approach is capable of inherently modeling sensor range, occlusion, latency, ghost objects, and object loss without explicit programming. Moreover, only few assumptions on the sensor properties are made; therefore, the technique is generic and can be applied to any object-list-generating sensor. The statistically equivalent implementation improvements presented herein render the approach real-time capable. Finally, the method is applied to an automotive radar system using test drives.},
	booktitle = {2015 16th {International} {Radar} {Symposium} ({IRS})},
	author = {Hirsenkorn, N. and Hanke, T. and Rauch, A. and Dehlink, B. and Rasshofer, R. and Biebl, E.},
	month = jun,
	year = {2015},
	keywords = {Automotive engineering, Electronic mail, Estimation, Kernel, Radar, Random variables, Real-time systems},
	pages = {131--136},
}

@article{linnhoff_refining_2021,
	title = {Refining {Object}-{Based} {Lidar} {Sensor} {Modeling} — {Challenging} {Ray} {Tracing} as the {Magic} {Bullet}},
	volume = {21},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2021.3115589},
	abstract = {Sensor and perception simulation is key for simulation based testing of automated driving functions. Depending on the testing use-case, different cause-effect chains for the specific sensor technology become relevant and the demanded computation times differ. In this work, a novel approach for object-based lidar simulation is introduced, identifying and modeling major sensor effects while balancing effect fidelity and computation time. With an explicitly designed static experiment, simple object based models are falsified, showing the need of a novel approach. Therefore, refined bounding boxes are designed and integrated into occlusion calculation. This new approach is compared to an advanced ray tracing simulation on an object output level. The comparison is conducted on the cause-effect chain of partial object occlusion, which has been identified as highly relevant. The modeling approach challenges ray tracing as the magic bullet for high-fidelity lidar front-end simulation. In direct comparison with the ray tracing approach, our novel approach stands out due to its significantly lower computation time. The newly developed object-based model is open source and publicly available at https://gitlab.com/tuda-fzd/perception-sensor-modeling/object-based-generic-perception-object-model.},
	number = {21},
	journal = {IEEE Sensors Journal},
	author = {Linnhoff, Clemens and Rosenberger, Philipp and Winner, Hermann},
	month = nov,
	year = {2021},
	keywords = {Automated driving, Computational modeling, Data models, Laser radar, Ray tracing, Sensors, Solid modeling, Tools, lidar, modeling, perception sensor, simulation},
	pages = {24238--24245},
}

@inproceedings{muckenhuber_object-based_2019,
	title = {Object-based sensor model for virtual testing of {ADAS}/{AD} functions},
	doi = {10.1109/ICCVE45908.2019.8965071},
	abstract = {A novel generic sensor model is introduced for testing and validation of ADAS/AD (Advanced Driver Assistance Systems/Autonomous Drive) functions. The model converts an incoming object list into a sensor specific object list and is suitable for all ADAS/AD perception sensors operating on object level. The field of view of the sensor is represented by a two dimensional polygon, that can be defined by a set of points. A simple ray-tracing method is applied to simulate coverage by objects. The model allows multiple range specifications for a single sensor depending on object type and classification capabilities. A look-up table is used to convert the object class definitions of the virtual environment into the class definitions of the considered sensor. Additional object parameters that are detected by the sensor may be included. False negative and false positive detections are generated by probabilistic functions. The parametrisation procedure of the sensor model is explained and depicted in an example using the data sheet of the Radar Continental ARS404. The model's capability to simulate a complete sensor set is demonstrated with the sensor set of the Renault Zoe that was used to collect the nuScenes data set. The sensor model is integrated into a virtual test-bed using Vires VTD and Open Simulation Interface.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Connected} {Vehicles} and {Expo} ({ICCVE})},
	author = {Muckenhuber, Stefan and Holzer, Hannes and Rübsam, Jonas and Stettinger, Georg},
	month = nov,
	year = {2019},
	keywords = {ADAS/AD function, sensor model, virtual testing},
	pages = {1--6},
}

@techreport{us_geological_survey_lidar_2012,
	type = {Techniques and {Methods}},
	title = {Lidar {Base} {Specification}: {Collection} {Requirements}},
	language = {en},
	institution = {U.S. Geological Survey},
	author = {{U.S. Geological Survey}},
	year = {2012},
}

@misc{instituto_geografico_de_informacion_geografica_pnoa_nodate,
	title = {{PNOA} {LiDAR}},
	url = {https://pnoa.ign.es/},
	abstract = {Geoportal web del Plan Nacional de Ortografía Aérea PNOA},
	language = {es-ES},
	urldate = {2022-11-18},
	author = {{Instituto Geográfico de Información Geográfica}},
	note = {Publication Title: Plan Nacional de Ortografía Aérea},
}

@article{lopez_gpu-accelerated_2022,
	title = {A {GPU}-{Accelerated} {Framework} for {Simulating} {LiDAR} {Scanning}},
	volume = {60},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2022.3165746},
	abstract = {In this work, we present an efficient graphics processing unit (GPU)-based light detection and ranging (LiDAR) scanner simulator. Laser-based scanning is a useful tool for applications ranging from reverse engineering or quality control at an object scale to large-scale environmental monitoring or topographic mapping. Beyond that, other specific applications require a large amount of LiDAR data during development, such as autonomous driving. Unfortunately, it is not easy to get a sufficient amount of ground-truth data due to time constraints and available resources. However, LiDAR simulation can generate classified data at a reduced cost. We propose a parameterized LiDAR to emulate a wide range of sensor models from airborne to terrestrial scanning. OpenGL’s compute shaders are used to massively generate beams emitted by the virtual LiDAR sensors and solve their collision with the surrounding environment even with multiple returns. Our work is mainly intended for the rapid generation of datasets for neural networks, consisting of hundreds of millions of points. The conducted tests show that the proposed approach outperforms a sequential LiDAR scanning. Its capabilities for generating huge labeled datasets have also been shown to improve previous studies.},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {López, Alfonso and Ogayar, Carlos J. and Jurado, Juan M. and Feito, Francisco R.},
	year = {2022},
	keywords = {General-purpose computing on graphics processing units (GPUs), Laser beams, Laser radar, Point cloud compression, Semantics, Sensors, Solid modeling, Three-dimensional displays, light detection and ranging (LiDAR) simulation, point cloud},
	pages = {1--18},
}

@inproceedings{lohani_generating_2007,
	title = {Generating {LiDAR} data in laboratory: {LiDAR} simulator},
	volume = {52},
	booktitle = {Int. {Arch}. {Photogramm}. {Remote} {Sens}.},
	author = {Lohani, Bharat and Mishra, R},
	month = jan,
	year = {2007},
}

@misc{noauthor_home_nodate,
	title = {Home - {LGSVL} {Simulator}},
	url = {https://www.svlsimulator.com/docs/archive/2020.06/},
	urldate = {2021-07-01},
}

@misc{noauthor_automatic_nodate,
	title = {Automatic detection and analysis of photovoltaic modules in aerial infrared imagery},
	url = {https://ieeexplore.ieee.org/document/7477658/},
	abstract = {Drone-based aerial thermography has become a convenient quality assessment tool for the precise localization of defective modules and cells in large photovoltaic-power plants. However, manual evaluation of aerial infrared recordings can be extremely time-consuming. Therefore, we propose an approach for automatic detection and analysis of photovoltaic modules in aerial infrared images. Significant temperature abnormalities such as hot spots and hot areas can be identified using our processing pipeline. To identify such defects, we first detect the individual modules in infrared images, and then use statistical tests to detect the defective modules. A quantitative evaluation of the detection and analysis pipeline on real-world, infrared recordings shows the applicability of our approach.},
	language = {en-US},
	urldate = {2021-05-09},
}

@misc{noauthor_thermography_nodate,
	title = {Thermography for early detection of faults on {PV} {Systems}},
	url = {https://kitawa.de/en/thermography-with-multicopter},
	urldate = {2021-05-09},
}

@misc{noauthor_pix4dmapper_2021,
	title = {{PIX4Dmapper}: {Professional} photogrammetry software for drone mapping},
	shorttitle = {{PIX4Dmapper}},
	url = {https://www.pix4d.com/product/pix4dmapper-photogrammetry-software},
	abstract = {The leading photogrammetry software for professional drone mapping. Transform any aerial and ground images into accurate, georeferenced maps and 3D models.},
	language = {en-US},
	urldate = {2021-10-16},
	year = {2021},
	note = {Publication Title: Pix4D},
}

@article{jarzabek-rychard_supervised_2020,
	title = {Supervised {Detection} of {Façade} {Openings} in {3D} {Point} {Clouds} with {Thermal} {Attributes}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/12/3/543},
	doi = {10.3390/rs12030543},
	abstract = {Targeted energy management and control is becoming an increasing concern in the building sector. Automatic analyses of thermal data, which minimize the subjectivity of the assessment and allow for large-scale inspections, are therefore of high interest. In this study, we propose an approach for a supervised extraction of fa\&ccedil;ade openings (windows and doors) from photogrammetric 3D point clouds attributed to RGB and thermal infrared (TIR) information. The novelty of the proposed approach is in the combination of thermal information with other available characteristics of data for a classification performed directly in 3D space. Images acquired in visible and thermal infrared spectra serve as input data for the camera pose estimation and the reconstruction of 3D scene geometry. To investigate the relevance of different information types to the classification performance, a Random Forest algorithm is applied to various sets of computed features. The best feature combination is then used as an input for a Conditional Random Field that enables us to incorporate contextual information and consider the interaction between the points. The evaluation executed on a per-point level shows that the fusion of all available information types together with context consideration allows us to extract objects with 90\% completeness and 95\% correctness. A respective assessment executed on a per-object level shows 97\% completeness and 88\% accuracy.},
	language = {en},
	number = {3},
	urldate = {2021-04-01},
	journal = {Remote Sensing},
	author = {Jarząbek-Rychard, Małgorzata and Lin, Dong and Maas, Hans-Gerd},
	month = jan,
	year = {2020},
	keywords = {3D point cloud, building façades, semantic classification, thermal infrared imagery},
	pages = {543},
}

@article{pepe_efficient_2020,
	title = {An {Efficient} {Pipeline} to {Obtain} {3D} {Model} for {HBIM} and {Structural} {Analysis} {Purposes} from {3D} {Point} {Clouds}},
	volume = {10},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/10/4/1235},
	doi = {10.3390/app10041235},
	abstract = {The aim of this work is to identify an efficient pipeline in order to build HBIM (heritage building information modelling) and create digital models to be used in structural analysis. To build accurate 3D models it is first necessary to perform a geomatics survey. This means performing a survey with active or passive sensors and, subsequently, accomplishing adequate post-processing of the data. In this way, it is possible to obtain a 3D point cloud of the structure under investigation. The next step, known as “scan-to-BIM (building information modelling)”, has led to the creation of an appropriate methodology that involved the use of Rhinoceros software and a few tools developed within this environment. Once the 3D model is obtained, the last step is the implementation of the structure in FEM (finite element method) and/or in HBIM software. In this paper, two case studies involving structures belonging to the cultural heritage (CH) environment are analysed: a historical church and a masonry bridge. In particular, for both case studies, the different phases were described involving the construction of the point cloud and, subsequently, the construction of a 3D model. This model is suitable both for structural analysis and for the parameterization of rheological and geometric information of each single element of the structure.},
	language = {en},
	number = {4},
	urldate = {2020-12-23},
	journal = {Applied Sciences},
	author = {Pepe, Massimiliano and Costantino, Domenica and Restuccia Garofalo, Alfredo},
	month = feb,
	year = {2020},
	pages = {1235},
}

@article{pejic_design_2013,
	title = {Design and optimisation of laser scanning for tunnels geometry inspection},
	volume = {37},
	issn = {08867798},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S088677981300062X},
	doi = {10.1016/j.tust.2013.04.004},
	language = {en},
	urldate = {2021-01-09},
	journal = {Tunnelling and Underground Space Technology},
	author = {Pejić, Marko},
	month = aug,
	year = {2013},
	pages = {199--206},
}

@article{olsen_situ_2015,
	title = {In {Situ} {Change} {Analysis} and {Monitoring} through {Terrestrial} {Laser} {Scanning}},
	volume = {29},
	issn = {0887-3801, 1943-5487},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%29CP.1943-5487.0000328},
	doi = {10.1061/(ASCE)CP.1943-5487.0000328},
	language = {en},
	number = {2},
	urldate = {2021-01-09},
	journal = {Journal of Computing in Civil Engineering},
	author = {Olsen, Michael J.},
	month = mar,
	year = {2015},
	pages = {04014040},
}

@article{yang_bim_2020,
	title = {{BIM} {Research} {Topics} and {Progress} in the {Construction} {Field}: {Analysis} {Based} on {Knowledge} {Mapping}},
	volume = {165},
	issn = {2267-1242},
	shorttitle = {{BIM} {Research} {Topics} and {Progress} in the {Construction} {Field}},
	url = {https://www.e3s-conferences.org/10.1051/e3sconf/202016503038},
	doi = {10.1051/e3sconf/202016503038},
	abstract = {The CNKI database and the WOS database are used as data retrieval sources for research literature related to BIM technology at home and abroad. Based on scientometrics and bibliometrics, through the analysis of keywords co-occurrence analysis, the hotspots and frontiers of BIM technology research in the construction field are quantitatively analysed, revealing the evolutionary trend of BIM technology research in the construction field. The research finds that BIM technology research in the construction field at home and abroad has roughly gone through three stages. Based on the combing of research frontiers and research hotspots, and analysis of their evolution trends, future research on BIM technology in the field of architecture will focus on the following directions: BIM technology is combined with the Internet of Things and cloud computing to conduct research, focusing on the application of BIM technology in risk management and the application of VR virtual reality technology.},
	urldate = {2021-01-02},
	journal = {E3S Web of Conferences},
	author = {Yang, Hongxiong and Wang, Duo},
	editor = {Qin, W. and Wang, L. and Yepes, V.},
	year = {2020},
	pages = {03038},
}

@article{ding_digital_2019,
	title = {A digital construction framework integrating building information modeling and reverse engineering technologies for renovation projects},
	volume = {102},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0926580518313384},
	doi = {10.1016/j.autcon.2019.02.012},
	language = {en},
	urldate = {2021-01-02},
	journal = {Automation in Construction},
	author = {Ding, Zhikun and Liu, Shan and Liao, Longhui and Zhang, Liang},
	month = jun,
	year = {2019},
	pages = {45--58},
}

@article{wang_integrating_2015,
	title = {Integrating {BIM} and {LiDAR} for {Real}-{Time} {Construction} {Quality} {Control}},
	volume = {79},
	issn = {0921-0296, 1573-0409},
	url = {http://link.springer.com/10.1007/s10846-014-0116-8},
	doi = {10.1007/s10846-014-0116-8},
	language = {en},
	number = {3-4},
	urldate = {2021-02-18},
	journal = {Journal of Intelligent \& Robotic Systems},
	author = {Wang, Jun and Sun, Weizhuo and Shou, Wenchi and Wang, Xiangyu and Wu, Changzhi and Chong, Heap-Yih and Liu, Yan and Sun, Cenfei},
	month = aug,
	year = {2015},
	pages = {417--432},
}

@article{kim_3d_2020,
	title = {{3D} {Point} {Cloud} and {BIM}-{Based} {Reconstruction} for {Evaluation} of {Project} by {As}-{Planned} and {As}-{Built}},
	volume = {12},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/9/1457},
	doi = {10.3390/rs12091457},
	abstract = {Progress management of a construction project can detect changes early by visualizing the progress of the project, as it is important to be capable of predicting the success or failure of future project objectives. However, to perform reliable progress management tasks, accurate measurement data is required. In this study, the basic principle of the evaluation of project progress was performed through the 3D point cloud and the 4D attributes of BIM. The evaluation of project progress proposed in this study was based on as-built data to assess the progress of the project site. The specific improvements via the proposed process for this study in the construction project-progress control area were as follows: (1) visualization of construction project progress, (2) calculation of project as-built quantity, and (3) evaluation of a project’s progress. This study improved the efficiency and productivity in the management of a construction project through detection of the progress process. It provided easy monitoring of the overall project status, such as productivity analysis, progress rate and quality verifications, and easy identification of the problems created and foreseeable engineering tasks.},
	language = {en},
	number = {9},
	urldate = {2020-12-23},
	journal = {Remote Sensing},
	author = {Kim, Seungho and Kim, Sangyong and Lee, Dong-Eun},
	month = may,
	year = {2020},
	pages = {1457},
}

@article{pucko_automated_2018,
	title = {Automated continuous construction progress monitoring using multiple workplace real time {3D} scans},
	volume = {38},
	issn = {14740346},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1474034617305086},
	doi = {10.1016/j.aei.2018.06.001},
	language = {en},
	urldate = {2021-01-02},
	journal = {Advanced Engineering Informatics},
	author = {Pučko, Zoran and Šuman, Nataša and Rebolj, Danijel},
	month = oct,
	year = {2018},
	pages = {27--40},
}

@article{honti_automated_2020,
	title = {Automated {Verification} of {Building} {Components} {Using} {BIM} {Models} and {Point} {Clouds}},
	volume = {28},
	issn = {1338-3973, 1210-3896},
	url = {https://content.sciendo.com/view/journals/sjce/28/3/article-p13.xml},
	doi = {10.2478/sjce-2020-0019},
	abstract = {Abstract One of the most important parts of construction work is the verification of the geometry of the parts of structures and buildings constructed. Today this procedure is often semi- or fully automated. The paper introduces an approach for the automated verification of parts of buildings, by comparing the design of a building (as-planned model), derived from a Building Information Model (BIM) in an Industry Foundation Classes (IFC) exchange format to a terrestrial laser scanning (TLS) point cloud (as-built model). The approach proposed has three main steps. The process begins with the acquisition of information from the as-planned model in the IFC exchange format; the second step is the automated (wall) plane segmentation from the point cloud. In the last step, the two models mentioned are compared to determine the deviations from the design, and the as-built wall flatness quantification is also executed. The potential of the proposed algorithm is shown in a case-study.},
	number = {3},
	urldate = {2020-12-23},
	journal = {Slovak Journal of Civil Engineering},
	author = {Honti, Richard and Erdélyi, Ján and Bariczová, Gabriela and Funtík, Tomáš and Mayer, Pavol},
	month = sep,
	year = {2020},
	pages = {13--19},
}

@article{comba_unsupervised_2018,
	title = {Unsupervised detection of vineyards by {3D} point-cloud {UAV} photogrammetry for precision agriculture},
	volume = {155},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169917315491},
	doi = {10.1016/j.compag.2018.10.005},
	abstract = {An effective management of precision viticulture processes relies on robust crop monitoring procedures and, in the near future, to autonomous machine for automatic site-specific crop managing. In this context, the exact detection of vineyards from 3D point-cloud maps, generated from unmanned aerial vehicles (UAV) multispectral imagery, will play a crucial role, e.g. both for achieve enhanced remotely sensed data and to manage path and operation of unmanned vehicles. In this paper, an innovative unsupervised algorithm for vineyard detection and vine-rows features evaluation, based on 3D point-cloud maps processing, is presented. The main results are the automatic detection of the vineyards and the local evaluation of vine rows orientation and of inter-rows spacing. The overall point-cloud processing algorithm can be divided into three mains steps: (1) precise local terrain surface and height evaluation of each point of the cloud, (2) point-cloud scouting and scoring procedure on the basis of a new vineyard likelihood measure, and, finally, (3) detection of vineyard areas and local features evaluation. The algorithm was found to be efficient and robust: reliable results were obtained even in the presence of dense inter-row grassing, many missing plants and steep terrain slopes. Performances of the algorithm were evaluated on vineyard maps at different phenological phase and growth stages. The effectiveness of the developed algorithm does not rely on the presence of rectilinear vine rows, being also able to detect vineyards with curvilinear vine row layouts.},
	language = {en},
	urldate = {2021-12-26},
	journal = {Computers and Electronics in Agriculture},
	author = {Comba, Lorenzo and Biglia, Alessandro and Ricauda Aimonino, Davide and Gay, Paolo},
	month = dec,
	year = {2018},
	keywords = {3D point-cloud modelling, Images processing, Precision viticulture, Remote sensing, UAV},
	pages = {84--95},
}

@inproceedings{comba_2d_2019,
	title = {{2D} and {3D} data fusion for crop monitoring in precision agriculture},
	doi = {10.1109/MetroAgriFor.2019.8909219},
	abstract = {Addressing the intrinsic variability within vineyards is a key factor to perform precision viticulture management. To this aim, new and more reliable methods for vineyard monitoring purposes must be defined. The introduction of Unmanned Aerial Vehicle (UAV) airborne sensors makes available a considerable amount of data with very high resolution, in terms of both spatial and temporal dimension. In this work, a data fusion approach for vigour characterization in vineyards is presented, which exploits the information provided by 2D multispectral aerial imagery, 3D point cloud crop models and aerial thermal imagery. A crucial phase of the procedure is the proper management of data provided by several sources, to achieve high consistency of the obtained huge dataset. The enhanced effectiveness of the proposed method to classify vines in different vigour classes exploiting multi source data was proved by an experimental campaign, considering 30 portions of vine rows, each made by 8 vines. Results showed that the error of the discriminant analysis using data fusion reach an improvement ranging from 67\% to 90\% with respect to a single data source, with a misclassification error rate of 3\%.},
	booktitle = {2019 {IEEE} {International} {Workshop} on {Metrology} for {Agriculture} and {Forestry} ({MetroAgriFor})},
	author = {Comba, Lorenzo and Biglia, Alessandro and Aimonino, Davide Ricauda and Barge, Paolo and Tortia, Cristina and Gay, Paolo},
	month = oct,
	year = {2019},
	keywords = {2D and 3D data fusion, Agriculture, Crop modelling, Data integration, Data models, Error analysis, Precision viticulture, Solid modeling, Three-dimensional displays, Two dimensional displays, UAV remote sensing},
	pages = {62--67},
}

@article{ribeiro-gomes_uncooled_2017,
	title = {Uncooled {Thermal} {Camera} {Calibration} and {Optimization} of the {Photogrammetry} {Process} for {UAV} {Applications} in {Agriculture}},
	volume = {17},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/17/10/2173},
	doi = {10.3390/s17102173},
	abstract = {The acquisition, processing, and interpretation of thermal images from unmanned aerial vehicles (UAVs) is becoming a useful source of information for agronomic applications because of the higher temporal and spatial resolution of these products compared with those obtained from satellites. However, due to the low load capacity of the UAV they need to mount light, uncooled thermal cameras, where the microbolometer is not stabilized to a constant temperature. This makes the camera precision low for many applications. Additionally, the low contrast of the thermal images makes the photogrammetry process inaccurate, which result in large errors in the generation of orthoimages. In this research, we propose the use of new calibration algorithms, based on neural networks, which consider the sensor temperature and the digital response of the microbolometer as input data. In addition, we evaluate the use of the Wallis filter for improving the quality of the photogrammetry process using structure from motion software. With the proposed calibration algorithm, the measurement accuracy increased from 3.55 °C with the original camera configuration to 1.37 °C. The implementation of the Wallis filter increases the number of tie-point from 58,000 to 110,000 and decreases the total positing error from 7.1 m to 1.3 m.},
	language = {en},
	number = {10},
	urldate = {2021-04-01},
	journal = {Sensors},
	author = {Ribeiro-Gomes, Krishna and Hernández-López, David and Ortega, José F. and Ballesteros, Rocío and Poblete, Tomás and Moreno, Miguel A.},
	month = oct,
	year = {2017},
	keywords = {image filtering, irrigation management, microbolometer, structure from motion, uncooled thermal camera calibration, unmanned aerial vehicle},
	pages = {2173},
}

@incollection{gang_framework_2020,
	address = {Cham},
	title = {A {Framework} for {BIM}-based {Quality} {Supervision} {Model} in {Project} {Management}},
	volume = {1002},
	isbn = {978-3-030-21254-4 978-3-030-21255-1},
	url = {http://link.springer.com/10.1007/978-3-030-21255-1_18},
	language = {en},
	urldate = {2020-12-28},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Management} {Science} and {Engineering} {Management}},
	publisher = {Springer International Publishing},
	author = {Gang, Jun and Feng, Chun and Shu, Wei},
	editor = {Xu, Jiuping and Ahmed, Syed Ejaz and Cooke, Fang Lee and Duca, Gheorghe},
	year = {2020},
	doi = {10.1007/978-3-030-21255-1_18},
	pages = {234--242},
}

@article{ham_phased_2020,
	title = {Phased {Reverse} {Engineering} {Framework} for {Sustainable} {Cultural} {Heritage} {Archives} {Using} {Laser} {Scanning} and {BIM}: {The} {Case} of the {Hwanggungwoo} ({Seoul}, {Korea})},
	volume = {12},
	issn = {2071-1050},
	shorttitle = {Phased {Reverse} {Engineering} {Framework} for {Sustainable} {Cultural} {Heritage} {Archives} {Using} {Laser} {Scanning} and {BIM}},
	url = {https://www.mdpi.com/2071-1050/12/19/8108},
	doi = {10.3390/su12198108},
	abstract = {This study proposed a phased reverse engineering framework to construct cultural heritage archives using laser scanning and a building information model (BIM). This framework includes acquisition of point cloud data through laser scanning. Unlike previous studies, in this study, a standard for authoring BIM data was established through comparative analysis of existing archives and point cloud data, and a method of building valuable BIM data as an information model was proposed. From a short-term perspective, additional archives such as member lists and drawings can be extracted from BIM data built as an information model. In addition, from a long-term perspective, a scenario for using the cultural heritage archive consisting of historical records, point cloud data, and BIM data was presented. These scenarios were verified through a case study. In particular, through the BIM data building and management method, relatively very light BIM data (499 MB) could be built based on point cloud data (more than 917 MB), which is a large amount of data.},
	language = {en},
	number = {19},
	urldate = {2020-12-23},
	journal = {Sustainability},
	author = {Ham, Namhyuk and Bae, Baek-Il and Yuh, Ok-Kyung},
	month = oct,
	year = {2020},
	pages = {8108},
}

@inproceedings{glavas_infrared_2017,
	title = {Infrared thermography in inspection of photovoltaic panels},
	doi = {10.1109/SST.2017.8188671},
	abstract = {Paper provides an overview of passive thermographic analysis of photovoltaic panels. Operation state of real photovoltaic system, power plant ETFOS 1, is described through detailed thermographic documentation. The importance of education needed for correct measurement and interpretation of thermodynamic state of photovoltaic (PV) modules has been emphasized. Influences that can change apparent temperature as reflection, transmission and emissivity of analyzed object are described. All parameters that influence on formation of thermographic records are analyzed. Problems in determining the actual temperature of PV module are illustrated with a scientific background. Through the analysis of 16 thermographs and their corresponding visual images, the state of the photovoltaic modules was presented. Three unexpected thermal patterns were observed and compared with the relevant literature.},
	booktitle = {2017 {International} {Conference} on {Smart} {Systems} and {Technologies} ({SST})},
	author = {Glavaš, Hrvoje and Vukobratović, Marko and Primorac, Mario and Muštran, Daniel},
	month = oct,
	year = {2017},
	keywords = {Cameras, Glass, IR Thermography, Photovoltaic systems, Reflection, Temperature, Temperature measurement, maintenance, photovoltaic},
	pages = {63--68},
}

@article{patraucean_state_2015,
	title = {State of research in automatic as-built modelling},
	volume = {29},
	issn = {14740346},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1474034615000026},
	doi = {10.1016/j.aei.2015.01.001},
	language = {en},
	number = {2},
	urldate = {2020-12-21},
	journal = {Advanced Engineering Informatics},
	author = {Pătrăucean, Viorica and Armeni, Iro and Nahangi, Mohammad and Yeung, Jamie and Brilakis, Ioannis and Haas, Carl},
	month = apr,
	year = {2015},
	pages = {162--171},
}

@article{tsouros_review_2019,
	title = {A {Review} on {UAV}-{Based} {Applications} for {Precision} {Agriculture}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2078-2489/10/11/349},
	doi = {10.3390/info10110349},
	abstract = {Emerging technologies such as Internet of Things (IoT) can provide significant potential in Smart Farming and Precision Agriculture applications, enabling the acquisition of real-time environmental data. IoT devices such as Unmanned Aerial Vehicles (UAVs) can be exploited in a variety of applications related to crops management, by capturing high spatial and temporal resolution images. These technologies are expected to revolutionize agriculture, enabling decision-making in days instead of weeks, promising significant reduction in cost and increase in the yield. Such decisions enable the effective application of farm inputs, supporting the four pillars of precision agriculture, i.e., apply the right practice, at the right place, at the right time and with the right quantity. However, the actual proliferation and exploitation of UAVs in Smart Farming has not been as robust as expected mainly due to the challenges confronted when selecting and deploying the relevant technologies, including the data acquisition and image processing methods. The main problem is that still there is no standardized workflow for the use of UAVs in such applications, as it is a relatively new area. In this article, we review the most recent applications of UAVs for Precision Agriculture. We discuss the most common applications, the types of UAVs exploited and then we focus on the data acquisition methods and technologies, appointing the benefits and drawbacks of each one. We also point out the most popular processing methods of aerial imagery and discuss the outcomes of each method and the potential applications of each one in the farming operations.},
	language = {en},
	number = {11},
	urldate = {2021-09-03},
	journal = {Information},
	author = {Tsouros, Dimosthenis C. and Bibi, Stamatia and Sarigiannidis, Panagiotis G.},
	month = nov,
	year = {2019},
	keywords = {IoT, Precision Agriculture, Smart Farming, UAS, UAV, Unmanned Aerial System, Unmanned Aerial Vehicle, image processing, remote sensing, review},
	pages = {349},
}

@article{lopez_framework_2021,
	title = {A framework for registering {UAV}-based imagery for crop-tracking in {Precision} {Agriculture}},
	volume = {97},
	issn = {0303-2434},
	url = {https://www.sciencedirect.com/science/article/pii/S030324342030917X},
	doi = {10.1016/j.jag.2020.102274},
	abstract = {Multiple types of images provide useful information about a crop, but image fusion is still a challenge in Precision Agriculture (PA). We describe a framework which manages a multi-layer registration model of heterogeneous images obtained by an unmanned aerial vehicle (UAV) by proposing pair-to-pair steps through a registration method invariant to intensity differences, allowing us to connect different aerial images with significant differences. Correction of deformed images is treated as a first step to end up with our registration algorithms. These methods conform the base of more advanced systems that combine 2D and spatial information, therefore it represents the link of several types of images. The evaluation shows the flexibility of our framework when dealing with different requirements. Effectiveness of the Enhanced Correlation Coefficient method is proved and thus shown as a suitable method for the registration of heterogeneous images.},
	language = {en},
	urldate = {2021-12-25},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {López, Alfonso and Jurado, Juan M. and Ogayar, Carlos J. and Feito, Francisco R.},
	month = may,
	year = {2021},
	keywords = {Image registration, Multispectral imagery, Thermal imagery, Tree recognition},
	pages = {102274},
}

@article{sledz_thermal_2018,
	title = {Thermal {IR} imaging: {Image} quality and orthophoto generation},
	volume = {42},
	copyright = {CC BY 4.0 Unported},
	shorttitle = {Thermal {IR} imaging},
	url = {https://www.repo.uni-hannover.de/handle/123456789/4116},
	doi = {10.15488/4082},
	abstract = {This paper deals with two aspects of photogrammetric processing of thermal images: image quality and 3D reconstruction quality. The first aspect of the paper relates to the influence of day light on Thermal InfraRed (TIR) images captured by an Unmanned Aerial Vehicle (UAV). Environmental factors such as ambient temperature and lack of sun light affect TIR image quality. We acquire image sequences of the same object during day and night and compare the generated orthophotos according to different metrics like contrast and signal-to-noise ratio (SNR). Our experiments show that performing TIR image acquisition during night time provides a better thermal contrast, regardless of whether we compute contrast over the whole image or over small patches. The second aspect investigated in this work is the potential of using TIR images for photogrammetric tasks such as the automatic generation of Digital Surface Models (DSM) and orthophotos. Due to the low geometrical resolution of a TIR camera and the low image quality in terms of contrast and noise compared to RGB images, the TIR DSM suffers from reconstruction errors and an orthophoto generated using the TIR DSM and TIR images is visibly influenced by those errors. We therefore include measurements of the UAVs positions during image capturing provided by a Global Navigation Satellite System (GNSS) receiver to retrieve position and orientation of TIR and RGB images in the same world coordinate system. To generate an orthophoto from TIR images, they are projected onto the DSM reconstructed from RGB images. This procedure leads to a TIR orthophoto of much higher quality in terms of geometrical correctness.},
	language = {eng},
	number = {1},
	urldate = {2021-09-03},
	journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives 42 (2018), Nr. 1},
	author = {Sledz, A. and Unger, J. and Heipke, C.},
	year = {2018},
	pages = {413--420},
}

@article{jurado_automatic_2020,
	title = {Automatic {Grapevine} {Trunk} {Detection} on {UAV}-{Based} {Point} {Cloud}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/18/3043},
	doi = {10.3390/rs12183043},
	abstract = {The optimisation of vineyards management requires efficient and automated methods able to identify individual plants. In the last few years, Unmanned Aerial Vehicles (UAVs) have become one of the main sources of remote sensing information for Precision Viticulture (PV) applications. In fact, high resolution UAV-based imagery offers a unique capability for modelling plant’s structure making possible the recognition of significant geometrical features in photogrammetric point clouds. Despite the proliferation of innovative technologies in viticulture, the identification of individual grapevines relies on image-based segmentation techniques. In that way, grapevine and non-grapevine features are separated and individual plants are estimated usually considering a fixed distance between them. In this study, an automatic method for grapevine trunk detection, using 3D point cloud data, is presented. The proposed method focuses on the recognition of key geometrical parameters to ensure the existence of every plant in the 3D model. The method was tested in different commercial vineyards and to push it to its limit a vineyard characterised by several missing plants along the vine rows, irregular distances between plants and occluded trunks by dense vegetation in some areas, was also used. The proposed method represents a disruption in relation to the state of the art, and is able to identify individual trunks, posts and missing plants based on the interpretation and analysis of a 3D point cloud. Moreover, a validation process was carried out allowing concluding that the method has a high performance, especially when it is applied to 3D point clouds generated in phases in which the leaves are not yet very dense (January to May). However, if correct flight parametrizations are set, the method remains effective throughout the entire vegetative cycle.},
	language = {en},
	number = {18},
	urldate = {2022-10-17},
	journal = {Remote Sensing},
	author = {Jurado, Juan M. and Pádua, Luís and Feito, Francisco R. and Sousa, Joaquim J.},
	month = jan,
	year = {2020},
	keywords = {3D vineyard structure, grapevine detection, photogrammetry, precision viticulture},
	pages = {3043},
}

@article{webster_three-dimensional_2018,
	title = {Three-dimensional thermal characterization of forest canopies using {UAV} photogrammetry},
	volume = {209},
	issn = {0034-4257},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425717304455},
	doi = {10.1016/j.rse.2017.09.033},
	abstract = {Measurements of vegetation structure have become a valuable tool for ecological research and environmental management. However, data describing the thermal 3D structure of canopies and how they vary both spatially and temporally remain sparse. Coincident RGB and thermal imagery from a UAV platform were collected of both a standalone tree and a relatively dense forest stand in the sub-alpine Eastern Swiss Alps. For the first time, SfM-MVS methods were used to develop 3D RGB and thermal point clouds of the two sites with point densities of 35,245 and 776 points per m2, respectively, compared to 78 points per m2 for an airborne LiDAR dataset of the same area. Despite the low resolution of the thermal imagery compared to RGB photosets, forest structural elements were accurately resolved in both point clouds. Improvements in the quality of the thermal 3D data were gained through the application of a distance filter based on the proximity of these data to the RGB 3D point data. Vertical temperature gradients of trees were negative with increasing height at the standalone tree, but were positive in the dense stand largely due to increased self-shading of incoming shortwave energy. Repeat surveys across a single morning during the snowmelt period revealed changes in the spatial distribution of canopy temperatures which are consistent with canopy warming from direct solar radiation. This is the first time that coincidentally acquired RGB and thermal imagery have been combined to generate separate RGB and thermal point clouds of 3D structures. These methods and findings demonstrate important implications for atmospheric, hydrological and ecological modeling, and have wide application for effective thermal measurements of remote environmental landscapes.},
	language = {en},
	urldate = {2021-09-03},
	journal = {Remote Sensing of Environment},
	author = {Webster, Clare and Westoby, Matthew and Rutter, Nick and Jonas, Tobias},
	month = may,
	year = {2018},
	keywords = {Canopy structure, Computer vision, Digital photogrammetry, Forest canopy temperature, Forest monitoring, Forest structure, Structure from motion, Temperature heterogeneity, Thermal infrared imagery, UAS, UAV, Unmanned aerial system, Unmanned aerial vehicle},
	pages = {835--847},
}

@article{grechi_3d_2021,
	title = {{3D} {Thermal} {Monitoring} of {Jointed} {Rock} {Masses} through {Infrared} {Thermography} and {Photogrammetry}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/13/5/957},
	doi = {10.3390/rs13050957},
	abstract = {The study of strain effects in thermally-forced rock masses has gathered growing interest from engineering geology researchers in the last decade. In this framework, digital photogrammetry and infrared thermography have become two of the most exploited remote surveying techniques in engineering geology applications because they can provide useful information concerning geomechanical and thermal conditions of these complex natural systems where the mechanical role of joints cannot be neglected. In this paper, a methodology is proposed for generating point clouds of rock masses prone to failure, combining the high geometric accuracy of RGB optical images and the thermal information derived by infrared thermography surveys. Multiple 3D thermal point clouds and a high-resolution RGB point cloud were separately generated and co-registered by acquiring thermograms at different times of the day and in different seasons using commercial software for Structure from Motion and point cloud analysis. Temperature attributes of thermal point clouds were merged with the reference high-resolution optical point cloud to obtain a composite 3D model storing accurate geometric information and multitemporal surface temperature distributions. The quality of merged point clouds was evaluated by comparing temperature distributions derived by 2D thermograms and 3D thermal models, with a view to estimating their accuracy in describing surface thermal fields. Moreover, a preliminary attempt was made to test the feasibility of this approach in investigating the thermal behavior of complex natural systems such as jointed rock masses by analyzing the spatial distribution and temporal evolution of surface temperature ranges under different climatic conditions. The obtained results show that despite the low resolution of the IR sensor, the geometric accuracy and the correspondence between 2D and 3D temperature measurements are high enough to consider 3D thermal point clouds suitable to describe surface temperature distributions and adequate for monitoring purposes of jointed rock mass.},
	language = {en},
	number = {5},
	urldate = {2021-09-03},
	journal = {Remote Sensing},
	author = {Grechi, Guglielmo and Fiorucci, Matteo and Marmoni, Gian Marco and Martino, Salvatore},
	month = jan,
	year = {2021},
	keywords = {infrared thermography, photogrammetry, rock mass, temperature},
	pages = {957},
}

@article{lee_developing_2019,
	title = {Developing {Inspection} {Methodology} of {Solar} {Energy} {Plants} by {Thermal} {Infrared} {Sensor} on {Board} {Unmanned} {Aerial} {Vehicles}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1996-1073/12/15/2928},
	doi = {10.3390/en12152928},
	abstract = {Photovoltaic (PV) power generation facilities have been built on various scales due to rapid growth in response to demand for renewable energy. Facilities built on diverse terrain and on such a scale are required to employ fast and accurate monitoring technology for stable electrical production and maintenance. The purpose of this study was to develop a technology to analyze the normal operation and failure of solar modules by acquiring images by attaching optical and thermal infrared sensors to unmanned aerial vehicles (UAVs) and producing orthographic images of temperature information. The results obtained in this study are as follows: (1) a method of using optical and thermal infrared sensors with different resolutions at the same time is able to produce accurate spatial information, (2) it is possible to produce orthographic images of thermal infrared images, (3) the analysis of the temperature fluctuation characteristics of the solar panel and cell showed that the abnormal module and cell displayed a larger temperature change than the normal module and cell, and (4) the abnormal heat generation of the panel and cell can be accurately discerned by the abnormal state panel and cell through the spatial distribution of the temperature. It is concluded that the inspection method of the solar module using the obtained UAV-based thermal infrared sensor can be useful for safety inspection and monitoring of the rapidly growing solar power generation facility.},
	language = {en},
	number = {15},
	urldate = {2021-05-09},
	journal = {Energies},
	author = {Lee, Dong Ho and Park, Jong Hwa},
	month = jan,
	year = {2019},
	keywords = {optical sensor, orthographic image, photovoltaic, temperature fluctuation, thermal infrared sensor, unmanned aerial vehicle},
	pages = {2928},
}

@article{bueno_4-plane_2018,
	title = {4-{Plane} congruent sets for automatic registration of as-is {3D} point clouds with {3D} {BIM} models},
	volume = {89},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0926580517301620},
	doi = {10.1016/j.autcon.2018.01.014},
	language = {en},
	urldate = {2021-01-02},
	journal = {Automation in Construction},
	author = {Bueno, Martín and Bosché, Frédéric and González-Jorge, Higinio and Martínez-Sánchez, Joaquín and Arias, Pedro},
	month = may,
	year = {2018},
	pages = {120--134},
}

@article{macher_point_2017,
	title = {From {Point} {Clouds} to {Building} {Information} {Models}: {3D} {Semi}-{Automatic} {Reconstruction} of {Indoors} of {Existing} {Buildings}},
	volume = {7},
	issn = {2076-3417},
	shorttitle = {From {Point} {Clouds} to {Building} {Information} {Models}},
	url = {http://www.mdpi.com/2076-3417/7/10/1030},
	doi = {10.3390/app7101030},
	language = {en},
	number = {10},
	urldate = {2020-12-31},
	journal = {Applied Sciences},
	author = {Macher, Hélène and Landes, Tania and Grussenmeyer, Pierre},
	month = oct,
	year = {2017},
	pages = {1030},
}

@article{thomson_mobile_2013,
	title = {Mobile {Laser} {Scanning} for {Indoor} {Modelling}},
	volume = {II-5/W2},
	issn = {2194-9050},
	url = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/II-5-W2/289/2013/},
	doi = {10.5194/isprsannals-II-5-W2-289-2013},
	language = {en},
	urldate = {2021-01-01},
	journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Thomson, C. and Apostolopoulos, G. and Backes, D. and Boehm, J.},
	month = oct,
	year = {2013},
	pages = {289--293},
}

@article{moyano_bringing_2020,
	title = {Bringing {BIM} to archaeological heritage: {Interdisciplinary} method/strategy and accuracy applied to a megalithic monument of the {Copper} {Age}},
	volume = {45},
	issn = {12962074},
	shorttitle = {Bringing {BIM} to archaeological heritage},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1296207420300467},
	doi = {10.1016/j.culher.2020.03.010},
	language = {en},
	urldate = {2020-12-23},
	journal = {Journal of Cultural Heritage},
	author = {Moyano, Juan and Odriozola, Carlos P. and Nieto-Julián, Juan E. and Vargas, Juan M. and Barrera, José A. and León, Javier},
	month = sep,
	year = {2020},
	pages = {303--314},
}

@article{martinez_automatic_2019,
	title = {Automatic {Selection} {Tool} of {Quality} {Control} {Specifications} for {Off}-site {Construction} {Manufacturing} {Products}: {A} {BIM}-based {Ontology} {Model} {Approach}},
	issn = {2562-5438},
	shorttitle = {Automatic {Selection} {Tool} of {Quality} {Control} {Specifications} for {Off}-site {Construction} {Manufacturing} {Products}},
	url = {https://www.journalofindustrializedconstruction.com/index.php/mocs/article/view/87},
	doi = {10.29173/mocs87},
	abstract = {Construction manufacturing specifications play an important role in assessing quality requirements on a construction project. However, working with these specifications can be overly complicated and error prone to the large amount of regulations and codes that need to be considered and their inter-dependencies. In building information modelling (BIM), the model is a digital representation of a complex construction product and contains precise product information data. The data is currently embedded into the model as properties for parametric building objects that are exchangeable among project operators. Some effort has been previously done to enhance the BIM model to obtain construction-oriented data and linking information that is crucial to manufacturing and quality control and assurance with BIM modelling still remains a challenge. This study proposes an extension to the current BIM-based product-oriented ontology model to include manufacturing processes and inspection, and quality control specifications. By automatically identifying which specifications are applicable to certain products and to extract the requirements imposed, this approach can support and enable automatic decision making in quality inspection and control tasks, which solely depend on information and knowledge from construction specifications. This approach is tested and validated using a light-gauge steel frame wall under Canadian construction standards and regulations.},
	urldate = {2020-12-28},
	journal = {Modular and Offsite Construction (MOC) Summit Proceedings},
	author = {Martinez, Pablo and Ahmad, Rafiq and Al-Hussein, Mohamed},
	month = may,
	year = {2019},
	pages = {141--148},
}

@article{warchol_concept_2019,
	title = {{THE} {CONCEPT} {OF} {LIDAR} {DATA} {QUALITY} {ASSESSMENT} {IN} {THE} {CONTEXT} {OF} {BIM} {MODELING}},
	volume = {XLII-1/W2},
	issn = {2194-9034},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-1-W2/61/2019/},
	doi = {10.5194/isprs-archives-XLII-1-W2-61-2019},
	abstract = {{\textbackslash}textlessp{\textbackslash}textgreater{\textbackslash}textlessstrong{\textbackslash}textgreaterAbstract.{\textbackslash}textless/strong{\textbackslash}textgreater LiDAR technology has revolutionized the area of 3D data acquisition. It is possible to obtain in a very fast and accurate way geometric and other information for a large area . Along with the development of design technology, LiDAR point clouds are often used to create inventory models of building objects and installations. This paper presents the possibilities of assessing LiDAR data for BIM modeling. The areas in which the assessment and description of obtained TLS data is important are presented. In addition to the attributes for assessing the quality of spatial data contained in the ISO 19157 standard, a density parameter was proposed. Examples of point clouds with different density for the architectural detail are presented in the final part of the work. For the attributes describing LiDAR data sets the levels of importance have been proposed for.{\textbackslash}textless/p{\textbackslash}textgreater},
	language = {en},
	urldate = {2020-12-28},
	journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Warchoł, A.},
	month = sep,
	year = {2019},
	pages = {61--66},
}

@article{bassier_topology_2020,
	title = {Topology {Reconstruction} of {BIM} {Wall} {Objects} from {Point} {Cloud} {Data}},
	volume = {12},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/11/1800},
	doi = {10.3390/rs12111800},
	abstract = {The processing of remote sensing measurements to Building Information Modeling (BIM) is a popular subject in current literature. An important step in the process is the enrichment of the geometry with the topology of the wall observations to create a logical model. However, this remains an unsolved task as methods struggle to deal with the noise, incompleteness and the complexity of point cloud data of building scenes. Current methods impose severe abstractions such as Manhattan-world assumptions and single-story procedures to overcome these obstacles, but as a result, a general data processing approach is still missing. In this paper, we propose a method that solves these shortcomings and creates a logical BIM model in an unsupervised manner. More specifically, we propose a connection evaluation framework that takes as input a set of preprocessed point clouds of a building’s wall observations and compute the best fit topology between them. We transcend the current state of the art by processing point clouds of both straight, curved and polyline-based walls. Also, we consider multiple connection types in a novel reasoning framework that decides which operations are best fit to reconstruct the topology of the walls. The geometry and topology produced by our method is directly usable by BIM processes as it is structured conform the IFC data structure. The experimental results conducted on the Stanford 2D-3D-Semantics dataset (2D-3D-S) show that the proposed method is a promising framework to reconstruct complex multi-story wall elements in an unsupervised manner.},
	language = {en},
	number = {11},
	urldate = {2020-12-23},
	journal = {Remote Sensing},
	author = {Bassier, Maarten and Vergauwen, Maarten},
	month = jun,
	year = {2020},
	pages = {1800},
}

@article{park_new_2007,
	title = {A {New} {Approach} for {Health} {Monitoring} of {Structures}: {Terrestrial} {Laser} {Scanning}},
	volume = {22},
	issn = {1093-9687, 1467-8667},
	shorttitle = {A {New} {Approach} for {Health} {Monitoring} of {Structures}},
	url = {http://doi.wiley.com/10.1111/j.1467-8667.2006.00466.x},
	doi = {10.1111/j.1467-8667.2006.00466.x},
	language = {en},
	number = {1},
	urldate = {2021-01-09},
	journal = {Computer-Aided Civil and Infrastructure Engineering},
	author = {Park, H. S. and Lee, H. M. and Adeli, Hojjat and Lee, I.},
	month = jan,
	year = {2007},
	pages = {19--30},
}

@article{shah_airsim_2017,
	title = {{AirSim}: {High}-{Fidelity} {Visual} and {Physical} {Simulation} for {Autonomous} {Vehicles}},
	shorttitle = {{AirSim}},
	url = {http://arxiv.org/abs/1705.05065},
	abstract = {Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.},
	urldate = {2022-02-08},
	journal = {arXiv:1705.05065 [cs]},
	author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
	month = jul,
	year = {2017},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@article{khanal_overview_2017,
	title = {An overview of current and potential applications of thermal remote sensing in precision agriculture},
	volume = {139},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169916310225},
	doi = {10.1016/j.compag.2017.05.001},
	abstract = {Precision agriculture (PA) utilizes tools and technologies to identify in-field soil and crop variability for improving farming practices and optimizing agronomic inputs. Traditionally, optical remote sensing (RS) that utilizes visible light and infrared regions of the electromagnetic spectrum has been used as an integral part of PA for crop and soil monitoring. Optical RS, however, is slow in differentiating stress levels in crops until visual symptoms become noticeable. Surface temperature is considered to be a rapid response variable that can indicate crop stresses prior to their visual symptoms. By measuring estimates of surface temperature, thermal RS has been found to be a promising tool for PA. Compared to optical RS, applications of thermal RS for PA have been limited. Until recently (i.e., before the advancement of low cost RS platforms such as unmanned aerial systems (UAVs)), the availability of high resolution thermal images was limited due to high acquisition costs. Given recent developments in UAVs, thermal images with high spatial and temporal resolutions have become available at a low cost, which has increased opportunities to understand in-field variability of crop and soil conditions useful for various agronomic decision-making. Before thermal RS is adopted as a routine tool for crop and environmental monitoring, there is a need to understand its current and potential applications as well as issues and concerns. This review focuses on current and potential applications of thermal RS in PA as well as some concerns relating to its application. The application areas of thermal RS in agriculture discussed here include irrigation scheduling, drought monitoring, crop disease detection, and mapping of soil properties, residues and tillage, field tiles, and crop maturity and yield. Some of the issues related to its application include spatial and temporal resolution, atmospheric conditions, and crop growth stages.},
	language = {en},
	urldate = {2021-04-01},
	journal = {Computers and Electronics in Agriculture},
	author = {Khanal, Sami and Fulton, John and Shearer, Scott},
	month = jun,
	year = {2017},
	keywords = {Monitoring, Precision agriculture, Remote sensing, Thermal sensing},
	pages = {22--32},
}

@inproceedings{addabbo_uav_2017,
	title = {A {UAV} infrared measurement approach for defect detection in photovoltaic plants},
	doi = {10.1109/MetroAeroSpace.2017.7999594},
	abstract = {In the last two decades, the increased production and installation of photovoltaic (PV) plants worldwide has asked for efficient low-cost methods for PV plant inspection to monitor their functionality and guaranteed their performance. To lower maintenance costs new systems have been thought to substitute human workers inspecting the PV plants. The employment of Unmanned Aerial Vehicles (UAVs) has allowed realizing a fast detection of defects and problems arisen in PV plants thanks to the fusion of computer vision algorithms and high accuracy Global Navigation Satellite System (GNSS) positioning techniques able to detect and tag anomalies and identify the defective panels. Authors in this paper intend to present the state-of-the-art in the Computer Vision field applied to PV plant inspection and to thermal anomalies detection over the panels. In addition, different data sets have been recorded and compared for geo-referencing the solar panels. They have been derived through the U-blox NEO-M8N installed on board of the UAV used for inspection. Although the U-blox NEO-M8N measures are less accurate than the classic RTK GNSS ones, the measurements obtained with this handset introduce a very interesting novelty since initial services of the Galileo constellation, supported by the NEO-M8N GNSS module, have become available only since last December. Future testing and validation will be performed by using geo-referenced data from the RTK GNSS receiver, that has been ordered with a specially customized antenna whose specifications have been properly designed and sent to the manufacturer for its fabrication. Next campaigns will allow to get results also from this RTK receiver and to properly validate the proposed algorithm, by comparing new results with those found through the employment of U-blox receiver.},
	booktitle = {2017 {IEEE} {International} {Workshop} on {Metrology} for {AeroSpace} ({MetroAeroSpace})},
	author = {Addabbo, Pia and Angrisano, Antonio and Bernardi, Mario Luca and Gagliarde, Graziano and Mennella, Alberto and Nisi, Marco and Ullo, Silvia},
	month = jun,
	year = {2017},
	keywords = {Cameras, Computer vision, GNSS receivers, Infrared (IR) measurements, Inspection, Monitoring, Payloads, Receivers, Unmanned Aerial Vehicles (UAVs), Unmanned aerial vehicles, geo-referencing, image processing, photovoltaic (PV) plant inspection},
	pages = {345--350},
}

@article{lanz_infrared_2015,
	title = {Infrared ({IR}) {Drone} for {Quick} and {Cheap} {PV} {Inspection}},
	url = {http://www.eupvsec-proceedings.com/proceedings?paper=33162},
	doi = {10.4229/EUPVSEC20152015-5CO.15.6},
	abstract = {OPERATIONS, PERFORMANCE AND RELIABILITY OF PHOTOVOLTAICS (FROM CELLS TO SYSTEMS), Operation of PV Systems and Plants, Infrared (IR) Drone for Quick and Cheap PV Inspection},
	language = {en},
	urldate = {2021-05-09},
	journal = {31st European Photovoltaic Solar Energy Conference and Exhibition},
	author = {Lanz, M. and Schüpbach, E. and Muntwyler, U.},
	month = nov,
	year = {2015},
	pages = {1804--1806},
}

@article{javadnejad_photogrammetric_2020,
	title = {A photogrammetric approach to fusing natural colour and thermal infrared {UAS} imagery in {3D} point cloud generation},
	volume = {41},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431161.2019.1641241},
	doi = {10.1080/01431161.2019.1641241},
	abstract = {The inclusion of thermal infrared (TIR) data in point clouds derived from unmanned aircraft system (UAS) imagery can benefit a variety of applications in which surface temperature and 3D geometry are both important discriminants of feature type and condition. Low resolution and narrow fields of view (FOV) of current consumer-grade TIR cameras on UAS, combined with the lack of sharpness and texture in many image regions, may cause failure or poor results from structure from motion (SfM) photogrammetric software, which has gained widespread use for generating point clouds from UAS imagery. This paper proposes a photogrammetric approach for generating 3D multispectral point clouds utilizing coacquired TIR-RGB images. A 3D point cloud is first generated from the RGB imagery using standard SfM procedures. Then the TIR attributes are assigned to points, where the image coordinates of the points in TIR images are estimated using transformation parameters obtained from co-registration procedures. To obtain RGB-to-TIR transformation parameters, this study tests 3D and 2D co-registration approaches. The latter produces better results due to the challenge of calibrating the TIR camera as required for the 3D approach. This proposed approach is advantageous for generating TIR point clouds without loss of photogrammetric precision compared with solely TIR-based SfM, as the 3D accuracy, point density, and reliability are greatly enhanced.},
	number = {1},
	urldate = {2022-09-01},
	journal = {International Journal of Remote Sensing},
	author = {Javadnejad, Farid and Gillins, Daniel T. and Parrish, Christopher E. and Slocum, Richard K.},
	month = jan,
	year = {2020},
	pages = {211--237},
}

@article{tyutyundzhiev_advanced_2015,
	title = {Advanced {PV} modules inspection using multirotor {UAV}},
	abstract = {Large-scale PV fields are a complicated target for monitoring and performance evaluation. In this paper, field experience with multirotor UAV has been reported. A X-frame quadcopter equipped with various VIS and IR cameras is used to detect defects responsible for performance degradation. Usually in laboratory environment, hi-res VIS photos, EL photos and far- IR photos of single module are used for diagnostic purposes. However, on-field PV plant testing procedures need faster methods for batch processing. Our investigations confirm that new robotics and avionics equipment for close-range flights over PV modules combined with cameras can reveal hidden problems of materials quality and device reliability. Various new tasks, as high resolution aerial photography, 3D photogrammetry, 3D field and building reconstruction can be involved as testing methods for performance optimization of photovoltaic installations.},
	language = {en},
	author = {Tyutyundzhiev, N and Lovchinov, K and Martínez-Moreno, F and Leloux, J and Narvarte, L},
	year = {2015},
	pages = {6},
}

@article{guarnieri_monitoring_2013,
	title = {Monitoring {Of} {Complex} {Structure} {For} {Structural} {Control} {Using} {Terrestrial} {Laser} {Scanning} ({Tls}) {And} {Photogrammetry}},
	volume = {7},
	issn = {1558-3058, 1558-3066},
	url = {http://www.tandfonline.com/doi/abs/10.1080/15583058.2011.606595},
	doi = {10.1080/15583058.2011.606595},
	language = {en},
	number = {1},
	urldate = {2021-01-09},
	journal = {International Journal of Architectural Heritage},
	author = {Guarnieri, Alberto and Milan, Nicola and Vettore, Antonio},
	month = jan,
	year = {2013},
	pages = {54--67},
}

@article{zarco-tejada_previsual_2018,
	title = {Previsual symptoms of {Xylella} fastidiosa infection revealed in spectral plant-trait alterations},
	volume = {4},
	copyright = {2018 The Author(s)},
	issn = {2055-0278},
	url = {https://www.nature.com/articles/s41477-018-0189-7},
	doi = {10.1038/s41477-018-0189-7},
	abstract = {Plant pathogens cause significant losses to agricultural yields and increasingly threaten food security1, ecosystem integrity and societies in general2–5. Xylella fastidiosa is one of the most dangerous plant bacteria worldwide, causing several diseases with profound impacts on agriculture and the environment6. Primarily occurring in the Americas, its recent discovery in Asia and Europe demonstrates that X. fastidiosa’s geographic range has broadened considerably, positioning it as a reemerging global threat that has caused socioeconomic and cultural damage7,8. X. fastidiosa can infect more than 350 plant species worldwide9, and early detection is critical for its eradication8. In this article, we show that changes in plant functional traits retrieved from airborne imaging spectroscopy and thermography can reveal X. fastidiosa infection in olive trees before symptoms are visible. We obtained accuracies of disease detection, confirmed by quantitative polymerase chain reaction, exceeding 80\% when high-resolution fluorescence quantified by three-dimensional simulations and thermal stress indicators were coupled with photosynthetic traits sensitive to rapid pigment dynamics and degradation. Moreover, we found that the visually asymptomatic trees originally scored as affected by spectral plant-trait alterations, developed X. fastidiosa symptoms at almost double the rate of the asymptomatic trees classified as not affected by remote sensing. We demonstrate that spectral plant-trait alterations caused by X. fastidiosa infection are detectable previsually at the landscape scale, a critical requirement to help eradicate some of the most devastating plant diseases worldwide.},
	language = {en},
	number = {7},
	urldate = {2021-09-03},
	journal = {Nature Plants},
	author = {Zarco-Tejada, P. J. and Camino, C. and Beck, P. S. A. and Calderon, R. and Hornero, A. and Hernández-Clemente, R. and Kattenborn, T. and Montes-Borrego, M. and Susca, L. and Morelli, M. and Gonzalez-Dugo, V. and North, P. R. J. and Landa, B. B. and Boscia, D. and Saponari, M. and Navas-Cortes, J. A.},
	month = jul,
	year = {2018},
	keywords = {Fluorescence, Imaging, Olea, Plant Diseases, Satellite Imagery, Spectrum Analysis, Thermography, Three-Dimensional, Xylella},
	pages = {432--439},
}

@article{jurado_semantic_2020,
	title = {Semantic {Segmentation} of {Natural} {Materials} on a {Point} {Cloud} {Using} {Spatial} and {Multispectral} {Features}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/20/8/2244},
	doi = {10.3390/s20082244},
	abstract = {The characterization of natural spaces by the precise observation of their material properties is highly demanded in remote sensing and computer vision. The production of novel sensors enables the collection of heterogeneous data to get a comprehensive knowledge of the living and non-living entities in the ecosystem. The high resolution of consumer-grade RGB cameras is frequently used for the geometric reconstruction of many types of environments. Nevertheless, the understanding of natural spaces is still challenging. The automatic segmentation of homogeneous materials in nature is a complex task because there are many overlapping structures and an indirect illumination, so the object recognition is difficult. In this paper, we propose a method based on fusing spatial and multispectral characteristics for the unsupervised classification of natural materials in a point cloud. A high-resolution camera and a multispectral sensor are mounted on a custom camera rig in order to simultaneously capture RGB and multispectral images. Our method is tested in a controlled scenario, where different natural objects coexist. Initially, the input RGB images are processed to generate a point cloud by applying the structure-from-motion (SfM) algorithm. Then, the multispectral images are mapped on the three-dimensional model to characterize the geometry with the reflectance captured from four narrow bands (green, red, red-edge and near-infrared). The reflectance, the visible colour and the spatial component are combined to extract key differences among all existing materials. For this purpose, a hierarchical cluster analysis is applied to pool the point cloud and identify the feature pattern for every material. As a result, the tree trunk, the leaves, different species of low plants, the ground and rocks can be clearly recognized in the scene. These results demonstrate the feasibility to perform a semantic segmentation by considering multispectral and spatial features with an unknown number of clusters to be detected on the point cloud. Moreover, our solution is compared to other method based on supervised learning in order to test the improvement of the proposed approach.},
	language = {en},
	number = {8},
	urldate = {2021-10-20},
	journal = {Sensors},
	author = {Jurado, J. M. and Cárdenas, J. L. and Ogayar, C. J. and Ortega, L. and Feito, F. R.},
	month = jan,
	year = {2020},
	keywords = {heterogeneous data fusion, material-based recognition, multispectral imaging, point cloud segmentation},
	pages = {2244},
}

@article{osroosh_economical_2018,
	title = {Economical thermal-{RGB} imaging system for monitoring agricultural crops},
	volume = {147},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169917310360},
	doi = {10.1016/j.compag.2018.02.018},
	abstract = {In this study, a low-cost thermal-RGB imager was developed for use in agricultural crop monitoring applications. It is weatherproof, and has a geo-referencing capability along with a power management panel that allows unattended field deployment of the systems for crop monitoring over extended period of time. The imager is made up of single-board Linux-based computer integrated with RGB and thermal imaging modules. The imager can be configured as FTP server to allow data transfer to/from a client computer. Developed was also the custom image-processing algorithm which overlays, aligns thermal and RGB images, and mask for the thermal image to remove the soil background and shaded leaves. The algorithm outputs are the average temperature of sunlit leaves and canopy coverage. Prior to field validation, the performance of ten thermal modules and four fully assembled RGB-thermal imagers were assessed under laboratory conditions. In the spring of 2017, two imagers were mounted on a center pivot retrofitted with Medium Elevation Spray Application (MESA) and Low Elevation Spray Application (LESA) systems in a mint field near Toppenish, WA. The thermal modules showed an accuracy of ±2.4 °C on average over a range of 0–50 °C of a blackbody target. Although accurate for larger canopies, the imperfect alignment of RGB and thermal images introduced significant errors in the calculations of sunlit leaves surface temperature in images with small canopy coverage. Further investigations revealed that the first peak of thermal image relative frequency histogram could be a more accurate representative of sunlit leaf surface temperature. Overall, the amended image-processing algorithm was able to successfully extract canopy surface temperature and percent canopy cover from a wide range of images captured during the crop growing season. The current design of imager allows for creating a network of imaging units in the field to obtain real-time surface temperature data from plant canopies. The system has the potential to be used for creating evapotranspiration and prescription maps in real-time, and irrigation scheduling.},
	language = {en},
	urldate = {2021-04-01},
	journal = {Computers and Electronics in Agriculture},
	author = {Osroosh, Yasin and Khot, Lav R. and Peters, R. Troy},
	month = apr,
	year = {2018},
	keywords = {Center pivot, Crop water status, Image processing algorithm, Irrigated crops, Thermal-RGB imaging},
	pages = {34--43},
}

@article{gade_thermal_2014,
	title = {Thermal cameras and applications: a survey},
	volume = {25},
	issn = {1432-1769},
	shorttitle = {Thermal cameras and applications},
	url = {https://doi.org/10.1007/s00138-013-0570-5},
	doi = {10.1007/s00138-013-0570-5},
	abstract = {Thermal cameras are passive sensors that capture the infrared radiation emitted by all objects with a temperature above absolute zero. This type of camera was originally developed as a surveillance and night vision tool for the military, but recently the price has dropped, significantly opening up a broader field of applications. Deploying this type of sensor in vision systems eliminates the illumination problems of normal greyscale and RGB cameras. This survey provides an overview of the current applications of thermal cameras. Applications include animals, agriculture, buildings, gas detection, industrial, and military applications, as well as detection, tracking, and recognition of humans. Moreover, this survey describes the nature of thermal radiation and the technology of thermal cameras.},
	language = {en},
	number = {1},
	urldate = {2021-09-03},
	journal = {Machine Vision and Applications},
	author = {Gade, Rikke and Moeslund, Thomas B.},
	month = jan,
	year = {2014},
	pages = {245--262},
}

@misc{xiao_transfer_2021,
	title = {Transfer {Learning} from {Synthetic} to {Real} {LiDAR} {Point} {Cloud} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2107.05399},
	abstract = {Knowledge transfer from synthetic to real data has been widely studied to mitigate data annotation constraints in various computer vision tasks such as semantic segmentation. However, the study focused on 2D images and its counterpart in 3D point clouds segmentation lags far behind due to the lack of large-scale synthetic datasets and effective transfer methods. We address this issue by collecting SynLiDAR, a large-scale synthetic LiDAR dataset that contains point-wise annotated point clouds with accurate geometric shapes and comprehensive semantic classes. SynLiDAR was collected from multiple virtual environments with rich scenes and layouts which consists of over 19 billion points of 32 semantic classes. In addition, we design PCT, a novel point cloud translator that effectively mitigates the gap between synthetic and real point clouds. Specifically, we decompose the synthetic-to-real gap into an appearance component and a sparsity component and handle them separately which improves the point cloud translation greatly. We conducted extensive experiments over three transfer learning setups including data augmentation, semi-supervised domain adaptation and unsupervised domain adaptation. Extensive experiments show that SynLiDAR provides a high-quality data source for studying 3D transfer and the proposed PCT achieves superior point cloud translation consistently across the three setups. SynLiDAR project page: {\textbackslash}textbackslashurl\{https://github.com/xiaoaoran/SynLiDAR\}},
	urldate = {2022-09-25},
	publisher = {arXiv},
	author = {Xiao, Aoran and Huang, Jiaxing and Guan, Dayan and Zhan, Fangneng and Lu, Shijian},
	month = dec,
	year = {2021},
	doi = {10.48550/arXiv.2107.05399},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{aguero_inside_2015,
	title = {Inside the {Virtual} {Robotics} {Challenge}: {Simulating} {Real}-{Time} {Robotic} {Disaster} {Response}},
	volume = {12},
	issn = {1558-3783},
	shorttitle = {Inside the {Virtual} {Robotics} {Challenge}},
	doi = {10.1109/TASE.2014.2368997},
	abstract = {This paper presents the software framework established to facilitate cloud-hosted robot simulation. The framework addresses the challenges associated with conducting a task-oriented and real-time robot competition, the Defense Advanced Research Projects Agency (DARPA) Virtual Robotics Challenge (VRC), designed to mimic reality. The core of the framework is the Gazebo simulator, a platform to simulate robots, objects, and environments, as well as the enhancements made for the VRC to maintain a high fidelity simulation using a high degree of freedom and multisensor robot. The other major component used is the CloudSim tool, designed to enhance the automation of robotics simulation using existing cloud technologies. The results from the VRC and a discussion are also detailed in this work. Note to Practitioners - Advances in robot simulation, cloud hosted infrastructure, and web technology have made it possible to accurately and efficiently simulate complex robots and environments on remote servers while providing realistic data streams for human-in-the-loop robot control. This paper presents the software and hardware frameworks established to facilitate cloud-hosted robot simulation, and addresses the challenges associated with conducting a task-oriented robot competition designed to mimic reality. The competition that spurred this innovation was the VRC, a precursor to the DARPA Robotics Challenge, in which teams from around the world utilized custom human-robot interfaces and control code to solve disaster response-related tasks in simulation. Winners of the VRC received both funding and access to Atlas, a humanoid robot developed by Boston Dynamics. The Gazebo simulator, an open source and high fidelity robot simulator, was improved upon to met the needs of the VRC competition. Additionally, CloudSim was created to act as an interface between users and the cloud-hosted simulations. As a result of this work, we have achieved automated deployment of cloud resources for robotic simulations, near real-time simulation performance, and simulation accuracy that closely mimics real hardware. These tools have been released under open source licenses and are freely available, and can be used to help reduce robot and algorithm design and development time, and increase robot software robustness.},
	number = {2},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Agüero, Carlos E. and Koenig, Nate and Chen, Ian and Boyer, Hugo and Peters, Steven and Hsu, John and Gerkey, Brian and Paepcke, Steffi and Rivero, Jose L. and Manzo, Justin and Krotkov, Eric and Pratt, Gill},
	month = apr,
	year = {2015},
	keywords = {Cloud robotics, Computational modeling, Computer architecture, Computers, Real-time systems, Robot sensing systems, Servers, real-time robot simulation, robotic disaster response},
	pages = {494--506},
}

@article{fekete_integration_2013,
	title = {Integration of three-dimensional laser scanning with discontinuum modelling for stability analysis of tunnels in blocky rockmasses},
	volume = {57},
	issn = {13651609},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1365160912001748},
	doi = {10.1016/j.ijrmms.2012.08.003},
	language = {en},
	urldate = {2021-01-09},
	journal = {International Journal of Rock Mechanics and Mining Sciences},
	author = {Fekete, Stephanie and Diederichs, Mark},
	month = jan,
	year = {2013},
	pages = {11--23},
}

@article{rahimian_cost-effective_2020,
	title = {Cost-{Effective} as-{Built} {BIM} {Modelling} {Using} {3D} {Point}- {Clouds} and {Photogrammetry}},
	volume = {4},
	issn = {26436876},
	url = {https://irispublishers.com/ctcse/fulltext/cost-effective-as-built-bim-modelling-using-3d-point-clouds-and-photogrammetry.ID.000599.php},
	doi = {10.33552/CTCSE.2020.04.000599},
	number = {5},
	urldate = {2020-12-28},
	journal = {Current Trends in Civil \& Structural Engineering},
	author = {Rahimian, Farzad Pour},
	year = {2020},
}

@article{zhang_detection_2017,
	title = {Detection and location of fouling on photovoltaic panels using a drone-mounted infrared thermography system},
	volume = {11},
	issn = {1931-3195, 1931-3195},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-applied-remote-sensing/volume-11/issue-1/016026/Detection-and-location-of-fouling-on-photovoltaic-panels-using-a/10.1117/1.JRS.11.016026.short},
	doi = {10.1117/1.JRS.11.016026},
	abstract = {Due to weathering and external forces, solar panels are subject to fouling and defects after a certain amount of time in service. These fouling and defects have direct adverse consequences such as low-power efficiency. Because solar power plants usually have large-scale photovoltaic (PV) panels, fast detection and location of fouling and defects across large PV areas are imperative. A drone-mounted infrared thermography system was designed and developed, and its ability to detect rapid fouling on large-scale PV panel systems was investigated. The infrared images were preprocessed using the K neighbor mean filter, and the single PV module on each image was recognized and extracted. Combining the local and global detection method, suspicious sites were located precisely. The results showed the flexible drone-mounted infrared thermography system to have a strong ability to detect the presence and determine the position of PV fouling. Drone-mounted infrared thermography also has good technical feasibility and practical value in the detection of PV fouling detection.},
	number = {1},
	urldate = {2021-05-09},
	journal = {Journal of Applied Remote Sensing},
	author = {Zhang, Peng and Zhang, Lifu and Wu, Taixia and Zhang, Hongming and Sun, Xuejian},
	month = feb,
	year = {2017},
	pages = {016026},
}

@inproceedings{jiang_hybrid_2020,
	address = {Tempe, Arizona},
	title = {A {Hybrid} {Approach} to {Semi}-{Automatic} {Conversion} of {3D} {Point} {Cloud} {Data} to {Building} {Information} {Model}},
	isbn = {978-0-7844-8285-8},
	url = {http://ascelibrary.org/doi/10.1061/9780784482858.045},
	doi = {10.1061/9780784482858.045},
	language = {en},
	urldate = {2020-12-23},
	booktitle = {Construction {Research} {Congress} 2020},
	publisher = {American Society of Civil Engineers},
	author = {Jiang, Jianan and Shen, Xuesong and Shi, Jianyong and Jiang, Ziang},
	month = nov,
	year = {2020},
	pages = {408--417},
}

@article{pour_rahimian_openbim-tango_2019,
	title = {{OpenBIM}-{Tango} integrated virtual showroom for offsite manufactured production of self-build housing},
	volume = {102},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0926580518302826},
	doi = {10.1016/j.autcon.2019.02.009},
	language = {en},
	urldate = {2021-02-18},
	journal = {Automation in Construction},
	author = {Pour Rahimian, Farzad and Chavdarova, Veselina and Oliver, Stephen and Chamo, Farhad and Potseluyko Amobi, Lilia},
	month = jun,
	year = {2019},
	pages = {1--16},
}

@article{weixing_extraction_2020,
	title = {Extraction of tunnel center line and cross-sections on fractional calculus, {3D} invariant moments and best-fit ellipse},
	volume = {128},
	issn = {00303992},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0030399219321371},
	doi = {10.1016/j.optlastec.2020.106220},
	language = {en},
	urldate = {2021-01-02},
	journal = {Optics \& Laser Technology},
	author = {Weixing, Wang and Weiwei, Chen and Kevin, Wang and Shuang, Li},
	month = aug,
	year = {2020},
	pages = {106220},
}

@techreport{zhang_tunnel_2020,
	type = {preprint},
	title = {Tunnel {Point} {Cloud} and {BIM} {Model} {Integration} for {Cross}-section {Monitoring}},
	url = {https://www.preprints.org/manuscript/202011.0537/v1},
	abstract = {This paper introduces a method for tunnel point cloud and BIM model integration and cross-section monitoring, providing information to analyse tunnel cross-sections and surrounding rock deformation, and support for tunnel maintenance and reconstruction. Three types of data are processed for the integration: laser scanning point cloud, BIM tunnel model and terrain model from oblique photogrammetry. An adaptive BIM modelling scheme is proposed for tunnels with alien structures. Precise spatial registration of the data sets is conducted by applying singular value decomposition (SVD) algorithm to calculate transformation parameters from the point cloud model to the BIM model. Since the tunnel central line has high-order derivability, a cross-section calculation method based on tangent vector is proposed to obtain the cross-sectional profile of tunnels at any mileage. The proposed method has been verified by applying it to a tunnel reconstruction project. The experiment results show that the tunnel point cloud and the BIM model were highly coincident after the integration. The developed program can effectively get the cross-section of the tunnel at any mileage, and correctly express the spatial relationship between the BIM tunnel, the point cloud of tunnel and the external mountainous terrain.},
	urldate = {2020-12-23},
	institution = {ENGINEERING},
	author = {Zhang, Wensheng and Hao, Ziqi and Guo, Dong and Gao, Yingkai and Wang, Jack Jianguo},
	month = nov,
	year = {2020},
	doi = {10.20944/preprints202011.0537.v1},
}

@article{olsen_terrestrial_2010,
	title = {Terrestrial {Laser} {Scanning}-{Based} {Structural} {Damage} {Assessment}},
	volume = {24},
	issn = {0887-3801, 1943-5487},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%29CP.1943-5487.0000028},
	doi = {10.1061/(ASCE)CP.1943-5487.0000028},
	language = {en},
	number = {3},
	urldate = {2021-01-09},
	journal = {Journal of Computing in Civil Engineering},
	author = {Olsen, Michael J. and Kuester, Falko and Chang, Barbara J. and Hutchinson, Tara C.},
	month = may,
	year = {2010},
	pages = {264--272},
}

@article{golparvar-fard_evaluation_2011,
	title = {Evaluation of image-based modeling and laser scanning accuracy for emerging automated performance monitoring techniques},
	volume = {20},
	issn = {09265805},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0926580511000707},
	doi = {10.1016/j.autcon.2011.04.016},
	language = {en},
	number = {8},
	urldate = {2021-01-09},
	journal = {Automation in Construction},
	author = {Golparvar-Fard, Mani and Bohn, Jeffrey and Teizer, Jochen and Savarese, Silvio and Peña-Mora, Feniosky},
	month = dec,
	year = {2011},
	pages = {1143--1155},
}

@article{antova_creation_2020,
	title = {Creation of {3D} {Geometry} in {Scan}-to-{CAD}/{BIM} {Environment}},
	volume = {609},
	issn = {1755-1315},
	url = {https://iopscience.iop.org/article/10.1088/1755-1315/609/1/012085},
	doi = {10.1088/1755-1315/609/1/012085},
	urldate = {2020-12-23},
	journal = {IOP Conference Series: Earth and Environmental Science},
	author = {Antova, Gergana and Tanev, Vatio},
	month = dec,
	year = {2020},
	pages = {012085},
}

@article{rocha_scan--bim_2020,
	title = {A {Scan}-to-{BIM} {Methodology} {Applied} to {Heritage} {Buildings}},
	volume = {3},
	issn = {2571-9408},
	url = {https://www.mdpi.com/2571-9408/3/1/4},
	doi = {10.3390/heritage3010004},
	abstract = {Heritage buildings usually have complex (non-parametric) geometries that turn their digitization through conventional methods in inaccurate and time-consuming processes. When it comes to the survey and representation of historical assets, remote sensing technologies have been playing key roles in the last few years: 3D laser scanning and photogrammetry surveys save time in the field, while proving to be extremely accurate at registering non-regular geometries of buildings. However, the efficient transformation of remote-sensing data into as-built parametric smart models is currently an unsolved challenge. A pragmatic and organized Historic Building Information Modeling (HBIM) methodology is essential in order to obtain a consistent model that can bring benefits and integrate conservation and restoration work. This article addresses the creation of an HBIM model of heritage assets using 3D laser scanning and photogrammetry. Our findings are illustrated in one case study: The Engine House Paços Reais in Lisbon. The paper first describes how and what measures should be taken to plan a careful scan-to-HBIM process. Second, the description of the remote-sensing survey campaign is conducted accordingly and is aimed at a BIM output, including the process of data alignment, cleaning, and merging. Finally, the HBIM modeling phase is described, based on point cloud data.},
	language = {en},
	number = {1},
	urldate = {2020-12-23},
	journal = {Heritage},
	author = {{Rocha} and {Mateus} and {Fernández} and {Ferreira}},
	month = feb,
	year = {2020},
	pages = {47--67},
}

@article{banfi_integration_2019,
	title = {The integration of a scan-to-{HBIM} process in {BIM} application: the development of an add-in to guide users in {Autodesk} {Revit}},
	volume = {XLII-2/W11},
	issn = {2194-9034},
	shorttitle = {{THE} {INTEGRATION} {OF} {A} {SCAN}-{TO}-{HBIM} {PROCESS} {IN} {BIM} {APPLICATION}},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2-W11/141/2019/},
	doi = {10.5194/isprs-archives-XLII-2-W11-141-2019},
	abstract = {{\textbackslash}textlessp{\textbackslash}textgreater{\textbackslash}textlessstrong{\textbackslash}textgreaterAbstract.{\textbackslash}textless/strong{\textbackslash}textgreater In recent years, the generative process of building information modeling (BIM) digital models oriented to the digitisation of heritage buildings has been supported by the development of new modeling tools, able to integrate the point cloud data produced by laser scanning and digital photogrammetry in major modeling software applications such as Autodesk Revit and Graphisoft Archicad. Architectural and structural elements of churches, castles, and historical monuments such as complex vaults, arches, decorations and ornaments, irregular walls with a variable section and wall stratigraphy require higher levels of detail (LOD) and information (LOI) than new buildings. Consequently, the structure of a BIM model oriented to represent heritage buildings (HBIM) required the definition of a new digital process capable of converting the traditional techniques to the generation of 'unique' digital models able to connect different type of information. Consequently, the generation of 'new' 3D objects able to follow the constructive logic of the detected artefact has required the establishment of new grades of generation (GOG) and accuracy (GOA) to reduce the time and cost of the scan-to-BIM process. The main challenge of this research was the integration of these new modeling requirements in BIM software through the development of an add-in for one of the most used BIM software (Autodesk Revit). Through the generation of the complex vaulted system of the Basilica of Collemaggio (L’Aquila, Italy) and one of the most famous monuments of northern Italy (Arch of Peace in Milan, Italy), the following research shows how it was possible to support users in the HBIM generation, reducing the modeling impact of complex shapes from point cloud data and increasing information sharing for different BIM-based analysis, disciplines and users.{\textbackslash}textless/p{\textbackslash}textgreater},
	language = {en},
	urldate = {2020-12-23},
	journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Banfi, F.},
	month = may,
	year = {2019},
	pages = {141--148},
}

@article{kokab_extracting_2019,
	title = {Extracting of {Cross} {Section} {Profiles} from {Complex} {Point} {Cloud} {Data} {Sets}},
	volume = {52},
	issn = {24058963},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2405896319309103},
	doi = {10.1016/j.ifacol.2019.10.055},
	language = {en},
	number = {10},
	urldate = {2021-01-02},
	journal = {IFAC-PapersOnLine},
	author = {Kokab, H. Setareh and Urbanic, R. Jill},
	year = {2019},
	pages = {346--351},
}

@inproceedings{gallardo_saavedra_aerial_2017,
	address = {Abu Dhabi},
	title = {Aerial {Thermographic} {Inspection} of {Photovoltaic} {Plants}: {Analysis} and {Selection} of the {Equipment}},
	isbn = {978-3-9814659-7-6},
	shorttitle = {Aerial {Thermographic} {Inspection} of {Photovoltaic} {Plants}},
	url = {http://proceedings.ises.org/citation?doi=swc.2017.20.03},
	doi = {10.18086/swc.2017.20.03},
	abstract = {In recent times, more and more countries are choosing the alternative of generating clean energy. The photovoltaic (PV) energy installed is rapidly increasing around the World. PV cells are made with semiconductor materials such as Si, GaAs, among others. Despite the quality controls in the manufacture and manipulation of the panels, damages occur during their manufacturing, installation, use or of wear due to environmental factors, doing it necessary a periodic review. Manual inspections become expensive and largely inefficient due to the big extensions of PV plants. For this reason, it is necessary to automate the inspection task. This paper presents a methodology for the selection of equipment used to make inspections of faults in solar panels using aerial thermography, based on the review of the state of the art and the latest equipment technology available.},
	language = {en},
	urldate = {2021-05-09},
	booktitle = {Proceedings of {SWC2017}/{SHC2017}},
	publisher = {International Solar Energy Society},
	author = {Gallardo Saavedra, Sara and Alfaro Mejia, Estefanía and Hernandez Callejo, Luis and Duque Pérez, Óscar and Loaiza Correa, Humberto and Franco Mejia, Edinson},
	year = {2017},
	pages = {1--9},
}

@article{andriasyan_point_2020,
	title = {From {Point} {Cloud} {Data} to {Building} {Information} {Modelling}: {An} {Automatic} {Parametric} {Workflow} for {Heritage}},
	volume = {12},
	issn = {2072-4292},
	shorttitle = {From {Point} {Cloud} {Data} to {Building} {Information} {Modelling}},
	url = {https://www.mdpi.com/2072-4292/12/7/1094},
	doi = {10.3390/rs12071094},
	abstract = {Building Information Modelling (BIM) is a globally adapted methodology by government organisations and builders who conceive the integration of the organisation, planning, development and the digital construction model into a single project. In the case of a heritage building, the Historic Building Information Modelling (HBIM) approach is able to cover the comprehensive restoration of the building. In contrast to BIM applied to new buildings, HBIM can address different models which represent either periods of historical interpretation, restoration phases or records of heritage assets over time. Great efforts are currently being made to automatically reconstitute the geometry of cultural heritage elements from data acquisition techniques such as Terrestrial Laser Scanning (TLS) or Structure From Motion (SfM) into BIM (Scan-to-BIM). Hence, this work advances on the parametric modelling from remote sensing point cloud data, which is carried out under the Rhino+Grasshopper-ArchiCAD combination. This workflow enables the automatic conversion of TLS and SFM point cloud data into textured 3D meshes and thus BIM objects to be included in the HBIM project. The accuracy assessment of this workflow yields a standard deviation value of 68.28 pixels, which is lower than other author’s precision but suffices for the automatic HBIM of the case study in this research.},
	language = {en},
	number = {7},
	urldate = {2020-12-23},
	journal = {Remote Sensing},
	author = {Andriasyan, Mesrop and Moyano, Juan and Nieto-Julián, Juan Enrique and Antón, Daniel},
	month = mar,
	year = {2020},
	pages = {1094},
}

@article{kim_sustainable_2020,
	title = {Sustainable {Application} of {Hybrid} {Point} {Cloud} and {BIM} {Method} for {Tracking} {Construction} {Progress}},
	volume = {12},
	issn = {2071-1050},
	url = {https://www.mdpi.com/2071-1050/12/10/4106},
	doi = {10.3390/su12104106},
	abstract = {Compared to the past, the complexity of construction-project progress has increased as the size of structures has become larger and taller. This has resulted in many unexpected problems with an increasing frequency of occurrence, such as various uncertainties and risk factors. Recently, research was conducted to solve the problem via integration with data-collection automation tools of construction-project-progress measurement. Most of the methods used spatial sensing technology. Thus, this study performed a review of the representative technologies applied to construction-project-progress data collection and identified the unique characteristics of each technology. The basic principle of the progress proposed in this study is its execution through the point cloud and the attributes of BIM, which were studied in five stages: (1) Acquisition of construction completion data using a point cloud, (2) production of a completed 3D model, (3) interworking of an as-planned BIM model and as-built model, (4) construction progress tracking via overlap of two 3D models, and (5) verification by comparison with actual data. This has confirmed that the technical limitations of the construction progress tracking through the point cloud do not exist, and that a fairly high degree of progress data which contains efficiency and accuracy can be collected.},
	language = {en},
	number = {10},
	urldate = {2020-12-23},
	journal = {Sustainability},
	author = {Kim, Seungho and Kim, Sangyong and Lee, Dong-Eun},
	month = may,
	year = {2020},
	pages = {4106},
}

@article{soilan_3d_2020,
	title = {{3D} {Point} {Cloud} to {BIM}: {Semi}-{Automated} {Framework} to {Define} {IFC} {Alignment} {Entities} from {MLS}-{Acquired} {LiDAR} {Data} of {Highway} {Roads}},
	volume = {12},
	issn = {2072-4292},
	shorttitle = {{3D} {Point} {Cloud} to {BIM}},
	url = {https://www.mdpi.com/2072-4292/12/14/2301},
	doi = {10.3390/rs12142301},
	abstract = {Building information modeling (BIM) is a process that has shown great potential in the building industry, but it has not reached the same level of maturity for transportation infrastructure. There is a standardization need for information exchange and management processes in the infrastructure that integrates BIM and Geographic Information Systems (GIS). Currently, the Industry Foundation Classes standard has harmonized different infrastructures under the Industry Foundation Classes (IFC) 4.3 release. Furthermore, the usage of remote sensing technologies such as laser scanning for infrastructure monitoring is becoming more common. This paper presents a semi-automated framework that takes as input a raw point cloud from a mobile mapping system, and outputs an IFC-compliant file that models the alignment and the centreline of each road lane in a highway road. The point cloud processing methodology is validated for two of its key steps, namely road marking processing and alignment and road line extraction, and a UML diagram is designed for the definition of the alignment entity from the point cloud data.},
	language = {en},
	number = {14},
	urldate = {2020-12-23},
	journal = {Remote Sensing},
	author = {Soilán, Mario and Justo, Andrés and Sánchez-Rodríguez, Ana and Riveiro, Belén},
	month = jul,
	year = {2020},
	pages = {2301},
}

@article{mahmood_bim-based_2020,
	title = {{BIM}-{Based} {Registration} and {Localization} of {3D} {Point} {Clouds} of {Indoor} {Scenes} {Using} {Geometric} {Features} for {Augmented} {Reality}},
	volume = {12},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/14/2302},
	doi = {10.3390/rs12142302},
	abstract = {Augmented reality can improve construction and facility management by visualizing an as-planned model on its corresponding surface for fast, easy, and correct information retrieval. This requires the localization registration of an as-built model in an as-planned model. However, the localization and registration of indoor environments fail, owing to self-similarity in an indoor environment, relatively large as-planned models, and the presence of additional unplanned objects. Therefore, this paper proposes a computer vision-based method to (1) homogenize indoor as-planned and as-built models, (2) reduce the search space of model matching, and (3) localize the structure (e.g., room) for registration of the scanned area in its as-planned model. This method extracts a representative horizontal cross section from the as-built and as-planned point clouds to make these models similar, restricts unnecessary transformation to reduce the search space, and corresponds the line features for the estimation of the registration transformation matrix. The performance of this method, in terms of registration accuracy, is evaluated on as-built point clouds of rooms and a hallway on a building floor. A rotational error of 0.005 rad and a translational error of 0.088 m are observed in the experiments. Hence, the geometric feature described on a representative cross section with transformation restrictions can be a computationally cost-effective solution for indoor localization and registration.},
	language = {en},
	number = {14},
	urldate = {2020-12-23},
	journal = {Remote Sensing},
	author = {Mahmood, Bilawal and Han, SangUk and Lee, Dong-Eun},
	month = jul,
	year = {2020},
	pages = {2302},
}

@article{wang_urban_2020,
	title = {Urban {3D} modeling with mobile laser scanning: a review},
	volume = {2},
	issn = {20965796},
	shorttitle = {Urban {3D} modeling with mobile laser scanning},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2096579620300395},
	doi = {10.1016/j.vrih.2020.05.003},
	language = {en},
	number = {3},
	urldate = {2021-01-01},
	journal = {Virtual Reality \& Intelligent Hardware},
	author = {Wang, Cheng and Wen, Chenglu and Dai, Yudi and Yu, Shangshu and Liu, Minghao},
	month = jun,
	year = {2020},
	pages = {175--212},
}

@inproceedings{schaer_accuracy_2007,
	title = {Accuracy estimation for laser point cloud including scanning geometry},
	booktitle = {Mobile {Mapping} {Symposium} 2007, {Padova}},
	author = {Schaer, Philipp and Skaloud, Jan and Landtwing, S and Legat, Klaus},
	year = {2007},
}

@misc{noauthor_simcenter_nodate,
	title = {Simcenter},
	url = {https://www.plm.automation.siemens.com/global/es/products/simcenter/},
	abstract = {Simcenter},
	urldate = {2021-07-01},
	note = {Publication Title: Siemens Digital Industries Software},
}

@article{nageem_predicting_2017,
	series = {7th {International} {Conference} on {Advances} in {Computing} \& {Communications}, {ICACC}-2017, 22-24 {August} 2017, {Cochin}, {India}},
	title = {Predicting the {Power} {Output} of a {Grid}-{Connected} {Solar} {Panel} {Using} {Multi}-{Input} {Support} {Vector} {Regression}},
	volume = {115},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050917319725},
	doi = {10.1016/j.procs.2017.09.143},
	abstract = {As the penetration of photovoltaic power is increasing, utilities are concerned about its impact on distribution grid. Due to the variable nature of solar power, predicting the power output of solar panel installation is important for its optimal use. This paper proposes a new method for forecasting the power output from a solar panel using multi input Support Vector Regression model. The performance has been analysed and compared with Analytical PV power forecasting model. Both the models are simulated and performance evaluation is done using MATLAB. Mean Absolute Percentage Error and Mean Absolute Error are used to assess forecasting models.},
	language = {en},
	urldate = {2021-05-10},
	journal = {Procedia Computer Science},
	author = {Nageem, Ruby and R, Jayabarathi},
	month = jan,
	year = {2017},
	keywords = {Mean Absolute Error(MAE), Mean Absolute Percentage Error(MAPE), Photovoltaic(PV) installation, Support Vector Regression(SVR), solar irradiance},
	pages = {723--730},
}

@article{liu_detection_2014,
	title = {Detection of {High}-{Speed} {Railway} {Subsidence} and {Geometry} {Irregularity} {Using} {Terrestrial} {Laser} {Scanning}},
	volume = {140},
	issn = {0733-9453, 1943-5428},
	url = {http://ascelibrary.org/doi/10.1061/%28ASCE%29SU.1943-5428.0000131},
	doi = {10.1061/(ASCE)SU.1943-5428.0000131},
	language = {en},
	number = {3},
	urldate = {2021-01-09},
	journal = {Journal of Surveying Engineering},
	author = {Liu, Chun and Li, Nan and Wu, Hangbin and Meng, Xiaolin},
	month = aug,
	year = {2014},
	pages = {04014009},
}

@inproceedings{liu_normal_2012,
	title = {Normal estimation for pointcloud using {GPU} based sparse tensor voting},
	doi = {10.1109/ROBIO.2012.6490949},
	abstract = {Normal estimation is the basis for most applications using pointcloud, such as segmentation. However, it is still a challenging problem regarding computational complexity and observation noise. In this paper, we propose a normal estimation method for pointcloud using results from tensor voting. Comparing with other approaches, we show it has smaller estimation error. Moreover, by varying the voting kernel size, we find it is a flexible approach for structure extraction as well. The results show that the proposed method is robust to noisy observation and missing data points as well. We use a GPU based implementation of Sparse Tensor Voting, which enables realtime calculation.},
	booktitle = {2012 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO})},
	author = {Liu, Ming and Pomerleau, François and Colas, Francis and Siegwart, Roland},
	month = dec,
	year = {2012},
	pages = {91--96},
}

@misc{noauthor_semantic3d_nodate,
	title = {{Semantic3D}},
	url = {http://www.semantic3d.net/},
	urldate = {2021-05-06},
}

@misc{noauthor_pix4dmapper_nodate,
	title = {{Pix4Dmapper} - {El} software líder en fotogrametría para mapeo profesional con drones},
	url = {https://www.pix4d.com/es/producto/pix4dmapper-fotogrametria-software},
	abstract = {Pix4Dmapper: El software líder en fotogrametría para mapeo profesional con drones},
	language = {es},
	urldate = {2021-05-06},
	note = {Publication Title: Pix4D},
}

@inproceedings{heikkila_four-step_1997,
	title = {A four-step camera calibration procedure with implicit image correction},
	doi = {10.1109/CVPR.1997.609468},
	abstract = {In geometrical camera calibration the objective is to determine a set of camera parameters that describe the mapping between 3-D reference coordinates and 2-D image coordinates. Various methods for camera calibration can be found from the literature. However surprisingly little attention has been paid to the whole calibration procedure, i.e., control point extraction from images, model fitting, image correction, and errors originating in these stages. The main interest has been in model fitting, although the other stages are also important. In this paper we present a four-step calibration procedure that is an extension to the two-step method. There is an additional step to compensate for distortion caused by circular features, and a step for correcting the distorted image coordinates. The image correction is performed with an empirical inverse model that accurately compensates for radial and tangential distortions. Finally, a linear method for solving the parameters of the inverse model is presented.},
	booktitle = {Proceedings of {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Heikkila, J. and Silven, O.},
	month = jun,
	year = {1997},
	keywords = {Calibration, Cameras, Closed-form solution, Error correction, Geometrical optics, Inverse problems, Machine vision, Mathematical model, Minimization methods, Nonlinear distortion},
	pages = {1106--1112},
}

@misc{noauthor_photogrammetric_nodate,
	title = {A photogrammetric approach to fusing natural colour and thermal infrared {UAS} imagery in {3D} point cloud generation: {International} {Journal} of {Remote} {Sensing}: {Vol} 41, {No} 1},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01431161.2019.1641241?journalCode=tres20},
	urldate = {2021-05-06},
}

@misc{noauthor_lidar_nodate,
	title = {Lidar measurement of snow depth: a review {\textbackslash}textbar {Journal} of {Glaciology} {\textbackslash}textbar {Cambridge} {Core}},
	url = {https://www.cambridge.org/core/journals/journal-of-glaciology/article/lidar-measurement-of-snow-depth-a-review/4419DF5C778946103080CB6187D434C0},
	urldate = {2021-05-06},
}

@misc{noauthor_opengl_nodate,
	title = {{OpenGL} - {The} {Industry} {Standard} for {High} {Performance} {Graphics}},
	url = {https://www.opengl.org//},
	urldate = {2021-05-06},
}

@article{moriarty_deploying_2019,
	title = {Deploying multispectral remote sensing for multi-temporal analysis of archaeological crop stress at {Ravenshall}, {Fife}, {Scotland}},
	volume = {26},
	copyright = {© 2018 John Wiley \& Sons, Ltd.},
	issn = {1099-0763},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/arp.1721},
	doi = {https://doi.org/10.1002/arp.1721},
	abstract = {Diminishing returns of archaeological crop marks in lowland areas from traditional observer-directed visible spectrum aerial survey with standard photographic cameras highlights a need to explore alternative approaches to maintain the effectiveness of survey programmes. Developments in low-cost multispectral remote sensing have in part been driven by the growth of precision agriculture and, whilst contributing to the intensification of land use, these technologies may offer new spectral and temporal capacities for detecting, recording and monitoring historic landscapes. However, there are significant challenges to the deployment of such approaches, not least the costs of data acquisition and uncertainty about the best conditions for data collection. This study assesses the effectiveness of the Parrot Sequoia, a relatively low-cost multispectral sensor recently developed for agricultural applications, for the detection of crop marks to inform archaeological survey. A series of observations were taken with the sensor mounted on an unmanned aerial vehicle (UAV) at Ravenshall, Fife, Scotland, between April and July 2017. The resulting reflectance maps are compared to red, green and blue (RGB) photographs taken with a Nikon D800E digital camera during seven light aircraft surveys, with the aim of testing the sensors' comparative ability to record crop mark developments over time. The contrast in reflectance between vegetation samples growing over buried archaeological remains and the surrounding field was assessed through separability in regional histogram values across different image band combinations. Separable values indicative of crop marks were found in both the multispectral and RGB results from June 2017 onwards. Several vegetation index (VI) maps, particularly the Simple Ratio (SR) and Normalised Difference Vegetation Index (NDVI), were found to be effective for distinguishing crop marks in the multispectral results. The Sequoia is a cost-effective sensor offering improved spectral resolution over the RGB photographs, showing potential for subtle crop mark detection across compact study areas.},
	language = {en},
	number = {1},
	urldate = {2021-05-05},
	journal = {Archaeological Prospection},
	author = {Moriarty, Charles and Cowley, Dave C. and Wade, Tom and Nichol, Caroline J.},
	year = {2019},
	keywords = {aerial photography, agriculture, archaeological survey, multispectral remote sensing, unmanned aerial vehicles (UAVs), vegetation indices},
	pages = {33--46},
}

@article{graciano_real-time_2018,
	title = {Real-time visualization of {3D} terrains and subsurface geological structures},
	volume = {115},
	issn = {0965-9978},
	url = {https://www.sciencedirect.com/science/article/pii/S0965997817304830},
	doi = {10.1016/j.advengsoft.2017.10.002},
	abstract = {Geological structures, both at the surface and subsurface levels, are typically represented by means of voxel data. This model presents a major drawback: its large storage requirements. In this paper, we address this problem and propose the use of a stack-based representation for geological surface-subsurface structures. Although this representation has been mainly used for volumetric terrain visualization in previous works, it has been used as an auxiliary data structure. Therefore, our main contribution in this work is its use as a first-class representation for both processing and visualization of surface and subsurface information. The proposed solution provides real-time visualization of volumetric terrains and subsurface geological structures represented as stacks using a compact data representation in the GPU. Different GPU memory implementations of the stacks have been described, discussing the tradeoffs between performance and storage efficiency. We also introduce a novel algorithm for the calculation of the surface normal vectors using a hybrid object-image space strategy. Moreover, important features for geoscientific applications such as visualization of boreholes or geological cross sections, and selective attenuation of strata have also been implemented in a straightforward way.},
	language = {en},
	urldate = {2021-05-05},
	journal = {Advances in Engineering Software},
	author = {Graciano, Alejandro and Rueda, Antonio J. and Feito, Francisco R.},
	month = jan,
	year = {2018},
	keywords = {GPU Memory management, Stack-based representation of terrains, Terrain modeling, Volume rendering},
	pages = {314--326},
}

@article{ortega_topological_2020,
	title = {Topological {Data} {Models} for {Virtual} {Management} of {Hidden} {Facilities} {Through} {Digital} {Reality}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2984035},
	abstract = {The integral management of facilities located in the subsoil or hidden in walls and ceilings entails numerous challenges in terms of accurate positioning and data update. We describe the characteristics of the information system designed to act in the life cycle of facilities networks. We focus on the data model design, whose core connects topologically the 3D geometric objects representing facilities components. This scheme enables CRUD (Create, Read, Update, Delete) operations directly on the 3D objects of the digital reality, and through desktop or mobile devices. The system is implemented in a client-server architecture. The server maintains the complete topological data model in a spatial database. This model is redesigned and replicated in the memory of any client device with graphical capabilities. Conventional computers can enable Virtual Reality, while mobile devices can access ubiquitously surrounding facilities through Augmented Reality. In addition, the design of a third data model allows the exchange of this information using a standard format such as CityGML. Current facilities networks have been used to test the CRUD operations functionality.},
	journal = {IEEE Access},
	author = {Ortega, Lidia M. and Jurado, Juan M. and Ruiz, José Luis López and Feito, Francisco R.},
	year = {2020},
	keywords = {3D-GIS, ARCore, BIM, CRUD operations, CityGML, Data models, Information systems, Mobile handsets, Solid modeling, Spatial databases, Three-dimensional displays, Urban areas, augmented reality, facilities management, underground infrastructures, virtual reality},
	pages = {62584--62600},
}

@incollection{noauthor_notitle_2017,
	isbn = {978-3-527-69330-6},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9783527693306.fmatter},
	abstract = {The prelims comprise: Half-Title Page Title Page Copyright Page Contents Preface to Second Edition Preface to First Edition List of Acronyms},
	language = {en},
	urldate = {2021-05-04},
	booktitle = {Infrared {Thermal} {Imaging}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2017},
	doi = {10.1002/9783527693306.fmatter},
	pages = {I--XXIV},
}

@misc{noauthor_pix4dmapper_nodate-1,
	title = {{Pix4Dmapper} - {El} software líder en fotogrametría para mapeo profesional con drones {\textbackslash}textbar {Pix4D}},
	url = {https://www.pix4d.com/es/producto/pix4dmapper-fotogrametria-software},
	urldate = {2021-05-04},
}

@misc{noauthor_infrared_nodate,
	title = {Infrared {Thermal} {Imaging} {\textbackslash}textbar {Wiley} {Online} {Books}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9783527693306},
	urldate = {2021-05-04},
}

@article{hu_parallel_2019,
	title = {Parallel {BVH} {Construction} {Using} {Locally} {Density} {Clustering}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2932151},
	abstract = {A novel bounding volume hierarchy (BVH) construction method based on locally dense clustering (LDC) was proposed for the low quality of BVH constructed in a complex scene with uneven distribution of primitives. The quality of the BVH was effectively improved by defining primitive density, analyzing the relation between density and traversal efficiency, selecting primitives with high density as the clustering center at the early stages of construction, and combining with top-down iterative clustering and bottom-up agglomerative clustering (AC). In order to effectively calculate primitive density, a local search strategy based on Morton coding was adopted. This strategy can quickly get approximate primitive density information through GPU. We evaluate the method and show that, in the complex building scene with uneven distribution of primitives, our method is 13\% higher in traversal speed and 11\% lower in surface area heuristic (SAH) cost. That means our method can improve the rendering speed of ray tracing effectively.},
	journal = {IEEE Access},
	author = {Hu, Yingsong and Wang, Weijian and Li, Dan and Zeng, Qingzhi and Hu, Yunfei},
	year = {2019},
	keywords = {Acceleration, Approximation algorithms, Clustering algorithms, Graphics processing units, Morton code, Primitive density, Ray tracing, Rendering (computer graphics), Three-dimensional displays, local search, parallel Construction},
	pages = {105827--105839},
}

@article{sun_modeling_2000,
	title = {Modeling lidar returns from forest canopies},
	volume = {38},
	issn = {1558-0644},
	abstract = {Remote sensing techniques that utilize light detection and ranging (lidar) provide unique data on canopy geometry and subcanopy topography. This type of information will lead to improved understanding of important structures and processes of Earth's vegetation cover. To understand the relation between canopy structure and the lidar return waveform, a three-dimensional (3D) model was developed and implemented. Detailed field measurements and forest growth model simulations of forest stands were used to parameterize this vegetation lidar waveform model. In the model, the crown shape of trees determines the vertical distribution of plant material and the corresponding lidar waveforms. Preliminary comparisons of averaged waveforms from an airborne lidar and model simulations shows that the shape of the measured waveform was more similar to simulations using an ellipsoid or hemi-ellipsoid shape. The observed slower decay of the airborne lidar waveforms than the simulated waveforms may indicate the existence of the understories and may also suggest that higher order scattering from the upper canopy may contribute to the lidar signals. The lidar waveforms from stands simulated from a forest growth model show the dependence of the waveform on stand structure.},
	number = {6},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Sun, G. and Ranson, K. J.},
	month = nov,
	year = {2000},
	keywords = {Atmospheric modeling, Geometry, Laser radar, Ray tracing, Remote sensing, Shape measurement, Sun, Surface emitting lasers, Surface topography, Vegetation mapping, backscatter, canopy geometry, canopy structure, crown shape, ellipsoid, forest, forest canopy, forest growth, forestry, geophysical measurement technique, geophysical techniques, hemi-ellipsoid, laser remote sensing, laser scattering, lidar, lidar return, optical radar, remote sensing by laser beam, simulation, subcanopy topography, three dimensional model, three dimensional structure, understory, vegetation mapping, vertical distribution, waveform},
	pages = {2617--2626},
}

@misc{noauthor_comprehensive_nodate,
	title = {A comprehensive review on automation in agriculture using artificial intelligence {\textbackslash}textbar {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S2589721719300182?token=BE2C11B9577B23A12950A89AC687A343CEF7F505244EBDD815B19F3015779D430D75709B2C1E4CC14238D82BA63803EB&originRegion=eu-west-1&originCreation=20210401125735},
	language = {en},
	urldate = {2021-04-01},
	doi = {10.1016/j.aiia.2019.05.004},
}

@article{sagan_uav-based_2019,
	title = {{UAV}-{Based} {High} {Resolution} {Thermal} {Imaging} for {Vegetation} {Monitoring}, and {Plant} {Phenotyping} {Using} {ICI} 8640 {P}, {FLIR} {Vue} {Pro} {R} 640, and {thermoMap} {Cameras}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/11/3/330},
	doi = {10.3390/rs11030330},
	abstract = {The growing popularity of Unmanned Aerial Vehicles (UAVs) in recent years, along with decreased cost and greater accessibility of both UAVs and thermal imaging sensors, has led to the widespread use of this technology, especially for precision agriculture and plant phenotyping. There are several thermal camera systems in the market that are available at a low cost. However, their efficacy and accuracy in various applications has not been tested. In this study, three commercially available UAV thermal cameras, including ICI 8640 P-series (Infrared Cameras Inc., USA), FLIR Vue Pro R 640 (FLIR Systems, USA), and thermoMap (senseFly, Switzerland) have been tested and evaluated for their potential for forest monitoring, vegetation stress detection, and plant phenotyping. Mounted on multi-rotor or fixed wing systems, these cameras were simultaneously flown over different experimental sites located in St. Louis, Missouri (forest environment), Columbia, Missouri (plant stress detection and phenotyping), and Maricopa, Arizona (high throughput phenotyping). Thermal imagery was calibrated using procedures that utilize a blackbody, handheld thermal spot imager, ground thermal targets, emissivity and atmospheric correction. A suite of statistical analyses, including analysis of variance (ANOVA), correlation analysis between camera temperature and plant biophysical and biochemical traits, and heritability were utilized in order to examine the sensitivity and utility of the cameras against selected plant phenotypic traits and in the detection of plant water stress. In addition, in reference to quantitative assessment of image quality from different thermal cameras, a non-reference image quality evaluator, which primarily measures image focus that is based on the spatial relationship of pixels in different scales, was developed. Our results show that (1) UAV-based thermal imaging is a viable tool in precision agriculture and (2) the three examined cameras are comparable in terms of their efficacy for plant phenotyping. Overall, accuracy, when compared against field measured ground temperature and estimating power of plant biophysical and biochemical traits, the ICI 8640 P-series performed better than the other two cameras, followed by FLIR Vue Pro R 640 and thermoMap cameras. Our results demonstrated that all three UAV thermal cameras provide useful temperature data for precision agriculture and plant phenotying, with ICI 8640 P-series presenting the best results among the three systems. Cost wise, FLIR Vue Pro R 640 is more affordable than the other two cameras, providing a less expensive option for a wide range of applications.},
	language = {en},
	number = {3},
	urldate = {2021-04-01},
	journal = {Remote Sensing},
	author = {Sagan, Vasit and Maimaitijiang, Maitiniyazi and Sidike, Paheding and Eblimit, Kevin and Peterson, Kyle T. and Hartling, Sean and Esposito, Flavio and Khanal, Kapil and Newcomb, Maria and Pauli, Duke and Ward, Rick and Fritschi, Felix and Shakoor, Nadia and Mockler, Todd},
	month = jan,
	year = {2019},
	keywords = {FLIR Vue Pro R 640, ICI 8640 P-series, Unmanned Aerial Vehicles, heritability analysis, plant phenotyping, thermal imaging, thermoMap, vegetation monitoring},
	pages = {330},
}

@article{aboutalebi_incorporation_2020,
	title = {Incorporation of {Unmanned} {Aerial} {Vehicle} ({UAV}) {Point} {Cloud} {Products} into {Remote} {Sensing} {Evapotranspiration} {Models}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/12/1/50},
	doi = {10.3390/rs12010050},
	abstract = {In recent years, the deployment of satellites and unmanned aerial vehicles (UAVs) has led to production of enormous amounts of data and to novel data processing and analysis techniques for monitoring crop conditions. One overlooked data source amid these efforts, however, is incorporation of 3D information derived from multi-spectral imagery and photogrammetry algorithms into crop monitoring algorithms. Few studies and algorithms have taken advantage of 3D UAV information in monitoring and assessment of plant conditions. In this study, different aspects of UAV point cloud information for enhancing remote sensing evapotranspiration (ET) models, particularly the Two-Source Energy Balance Model (TSEB), over a commercial vineyard located in California are presented. Toward this end, an innovative algorithm called Vegetation Structural-Spectral Information eXtraction Algorithm (VSSIXA) has been developed. This algorithm is able to accurately estimate height, volume, surface area, and projected surface area of the plant canopy solely based on point cloud information. In addition to biomass information, it can add multi-spectral UAV information to point clouds and provide spectral-structural canopy properties. The biomass information is used to assess its relationship with in situ Leaf Area Index (LAI), which is a crucial input for ET models. In addition, instead of using nominal field values of plant parameters, spatial information of fractional cover, canopy height, and canopy width are input to the TSEB model. Therefore, the two main objectives for incorporating point cloud information into remote sensing ET models for this study are to (1) evaluate the possible improvement in the estimation of LAI and biomass parameters from point cloud information in order to create robust LAI maps at the model resolution and (2) assess the sensitivity of the TSEB model to using average/nominal values versus spatially-distributed canopy fractional cover, height, and width information derived from point cloud data. The proposed algorithm is tested on imagery from the Utah State University AggieAir sUAS Program as part of the ARS-USDA GRAPEX Project (Grape Remote sensing Atmospheric Profile and Evapotranspiration eXperiment) collected since 2014 over multiple vineyards located in California. The results indicate a robust relationship between in situ LAI measurements and estimated biomass parameters from the point cloud data, and improvement in the agreement between TSEB model output of ET with tower measurements when employing LAI and spatially-distributed canopy structure parameters derived from the point cloud data.},
	language = {en},
	number = {1},
	urldate = {2021-04-01},
	journal = {Remote Sensing},
	author = {Aboutalebi, Mahyar and Torres-Rua, Alfonso F. and McKee, Mac and Kustas, William P. and Nieto, Hector and Alsina, Maria Mar and White, Alex and Prueger, John H. and McKee, Lynn and Alfieri, Joseph and Hipps, Lawrence and Coopmans, Calvin and Dokoozlian, Nick},
	month = jan,
	year = {2020},
	keywords = {AggieAir, GRAPEX, LAI, TSEB, UAS, UAV, VSSIXA, evapotranspiration (ET), point-cloud},
	pages = {50},
}

@inproceedings{wang_you_2022,
	address = {New York, NY, USA},
	series = {{MM} '22},
	title = {You {Only} {Hypothesize} {Once}: {Point} {Cloud} {Registration} with {Rotation}-equivariant {Descriptors}},
	isbn = {978-1-4503-9203-7},
	shorttitle = {You {Only} {Hypothesize} {Once}},
	url = {https://doi.org/10.1145/3503161.3548023},
	doi = {10.1145/3503161.3548023},
	abstract = {In this paper, we propose a novel local descriptor-based framework, called You Only Hypothesize Once (YOHO), for the registration of two unaligned point clouds. In contrast to most existing local descriptors which rely on a fragile local reference frame to gain rotation invariance, the proposed descriptor achieves the rotation invariance by recent technologies of group equivariant feature learning, which brings more robustness to point density and noise. Meanwhile, the descriptor in YOHO also has a rotation-equivariant part, which enables us to estimate the registration from just one correspondence hypothesis. Such property reduces the searching space for feasible transformations, thus greatly improving both the accuracy and the efficiency of YOHO. Extensive experiments show that YOHO achieves superior performances with much fewer needed RANSAC iterations on four widely-used datasets, the 3DMatch/3DLoMatch datasets, the ETH dataset and the WHU-TLS dataset. More details are shown in our project page: https://hpwang-whu.github.io/YOHO/.},
	urldate = {2023-03-15},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Haiping and Liu, Yuan and Dong, Zhen and Wang, Wenping},
	month = oct,
	year = {2022},
	keywords = {3d registration, point cloud registration, rotation equivariance, scene reconstruction, shape descriptor},
	pages = {1630--1641},
}

@article{winiwarter_virtual_2022,
	title = {Virtual laser scanning with {HELIOS}++: {A} novel take on ray tracing-based simulation of topographic full-waveform {3D} laser scanning},
	volume = {269},
	issn = {0034-4257},
	shorttitle = {Virtual laser scanning with {HELIOS}++},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425721004922},
	doi = {10.1016/j.rse.2021.112772},
	abstract = {Topographic laser scanning is a remote sensing method to create detailed 3D point cloud representations of the Earth's surface. Since data acquisition is expensive, simulations can complement real data given certain premises are met: (i) models of 3D scene and scanner are available and (ii) modelling of the beam-scene interaction is simplified to a computationally feasible while physically realistic level. A number of laser scanning simulators for different purposes exist, which we enrich by presenting HELIOS++. HELIOS++ is an open-source simulation framework for terrestrial static, mobile, UAV-based and airborne laser scanning implemented in C++. The HELIOS++ concept provides a flexible solution for the trade-off between physical accuracy (realism) and computational complexity (runtime, memory footprint), as well as ease of use and of configuration. Features of HELIOS++ include the availability of Python bindings (pyhelios) for controlling simulations, and a range of model types for 3D scene representation. Such model types include meshes, digital terrain models, point clouds and partially transmissive voxels, which are especially useful in laser scanning simulations of vegetation. In a scene, object models of different types can be combined, so that representations spanning multiple spatial scales in different resolutions and levels of detail are possible. HELIOS++ follows a modular design, where the core components of platform, scene, and scanner can be individually interchanged, and easily configured. HELIOS++ further allows the simulation of beam divergence using a subsampling strategy, and is able to create full-waveform outputs as a basis for detailed analysis. We show how HELIOS++ positions among other VLS software in terms of input model support and simulation of beam divergence in a literature survey. We also perform a direct comparison of simulations with DART, where we employ a scene from the Radiative Transfer Model Intercomparison (RAMI). This example shows that HELIOS++ takes about 10 times longer than DART for parsing and preparing the 3D scene, but performs about 314,000 times faster in the beam simulation, achieving 200,000rays/s. Comparing HELIOS++ to its predecessor, HELIOS, revealed reduced runtimes by up to 99\%. Virtually scanned point clouds may be used for a broad range of applications as shown in literature. We could identify four main categories of use cases prevailing at present, which benefit from simulated LiDAR point clouds: data acquisition planning, method evaluation, method training and sensing experimentation. We conclude that a general-purpose LiDAR simulator can be employed for many different scientific applications, as long as it is ensured that the simulation adequately represents reality, which is specific to the given research question.},
	language = {en},
	urldate = {2023-03-10},
	journal = {Remote Sensing of Environment},
	author = {Winiwarter, Lukas and Esmorís Pena, Alberto Manuel and Weiser, Hannah and Anders, Katharina and Martínez Sánchez, Jorge and Searle, Mark and Höfle, Bernhard},
	month = feb,
	year = {2022},
	keywords = {Data generation, Diffuse media, LiDAR simulation, Point cloud, Software, Vegetation modelling, Voxel},
	pages = {112772},
}

@misc{qi_pointnet_2017,
	title = {{PointNet}++: {Deep} {Hierarchical} {Feature} {Learning} on {Point} {Sets} in a {Metric} {Space}},
	shorttitle = {{PointNet}++},
	url = {http://arxiv.org/abs/1706.02413},
	doi = {10.48550/arXiv.1706.02413},
	abstract = {Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	month = jun,
	year = {2017},
	note = {arXiv:1706.02413 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{xu_spidercnn_2018,
	title = {{SpiderCNN}: {Deep} {Learning} on {Point} {Sets} with {Parameterized} {Convolutional} {Filters}},
	shorttitle = {{SpiderCNN}},
	url = {http://arxiv.org/abs/1803.11527},
	doi = {10.48550/arXiv.1803.11527},
	abstract = {Deep neural networks have enjoyed remarkable success for various vision tasks, however it remains challenging to apply CNNs to domains lacking a regular underlying structures such as 3D point clouds. Towards this we propose a novel convolutional architecture, termed SpiderCNN, to efficiently extract geometric features from point clouds. SpiderCNN is comprised of units called SpiderConv, which extend convolutional operations from regular grids to irregular point sets that can be embedded in R{\textasciicircum}n, by parametrizing a family of convolutional filters. We design the filter as a product of a simple step function that captures local geodesic information and a Taylor polynomial that ensures the expressiveness. SpiderCNN inherits the multi-scale hierarchical architecture from classical CNNs, which allows it to extract semantic deep features. Experiments on ModelNet40 demonstrate that SpiderCNN achieves state-of-the-art accuracy 92.4\% on standard benchmarks, and shows competitive performance on segmentation task.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Xu, Yifan and Fan, Tianqi and Xu, Mingye and Zeng, Long and Qiao, Yu},
	month = sep,
	year = {2018},
	note = {arXiv:1803.11527 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{qi_frustum_2018,
	title = {Frustum {PointNets} for {3D} {Object} {Detection} from {RGB}-{D} {Data}},
	url = {http://arxiv.org/abs/1711.08488},
	doi = {10.48550/arXiv.1711.08488},
	abstract = {In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Qi, Charles R. and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J.},
	month = apr,
	year = {2018},
	note = {arXiv:1711.08488 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{windrim_unsupervised_2023,
	title = {Unsupervised ore/waste classification on open-cut mine faces using close-range hyperspectral data},
	volume = {14},
	issn = {1674-9871},
	url = {https://www.sciencedirect.com/science/article/pii/S1674987123000294},
	doi = {10.1016/j.gsf.2023.101562},
	abstract = {The remote mapping of minerals and discrimination of ore and waste on surfaces are important tasks for geological applications such as those in mining. Such tasks have become possible using ground-based, close-range hyperspectral sensors which can remotely measure the reflectance properties of the environment with high spatial and spectral resolution. However, autonomous mapping of mineral spectra measured on an open-cut mine face remains a challenging problem due to the subtleness of differences in spectral absorption features between mineral and rock classes as well as variability in the illumination of the scene. An additional layer of difficulty arises when there is no annotated data available to train a supervised learning algorithm. A pipeline for unsupervised mapping of spectra on a mine face is proposed which draws from several recent advances in the hyperspectral machine learning literature. The proposed pipeline brings together unsupervised and self-supervised algorithms in a unified system to map minerals on a mine face without the need for human-annotated training data. The pipeline is evaluated with a hyperspectral image dataset of an open-cut mine face comprising mineral ore martite and non-mineralised shale. The combined system is shown to produce a superior map to its constituent algorithms, and the consistency of its mapping capability is demonstrated using data acquired at two different times of day.},
	language = {en},
	number = {4},
	urldate = {2023-03-10},
	journal = {Geoscience Frontiers},
	author = {Windrim, Lloyd and Melkumyan, Arman and Murphy, Richard J. and Chlingaryan, Anna and Leung, Raymond},
	month = jul,
	year = {2023},
	keywords = {Convolutional neural networks, Hyperspectral imaging, Illumination invariance, Machine learning, Mineral mapping, Open-cut mine face},
	pages = {101562},
}

@article{gao_cbff-net_2023,
	title = {{CBFF}-{Net}: {A} {New} {Framework} for {Efficient} and {Accurate} {Hyperspectral} {Object} {Tracking}},
	issn = {1558-0644},
	shorttitle = {{CBFF}-{Net}},
	doi = {10.1109/TGRS.2023.3253173},
	abstract = {Visual object tracking is a fundamental task in computer vision, and thrived in recent decades. With the development of snapshot hyperspectral sensors, efforts have been made to exploit tracking the object with hyperspectral (HS) videos to overcome the inherent limitation of RGB images. Existing HS tracking algorithms extract the deep features from image data separately, which break the interaction information between bands. Therefore, the discrimination ability of HS trackers is limited and the efficiency of the existing HS algorithms is low. In this paper, a novel algorithm (CBFF-Net) is proposed for HS object tracking to improve the discrimination ability and reduce the computational complexity. Specifically, the backbone and head network are implemented with modules of a transferred RGB object tracking network to carry out the HS target tracking task while maintaining the discrimination ability learned from RGB data. Moreover, a bi-directional multiple deep feature fusion (BMDFF) module is proposed to fuse the features extracted from different bands of the HS images, and a cross-band group attention (CBGA) module is introduced to learn interaction information across bands of the HS images. Experiments results indicate the superiority in performance of CBFF-Net, and it runs at 24 frames per second.},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Gao, Long and Liu, Pan and Jiang, Yan and Xie, Weiying and Lei, Jie and Li, Yunsong and Du, Qian},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Attention Learning, Data mining, Feature extraction, Hyperspectral Object Tracking, Multiple Features Fusion, Object tracking, Target tracking, Task analysis, Training, Transformer-based, Videos},
	pages = {1--1},
}

@article{meister_performance_2022,
	title = {Performance {Comparison} of {Bounding} {Volume} {Hierarchies} for {GPU} {Ray} {Tracing}},
	volume = {11},
	issn = {2331-7418},
	url = {http://jcgt.org/published/0011/04/01/},
	number = {4},
	journal = {Journal of Computer Graphics Techniques (JCGT)},
	author = {Meister, Daniel and Bittner, Jiří},
	month = oct,
	year = {2022},
	pages = {1--19},
}

@inproceedings{boesch_thermal_2017,
	title = {Thermal remote sensing with {UAV}-based workflows},
	volume = {XLII-2-W6},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2-W6/41/2017/},
	doi = {10.5194/isprs-archives-XLII-2-W6-41-2017},
	abstract = {{\textbackslash}textlessp{\textbackslash}textgreater{\textbackslash}textlessstrong class="journal-contentHeaderColor"{\textbackslash}textgreaterAbstract.{\textbackslash}textless/strong{\textbackslash}textgreater Climate change will have a significant influence on vegetation health and growth. Predictions of higher mean summer temperatures and prolonged summer draughts may pose a threat to agriculture areas and forest canopies. Rising canopy temperatures can be an indicator of plant stress because of the closure of stomata and a decrease in the transpiration rate. {\textbackslash}textlessbr{\textbackslash}textgreater{\textbackslash}textlessbr{\textbackslash}textgreater Thermal cameras are available for decades, but still often used for single image analysis, only in oblique view manner or with visual evaluations of video sequences. {\textbackslash}textlessbr{\textbackslash}textgreater{\textbackslash}textlessbr{\textbackslash}textgreater Therefore remote sensing using a thermal camera can be an important data source to understand transpiration processes. {\textbackslash}textlessbr{\textbackslash}textgreater{\textbackslash}textlessbr{\textbackslash}textgreater Photogrammetric workflows allow to process thermal images similar to RGB data. But low spatial resolution of thermal cameras, significant optical distortion and typically low contrast require an adapted workflow. Temperature distribution in forest canopies is typically completely unknown and less distinct than for urban or industrial areas, where metal constructions and surfaces yield high contrast and sharp edge information. {\textbackslash}textlessbr{\textbackslash}textgreater{\textbackslash}textlessbr{\textbackslash}textgreater The aim of this paper is to investigate the influence of interior camera orientation, tie point matching and ground control points on the resulting accuracy of bundle adjustment and dense cloud generation with a typically used photogrammetric workflow for UAVbased thermal imagery in natural environments.{\textbackslash}textless/p{\textbackslash}textgreater},
	language = {English},
	urldate = {2021-10-10},
	booktitle = {The {International} {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Boesch, R.},
	month = aug,
	year = {2017},
	pages = {41--46},
}

@inproceedings{khan_detection_2017,
	title = {Detection of defects in solar panels using thermal imaging by {PCA} and {ICA} method},
	url = {https://www.semanticscholar.org/paper/DETECTION-OF-DEFECTS-IN-SOLAR-PANELS-USING-THERMAL-Khan-Gautam/b5601eecb97a788c0f99e5e71f85e5d9a59df2d5},
	abstract = {For good efficiency, fast, reliable and smooth operation of any process we need a failure free operation. It gives a high production and also ensures high return on investments. A failure free operation is of fundamental importance for modern commercial solar power plants to achieve higher power generation efficiency and longer panel life. So a simple and reliable panel evaluation method is required to ensure that. By using thermal infrared imaging, glitches or defects in the solar panels can be easily detected without having to incorporate expensive electrical detection circuitry. In this paper, we propose a solar panel defect detection system based on thermal imaging, which automates the inspection process and mitigates the need for manual panel inspection in a large solar farm. So in this way solar panels can be checked while in operation without disturbing the continuity of operation. So it saves lot of time and cost of detection.},
	urldate = {2023-03-07},
	author = {Khan, F. A. and Gautam, Brijesh Kumar and Singh, Sanjay},
	year = {2017},
}

@article{deng_uav-based_2018,
	title = {{UAV}-based multispectral remote sensing for precision agriculture: {A} comparison between different cameras},
	volume = {146},
	issn = {0924-2716},
	shorttitle = {{UAV}-based multispectral remote sensing for precision agriculture},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271618302533},
	doi = {10.1016/j.isprsjprs.2018.09.008},
	abstract = {Unmanned aerial vehicle (UAV)-based multispectral remote sensing has shown great potential for precision agriculture. However, there are many problems in data acquisition, processing and application, which have stunted its development. In this study, a narrowband Mini-MCA6 multispectral camera and a sunshine-sensor-equipped broadband Sequoia multispectral camera were mounted on a multirotor micro-UAV. They were used to simultaneously collect multispectral imagery and soil–plant analysis development (SPAD) values of maize at multiple sampling points in the field, in addition to the spectral reflectances of six standard diffuse reflectance panels with different reflectance values (4.5\%, 20\%, 30\%, 40\%, 60\% and 65\%). The accuracies of the reflectance and vegetation indices (VIs) derived from the imagery were compared, and the effectiveness and accuracy of the SPAD prediction from the normalized difference vegetation index (NDVI) and red-edge NDVI (reNDVI) under different nitrogen treatments were examined at the plot level. The results show that the narrowband Mini-MCA6 camera could produce more accurate reflectance values than the broadband Sequoia camera, but only if the appropriate calibration method (the nonlinear subband empirical line method) was adopted, especially in visible (blue, green and red) bands. However, the accuracy of the VIs was not completely dependent on the accuracy of the reflectance, i.e., the NDVI from Mini-MCA6 was slightly better than that from Sequoia, whereas Sequoia produced more accurate reNDVI than did Mini-MCA6. At the plot level, reNDVI performed better than NDVI in SPAD prediction regardless of which camera was employed. Moreover, the reNDVI had relatively low sensitivity to the vegetation coverage and was insignificantly affected by environmental factors (e.g., exposed sandy soil). This study indicates that UAV multispectral remote sensing technology is instructive for precision agriculture, but more effort is needed regarding calibration methods for vegetation, postprocessing techniques and robust quantitative studies.},
	language = {en},
	urldate = {2023-03-07},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Deng, Lei and Mao, Zhihui and Li, Xiaojuan and Hu, Zhuowei and Duan, Fuzhou and Yan, Yanan},
	month = dec,
	year = {2018},
	keywords = {Multispectral camera, Remote sensing, SPAD value, Unmanned Aerial Vehicle (UAV), Vegetation index},
	pages = {124--136},
}

@article{tsai_accelerated_2017,
	title = {An accelerated image matching technique for {UAV} orthoimage registration},
	volume = {128},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271617302277},
	doi = {10.1016/j.isprsjprs.2017.03.017},
	abstract = {Using an Unmanned Aerial Vehicle (UAV) drone with an attached non-metric camera has become a popular low-cost approach for collecting geospatial data. A well-georeferenced orthoimage is a fundamental product for geomatics professionals. To achieve high positioning accuracy of orthoimages, precise sensor position and orientation data, or a number of ground control points (GCPs), are often required. Alternatively, image registration is a solution for improving the accuracy of a UAV orthoimage, as long as a historical reference image is available. This study proposes a registration scheme, including an Accelerated Binary Robust Invariant Scalable Keypoints (ABRISK) algorithm and spatial analysis of corresponding control points for image registration. To determine a match between two input images, feature descriptors from one image are compared with those from another image. A “Sorting Ring” is used to filter out uncorrected feature pairs as early as possible in the stage of matching feature points, to speed up the matching process. The results demonstrate that the proposed ABRISK approach outperforms the vector-based Scale Invariant Feature Transform (SIFT) approach where radiometric variations exist. ABRISK is 19.2 times and 312 times faster than SIFT for image sizes of 1000×1000 pixels and 4000×4000 pixels, respectively. ABRISK is 4.7 times faster than Binary Robust Invariant Scalable Keypoints (BRISK). Furthermore, the positional accuracy of the UAV orthoimage after applying the proposed image registration scheme is improved by an average of root mean square error (RMSE) of 2.58m for six test orthoimages whose spatial resolutions vary from 6.7cm to 10.7cm.},
	language = {en},
	urldate = {2023-03-07},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Tsai, Chung-Hsien and Lin, Yu-Ching},
	month = jun,
	year = {2017},
	keywords = {BRISK, Image registration, UAV},
	pages = {130--145},
}

@article{li_modeling_2011,
	title = {Modeling and generating moving trees from video},
	volume = {30},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2070781.2024161},
	doi = {10.1145/2070781.2024161},
	abstract = {We present a probabilistic approach for the automatic production of tree models with convincing 3D appearance and motion. The only input is a video of a moving tree that provides us an initial dynamic tree model, which is used to generate new individual trees of the same type. Our approach combines global and local constraints to construct a dynamic 3D tree model from a 2D skeleton. Our modeling takes into account factors such as the shape of branches, the overall shape of the tree, and physically plausible motion. Furthermore, we provide a generative model that creates multiple trees in 3D, given a single example model. This means that users no longer have to make each tree individually, or specify rules to make new trees. Results with different species are presented and compared to both reference input data and state of the art alternatives.},
	number = {6},
	urldate = {2023-03-07},
	journal = {ACM Transactions on Graphics},
	author = {Li, Chuan and Deussen, Oliver and Song, Yi-Zhe and Willis, Phil and Hall, Peter},
	year = {2011},
	keywords = {generative model, tree modeling and animation},
	pages = {1--12},
}

@inproceedings{correa_gfkuts_2020,
	title = {{GFkuts}: a novel multispectral image segmentation method applied to precision agriculture},
	shorttitle = {{GFkuts}},
	doi = {10.1109/OMICAS52284.2020.9535659},
	abstract = {Image segmentation enables the precise extraction of several crop traits from multispectral aerial imagery. This paper presents a novel segmentation technique called GFKuts. The method integrates a graph-based optimization algorithm with a k-means Monte Carlo approach. Here, we evaluate the performance of the proposed method against other approaches for image segmentation found in the specialized literature. Results report an improvement on the F1-score accuracy in terms of crop canopy segmentation. These findings are promising for the precise calculation of vegetative indices and other crop trait features.},
	booktitle = {2020 {Virtual} {Symposium} in {Plant} {Omics} {Sciences} ({OMICAS})},
	author = {Correa, Edgar S. and Calderon, Francisco and Colorado, Julian D.},
	month = nov,
	year = {2020},
	keywords = {Cameras, Crops, Image segmentation, Measurement, Multispectral imagery, Tools, Toxicology, Training, image segmentation, precision agriculture},
	pages = {1--6},
}

@article{kim_high-resolution_2022,
	title = {High-resolution hyperspectral imagery from pushbroom scanners on unmanned aerial systems},
	volume = {9},
	issn = {2049-6060},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/gdj3.133},
	doi = {10.1002/gdj3.133},
	abstract = {Hyperspectral data are gaining popularity in remote sensing and signal processing communities because of the increased spectral information relative to multispectral data. Several airborne and spaceborne hyperspectral datasets are publicly available, facilitating the development of various applications and algorithms. However, hyperspectral data are usually limited by their narrow, highly correlated and contiguous spectral bands in both processing and analysis. Moreover, the resolution of available hyperspectral datasets is not sufficiently high for the identification of small objects. Nevertheless, with the rapidly advancing technology, hyperspectral imaging systems can now be mounted on small aerial vehicles for detecting small objects at low altitude. To properly handle these high spectral and spatial resolution data, new or redesigned data processing or analysis pipelines must be developed, but such datasets are limited. In this study, we describe two hyperspectral datasets acquired by a drone and evaluate their radiometric and geometric quality. Based on appropriate data acquisition and processing approaches, our datasets are expected to be useful as testbeds for new algorithms and applications.},
	language = {en},
	number = {2},
	urldate = {2023-03-07},
	journal = {Geoscience Data Journal},
	author = {Kim, Jae-In and Chi, Junhwa and Masjedi, Ali and Flatt, John Evan and Crawford, Melba M. and Habib, Ayman F. and Lee, Joohan and Kim, Hyun-Cheol},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/gdj3.133},
	keywords = {geometric correction, hyperspectral, permafrost, radiometric correction, unmanned aerial vehicle},
	pages = {221--234},
}

@article{conde-rodriguez_modelling_2021,
	title = {Modelling {Material} {Microstructure} {Using} the {Perlin} {Noise} {Function}},
	volume = {40},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14182},
	doi = {10.1111/cgf.14182},
	abstract = {This paper introduces a precise and easy to use method for defining the microstructure of heterogeneous solids. This method is based on the concept of Heterogeneous Composite Bézier Hyperpatch, and allows to accurately represent the primary material proportions, as well as the size and shape of the material phases. The solid microstructure is modelled using two functions: a material distribution function (to compute the portion of the solid volume occupied by each primary material), and a modified Perlin noise function that determines the shape and size of each primary material phase. With this method, the position and orientation of the solid in the modeling space does not affect the portion of its volume that is occupied by each primary material, nor the shape and size of the phases. However, the solid microstructure is coherently and automatically modified when the shape of the solid is edited. Regarding continuity, this method allows to define to which extent continuity (both in shape and material distribution) has to be preserved at the junction of the cells that compose the solid. This makes modeling geometrically complex figures very easy.},
	language = {en},
	number = {1},
	urldate = {2023-03-07},
	journal = {Computer Graphics Forum},
	author = {Conde-Rodríguez, F. and García-Fernández, Á-.l. and Torres, J.c.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14182},
	keywords = {heterogeneous composite Bézier hyperpatch, heterogeneous solid modeling, material microstructure},
	pages = {195--208},
}

@inproceedings{dotenco_automatic_2016,
	title = {Automatic detection and analysis of photovoltaic modules in aerial infrared imagery},
	doi = {10.1109/WACV.2016.7477658},
	abstract = {Drone-based aerial thermography has become a convenient quality assessment tool for the precise localization of defective modules and cells in large photovoltaic-power plants. However, manual evaluation of aerial infrared recordings can be extremely time-consuming. Therefore, we propose an approach for automatic detection and analysis of photovoltaic modules in aerial infrared images. Significant temperature abnormalities such as hot spots and hot areas can be identified using our processing pipeline. To identify such defects, we first detect the individual modules in infrared images, and then use statistical tests to detect the defective modules. A quantitative evaluation of the detection and analysis pipeline on real-world, infrared recordings shows the applicability of our approach.},
	booktitle = {2016 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Dotenco, Sergiu and Dalsass, Manuel and Winkler, Ludwig and Würzner, Tobias and Brabec, Christoph and Maier, Andreas and Gallwitz, Florian},
	month = mar,
	year = {2016},
	keywords = {Cameras, Histograms, Kernel, Photovoltaic systems, Temperature distribution},
	pages = {1--9},
}

@article{sun_poisson_2004,
	title = {Poisson matting},
	volume = {23},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/1015706.1015721},
	doi = {10.1145/1015706.1015721},
	abstract = {In this paper, we formulate the problem of natural image matting as one of solving Poisson equations with the matte gradient field. Our approach, which we call Poisson matting, has the following advantages. First, the matte is directly reconstructed from a continuous matte gradient field by solving Poisson equations using boundary information from a user-supplied trimap. Second, by interactively manipulating the matte gradient field using a number of filtering tools, the user can further improve Poisson matting results locally until he or she is satisfied. The modified local result is seamlessly integrated into the final result. Experiments on many complex natural images demonstrate that Poisson matting can generate good matting results that are not possible using existing matting techniques.},
	number = {3},
	urldate = {2023-03-07},
	journal = {ACM Transactions on Graphics},
	author = {Sun, Jian and Jia, Jiaya and Tang, Chi-Keung and Shum, Heung-Yeung},
	year = {2004},
	keywords = {Poisson equation, alpha channel, image compositing, matting},
	pages = {315--321},
}

@book{furukawa_multi-view_2015,
	address = {Boston Delft},
	title = {Multi-{View} {Stereo}: {A} {Tutorial}},
	isbn = {978-1-60198-836-2},
	shorttitle = {Multi-{View} {Stereo}},
	abstract = {Multi-View Stereo: A Tutorial presents a hands-on view of the field of multi-view stereo with a focus on practical algorithms. Multi-view stereo algorithms are able to construct highly detailed 3D models from images alone. They take a possibly very large set of images and construct a 3D plausible geometry that explains the images under some reasonable assumptions, the most important being scene rigidity. Multi-View Stereo: A Tutorial frames the multiview stereo problem as an image/geometry consistency optimization problem. It describes in detail its main two ingredients: robust implementations of photometric consistency measures, and efficient optimization algorithms. It then presents how these main ingredients are used by some of the most successful algorithms, applied into real applications, and deployed as products in the industry. Finally, it describes more advanced approaches exploiting domain-specific knowledge such as structural priors, and gives an overview of the remaining challenges and future research directions.},
	language = {Inglés},
	author = {Furukawa, Yasutaka and Hernández, Carlos},
	month = may,
	year = {2015},
}

@inproceedings{soldado_overview_2012,
	title = {An {Overview} of {BRDF} {Models}},
	url = {https://www.semanticscholar.org/paper/An-Overview-of-BRDF-Models-Soldado-Almagro/bcec7d78b4e78f7ad2dbf216324028cb0f79bdaa},
	abstract = {The authors have been partially supported by 
the Spanish Research Program under project 
TIN2004-07672-C03-02 and the Andalusian Research 
Program under project P08-TIC-03717.},
	urldate = {2023-03-07},
	author = {Soldado, Rosana Montes and Almagro, Carlos Ureña},
	month = mar,
	year = {2012},
}

@inproceedings{briechle_classification_2020,
	title = {Classification of tree species and standing dead trees by fusing {UAV}-based {LiDAR} data and multispectral imagery in the {3D} deep neural network {PointNet}++},
	volume = {V-2-2020},
	url = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/V-2-2020/203/2020/},
	doi = {10.5194/isprs-annals-V-2-2020-203-2020},
	abstract = {{\textless}p{\textgreater}{\textless}strong class="journal-contentHeaderColor"{\textgreater}Abstract.{\textless}/strong{\textgreater} Knowledge of tree species mapping and of dead wood in particular is fundamental to managing our forests. Although individual tree-based approaches using lidar can successfully distinguish between deciduous and coniferous trees, the classification of multiple tree species is still limited in accuracy. Moreover, the combined mapping of standing dead trees after pest infestation is becoming increasingly important. New deep learning methods outperform baseline machine learning approaches and promise a significant accuracy gain for tree mapping. In this study, we performed a classification of multiple tree species (pine, birch, alder) and standing dead trees with crowns using the 3D deep neural network (DNN) PointNet++ along with UAV-based lidar data and multispectral (MS) imagery. Aside from 3D geometry, we also integrated laser echo pulse width values and MS features into the classification process. In a preprocessing step, we generated the 3D segments of single trees using a 3D detection method. Our approach achieved an overall accuracy (OA) of 90.2\% and was clearly superior to a baseline method using a random forest classifier and handcrafted features (OA = 85.3\%). All in all, we demonstrate that the performance of the 3D DNN is highly promising for the classification of multiple tree species and standing dead trees in practice.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2023-03-01},
	booktitle = {{ISPRS} {Annals} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Briechle, S. and Krzystek, P. and Vosselman, G.},
	month = aug,
	year = {2020},
	note = {ISSN: 2194-9042},
	pages = {203--210},
}

@inproceedings{kalisperakis_leaf_2015,
	title = {Leaf area index estimation in vineyards from {UAV} hyperspectral data, {2D} image mosaics and {3D} canopy surface models},
	volume = {XL-1-W4},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-1-W4/299/2015/},
	doi = {10.5194/isprsarchives-XL-1-W4-299-2015},
	abstract = {{\textless}p{\textgreater}{\textless}strong class="journal-contentHeaderColor"{\textgreater}Abstract.{\textless}/strong{\textgreater} The indirect estimation of leaf area index (LAI) in large spatial scales is crucial for several environmental and agricultural applications. To this end, in this paper, we compare and evaluate LAI estimation in vineyards from different UAV imaging datasets. In particular, canopy levels were estimated from i.e., (\textit{i}) hyperspectral data, (\textit{ii}) 2D RGB orthophotomosaics and (\textit{iii}) 3D crop surface models. The computed canopy levels have been used to establish relationships with the measured LAI (ground truth) from several vines in Nemea, Greece. The overall evaluation indicated that the estimated canopy levels were correlated (\textit{r}$^{\textrm{2}}$ {\textgreater} 73\%) with the in-situ, ground truth LAI measurements. As expected the lowest correlations were derived from the calculated greenness levels from the 2D RGB orthomosaics. The highest correlation rates were established with the hyperspectral canopy greenness and the 3D canopy surface models. For the later the accurate detection of canopy, soil and other materials in between the vine rows is required. All approaches tend to overestimate LAI in cases with sparse, weak, unhealthy plants and canopy.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2023-03-01},
	booktitle = {The {International} {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Kalisperakis, I. and Stentoumis, Ch and Grammatikopoulos, L. and Karantzalos, K.},
	month = aug,
	year = {2015},
	note = {ISSN: 1682-1750},
	pages = {299--303},
}

@inproceedings{zainuddin_3d_2019,
	title = {{3D} modeling for rock art documentation using lightweight multispectral camera},
	volume = {XLII-2-W9},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2-W9/787/2019/},
	doi = {10.5194/isprs-archives-XLII-2-W9-787-2019},
	abstract = {{\textless}p{\textgreater}{\textless}strong class="journal-contentHeaderColor"{\textgreater}Abstract.{\textless}/strong{\textgreater} This paper discusses the use of the lightweight multispectral camera to acquire three-dimensional data for rock art documentation application. The camera consists of five discrete bands, used for taking the motifs of the rock art paintings on a big structure of a cave based on the close-range photogrammetry technique. The captured images then processed using commercial structure-from-motion photogrammetry software, which automatically extracts the tie point. The extracted tie points were then used as input to generate a dense point cloud based on the multi-view stereo (MVS) and produced the multispectral 3D model, and orthophotos in a different wavelength. For comparison, the paintings and the wall surface also observed by using terrestrial laser scanner which capable of recording thousands of points in a short period of time with high accuracy. The cloud-to-cloud comparison between multispectral and TLS 3D point cloud show a sub-cm discrepancy, considering the used of the natural features as control target during 3D construction. Nevertheless, the processing also provides photorealistic orthophoto, indicates the advantages of the multispectral camera in generating dense 3D point cloud as TLS, photorealistic 3D model as RGB optic camera, and also with the multiwavelength output.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2023-03-01},
	booktitle = {The {International} {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Zainuddin, K. and Majid, Z. and Ariff, M. F. M. and Idris, K. M. and Abbas, M. A. and Darwin, N.},
	month = jan,
	year = {2019},
	note = {ISSN: 1682-1750},
	pages = {787--793},
}

@inproceedings{roscher_detection_2016,
	title = {Detection of disease symptoms on hyperspectral {3D} plant models},
	volume = {III-7},
	url = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/III-7/89/2016/},
	doi = {10.5194/isprs-annals-III-7-89-2016},
	abstract = {{\textless}p{\textgreater}{\textless}strong class="journal-contentHeaderColor"{\textgreater}Abstract.{\textless}/strong{\textgreater} We analyze the benefit of combining hyperspectral images information with 3D geometry information for the detection of \textit{Cercospora} leaf spot disease symptoms on sugar beet plants. Besides commonly used one-class Support Vector Machines, we utilize an unsupervised sparse representation-based approach with group sparsity prior. Geometry information is incorporated by representing each sample of interest with an inclination-sorted dictionary, which can be seen as an 1D topographic dictionary. We compare this approach with a sparse representation based approach without geometry information and One-Class Support Vector Machines. One-Class Support Vector Machines are applied to hyperspectral data without geometry information as well as to hyperspectral images with additional pixelwise inclination information. Our results show a gain in accuracy when using geometry information beside spectral information regardless of the used approach. However, both methods have different demands on the data when applied to new test data sets. One-Class Support Vector Machines require full inclination information on test and training data whereas the topographic dictionary approach only need spectral information for reconstruction of test data once the dictionary is build by spectra with inclination.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2023-03-01},
	booktitle = {{ISPRS} {Annals} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Roscher, Ribana and Behmann, Jan and Mahlein, Anne-Katrin and Dupuis, Jan and Kuhlmann, Heiner and Plümer, Lutz},
	month = jun,
	year = {2016},
	note = {ISSN: 2194-9042},
	pages = {89--96},
}

@inproceedings{berra_advances_2020,
	title = {Advances and challenges of {UAV} {SfM}-{MVS} photogrammetry and remote sensing: short review},
	volume = {XLII-3-W12-2020},
	shorttitle = {{ADVANCES} {AND} {CHALLENGES} {OF} {UAV} {SFM} {MVS} {PHOTOGRAMMETRY} {AND} {REMOTE} {SENSING}},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-3-W12-2020/267/2020/},
	doi = {10.5194/isprs-archives-XLII-3-W12-2020-267-2020},
	abstract = {{\textless}p{\textgreater}{\textless}strong class="journal-contentHeaderColor"{\textgreater}Abstract.{\textless}/strong{\textgreater} Interest in Unnamed Aerial Vehicle (UAV)-sourced data and Structure-from-Motion (SfM) and Multi-View-Stereo (MVS) photogrammetry has seen a dramatic expansion over the last decade, revolutionizing the fields of aerial remote sensing and mapping. This literature review provides a summary overview on the recent developments and applications of light-weight UAVs and on the widely-accepted SfM - MVS approach. Firstly, the advantages and limitations of UAV remote sensing systems are discussed, followed by an identification of the different UAV and miniaturised sensor models applied to numerous disciplines, showing the range of systems and sensor types utilised recently. Afterwards, a concise list of advantages and challenges of UAV SfM-MVS is provided and discussed. Overall, the accuracy and quality of the SfM-MVS-derived products (e.g. orthomosaics, digital surface model) depends on the quality of the UAV data set, characteristics of the study area and processing tools used. Continued development and investigation are necessary to better determine the quality, precision and accuracy of UAV SfM-MVS derived outputs.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2023-03-05},
	booktitle = {The {International} {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Berra, E. F. and Peppa, M. V.},
	month = dec,
	year = {2020},
	note = {ISSN: 1682-1750},
	pages = {267--272},
}

@inproceedings{zeng_deep_2019,
	title = {Deep {Surface} {Normal} {Estimation} {With} {Hierarchical} {RGB}-{D} {Fusion}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Zeng_Deep_Surface_Normal_Estimation_With_Hierarchical_RGB-D_Fusion_CVPR_2019_paper.html},
	urldate = {2023-03-03},
	author = {Zeng, Jin and Tong, Yanfeng and Huang, Yunmu and Yan, Qiong and Sun, Wenxiu and Chen, Jing and Wang, Yongtian},
	year = {2019},
	pages = {6153--6162},
}

@article{boulch_deep_2016,
	title = {Deep {Learning} for {Robust} {Normal} {Estimation} in {Unstructured} {Point} {Clouds}},
	volume = {35},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12983},
	doi = {10.1111/cgf.12983},
	abstract = {Normal estimation in point clouds is a crucial first step for numerous algorithms, from surface reconstruction and scene understanding to rendering. A recurrent issue when estimating normals is to make appropriate decisions close to sharp features, not to smooth edges, or when the sampling density is not uniform, to prevent bias. Rather than resorting to manually-designed geometric priors, we propose to learn how to make these decisions, using ground-truth data made from synthetic scenes. For this, we project a discretized Hough space representing normal directions onto a structure amenable to deep learning. The resulting normal estimation method outperforms most of the time the state of the art regarding robustness to outliers, to noise and to point density variation, in the presence of sharp edges, while remaining fast, scaling up to millions of points.},
	language = {en},
	number = {5},
	urldate = {2023-03-03},
	journal = {Computer Graphics Forum},
	author = {Boulch, Alexandre and Marlet, Renaud},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12983},
	pages = {281--290},
}

@inproceedings{lenssen_deep_2020,
	title = {Deep {Iterative} {Surface} {Normal} {Estimation}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Lenssen_Deep_Iterative_Surface_Normal_Estimation_CVPR_2020_paper.html},
	urldate = {2023-03-03},
	author = {Lenssen, Jan Eric and Osendorfer, Christian and Masci, Jonathan},
	year = {2020},
	pages = {11247--11256},
}

@article{stumpfegger_gpu_2022,
	title = {{GPU} accelerated scalable parallel coordinates plots},
	volume = {109},
	issn = {0097-8493},
	url = {https://www.sciencedirect.com/science/article/pii/S0097849322001868},
	doi = {10.1016/j.cag.2022.10.008},
	abstract = {Parallel coordinates are a powerful technique to visually analyze multi-parameter data, i.e., sets of datapoints with potentially many associated parameter values per datapoint. When these sets are large, line rendering becomes a severe performance bottleneck, and since many lines fall into the same pixel the numerical precision of the color buffer is quickly reached. We propose a scalable GPU realization of parallel coordinates building upon 2D pairwise attribute bins, to significantly reduce the number of lines to be rendered. Our approach comprises a GPU compute pipeline that combines shader-based scattering with atomic increment operations to efficiently count how often a line is drawn. These counts are then used to draw all pairwise sub-plots in the parallel coordinates plot, by analytically calculating the opacity for each count and rendering a line with end points determined by the 2D coordinates of the bin. In this way, framebuffer precision issues that are paramount in classical approaches can be overcome. We demonstrate the efficiency of the proposed realization for visualizing a weather forecast ensemble comprising 2.7 billion datapoints, each carrying 7 prognostic floating-point variables like temperature, precipitation and pressure, plus spatial and simulation input variables. We compare our pipeline to a rasterization-based approach regarding performance, and demonstrate interactive brushing at 4 s per frame at full HD viewport resolution.},
	language = {en},
	urldate = {2023-03-02},
	journal = {Computers \& Graphics},
	author = {Stumpfegger, Josef and Höhlein, Kevin and Craig, George and Westermann, Rüdiger},
	month = dec,
	year = {2022},
	keywords = {Ensemble analysis, GPU scattering, Parallel coordinates},
	pages = {111--120},
}

@article{baek_accelerated_2020,
	title = {An accelerated rendering scheme for massively large point cloud data},
	volume = {76},
	issn = {1573-0484},
	url = {https://doi.org/10.1007/s11227-019-03114-y},
	doi = {10.1007/s11227-019-03114-y},
	abstract = {In the field of large-scale data visualization, the graphics rendering speed is one of the most important factors for its application development. Since the large-scale data visualization usually requires three-dimensional representations, the three-dimensional graphics libraries such as OpenGL and DirectX have been widely used. In this paper, we suggest a new way of accelerated rendering, through directly using the direct rendering manager packets. Current three-dimensional graphics features are focused on the efficiency of general purpose rendering pipelines. In contrast, we concentrated on the speed-up of the special-purpose rendering pipeline, for point cloud rendering. Our result shows that we achieved our purpose effectively.},
	language = {en},
	number = {10},
	urldate = {2023-03-02},
	journal = {The Journal of Supercomputing},
	author = {Baek, Nakhoon and Yoo, Kwan-Hee},
	month = oct,
	year = {2020},
	keywords = {Direct rendering manager, Graphics acceleration, Large-scale data visualization, Point rendering},
	pages = {8313--8323},
}

@misc{white_cascaded_2021,
	title = {Cascaded {Shadow} {Maps}},
	url = {https://learn.microsoft.com/en-us/windows/win32/dxtecharts/cascaded-shadow-maps},
	abstract = {Cascaded shadow maps (CSMs) are the best way to combat one of the most prevalent errors with shadowing perspective aliasing.},
	language = {en-us},
	urldate = {2022-11-09},
	author = {White, Steven and Natalie, Jessie and Coulter, David and Jakobs, Mike and Satran, Michael},
	month = apr,
	year = {2021},
}

@article{ekanayake_constrained_2021,
	title = {Constrained {Nonnegative} {Matrix} {Factorization} for {Blind} {Hyperspectral} {Unmixing} {Incorporating} {Endmember} {Independence}},
	volume = {14},
	issn = {2151-1535},
	doi = {10.1109/JSTARS.2021.3126664},
	abstract = {Hyperspectral unmixing (HU) has become an important technique in exploiting hyperspectral data since it decomposes a mixed pixel into a collection of endmembers weighted by fractional abundances. The endmembers of a hyperspectral image (HSI) are more likely to be generated by independent sources and be mixed in a macroscopic degree before arriving at the sensor element of the imaging spectrometer as mixed spectra. Over the past few decades, many attempts have focused on imposing auxiliary regularizes on the conventional nonnegative matrix factorization (NMF) framework in order to effectively unmix these mixed spectra. As a promising step toward finding an optimum regularizer to extract endmembers, this article presents a novel blind HU algorithm, referred to as kurtosis-based smooth nonnegative matrix factorization (KbSNMF) which incorporates a novel regularizer based on the statistical independence of the probability density functions of endmember spectra. Imposing this regularizer on the conventional NMF framework promotes the extraction of independent endmembers while further enhancing the parts-based representation of data. Experiments conducted on diverse synthetic HSI datasets (with numerous numbers of endmembers, spectral bands, pixels, and noise levels) and three standard real HSI datasets demonstrate the validity of the proposed KbSNMF algorithm compared to several state-of-the-art NMF-based HU baselines. The proposed algorithm exhibits superior performance especially in terms of extracting endmember spectra from hyperspectral data; therefore, it could uplift the performance of recent deep learning HU methods which utilize the endmember spectra as supervisory input data for abundance extraction.},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Ekanayake, E. M. M. B. and Weerasooriya, H. M. H. K. and Ranasinghe, D. Y. L. and Herath, S. and Rathnayake, B. and Godaliyadda, G. M. R. I. and Ekanayake, M. P. B. and Herath, H. M. V. R.},
	year = {2021},
	note = {Conference Name: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	keywords = {Approximation algorithms, Blind source separation, Data mining, Feature extraction, Gaussianity, Hyperspectral imaging, Matrix decomposition, Pragmatics, constrained, endmember independence, hyperspectral unmixing (HU), kurtosis, nonnegative matrix factorization (NMF)},
	pages = {11853--11869},
}

@article{van_oosterom_organizing_2022,
	title = {Organizing and visualizing point clouds with continuous levels of detail},
	volume = {194},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271622002647},
	doi = {10.1016/j.isprsjprs.2022.10.004},
	abstract = {Point clouds contain high detail and high accuracy geometry representation of the scanned Earth surface parts. To manage the huge amount of data, the point clouds are traditionally organized on location and map-scale; e.g. in an octree structure, where top-levels of the tree contain few points suitable for small scale overviews and lower levels of the tree contain more points suitable for large scale detailed views. The drawback of this solution is that it is based on discrete levels, causing visual artifacts in the form of data density shocks when creating the commonly used perspective views. This paper presents a method based on an optimized distribution of points over continuous levels, avoiding the visualization shocks. The traditional distribution ratio’s of data amounts over discrete levels of raster or vector data is considered the reference. How to convert this to point clouds with continuous levels (still benefiting from the proven advantages of the data distribution in discrete levels for efficient access at a wide range of scales)? In our solution, for each point a cLoD (continuous Level of Detail) value is computed and added as dimension to the point. A SFC (Space Filling Curve)-based nD data clustering technique can be used to organize the points, so that they can be efficiently queried. It should be noted that also other multi-dimensional indexing and clustering techniques could be applied to realize continuous levels based on the cLoD value. Besides the mathematical foundation of the approach also several implementations are described, varying from a 3D web-browser based solution to an augmented reality point cloud app in a mobile phone. The cLoD enables interactive real-time visualization using perspective views without data density shocks, while supporting continuous zoom-in/out and progressive data streaming between server and client. The described cLoD based approach is generic and supports different types of point clouds: from airborne, terrestrial, mobile and indoor laser scanning, but also from dense matching optical imagery or multi-beam echo soundings.},
	language = {en},
	urldate = {2023-03-02},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {van Oosterom, Peter and van Oosterom, Simon and Liu, Haicheng and Thompson, Rod and Meijers, Martijn and Verbree, Edward},
	month = dec,
	year = {2022},
	keywords = {Continuous level of detail (cLoD), Perspective view selection, Space Filling Curve (SFC), nD point clouds},
	pages = {119--131},
}

@misc{schutz_gpu-accelerated_2023,
	title = {{GPU}-{Accelerated} {LOD} {Generation} for {Point} {Clouds}},
	url = {http://arxiv.org/abs/2302.14801},
	doi = {10.48550/arXiv.2302.14801},
	abstract = {About: We introduce a GPU-accelerated LOD construction process that creates a hybrid voxel-point-based variation of the widely used layered point cloud (LPC) structure for LOD rendering and streaming. The massive performance improvements provided by the GPU allow us to improve the quality of lower LODs via color filtering while still increasing construction speed compared to the non-filtered, CPU-based state of the art. Background: LOD structures are required to render hundreds of millions to trillions of points, but constructing them takes time. Results: LOD structures suitable for rendering and streaming are constructed at rates of about 1 billion points per second (with color filtering) to 4 billion points per second (sample-picking/random sampling, state of the art) on an RTX 3090 -- an improvement of a factor of 80 to 400 times over the CPU-based state of the art (12 million points per second). Due to being in-core, model sizes are limited to about 500 million points per 24GB memory. Discussion: Our method currently focuses on maximizing in-core construction speed on the GPU. Issues such as out-of-core construction of arbitrarily large data sets are not addressed, but we expect it to be suitable as a component of bottom-up out-of-core LOD construction schemes.},
	urldate = {2023-03-01},
	publisher = {arXiv},
	author = {Schütz, Markus and Kerbl, Bernhard and Klaus, Philip and Wimmer, Michael},
	month = feb,
	year = {2023},
	note = {arXiv:2302.14801 [cs]},
	keywords = {Computer Science - Graphics},
}

@article{lin_detection_2019,
	title = {Detection of {Pine} {Shoot} {Beetle} ({PSB}) {Stress} on {Pine} {Forests} at {Individual} {Tree} {Level} using {UAV}-{Based} {Hyperspectral} {Imagery} and {Lidar}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/11/21/2540},
	doi = {10.3390/rs11212540},
	abstract = {In recent years, the outbreak of the pine shoot beetle (PSB), Tomicus spp., has caused serious shoots damage and the death of millions of trees in Yunnan pine forests in southwestern China. It is urgent to develop a convincing approach to accurately assess the shoot damage ratio (SDR) for monitoring the PSB insects at an early stage. Unmanned airborne vehicles (UAV)-based sensors, including hyperspectral imaging (HI) and lidar, have very high spatial and spectral resolutions, which are very useful to detect forest health. However, very few studies have utilized HI and lidar data to estimate SDRs and compare the predictive power for mapping PSB damage at the individual tree level. Additionally, the data fusion of HI and lidar may improve the detection accuracy, but it has not been well studied. In this study, UAV-based HI and lidar data were fused to detect PSB. We systematically evaluated the potential of a hyperspectral approach (only-HI data), a lidar approach (only-lidar data), and a combined approach (HI plus lidar data) to characterize PSB damage of individual trees using the Random Forest (RF) algorithm, separately. The most innovative point is the proposed new method to extract the three dimensional (3D) shadow distribution of each tree crown based on a lidar point cloud and the 3D radiative transfer model RAPID. The results show that: (1) for the accuracy of estimating the SDR of individual trees, the lidar approach (R2 = 0.69, RMSE = 12.28\%) performed better than hyperspectral approach (R2 = 0.67, RMSE = 15.87\%), and in addition, it was useful to detect dead trees with an accuracy of 70\%; (2) the combined approach has the highest accuracy (R2 = 0.83, RMSE = 9.93\%) for mapping PSB damage degrees; and (3) when combining HI and lidar data to predict SDRs, two variables have the most contributions, which are the leaf chlorophyll content (Cab) derived from hyperspectral data and the return intensity of the top of shaded crown (Int\_Shd\_top) from lidar metrics. This study confirms the high possibility to accurately predict SDRs at individual tree level if combining HI and lidar data. The 3D radiative transfer model can determine the 3D crown shadows from lidar, which is a key information to combine HI and lidar. Therefore, our study provided a guidance to combine the advantages of hyperspectral and lidar data to accurately measure the health of individual trees, enabling us to prioritize areas for forest health promotion. This method may also be used for other 3D land surfaces, like urban areas.},
	language = {en},
	number = {21},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Lin, Qinan and Huang, Huaguo and Wang, Jingxu and Huang, Kan and Liu, Yangyang},
	month = jan,
	year = {2019},
	note = {Number: 21
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {hyperspectral imaging, lidar, pine shoot beetles, random forest, shoot damage ratio},
	pages = {2540},
}

@inproceedings{cao_3d_2019,
	address = {New York, NY, USA},
	series = {{Web3D} '19},
	title = {{3D} {Point} {Cloud} {Compression}: {A} {Survey}},
	isbn = {978-1-4503-6798-1},
	shorttitle = {{3D} {Point} {Cloud} {Compression}},
	url = {https://doi.org/10.1145/3329714.3338130},
	doi = {10.1145/3329714.3338130},
	abstract = {In recent years, 3D point clouds have enjoyed a great popularity for representing both static and dynamic 3D objects. When compared to 3D meshes, they offer the advantage of providing a simpler, denser and more close-to-reality representation. However, point clouds always carry a huge amount of data. For a typical example of a point cloud with 0.7 million points per 3D frame at 30 fps, the point cloud raw video needs a bandwidth around 500MB/s. Thus, efficient compression methods are mandatory for ensuring the storage/transmission of such data, which include both geometry and attribute information. In the last years, the issue of 3D point cloud compression (3D-PCC) has emerged as a new field of research. In addition, an ISO/MPEG standardization process on 3D-PCC is currently on-going. In this paper, a comprehensive overview of the 3D-PCC state-of-the-art methods is proposed. Different families of approaches are identified, described in details and summarized, including 1D traversal compression, 2D-oriented techniques, which take leverage of existing 2D image/video compression technologies and finally purely 3D approaches, based on a direct analysis of the 3D data.},
	urldate = {2023-03-01},
	booktitle = {The 24th {International} {Conference} on {3D} {Web} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Cao, Chao and Preda, Marius and Zaharia, Titus},
	month = jul,
	year = {2019},
	keywords = {3D point cloud, compression, survey},
	pages = {1--9},
}

@article{angel_automated_2020,
	title = {Automated {Georectification} and {Mosaicking} of {UAV}-{Based} {Hyperspectral} {Imagery} from {Push}-{Broom} {Sensors}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/1/34},
	doi = {10.3390/rs12010034},
	abstract = {Hyperspectral systems integrated on unmanned aerial vehicles (UAV) provide unique opportunities to conduct high-resolution multitemporal spectral analysis for diverse applications. However, additional time-consuming rectification efforts in postprocessing are routinely required, since geometric distortions can be introduced due to UAV movements during flight, even if navigation/motion sensors are used to track the position of each scan. Part of the challenge in obtaining high-quality imagery relates to the lack of a fast processing workflow that can retrieve geometrically accurate mosaics while optimizing the ground data collection efforts. To address this problem, we explored a computationally robust automated georectification and mosaicking methodology. It operates effectively in a parallel computing environment and evaluates results against a number of high-spatial-resolution datasets (mm to cm resolution) collected using a push-broom sensor and an associated RGB frame-based camera. The methodology estimates the luminance of the hyperspectral swaths and coregisters these against a luminance RGB-based orthophoto. The procedure includes an improved coregistration strategy by integrating the Speeded-Up Robust Features (SURF) algorithm, with the Maximum Likelihood Estimator Sample Consensus (MLESAC) approach. SURF identifies common features between each swath and the RGB-orthomosaic, while MLESAC fits the best geometric transformation model to the retrieved matches. Individual scanlines are then geometrically transformed and merged into a single spatially continuous mosaic reaching high positional accuracies only with a few number of ground control points (GCPs). The capacity of the workflow to achieve high spatial accuracy was demonstrated by examining statistical metrics such as RMSE, MAE, and the relative positional accuracy at 95\% confidence level. Comparison against a user-generated georectification demonstrates that the automated approach speeds up the coregistration process by 85\%.},
	language = {en},
	number = {1},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Angel, Yoseline and Turner, Darren and Parkes, Stephen and Malbeteau, Yoann and Lucieer, Arko and McCabe, Matthew F.},
	month = jan,
	year = {2020},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {UAV, georectification, hyperspectral imaging, mosaicking, push-broom},
	pages = {34},
}

@article{cao_object-based_2018,
	title = {Object-{Based} {Mangrove} {Species} {Classification} {Using} {Unmanned} {Aerial} {Vehicle} {Hyperspectral} {Images} and {Digital} {Surface} {Models}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/10/1/89},
	doi = {10.3390/rs10010089},
	abstract = {Mangroves are one of the most important coastal wetland ecosystems, and the compositions and distributions of mangrove species are essential for conservation and restoration efforts. Many studies have explored this topic using remote sensing images that were obtained by satellite-borne and airborne sensors, which are known to be efficient for monitoring the mangrove ecosystem. With improvements in carrier platforms and sensor technology, unmanned aerial vehicles (UAVs) with high-resolution hyperspectral images in both spectral and spatial domains have been used to monitor crops, forests, and other landscapes of interest. This study aims to classify mangrove species on Qi’ao Island using object-based image analysis techniques based on UAV hyperspectral images obtained from a commercial hyperspectral imaging sensor (UHD 185) onboard a UAV platform. First, the image objects were obtained by segmenting the UAV hyperspectral image and the UAV-derived digital surface model (DSM) data. Second, spectral features, textural features, and vegetation indices (VIs) were extracted from the UAV hyperspectral image, and the UAV-derived DSM data were used to extract height information. Third, the classification and regression tree (CART) method was used to selection bands, and the correlation-based feature selection (CFS) algorithm was employed for feature reduction. Finally, the objects were classified into different mangrove species and other land covers based on their spectral and spatial characteristic differences. The classification results showed that when considering the three features (spectral features, textural features, and hyperspectral VIs), the overall classification accuracies of the two classifiers used in this paper, i.e., k-nearest neighbor (KNN) and support vector machine (SVM), were 76.12\% (Kappa = 0.73) and 82.39\% (Kappa = 0.801), respectively. After incorporating tree height into the classification features, the accuracy of species classification increased, and the overall classification accuracies of KNN and SVM reached 82.09\% (Kappa = 0.797) and 88.66\% (Kappa = 0.871), respectively. It is clear that SVM outperformed KNN for mangrove species classification. These results also suggest that height information is effective for discriminating mangrove species with similar spectral signatures, but different heights. In addition, the classification accuracy and performance of SVM can be further improved by feature reduction. The overall results provided evidence for the effectiveness and potential of UAV hyperspectral data for mangrove species identification.},
	language = {en},
	number = {1},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Cao, Jingjing and Leng, Wanchun and Liu, Kai and Liu, Lin and He, Zhi and Zhu, Yuanhui},
	month = jan,
	year = {2018},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {hyperspectral remote sensing, mangrove species classification, object-based image analysis (OBIA), tree height, unmanned aerial vehicle (UAV)},
	pages = {89},
}

@article{li_combining_2020,
	title = {Combining {Kriging} {Interpolation} to {Improve} the {Accuracy} of {Forest} {Aboveground} {Biomass} {Estimation} {Using} {Remote} {Sensing} {Data}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3008686},
	abstract = {The accurate estimation of forest biomass is very significant to the study of regional ecosystems. China's National Forest Continuous Inventory data and Landsat 8 data were used to establish a linear regression (LR) model for the estimation of forest aboveground biomass (AGB) based on forest type. The original results of the LR yielded upon conducting AGB estimation were insufficient. Therefore, an interpolation map of the residuals of the observed and predicted AGB was used to correct the error of the original LR model. There is a highly positive result of the accuracy of the corrected AGB map. First, the AGB estimation based on forest type could effectively improve its accuracy. For example, significant improvements were made in the estimations of broadleaf, coniferous, and mixed forests compared to that of the total vegetation conducted using the original LR model. Second, semivariance analysis should be conducted before spatial interpolation using the Kriging method to determine optimal semivariogram models and parameters. The optimal semivariogram model for broadleaf and total forests was the exponential model, while that for the coniferous and mixed forests was the spherical model. Third, combining Kriging interpolation predicted the AGB map effectively and reduced the under- and overestimation of AGB, although it did not fully eliminate this limitation. The R2 values of broadleaf, coniferous, and mixed forests were improved to 0.897, 0.856, and 0.826, respectively. Overall, the methods used in this study provide an effective approach towards improving the accuracy of AGB estimations by reducing under- and overestimation based on remote sensing data and increasing the ability to monitor the forest ecosystem.},
	journal = {IEEE Access},
	author = {Li, Yingchang and Li, Mingyang and Liu, Zhenzhen and Li, Chao},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Aboveground biomass, Artificial satellites, Biological system modeling, Biomass, Estimation, Forestry, Interpolation, Landsat 8, Remote sensing, ordinary Kriging, semivariance analysis, stepwise regression, subtropical forest},
	pages = {128124--128139},
}

@article{jiang_efficient_2020,
	title = {Efficient structure from motion for large-scale {UAV} images: {A} review and a comparison of {SfM} tools},
	volume = {167},
	issn = {0924-2716},
	shorttitle = {Efficient structure from motion for large-scale {UAV} images},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271620301131},
	doi = {10.1016/j.isprsjprs.2020.04.016},
	abstract = {Unmanned aerial vehicle (UAV) images have gained extensive attention in varying fields, and the Structure from Motion (SfM) technique has become the gold standard for aerial triangulation of UAV images. With increasing data volume caused by the use of multi-view and high-resolution imaging systems and the enhancement of UAV platform’s endurance, the capability for orientation of large-scale UAV images is becoming a prominent and necessary feature for SfM-based solutions. A classical SfM pipeline consists of three major steps, i.e., (i) feature extraction for an individual image, (ii) feature matching for each image pair, and (iii) parameter solving based on iterative bundle adjustment. Most of the time costs are consumed in the second and third steps. This can be explained from three main aspects. First, for feature matching the large number of images and high overlapping degrees cause high combinational complexity of match pairs. Second, the efficiency of commonly utilized techniques for outlier removal would be seriously degenerated because of high outlier ratios of initial matches. Third, for parameter solving of camera poses and scene structures, the iterative execution of bundle adjustment (BA) leads to high computational costs in the incremental SfM workflow. Thus, this paper gives a systematic survey of the state-of-the-art for match pair selection from both ordered and unordered datasets, for outlier removal of initial matches dominated by outliers, and for efficiency improvement of BA, and conducts an experimental evaluation for six well-known SfM-based software packages on UAV image orientation.},
	language = {en},
	urldate = {2023-03-01},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Jiang, San and Jiang, Cheng and Jiang, Wanshou},
	month = sep,
	year = {2020},
	keywords = {Bundle adjustment, Divide-and-conquer, Match pair selection, Outlier removal, Structure-from-motion, Unmanned aerial vehicle},
	pages = {230--251},
}

@article{nevalainen_individual_2017,
	title = {Individual {Tree} {Detection} and {Classification} with {UAV}-{Based} {Photogrammetric} {Point} {Clouds} and {Hyperspectral} {Imaging}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/9/3/185},
	doi = {10.3390/rs9030185},
	abstract = {Small unmanned aerial vehicle (UAV) based remote sensing is a rapidly evolving technology. Novel sensors and methods are entering the market, offering completely new possibilities to carry out remote sensing tasks. Three-dimensional (3D) hyperspectral remote sensing is a novel and powerful technology that has recently become available to small UAVs. This study investigated the performance of UAV-based photogrammetry and hyperspectral imaging in individual tree detection and tree species classification in boreal forests. Eleven test sites with 4151 reference trees representing various tree species and developmental stages were collected in June 2014 using a UAV remote sensing system equipped with a frame format hyperspectral camera and an RGB camera in highly variable weather conditions. Dense point clouds were measured photogrammetrically by automatic image matching using high resolution RGB images with a 5 cm point interval. Spectral features were obtained from the hyperspectral image blocks, the large radiometric variation of which was compensated for by using a novel approach based on radiometric block adjustment with the support of in-flight irradiance observations. Spectral and 3D point cloud features were used in the classification experiment with various classifiers. The best results were obtained with Random Forest and Multilayer Perceptron (MLP) which both gave 95\% overall accuracies and an F-score of 0.93. Accuracy of individual tree identification from the photogrammetric point clouds varied between 40\% and 95\%, depending on the characteristics of the area. Challenges in reference measurements might also have reduced these numbers. Results were promising, indicating that hyperspectral 3D remote sensing was operational from a UAV platform even in very difficult conditions. These novel methods are expected to provide a powerful tool for automating various environmental close-range remote sensing tasks in the very near future.},
	language = {en},
	number = {3},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Nevalainen, Olli and Honkavaara, Eija and Tuominen, Sakari and Viljanen, Niko and Hakala, Teemu and Yu, Xiaowei and Hyyppä, Juha and Saari, Heikki and Pölönen, Ilkka and Imai, Nilton N. and Tommaselli, Antonio M. G.},
	month = mar,
	year = {2017},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {UAV, classification, forest, hyperspectral, photogrammetry, point cloud, radiometry},
	pages = {185},
}

@article{liu_hyperspectral_2020,
	title = {Hyperspectral imaging and {3D} technologies for plant phenotyping: {From} satellite to close-range sensing},
	volume = {175},
	issn = {0168-1699},
	shorttitle = {Hyperspectral imaging and {3D} technologies for plant phenotyping},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169919318848},
	doi = {10.1016/j.compag.2020.105621},
	abstract = {High-throughput phenotyping technologies in controlled environments or field conditions have proven to be extremely useful in unravelling key quantitative traits of plants for breeding. Among many plant phenotyping methods, hyperspectral imaging (HSI) and three-dimensional (3D) sensing are the fastest growing and promising approaches for measuring multiple plant parameters. There are many types of HSI and 3D sensors available with each being designed for a specific purpose. Also, the same sensor could be set up and calibrated in different ways to measure different plant parameters on various platforms. This review aims to guide the use of HSI and 3D sensing technologies for plant phenotyping. It first introduces the preliminary knowledge of HSI and 3D sensing for plant phenotyping. In addition, it provides the detail of plant phenotyping using different HSI and 3D sensors on various platforms with different scales. Lastly, the problems and challenges of close-range HSI and 3D modelling of plants are discussed and potential solutions are suggested.},
	language = {en},
	urldate = {2023-03-01},
	journal = {Computers and Electronics in Agriculture},
	author = {Liu, Huajian and Bruning, Brooke and Garnett, Trevor and Berger, Bettina},
	month = aug,
	year = {2020},
	keywords = {3D sensing, Hyperspectral imaging, Plant phenotyping, Remote sensing, Sensor fusion},
	pages = {105621},
}

@article{iglhaut_structure_2019,
	title = {Structure from {Motion} {Photogrammetry} in {Forestry}: a {Review}},
	volume = {5},
	issn = {2198-6436},
	shorttitle = {Structure from {Motion} {Photogrammetry} in {Forestry}},
	url = {https://doi.org/10.1007/s40725-019-00094-3},
	doi = {10.1007/s40725-019-00094-3},
	abstract = {The adoption of Structure from Motion photogrammetry (SfM) is transforming the acquisition of three-dimensional (3D) remote sensing (RS) data in forestry. SfM photogrammetry enables surveys with little cost and technical expertise. We present the theoretical principles and practical considerations of this technology and show opportunities that SfM photogrammetry offers for forest practitioners and researchers.},
	language = {en},
	number = {3},
	urldate = {2023-03-01},
	journal = {Current Forestry Reports},
	author = {Iglhaut, Jakob and Cabo, Carlos and Puliti, Stefano and Piermattei, Livia and O’Connor, James and Rosette, Jacqueline},
	month = sep,
	year = {2019},
	keywords = {Close-range photogrammetry (CRP), Forest health, Forest inventory, Point cloud, SfM, UAV},
	pages = {155--168},
}

@article{ahmad_review_2021,
	title = {A {Review} of {Crop} {Water} {Stress} {Assessment} {Using} {Remote} {Sensing}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/13/20/4155},
	doi = {10.3390/rs13204155},
	abstract = {Currently, the world is facing high competition and market risks in improving yield, crop illness, and crop water stress. This could potentially be addressed by technological advancements in the form of precision systems, improvements in production, and through ensuring the sustainability of development. In this context, remote-sensing systems are fully equipped to address the complex and technical assessment of crop production, security, and crop water stress in an easy and efficient way. They provide simple and timely solutions for a diverse set of ecological zones. This critical review highlights novel methods for evaluating crop water stress and its correlation with certain measurable parameters, investigated using remote-sensing systems. Through an examination of previous literature, technologies, and data, we review the application of remote-sensing systems in the analysis of crop water stress. Initially, the study presents the relationship of relative water content (RWC) with equivalent water thickness (EWT) and soil moisture crop water stress. Evapotranspiration and sun-induced chlorophyll fluorescence are then analyzed in relation to crop water stress using remote sensing. Finally, the study presents various remote-sensing technologies used to detect crop water stress, including optical sensing systems, thermometric sensing systems, land-surface temperature-sensing systems, multispectral (spaceborne and airborne) sensing systems, hyperspectral sensing systems, and the LiDAR sensing system. The study also presents the future prospects of remote-sensing systems in analyzing crop water stress and how they could be further improved.},
	language = {en},
	number = {20},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Ahmad, Uzair and Alvino, Arturo and Marino, Stefano},
	month = jan,
	year = {2021},
	note = {Number: 20
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {LiDAR, crop water stress, hyperspectral, multispectral, optical sensing, remote sensing, sentinel-1, soil moisture, thermometric sensing},
	pages = {4155},
}

@article{jurado_multispectral_2020,
	title = {Multispectral {Mapping} on {3D} {Models} and {Multi}-{Temporal} {Monitoring} for {Individual} {Characterization} of {Olive} {Trees}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/7/1106},
	doi = {10.3390/rs12071106},
	abstract = {3D plant structure observation and characterization to get a comprehensive knowledge about the plant status still poses a challenge in Precision Agriculture (PA). The complex branching and self-hidden geometry in the plant canopy are some of the existing problems for the 3D reconstruction of vegetation. In this paper, we propose a novel application for the fusion of multispectral images and high-resolution point clouds of an olive orchard. Our methodology is based on a multi-temporal approach to study the evolution of olive trees. This process is fully automated and no human intervention is required to characterize the point cloud with the reflectance captured by multiple multispectral images. The main objective of this work is twofold: (1) the multispectral image mapping on a high-resolution point cloud and (2) the multi-temporal analysis of morphological and spectral traits in two flight campaigns. Initially, the study area is modeled by taking multiple overlapping RGB images with a high-resolution camera from an unmanned aerial vehicle (UAV). In addition, a UAV-based multispectral sensor is used to capture the reflectance for some narrow-bands (green, near-infrared, red, and red-edge). Then, the RGB point cloud with a high detailed geometry of olive trees is enriched by mapping the reflectance maps, which are generated for every multispectral image. Therefore, each 3D point is related to its corresponding pixel of the multispectral image, in which it is visible. As a result, the 3D models of olive trees are characterized by the observed reflectance in the plant canopy. These reflectance values are also combined to calculate several vegetation indices (NDVI, RVI, GRVI, and NDRE). According to the spectral and spatial relationships in the olive plantation, segmentation of individual olive trees is performed. On the one hand, plant morphology is studied by a voxel-based decomposition of its 3D structure to estimate the height and volume. On the other hand, the plant health is studied by the detection of meaningful spectral traits of olive trees. Moreover, the proposed methodology also allows the processing of multi-temporal data to study the variability of the studied features. Consequently, some relevant changes are detected and the development of each olive tree is analyzed by a visual-based and statistical approach. The interactive visualization and analysis of the enriched 3D plant structure with different spectral layers is an innovative method to inspect the plant health and ensure adequate plantation sustainability.},
	language = {en},
	number = {7},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Jurado, J. M. and Ortega, L. and Cubillas, J. J. and Feito, F. R.},
	month = jan,
	year = {2020},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D olive tree models, heterogeneous data fusion, multi-temporal analysis, multispectral imaging, unmanned aerial vehicles},
	pages = {1106},
}

@article{nezami_tree_2020,
	title = {Tree {Species} {Classification} of {Drone} {Hyperspectral} and {RGB} {Imagery} with {Deep} {Learning} {Convolutional} {Neural} {Networks}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/7/1070},
	doi = {10.3390/rs12071070},
	abstract = {Interest in drone solutions in forestry applications is growing. Using drones, datasets can be captured flexibly and at high spatial and temporal resolutions when needed. In forestry applications, fundamental tasks include the detection of individual trees, tree species classification, biomass estimation, etc. Deep neural networks (DNN) have shown superior results when comparing with conventional machine learning methods such as multi-layer perceptron (MLP) in cases of huge input data. The objective of this research is to investigate 3D convolutional neural networks (3D-CNN) to classify three major tree species in a boreal forest: pine, spruce, and birch. The proposed 3D-CNN models were employed to classify tree species in a test site in Finland. The classifiers were trained with a dataset of 3039 manually labelled trees. Then the accuracies were assessed by employing independent datasets of 803 records. To find the most efficient set of feature combination, we compare the performances of 3D-CNN models trained with hyperspectral (HS) channels, Red-Green-Blue (RGB) channels, and canopy height model (CHM), separately and combined. It is demonstrated that the proposed 3D-CNN model with RGB and HS layers produces the highest classification accuracy. The producer accuracy of the best 3D-CNN classifier on the test dataset were 99.6\%, 94.8\%, and 97.4\% for pines, spruces, and birches, respectively. The best 3D-CNN classifier produced {\textasciitilde}5\% better classification accuracy than the MLP with all layers. Our results suggest that the proposed method provides excellent classification results with acceptable performance metrics for HS datasets. Our results show that pine class was detectable in most layers. Spruce was most detectable in RGB data, while birch was most detectable in the HS layers. Furthermore, the RGB datasets provide acceptable results for many low-accuracy applications.},
	language = {en},
	number = {7},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Nezami, Somayeh and Khoramshahi, Ehsan and Nevalainen, Olli and Pölönen, Ilkka and Honkavaara, Eija},
	month = jan,
	year = {2020},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D convolutional neural networks, deep learning, drone imagery, hyperspectral image classification, tree species classification},
	pages = {1070},
}

@article{jurado_impact_2020,
	title = {The {Impact} of {Canopy} {Reflectance} on the {3D} {Structure} of {Individual} {Trees} in a {Mediterranean} {Forest}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/9/1430},
	doi = {10.3390/rs12091430},
	abstract = {The characterization of 3D vegetation structures is an important topic, which has been addressed by recent research in remote sensing. The forest inventory requires the proper extraction of accurate structural and functional features of individual trees. This paper presents a novel methodology to study the impact of the canopy reflectance on the 3D tree structure. A heterogeneous natural environment in a Mediterranean forest, in which various tree species (pine, oak and eucalyptus) coexist, was covered using a high-resolution digital camera and a multispectral sensor. These devices were mounted on an Unmanned Aerial Vehicle (UAV) in order to observe the tree architecture and the spectral reflectance at the same time. The Structure from Motion (SfM) method was applied to model the 3D structures using RGB images from the high-resolution camera. The geometric accuracy of the resulting point cloud was validated by georeferencing the study area through multiple ground control points (GCPs). Then, the point cloud was enriched with the reflected light in four narrow-bands (green, near-infrared, red and red-edge). Furthermore, the Normalized Difference Vegetation Index (NDVI) was calculated in order to measure the tree vigor. A comprehensive analysis based on structural and spectral features of individual trees was proposed. A spatial segmentation was developed to detect single-trees in a forest and for each one to identify the crown and trunk. Consequently, structural parameters were extracted, such as the tree height, the diameter at breast height (DBH) and the crown volume. The validation of these measurements was performed by field data, which were taken using a Total Station (TS). In addition, these characteristics were correlated with the mean reflectance in the tree canopy. Regarding the observed tree species, a statistical analysis was carried out to study the impact of reflectance on the 3D tree structure. By applying our method, a more detailed knowledge of forest dynamics can be gained and the impact of available solar irradiance on single-trees can be analyzed.},
	language = {en},
	number = {9},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Jurado, J. M. and Ramos, M. I. and Enríquez, C. and Feito, F. R.},
	month = jan,
	year = {2020},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D forest structure, forest inventory, heterogeneous data fusion, multispectral imaging, spectral reflectance},
	pages = {1430},
}

@book{ruiz_multispectral_2019,
	title = {Multispectral {Registration}, {Undistortion} and {Tree} {Detection} for {Precision} {Agriculture}},
	isbn = {978-3-03868-093-2},
	url = {https://diglib.eg.org:443/xmlui/handle/10.2312/ceig20191209},
	abstract = {Multi-lens multispectral cameras allow us to record multispectral information for a whole area of terrain, even though we may only need the vegetation data. Based on the intensity of each multispectral image we can retrieve the contours of the trees that appear on the recorded terrain. However, multispectral cameras use a physically different lens for each range of wavelengths and misregistration effects could appear due to the different viewing positions. As these types of lenses are dedicated to capture larger areas of terrain, their focal distance is lower and because of this we get what is called a fisheye distortion. Therefore if we want to retrieve the shape of each tree and its multispectral data we need to process the channels so them all are representated as undistorted images under a same reference system.},
	language = {en},
	urldate = {2023-02-28},
	publisher = {The Eurographics Association},
	author = {Ruiz, Alfonso López and Rodríguez, Juan Manuel Jurado and Anguita, Carlos Javier Ogayar and Higueruela, Francisco Ramón Feito},
	year = {2019},
	doi = {10.2312/ceig.20191209},
	note = {Accepted: 2019-06-25T16:20:49Z},
}

@article{terentev_current_2022,
	title = {Current {State} of {Hyperspectral} {Remote} {Sensing} for {Early} {Plant} {Disease} {Detection}: {A} {Review}},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {Current {State} of {Hyperspectral} {Remote} {Sensing} for {Early} {Plant} {Disease} {Detection}},
	url = {https://www.mdpi.com/1424-8220/22/3/757},
	doi = {10.3390/s22030757},
	abstract = {The development of hyperspectral remote sensing equipment, in recent years, has provided plant protection professionals with a new mechanism for assessing the phytosanitary state of crops. Semantically rich data coming from hyperspectral sensors are a prerequisite for the timely and rational implementation of plant protection measures. This review presents modern advances in early plant disease detection based on hyperspectral remote sensing. The review identifies current gaps in the methodologies of experiments. A further direction for experimental methodological development is indicated. A comparative study of the existing results is performed and a systematic table of different plants’ disease detection by hyperspectral remote sensing is presented, including important wave bands and sensor model information.},
	language = {en},
	number = {3},
	urldate = {2023-03-01},
	journal = {Sensors},
	author = {Terentev, Anton and Dolzhenko, Viktor and Fedotov, Alexander and Eremenko, Danila},
	month = jan,
	year = {2022},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {cereals, citrus, early detection, hyperspectral, oil palm, plant diseases, remote sensing, solanaceae},
	pages = {757},
}

@inproceedings{feng_range_2016,
	title = {Range calibration of airborne profiling radar used in forest inventory},
	doi = {10.1109/IGARSS.2016.7730742},
	abstract = {In this paper, a range calibration and modulation sweep linearity verification method of a helicopter-borne FM-CW (Frequency-Modulated Continuous Waveform) profiling radar are presented. The radar is designed to collect backscatter signal waveforms to build target stand profiles. Various forest elements e.g. tree height; density; species, etc. can be evaluated through this profiling radar. This paper investigates the relation between target range and the received corresponding backscatter signal frequency and verifies the linearity of modulated RF (radio frequency) signal. The calibration result can be achieved based on a ground calibration test with restricted distance rather than the full measurement range. And the calibrated output can be directly adopted for airborne forestry inventory. The field test proves that the proposed calibration method is applicable for FMCW radar calibration using a centimeter level accurate fly trajectory with the help of Global Navigation Satellite System (GNSS) and IMU (Inertial Measurement Units) and a DEM (Digital Terrain Model) of the test field.},
	booktitle = {2016 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium} ({IGARSS})},
	author = {Feng, Ziyi and Chen, Yuwei and Hakala, Teemu and Hyyppä, Juha},
	month = jul,
	year = {2016},
	note = {ISSN: 2153-7003},
	keywords = {Backscatter, Calibration, Frequency measurement, Frequency modulation, Linearity, Remote Sensing, Spaceborne radar, airborne radar, frequency modulation linearity, range calibration},
	pages = {6672--6675},
}

@article{chauhan_remote_2019,
	title = {Remote sensing-based crop lodging assessment: {Current} status and perspectives},
	volume = {151},
	issn = {0924-2716},
	shorttitle = {Remote sensing-based crop lodging assessment},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271619300747},
	doi = {10.1016/j.isprsjprs.2019.03.005},
	abstract = {Rapid and quantitative assessment of crop lodging is important for understanding the causes of the phenomena, improving crop management, making better production and supporting loss estimates in general. Accurate information on the location and timing of crop lodging is valuable for farmers, agronomists, insurance loss adjusters, and policymakers. Lodging studies can be performed to assess the impact of lodging events or to model the risk of occurrence, both of which rely on information that can be acquired by field observations, from meteorological data and from remote sensing (RS). While studies applying RS data to assess crop lodging dates back three decades, there has been no comprehensive review of the status, potential, current approaches, and challenges in this domain. In this position paper, we review the trends in field/lab-based and RS-based studies for crop lodging assessment and discuss the strengths and weaknesses of current approaches. Theoretical background on crop lodging is presented, and the scope of RS in assessing plant characteristics associated with lodging is reviewed and discussed. The review focuses on RS-based studies, grouping them according to the platform deployed (i.e., ground-based, airborne and spaceborne), with an emphasis on analyzing the pros and cons of the technology. Finally, the challenges, research gaps, perspectives for future research, and an outlook on new sensors and platforms are presented to provide state-of-the-art and future scenarios of RS in lodging assessment. Our review reveals that the use of RS techniques in crop lodging assessment is still in an experimental stage. However, there is increasing interest within the RS scientific community (based on the increased rate of publications over time) to investigate its use for crop lodging detection and risk mapping. The existing satellite-based lodging assessment studies are very few, and the operational application of the current approaches over large spatial extents seems to be the biggest challenge. We identify opportunities for future studies that can develop quantitative models for estimating lodging severity and mapping lodging risk using RS data.},
	language = {en},
	urldate = {2023-03-01},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Chauhan, Sugandh and Darvishzadeh, Roshanak and Boschetti, Mirco and Pepe, Monica and Nelson, Andrew},
	month = may,
	year = {2019},
	keywords = {Airborne, Crop lodging, Lodging detection, Remote sensing, Risk mapping, Satellite},
	pages = {124--140},
}

@inproceedings{nieto_3d_2010,
	title = {{3D} geological modelling using laser and hyperspectral data},
	doi = {10.1109/IGARSS.2010.5651553},
	abstract = {This paper presents a ground based system for mapping the geology and the geometry of the environment remotely. The main objective of this work is to develop a framework for a mobile robotic platform that can build 3D geological maps. We investigate classification and registration algorithms that can work without any manual intervention. The system capabilities are demonstrated with data acquired from a working mine environment. Geological maps are built by applying classification techniques to hyperspectral images of the rocks' surface. The result from the classification is then fused with laser images to form the 3D geological models of the environment.},
	booktitle = {2010 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Nieto, Juan I. and Monteiro, Sildomar T. and Viejo, Diego},
	month = jul,
	year = {2010},
	note = {ISSN: 2153-7003},
	keywords = {Cameras, Geology, Hyperspectral imaging, Laser modes, Materials, Three dimensional displays},
	pages = {4568--4571},
}

@article{muller_super-resolution_2020,
	title = {Super-resolution of multispectral satellite images using convolutional neural networks},
	volume = {V-1-2020},
	issn = {2194-9050},
	url = {http://arxiv.org/abs/2002.00580},
	doi = {10.5194/isprs-annals-V-1-2020-33-2020},
	abstract = {Super-resolution aims at increasing image resolution by algorithmic means and has progressed over the recent years due to advances in the fields of computer vision and deep learning. Convolutional Neural Networks based on a variety of architectures have been applied to the problem, e.g. autoencoders and residual networks. While most research focuses on the processing of photographs consisting only of RGB color channels, little work can be found concentrating on multi-band, analytic satellite imagery. Satellite images often include a panchromatic band, which has higher spatial resolution but lower spectral resolution than the other bands. In the field of remote sensing, there is a long tradition of applying pan-sharpening to satellite images, i.e. bringing the multispectral bands to the higher spatial resolution by merging them with the panchromatic band. To our knowledge there are so far no approaches to super-resolution which take advantage of the panchromatic band. In this paper we propose a method to train state-of-the-art CNNs using pairs of lower-resolution multispectral and high-resolution pan-sharpened image tiles in order to create super-resolved analytic images. The derived quality metrics show that the method improves information content of the processed images. We compare the results created by four CNN architectures, with RedNet30 performing best.},
	urldate = {2023-03-01},
	journal = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Müller, M. U. and Ekhtiari, N. and Almeida, R. M. and Rieke, C.},
	month = aug,
	year = {2020},
	note = {arXiv:2002.00580 [cs, eess]},
	keywords = {68-06, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, I.4.3},
	pages = {33--40},
}

@article{adao_hyperspectral_2017,
	title = {Hyperspectral {Imaging}: {A} {Review} on {UAV}-{Based} {Sensors}, {Data} {Processing} and {Applications} for {Agriculture} and {Forestry}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {Hyperspectral {Imaging}},
	url = {https://www.mdpi.com/2072-4292/9/11/1110},
	doi = {10.3390/rs9111110},
	abstract = {Traditional imagery—provided, for example, by RGB and/or NIR sensors—has proven to be useful in many agroforestry applications. However, it lacks the spectral range and precision to profile materials and organisms that only hyperspectral sensors can provide. This kind of high-resolution spectroscopy was firstly used in satellites and later in manned aircraft, which are significantly expensive platforms and extremely restrictive due to availability limitations and/or complex logistics. More recently, UAS have emerged as a very popular and cost-effective remote sensing technology, composed of aerial platforms capable of carrying small-sized and lightweight sensors. Meanwhile, hyperspectral technology developments have been consistently resulting in smaller and lighter sensors that can currently be integrated in UAS for either scientific or commercial purposes. The hyperspectral sensors’ ability for measuring hundreds of bands raises complexity when considering the sheer quantity of acquired data, whose usefulness depends on both calibration and corrective tasks occurring in pre- and post-flight stages. Further steps regarding hyperspectral data processing must be performed towards the retrieval of relevant information, which provides the true benefits for assertive interventions in agricultural crops and forested areas. Considering the aforementioned topics and the goal of providing a global view focused on hyperspectral-based remote sensing supported by UAV platforms, a survey including hyperspectral sensors, inherent data processing and applications focusing both on agriculture and forestry—wherein the combination of UAV and hyperspectral sensors plays a center role—is presented in this paper. Firstly, the advantages of hyperspectral data over RGB imagery and multispectral data are highlighted. Then, hyperspectral acquisition devices are addressed, including sensor types, acquisition modes and UAV-compatible sensors that can be used for both research and commercial purposes. Pre-flight operations and post-flight pre-processing are pointed out as necessary to ensure the usefulness of hyperspectral data for further processing towards the retrieval of conclusive information. With the goal of simplifying hyperspectral data processing—by isolating the common user from the processes’ mathematical complexity—several available toolboxes that allow a direct access to level-one hyperspectral data are presented. Moreover, research works focusing the symbiosis between UAV-hyperspectral for agriculture and forestry applications are reviewed, just before the paper’s conclusions.},
	language = {en},
	number = {11},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Adão, Telmo and Hruška, Jonáš and Pádua, Luís and Bessa, José and Peres, Emanuel and Morais, Raul and Sousa, Joaquim João},
	month = nov,
	year = {2017},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {UAS, UAV, agriculture, agroforestry, forestry, hyperspectral, hyperspectral data processing, hyperspectral sensors},
	pages = {1110},
}

@article{honkavaara_band_2017,
	title = {Band registration of tuneable frame format hyperspectral {UAV} imagers in complex scenes},
	volume = {134},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271616305378},
	doi = {10.1016/j.isprsjprs.2017.10.014},
	abstract = {A recent revolution in miniaturised sensor technology has provided markets with novel hyperspectral imagers operating in the frame format principle. In the case of unmanned aerial vehicle (UAV) based remote sensing, the frame format technology is highly attractive in comparison to the commonly utilised pushbroom scanning technology, because it offers better stability and the possibility to capture stereoscopic data sets, bringing an opportunity for 3D hyperspectral object reconstruction. Tuneable filters are one of the approaches for capturing multi- or hyperspectral frame images. The individual bands are not aligned when operating a sensor based on tuneable filters from a mobile platform, such as UAV, because the full spectrum recording is carried out in the time-sequential principle. The objective of this investigation was to study the aspects of band registration of an imager based on tuneable filters and to develop a rigorous and efficient approach for band registration in complex 3D scenes, such as forests. The method first determines the orientations of selected reference bands and reconstructs the 3D scene using structure-from-motion and dense image matching technologies. The bands, without orientation, are then matched to the oriented bands accounting the 3D scene to provide exterior orientations, and afterwards, hyperspectral orthomosaics, or hyperspectral point clouds, are calculated. The uncertainty aspects of the novel approach were studied. An empirical assessment was carried out in a forested environment using hyperspectral images captured with a hyperspectral 2D frame format camera, based on a tuneable Fabry-Pérot interferometer (FPI) on board a multicopter and supported by a high spatial resolution consumer colour camera. A theoretical assessment showed that the method was capable of providing band registration accuracy better than 0.5-pixel size. The empirical assessment proved the performance and showed that, with the novel method, most parts of the band misalignments were less than the pixel size. Furthermore, it was shown that the performance of the band alignment was dependent on the spatial distance from the reference band.},
	language = {en},
	urldate = {2023-03-01},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Honkavaara, Eija and Rosnell, Tomi and Oliveira, Raquel and Tommaselli, Antonio},
	month = dec,
	year = {2017},
	keywords = {Geometry, Hyperspectral imaging, Photogrammetry, Registration, UAV},
	pages = {96--109},
}

@article{guimaraes_forestry_2020,
	title = {Forestry {Remote} {Sensing} from {Unmanned} {Aerial} {Vehicles}: {A} {Review} {Focusing} on the {Data}, {Processing} and {Potentialities}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {Forestry {Remote} {Sensing} from {Unmanned} {Aerial} {Vehicles}},
	url = {https://www.mdpi.com/2072-4292/12/6/1046},
	doi = {10.3390/rs12061046},
	abstract = {Currently, climate change poses a global threat, which may compromise the sustainability of agriculture, forestry and other land surface systems. In a changing world scenario, the economic importance of Remote Sensing (RS) to monitor forests and agricultural resources is imperative to the development of agroforestry systems. Traditional RS technologies encompass satellite and manned aircraft platforms. These platforms are continuously improving in terms of spatial, spectral, and temporal resolutions. The high spatial and temporal resolutions, flexibility and lower operational costs make Unmanned Aerial Vehicles (UAVs) a good alternative to traditional RS platforms. In the management process of forests resources, UAVs are one of the most suitable options to consider, mainly due to: (1) low operational costs and high-intensity data collection; (2) its capacity to host a wide range of sensors that could be adapted to be task-oriented; (3) its ability to plan data acquisition campaigns, avoiding inadequate weather conditions and providing data availability on-demand; and (4) the possibility to be used in real-time operations. This review aims to present the most significant UAV applications in forestry, identifying the appropriate sensors to be used in each situation as well as the data processing techniques commonly implemented.},
	language = {en},
	number = {6},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Guimarães, Nathalie and Pádua, Luís and Marques, Pedro and Silva, Nuno and Peres, Emanuel and Sousa, Joaquim J.},
	month = jan,
	year = {2020},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {LiDAR, fire monitoring, forest health monitoring, forest inventory, photogrammetry, post-fire monitoring, sensing payloads, tree species classification},
	pages = {1046},
}

@article{padua_uas_2017,
	title = {{UAS}, sensors, and data processing in agroforestry: a review towards practical applications},
	volume = {38},
	issn = {0143-1161},
	shorttitle = {{UAS}, sensors, and data processing in agroforestry},
	url = {https://doi.org/10.1080/01431161.2017.1297548},
	doi = {10.1080/01431161.2017.1297548},
	abstract = {The aim of this study is twofold: first, to present a survey of the actual and most advanced methods related to the use of unmanned aerial systems (UASs) that emerged in the past few years due to the technological advancements that allowed the miniaturization of components, leading to the availability of small-sized unmanned aerial vehicles (UAVs) equipped with Global Navigation Satellite Systems (GNSS) and high quality and cost-effective sensors; second, to advice the target audience – mostly farmers and foresters – how to choose the appropriate UAV and imaging sensor, as well as suitable approaches to get the expected and needed results of using technological tools to extract valuable information about agroforestry systems and its dynamics, according to their parcels’ size and crop’s types.Following this goal, this work goes beyond a survey regarding UAS and their applications, already made by several authors. It also provides recommendations on how to choose both the best sensor and UAV, in according with the required application. Moreover, it presents what can be done with the acquired sensors’ data through theuse of methods, procedures, algorithms and arithmetic operations. Finally, some recent applications in the agroforestry research area are presented, regarding the main goal of each analysed studies, the used UAV, sensors, and the data processing stage to reach conclusions.},
	number = {8-10},
	urldate = {2023-03-01},
	journal = {International Journal of Remote Sensing},
	author = {Pádua, Luís and Vanko, Jakub and Hruška, Jonáš and Adão, Telmo and Sousa, Joaquim J. and Peres, Emanuel and Morais, Raul},
	month = may,
	year = {2017},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01431161.2017.1297548},
	pages = {2349--2391},
}

@article{benson_model-based_2021,
	title = {Model-{Based} {Estimation} of {Forest} {Canopy} {Height} and {Biomass} in the {Canadian} {Boreal} {Forest} {Using} {Radar}, {LiDAR}, and {Optical} {Remote} {Sensing}},
	volume = {59},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2020.3018638},
	abstract = {One of the fundamental technical challenges of any new spaceborne vegetation remote sensing mission is the determination of what sensor(s) to place onboard and what, if any, overlapping modes of operation they will employ as each onboard sensor adds significant cost to the overall mission. In this article, the remote sensing of forest parameters using multimodal remote sensing is presented. In particular, polarimetric radar, Light Detection And Ranging (LiDAR), and near-IR passive optical sensing platforms are employed in conjunction with physics-based models. These models are used to accurately estimate forest aboveground biomass as well as canopy height in homogeneous areas. It is shown that this proposed method is capable of achieving high accuracy estimates while using minimal ancillary data in the estimation process. We present a method to combine measured data sets with our geometric and electromagnetic sensor models to develop a forest parameter estimation algorithm that fuses multimodal remote sensing technologies with a minimal amount of ground information and yields an accurate estimate of forest structure including dry biomass and canopy height with rms errors of 1.6 kg/m2 and 1.68 m respectively.},
	number = {6},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Benson, Michael L. and Pierce, Leland and Bergen, Kathleen and Sarabandi, Kamal},
	month = jun,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Biological system modeling, Biomass, Forestry, Landsat5 TM, Laser radar, Light Detection And Ranging (LiDAR), Michigan fractal-tree model (MFTM), Optical sensors, Remote sensing, Synthetic aperture radar, canopy height, feature estimation, fractal trees, remote sensing, simulation, slicer, synthetic aperture radar (SAR)},
	pages = {4635--4653},
}

@article{li_assessing_2017,
	title = {Assessing the {Utility} of {Uav}-{Borne} {Hyperspectral} {Image} and {Photogrammetry} {Derived} 3d {Data} for {Wetland} {Species} {Distribution} {Quick} {Mapping}},
	volume = {42W6},
	issn = {2194-9034       The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	url = {https://ui.adsabs.harvard.edu/abs/2017ISPAr42W6..209L},
	doi = {10.5194/isprs-archives-XLII-2-W6-209-2017},
	abstract = {Lightweight unmanned aerial vehicle (UAV) loaded with novel sensors offers a low cost and minimum risk solution for data acquisition in complex environment. This study assessed the performance of UAV-based hyperspectral image and digital surface model (DSM) derived from photogrammetric point clouds for 13 species classification in wetland area of Hong Kong. Multiple feature reduction methods and different classifiers were compared. The best result was obtained when transformed components from minimum noise fraction (MNF) and DSM were combined in support vector machine (SVM) classifier. Wavelength regions at chlorophyll absorption green peak, red, red edge and Oxygen absorption at near infrared were identified for better species discrimination. In addition, input of DSM data reduces overestimation of low plant species and misclassification due to the shadow effect and inter-species morphological variation. This study establishes a framework for quick survey and update on wetland environment using UAV system. The findings indicate that the utility of UAV-borne hyperspectral and derived tree height information provides a solid foundation for further researches such as biological invasion monitoring and bio-parameters modelling in wetland.},
	urldate = {2023-03-01},
	journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Li, Q. S. and Wong, F. K. K. and Fung, T.},
	month = aug,
	year = {2017},
	note = {ADS Bibcode: 2017ISPAr42W6..209L},
	pages = {209--215},
}

@article{pires_individual_2022,
	title = {Individual tree detection and estimation of stem attributes with mobile laser scanning along boreal forest roads},
	volume = {187},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271622000697},
	doi = {10.1016/j.isprsjprs.2022.03.004},
	abstract = {The collection of field-reference data is a key task in remote sensing-based forest inventories. However, traditional methods of collection demand extensive personnel resources. Thus, field-reference data collection would benefit from more automated methods. In this study, we proposed a method for individual tree detection (ITD) and stem attribute estimation based on a car-mounted mobile laser scanner (MLS) operating along forest roads. We assessed its performance in six ranges with increasing mean distance from the roadside. We used a Riegl VUX-1LR sensor operating with high repetition rate, thus providing detailed cross sections of the stems. The algorithm we propose was designed for this sensor configuration, identifying the cross sections (or arcs) in the point cloud and aggregating those into single trees. Furthermore, we estimated diameter at breast height (DBH), stem profiles, and stem volume for each detected tree. The accuracy of ITD, DBH, and stem volume estimates varied with the trees’ distance from the road. In general, the proximity to the sensor of branches 0–10 m from the road caused commission errors in ITD and over estimation of stem attributes in this zone. At 50–60 m from roadside, stems were often occluded by branches, causing omissions and underestimation of stem attributes in this area. ITD’s precision and sensitivity varied from 82.8\% to 100\% and 62.7\% to 96.7\%, respectively. The RMSE of DBH estimates ranged from 1.81 cm (6.38\%) to 4.84 cm (16.9\%). Stem volume estimates had RMSEs ranging from 0.0800 m3 (10.1\%) to 0.190 m3 (25.7\%), depending on the distance to the sensor. The average proportion of detected reference volume was highly affected by the performance of ITD in the different zones. This proportion was highest from 0 to 10 m (113\%), a zone that concentrated most ITD commission errors, and lowest from 50 to 60 m (66.6\%), mostly due to the omission errors in this area. In the other zones, the RMSE ranged from 87.5\% to 98.5\%. These accuracies are in line with those obtained by other state-of-the-art MLS and terrestrial laser scanner (TLS) methods. The car-mounted MLS system used has the potential to collect data efficiently in large-scale inventories, being able to scan approximately 80 ha of forests per day depending on the survey setup. This data collection method could be used to increase the amount of field-reference data available in remote sensing-based forest inventories, improve models for area-based estimations, and support precision forestry development.},
	language = {en},
	urldate = {2023-03-01},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Pires, Raul de Paula and Olofsson, Kenneth and Persson, Henrik Jan and Lindberg, Eva and Holmgren, Johan},
	month = may,
	year = {2022},
	keywords = {Automatic stem detection, Car-mounted, MLS, Stem diameter, Stem volume},
	pages = {211--224},
}

@article{rahlf_digital_2017,
	title = {Digital aerial photogrammetry can efficiently support large-area forest inventories in {Norway}},
	volume = {90},
	issn = {0015-752X},
	url = {https://doi.org/10.1093/forestry/cpx027},
	doi = {10.1093/forestry/cpx027},
	abstract = {The use of digital aerial photogrammetry (DAP) for forest inventory purposes has been widely studied and can produce comparable accuracy compared with airborne laser scanning (ALS) in small, homogeneous areas. However, the accuracy of DAP for large scale applications with heterogeneous terrain and forest vegetation has not yet been reported. In this study we examined the accuracy of timber volume, biomass and basal area prediction models based on DAP and national forest inventory (NFI) data on a large area in central Norway. Two separate point clouds were derived from aerial image acquisitions of 2010 and 2013. Vegetation heights were extracted by subtracting terrain elevation derived from ALS. A large number of NFI sample plots (483) measured between 2010 and 2014 were used as reference data to fit linear models for timber volume, biomass and basal area with height metrics derived from the DAP data as explanatory variables. Variables describing the heterogeneous environmental and image acquisition conditions were calculated and their influence on the model accuracy was tested. The results showed that forest parameter prediction using DAP works well when applied to a large area. The model fits of the timber volume, biomass and basal area models were good with R2 of 0.80, 0.81, 0.81 and RMSEs of 41.43 m3 ha−1 (55\% of the mean observed value), 32.49 t ha−1 (47\%), 5.19 m2 ha−1 (41\%), respectively. Only a small proportion of the variation could be attributed to the heterogeneous conditions. The inclusion of the relative sun inclination led to an improvement of the model RMSEs by 2\% of the mean observed values. The relatively low cost and stability across large areas make DAP an attractive source of auxiliary information for large scale forest inventories.},
	number = {5},
	urldate = {2023-03-01},
	journal = {Forestry: An International Journal of Forest Research},
	author = {Rahlf, Johannes and Breidenbach, Johannes and Solberg, Svein and Næsset, Erik and Astrup, Rasmus},
	month = dec,
	year = {2017},
	pages = {710--718},
}

@article{su_spatial_2016,
	title = {Spatial distribution of forest aboveground biomass in {China}: {Estimation} through combination of spaceborne lidar, optical imagery, and forest inventory data},
	volume = {173},
	issn = {0034-4257},
	shorttitle = {Spatial distribution of forest aboveground biomass in {China}},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425715302236},
	doi = {10.1016/j.rse.2015.12.002},
	abstract = {The global forest ecosystem, which acts as a large carbon sink, plays an important role in modeling the global carbon balance. An accurate estimation of the total forest carbon stock in the aboveground biomass (AGB) is therefore necessary for improving our understanding of carbon dynamics, especially against the background of global climate change. The forest area of China is among the top five globally. However, because of limitations in forest AGB mapping methods and the availability of ground inventory data, there is still a lack in the nationwide wall-to-wall forest AGB estimation map for China. In this study, we collected over 8000 ground inventory records from published literatures, and developed an AGB mapping method using a combination of these ground inventory data, Geoscience Laser Altimeter System (GLAS)/Ice, Cloud, and Land Elevation Satellite (ICESat) data, optical imagery, climate surfaces, and topographic data. An uncertainty field model was introduced into the forest AGB mapping procedure to minimize the influence of plot location uncertainty. Our nationwide wall-to-wall forest AGB mapping results show that the forest AGB density in China is 120Mg/ha on average, with a standard deviation of 61Mg/ha. Evaluation with an independent ground inventory dataset showed that our proposed method can accurately map wall-to-wall forest AGB across a large landscape. The adjusted coefficient of determination (R2) and root-mean-square error between our predicted results and the validation dataset were 0.75 and 42.39Mg/ha, respectively. This new method and the resulting nationwide wall-to-wall forest AGB map will help to improve the accuracy of carbon dynamic predictions in China.},
	language = {en},
	urldate = {2023-03-01},
	journal = {Remote Sensing of Environment},
	author = {Su, Yanjun and Guo, Qinghua and Xue, Baolin and Hu, Tianyu and Alvarez, Otto and Tao, Shengli and Fang, Jingyun},
	month = feb,
	year = {2016},
	keywords = {China, Forest aboveground biomass, GLAS/ICESat, Ground inventory, Lidar},
	pages = {187--199},
}

@article{weiser_impact_2022,
	title = {Impact of {Volcanic} {Sulfur} {Emissions} on the {Pine} {Forest} of {La} {Palma}, {Spain}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-4907},
	url = {https://www.mdpi.com/1999-4907/13/2/299},
	doi = {10.3390/f13020299},
	abstract = {In autumn 2021, the largest volcanic eruption on the island of La Palma in historic records took place. The Canary Islands are of volcanic origin and eruptions have always constituted part of their natural disturbance regime. Until recently, their impacts could not be directly observed and studied. Influence of the emission of phytotoxic gases on biodiversity and ecosystem dynamics was hitherto unknown. The recent eruption is still being intensely monitored. We used Sentinel-2 remote sensing data to analyze the spatial extent and intensity of the impact related to sulfuric emissions, aiming to understand the damage patterns in Canary pine forest. The emissions damaged 10\% of that forest and affected 5.3\% of the Natura 2000 protected areas. We concluded that this is largely due to the toxic effects of the enormous emissions of SO2. We found a clear correlation between the change in the normalized difference vegetation index (NDVI) and distance from the eruption. This pattern was weakly anisotropic, with stronger damage in southern directions. Counteracting effects, such as ash deposition, were largely excluded by combining NDVI change detection with tree cover density. We expect that vegetation damage will be transient. P. canariensis can resprout after forest fires, where most leaves are lost. Consequently, our assessment can serve as a reference for future ecosystem regeneration.},
	language = {en},
	number = {2},
	urldate = {2023-03-01},
	journal = {Forests},
	author = {Weiser, Frank and Baumann, Esther and Jentsch, Anke and Medina, Félix Manuel and Lu, Meng and Nogales, Manuel and Beierkuhnlein, Carl},
	month = feb,
	year = {2022},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {\textit{Pinus canariensis}, Cumbre Vieja, Sentinel-2, chlorosis, natural disturbances, natural pollution, oceanic island, pine forest, sulfur, volcanic eruption, volcanism},
	pages = {299},
}

@article{jurado_efficient_2021,
	title = {An {Efficient} {Method} for {Generating} {UAV}-{Based} {Hyperspectral} {Mosaics} {Using} {Push}-{Broom} {Sensors}},
	volume = {14},
	issn = {2151-1535},
	doi = {10.1109/JSTARS.2021.3088945},
	abstract = {Hyperspectral sensors mounted in unmanned aerial vehicles offer new opportunities to explore high-resolution multitemporal spectral analysis in remote sensing applications. Nevertheless, the use of hyperspectral data still poses challenges mainly in postprocessing to correct from high geometric deformation of images. In general, the acquisition of high-quality hyperspectral imagery is achieved through a time-consuming and complex processing workflow. However, this effort is mandatory when using hyperspectral imagery in a multisensor data fusion perspective, such as with thermal infrared imagery or photogrammetric point clouds. Push-broom hyperspectral sensors provide high spectral resolution data, but its scanning acquisition architecture imposes more challenges to create geometrically accurate mosaics from multiple hyperspectral swaths. In this article, an efficient method is presented to correct geometrical distortions on hyperspectral swaths from push-broom sensors by aligning them with an RGB photogrammetric orthophoto mosaic. The proposed method is based on an iterative approach to align hyperspectral swaths with an RGB photogrammetric orthophoto mosaic. Using as input preprocessed hyperspectral swaths, apart from the need of introducing some control points, the workflow is fully automatic and consists of: adaptive swath subdivision into multiple fragments; detection of significant image features; estimation of valid matches between individual swaths and the RGB orthophoto mosaic; and calculation of the best geometric transformation model to the retrieved matches. As a result, geometrical distortions of hyperspectral swaths are corrected and an orthomosaic is generated. This methodology provides an expedite solution able to produce a hyperspectral mosaic with an accuracy ranging from two to five times the ground sampling distance of the high-resolution RGB orthophoto mosaic, enabling the hyperspectral data integration with data from other sensors for multiple applications.},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Jurado, Juan M. and Pádua, Luís and Hruška, Jonas and Feito, Francisco R. and Sousa, Joaquim J.},
	year = {2021},
	note = {Conference Name: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	keywords = {Earth, Feature extraction, Hyperspectral imaging, Image sensors, Sensors, Software, Spatial resolution, mosaicking, push-broom sensor, unmanned aerial vehicles (UAVs)},
	pages = {6515--6531},
}

@inproceedings{debevec_efficient_1998,
	address = {Vienna},
	series = {Eurographics},
	title = {Efficient {View}-{Dependent} {Image}-{Based} {Rendering} with {Projective} {Texture}-{Mapping}},
	isbn = {978-3-7091-6453-2},
	doi = {10.1007/978-3-7091-6453-2_10},
	abstract = {This paper presents how the image-based rendering technique ofview dependent texture-mapping (VDTM) can be efficiently implementedusing projective texture mapping, a feature commonly available inpolygon graphics hardware. VDTM is a technique for generating novelviews of a scene with approximately known geometry making maximal useof a sparse set of original views. The original presentation of VDTMby Debevec, Taylor, and Malik required significant perpixelcomputation and did not scale well with the number of original images.In our technique, we precompute for each polygon the set of originalimages in which it is visibile and create a “view map” datastructure that encodes the best texture map to use for a regularlysampled set of possible viewing directions. To generate a novel view,the view map for each polygon is queried to determine a set of no morethan three original images to blend together in order to render thepolygon with projective texture-mapping. Invisible triangles areshaded using an object-space hole-filling method. We show how therendering process can be streamlined for implementation on standardpolygon graphics hardware. We present results of using the method torender a large-scale model of the Berkeley bell tower and itssurrounding campus enironment.},
	language = {en},
	booktitle = {Rendering {Techniques} ’98},
	publisher = {Springer},
	author = {Debevec, Paul and Yu, Yizhou and Borshukov, George},
	editor = {Drettakis, George and Max, Nelson},
	year = {1998},
	keywords = {Camera Position, Graphic Hardware, Original View, Texture Mapping, Vertex Color},
	pages = {105--116},
}

@misc{noauthor_sen2cor_nodate,
	title = {{Sen2Cor} – {STEP}},
	url = {https://step.esa.int/main/snap-supported-plugins/sen2cor/},
	language = {en-US},
	urldate = {2023-02-28},
}

@inproceedings{everitt_interactive_2001,
	title = {Interactive {Order}-{Independent} {Transparency}},
	url = {https://www.semanticscholar.org/paper/Interactive-Order-Independent-Transparency-Everitt/99b8940b5a6dab8527198e966c0eb7e2a02ee28c},
	abstract = {Correctly rendering non-refractive transparent surfaces with core OpenGL functionality [9] has the vexing requirements of depth-sorted traversal and nonintersecting polygons. This is frustrating for most application developers using OpenGL because the natural order of scene traversal (usually one object at a time) rarely satisfies these requirements. Objects can be complex, with their own transformation hierarchies. Even more troublesome, with advanced graphics hardware, the vertices and fragments of objects may be altered by user-defined per-vertex or per-fragment operations within the GPU. When these features are employed, it becomes intractable to guarantee that fragments will arrive in sorted order for each pixel. The technique presented here solves the problem of order dependence by using a technique we call depth peeling. Depth peeling is a fragment-level depth sorting technique described by Mammen using Virtual Pixel Maps [7] and by Diefenbach using a dual depth buffer [3]. Though no dual depth buffer hardware fitting Diefenbach’s description exists, Bastos observed that shadow mapping hardware in conjunction with alpha test can be used to achieve the same effect [2]. Using this variation of depth peeling, each unique depth in the scene is extracted into layers, and the layers are composited in depth-sorted order to produce the correctly blended final image. The peeling of a layer requires a single order-independent pass over the scene. Figure 1 contrasts correct and incorrect rendering of transparent surfaces. (a) (b)},
	urldate = {2023-02-28},
	author = {Everitt, C.},
	year = {2001},
}

@book{dachsbacher_translucent_2003,
	title = {Translucent {Shadow} {Maps}},
	isbn = {978-3-905673-03-6},
	url = {https://diglib.eg.org:443/xmlui/handle/10.2312/EGWR.EGWR03.197-201},
	abstract = {Shadow maps are a very efficient means to add shadows to arbitrary scenes. In this paper, we introduce Translucent Shadow Maps, an extension to shadow maps which allows very efficient rendering of sub-surface scattering. Translucent Shadow Maps contain depth and incident light information. Sub-surface scattering is computed on-the-fly during rendering by filtering the shadow map neighborhood. This filtering is done efficiently using a hierarchical approach. We describe optimizations for an implementation of Translucent Shadow Maps on contemporary graphics hardware, that can render complex translucent objects with varying light and material properties in real-time.},
	language = {en},
	urldate = {2023-02-28},
	publisher = {The Eurographics Association},
	author = {Dachsbacher, Carsten and Stamminger, Marc},
	year = {2003},
	doi = {10.2312/EGWR/EGWR03/197-201},
	note = {Accepted: 2014-01-27T14:22:49Z
ISSN: 1727-3463},
}

@misc{gani_multispectral_2021,
	title = {Multispectral {Object} {Detection} with {Deep} {Learning}},
	url = {http://arxiv.org/abs/2102.03115},
	doi = {10.48550/arXiv.2102.03115},
	abstract = {Object detection in natural scenes can be a challenging task. In many real-life situations, the visible spectrum is not suitable for traditional computer vision tasks. Moving outside the visible spectrum range, such as the thermal spectrum or the near-infrared (NIR) images, is much more beneficial in low visibility conditions, NIR images are very helpful for understanding the object's material quality. In this work, we have taken images with both the Thermal and NIR spectrum for the object detection task. As multi-spectral data with both Thermal and NIR is not available for the detection task, we needed to collect data ourselves. Data collection is a time-consuming process, and we faced many obstacles that we had to overcome. We train the YOLO v3 network from scratch to detect an object from multi-spectral images. Also, to avoid overfitting, we have done data augmentation and tune hyperparameters.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Gani, Md Osman and Kuiry, Somenath and Das, Alaka and Nasipuri, Mita and Das, Nibaran},
	month = feb,
	year = {2021},
	note = {arXiv:2102.03115 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{qadeer_spatio-temporal_2021,
	title = {Spatio-{Temporal} {Crop} {Classification} {On} {Volumetric} {Data}},
	doi = {10.1109/ICIP42928.2021.9506046},
	abstract = {Large-area crop classification using multi-spectral imagery is a widely studied problem for several decades and is generally addressed using classical Random Forest classifier. Recently, deep convolutional neural networks (DCNN) have been proposed. However, these methods only achieved results comparable with Random Forest. In this work, we present a novel CNN based architecture for large-area crop classification. Our methodology combines both spatio-temporal analysis via 3D CNN as well as temporal analysis via 1D CNN. We evaluated the efficacy of our approach on Yolo and Imperial county benchmark datasets. Our combined strategy outperforms both classical as well as recent DCNN based methods in terms of classification accuracy by 2\% while maintaining a minimum number of parameters and the lowest inference time.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Qadeer, Muhammad Usman and Saeed, Salar and Taj, Murtaza and Muhammad, Abubakr},
	month = sep,
	year = {2021},
	note = {ISSN: 2381-8549},
	keywords = {Benchmark testing, CNN, Conferences, Crop Classification, Developing countries, Image processing, Satellite data, Satellites, Solid modeling, Three-dimensional displays},
	pages = {3812--3816},
}

@article{lombardo_temperature_2020,
	title = {Temperature and {Emissivity} {Separation} ‘{Draping}’ {Algorithm} {Applied} to {Hyperspectral} {Infrared} {Data}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/12/2046},
	doi = {10.3390/rs12122046},
	abstract = {In the presented work, the spectral emissivity of basaltic melts at magmatic temperatures was retrieved in a laboratory-controlled experiment by measuring their spectral radiance. Granulated bombs of Etnean basalts were melted and the radiant energy from the melting surface was recorded by a portable spectroradiometer in the short wavelength infrared (SWIR) spectral range between 1500 and 2500 nm. The Draping algorithm, an improved algorithm for temperature and emissivity separation, was applied for the first time to SWIR hyperspectral data in order to take into account the non-uniform temperature distribution of the melt surface and, at the same time, solving the two temperatures and the spectral emissivity. The results have been validated by comparing our results with the emissivity measured at a "lava simulator". Basalt spectral emissivity does not vary significantly at magmatic temperature, but shows an absorption feature in the range 2180–2290 nm, an atmospheric window pivotal for the IR remote sensing of active volcanoes.},
	language = {en},
	number = {12},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Lombardo, Valerio and Pick, Leonie and Spinetti, Claudia and Tadeucci, Jacopo and Zakšek, Klemen},
	month = jan,
	year = {2020},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {draping, lab experiments, temperature–emissivity separation, thermal remote sensing},
	pages = {2046},
}

@article{chan_drone_2021,
	title = {The {Drone} {Based} {Hyperspectral} {Imaging} {System} for {Precision} {Agriculture}},
	copyright = {Copyright (c) 2021},
	url = {https://www.nveo.org/index.php/journal/article/view/1703},
	abstract = {Precision agriculture using remote sensing can be traced back to 1980’s with various handheld sensors as well as sensors mounted on satellite and airborne platform. Precision agriculture or precision farming is a farming management concept which combines conventional farming method with new technologies such as satellite/radar imaginary and information technology. Unlike conventional farming, precision agriculture integrates a suite of technological tools such as radar images, agriculture monitoring system, and high technology machines into farming practice. The aim is to minimize the crops production costs and maximize the production outputs all in benefiting the farmers. In this project, an experimental study on disease detection and nutrient extraction of plantation such as oil palm n Malaysia will be carried out using a drone based hyperspectral imaging system. In this project, an experimental study on disease detection and nutrient extraction of plantation such as oil palm in Malaysia will be carried out using a drone based hyperspectral imaging system. The major advantages of this hyperspectral sensors are light weight (approximate 250g for sensor) and larger number of band selection (from 600nm to 1000nm) compare to conventional multispectral camera. Therefore, it enables the usage of smaller airborne platform and further reduce the development cost and operation cost. A customised drone has been designed and developed to carry the hyperspectral imaging system. Preliminary testing has been performed in laboratory and oil palm plantation to verify both multirotor drone and hyperspectral imaging system. Initial results show that the hyperspectral data are suitable to be used for differentiation of the healthiness level of the oil palm plantation. In order to achieve the target of disease detection and nutrient extraction, timely ground truth data will be collected together with the field experiment using drone based hyperspectral camera. Data analysis on the obtained ground truth data need to be carried out to study the correlation properties of the hyperspectral images with various oil palm plantation condition. This research is supported by MOSTI International Collaboration Fund, IF0719A1102.},
	language = {en},
	urldate = {2023-03-01},
	journal = {NVEO - NATURAL VOLATILES \& ESSENTIAL OILS Journal {\textbar} NVEO},
	author = {Chan, Yee Kit and Koo, Voon Chet and Choong, Edmund Hou Kheat and Lim, Chee Siong},
	month = nov,
	year = {2021},
	pages = {5561--5573},
}

@article{sankey_uav_2018,
	title = {{UAV} hyperspectral and lidar data and their fusion for arid and semi-arid land vegetation monitoring},
	volume = {4},
	issn = {2056-3485},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rse2.44},
	doi = {10.1002/rse2.44},
	abstract = {Unmanned aerial vehicles (UAVs) provide a new research tool to obtain high spatial and temporal resolution imagery at a reduced cost. Rapid advances in miniature sensor technology are leading to greater potentials for ecological research. We demonstrate one of the first applications of UAV lidar and hyperspectral imagery and a fusion method for individual plant species identification and 3D characterization at submeter scales in south-eastern Arizona, USA. The UAV lidar scanner characterized the individual vegetation canopy structure and bare ground elevation, whereas the hyperspectral sensor provided species-specific spectral signatures for the dominant and target species at our study area in leaf-on condition. We hypothesized that the fusion of the two different data sources would perform better than either data type alone in the arid and semi-arid ecosystems with sparse vegetation. The fusion approach provides 84–89\% overall accuracy (kappa values of 0.80–0.86) in target species classification at the canopy scale, leveraging a wide range of target spectral responses in the hyperspectral data and a high point density (50 points/m2) in the lidar data. In comparison, the hyperspectral image classification alone produced 72–76\% overall accuracies (kappa values of 0.70 and 0.71). The UAV lidar-derived digital elevation model (DEM) is also strongly correlated with manned airborne lidar-derived DEM (R2 = 0.98 and 0.96), but was obtained at a lower cost. The lidar and hyperspectral data as well as the fusion method demonstrated here can be widely applied across a gradient of vegetation and topography to monitor and detect ecological changes at a local scale.},
	language = {en},
	number = {1},
	urldate = {2023-02-28},
	journal = {Remote Sensing in Ecology and Conservation},
	author = {Sankey, Temuulen T. and McVay, Jason and Swetnam, Tyson L. and McClaran, Mitchel P. and Heilman, Philip and Nichols, Mary},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rse2.44},
	keywords = {3D modeling, UAV, high-resolution DEM, lidar, species identification, vegetation monitoring},
	pages = {20--33},
}

@inproceedings{cunha_prediction_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Prediction of the {Vigor} and {Health} of {Peach} {Tree} {Orchard}},
	isbn = {978-3-030-86970-0},
	doi = {10.1007/978-3-030-86970-0_38},
	abstract = {New technologies are a great support for decision-making for agricultural producers. An example is the analysis of orchards by way of digital image processing. The processing of multispectral images captured by drones allows the evaluation of the health or vigor of the fruit trees. This work presents a proposal to evaluate the vigor and health of trees in a peach orchard using multispectral images, an algorithm for segmentation of trees canopy, and application of vegetable indexes. For canopy segmentation, the Faster R-CNN convolutional neural network model was used. To predict the health of the peach trees, the vegetable indexes NDVI, GNDVI, NDRE, and REGNDVI were calculated. The values of the NDVI, GNDVI, NDRE, REGNDVI indexes obtained for the healthiest tree were 0.94, 0.86, 0.58, and 0.57, respectively. With the application of this method, it was possible to conclude that the use of multispectral images together with image processing algorithms, artificial intelligence, and plant indexes, allows providing relevant information about the vigor or health of the cultures serving to support the decision making in agricultural activities, helping the optimization of resources, reduction of time and cost, maximizing production, facilitating the work of agricultural explorers.},
	language = {en},
	booktitle = {Computational {Science} and {Its} {Applications} – {ICCSA} 2021},
	publisher = {Springer International Publishing},
	author = {Cunha, João and Gaspar, Pedro D. and Assunção, Eduardo and Mesquita, Ricardo},
	editor = {Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Garau, Chiara and Blečić, Ivan and Taniar, David and Apduhan, Bernady O. and Rocha, Ana Maria A. C. and Tarantino, Eufemia and Torre, Carmelo Maria},
	year = {2021},
	keywords = {Canopy segmentation, Faster R-CNN, Health, Image processing, Multispectral images, Vegetation indexes, Vigor},
	pages = {541--551},
}

@book{rodriguez_spectral_2021,
	title = {Spectral characterization and semantic segmentation of complex {3D} models in natural environments},
	copyright = {Licencia Reconocimiento-NoComercial-SinObraDerivada 3.0 España},
	isbn = {978-84-9159-379-9},
	url = {http://ruja.ujaen.es/jspui/handle/10953/1046},
	abstract = {Esta tesis doctoral presenta sus principales contribuciones en la caracterización de escenarios 3D a partir 
de imágenes multiespectrales y la segmentación semántica de nubes de puntos. Como resultado de esta 
investigación, se ha desarrollado el software GEU (Geospatial and Envi-ronmental tools of University of 
Jaén) que integra el conjunto de métodos desarrollados. Dicha tesis doctoral es presentada por 
compendio de publicaciones. La primera parte de la tesis se centra en la fusión de variables espectrales 
y morfológicas de estructuras arbóreas que han sido reconstruidas del mundo real. Como áreas de estudio se utilizan una plantación de olivar y una zona forestal. La segunda parte de la tesis se centra en la explotación del conjunto de características intrínsecas a los modelos 3D con el fin de proponer avances 
en la segmentación semántica de distintos tipos de materiales y el reconocimiento de patrones 
geométricos en viñedos.},
	language = {eng},
	urldate = {2023-03-01},
	publisher = {Jaén : Universidad de Jaén},
	author = {Rodríguez, Jurado and Manuel, Juan},
	month = jan,
	year = {2021},
	note = {Accepted: 2021-01-19T11:59:29Z},
}

@inproceedings{ferrera_hyperspectral_2021,
	title = {Hyperspectral {3D} {Mapping} of {Underwater} {Environments}},
	doi = {10.1109/ICCVW54120.2021.00413},
	abstract = {Hyperspectral imaging has been increasingly used for underwater survey applications over the past years. As many hyperspectral cameras work as push-broom scanners, their use is usually limited to the creation of photo-mosaics based on a flat surface approximation and by interpolating the camera pose from dead-reckoning navigation. Yet, because of drift in the navigation and the mostly wrong flat surface assumption, the quality of the obtained photo-mosaics is often too low to support adequate analysis. In this paper we present an initial method for creating hyper-spectral 3D reconstructions of underwater environments. By fusing the data gathered by a classical RGB camera, an inertial navigation system and a hyperspectral push- broom camera, we show that the proposed method creates highly accurate 3D reconstructions with hyperspectral textures. We propose to combine techniques from simultaneous localization and mapping, structure-from-motion and 3D reconstruction and advantageously use them to create 3D models with hyperspectral texture, allowing us to overcome the flat surface assumption and the classical limitation of dead-reckoning navigation.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCVW})},
	author = {Ferrera, Maxime and Arnaubec, Aurélien and Istenič, Klemen and Gracias, Nuno and Bajjouk, Touria},
	month = oct,
	year = {2021},
	note = {ISSN: 2473-9944},
	keywords = {Cameras, Dead reckoning, Production, Simultaneous localization and mapping, Solid modeling, Surface reconstruction, Three-dimensional displays},
	pages = {3696--3705},
}

@article{yao_unmanned_2019,
	title = {Unmanned {Aerial} {Vehicle} for {Remote} {Sensing} {Applications}—{A} {Review}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/11/12/1443},
	doi = {10.3390/rs11121443},
	abstract = {The unmanned aerial vehicle (UAV) sensors and platforms nowadays are being used in almost every application (e.g., agriculture, forestry, and mining) that needs observed information from the top or oblique views. While they intend to be a general remote sensing (RS) tool, the relevant RS data processing and analysis methods are still largely ad-hoc to applications. Although the obvious advantages of UAV data are their high spatial resolution and flexibility in acquisition and sensor integration, there is in general a lack of systematic analysis on how these characteristics alter solutions for typical RS tasks such as land-cover classification, change detection, and thematic mapping. For instance, the ultra-high-resolution data (less than 10 cm of Ground Sampling Distance (GSD)) bring more unwanted classes of objects (e.g., pedestrian and cars) in land-cover classification; the often available 3D data generated from photogrammetric images call for more advanced techniques for geometric and spectral analysis. In this paper, we perform a critical review on RS tasks that involve UAV data and their derived products as their main sources including raw perspective images, digital surface models, and orthophotos. In particular, we focus on solutions that address the “new” aspects of the UAV data including (1) ultra-high resolution; (2) availability of coherent geometric and spectral data; and (3) capability of simultaneously using multi-sensor data for fusion. Based on these solutions, we provide a brief summary of existing examples of UAV-based RS in agricultural, environmental, urban, and hazards assessment applications, etc., and by discussing their practical potentials, we share our views in their future research directions and draw conclusive remarks.},
	language = {en},
	number = {12},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Yao, Huang and Qin, Rongjun and Chen, Xiaoyu},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {UAVs, data analysis, remote sensing applications},
	pages = {1443},
}

@article{heckbert_survey_1986,
	title = {Survey of {Texture} {Mapping}},
	volume = {6},
	issn = {1558-1756},
	doi = {10.1109/MCG.1986.276672},
	abstract = {Texture mapping is one of the most successful new techniques in high-quality image synthesis. It can enchance the visual richness of raster-scan images immensely while entailing only a relatively smann increase in computation. The technique has been applied to a number of surface attributes: surface color, surface normal, specularity, transparency, illumination, and surface displacement-to name a few. Although the list is potentially endless, the techniques of texture mapping are essentially the same in all cases. This article surveys the fundamentals of texture mapping, which can be spilt into two topics: the geometric mapping that warps a texture onto a surface, and the filtering necessary to avoid aliasing. An extensive bibliography is included.},
	number = {11},
	journal = {IEEE Computer Graphics and Applications},
	author = {Heckbert, Paul S.},
	month = nov,
	year = {1986},
	note = {Conference Name: IEEE Computer Graphics and Applications},
	keywords = {Image texture, Lighting, Optical reflection, Optical surface waves, Rough surfaces, Surface roughness, Surface texture, Surface waves, Terminology, Vegetation mapping},
	pages = {56--67},
}

@article{padua_effectiveness_2020,
	title = {Effectiveness of {Sentinel}-2 in {Multi}-{Temporal} {Post}-{Fire} {Monitoring} {When} {Compared} with {UAV} {Imagery}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2220-9964},
	url = {https://www.mdpi.com/2220-9964/9/4/225},
	doi = {10.3390/ijgi9040225},
	abstract = {Unmanned aerial vehicles (UAVs) have become popular in recent years and are now used in a wide variety of applications. This is the logical result of certain technological developments that occurred over the last two decades, allowing UAVs to be equipped with different types of sensors that can provide high-resolution data at relatively low prices. However, despite the success and extraordinary results achieved by the use of UAVs, traditional remote sensing platforms such as satellites continue to develop as well. Nowadays, satellites use sophisticated sensors providing data with increasingly improving spatial, temporal and radiometric resolutions. This is the case for the Sentinel-2 observation mission from the Copernicus Programme, which systematically acquires optical imagery at high spatial resolutions, with a revisiting period of five days. It therefore makes sense to think that, in some applications, satellite data may be used instead of UAV data, with all the associated benefits (extended coverage without the need to visit the area). In this study, Sentinel-2 time series data performances were evaluated in comparison with high-resolution UAV-based data, in an area affected by a fire, in 2017. Given the 10-m resolution of Sentinel-2 images, different spatial resolutions of the UAV-based data (0.25, 5 and 10 m) were used and compared to determine their similarities. The achieved results demonstrate the effectiveness of satellite data for post-fire monitoring, even at a local scale, as more cost-effective than UAV data. The Sentinel-2 results present a similar behavior to the UAV-based data for assessing burned areas.},
	language = {en},
	number = {4},
	urldate = {2023-03-01},
	journal = {ISPRS International Journal of Geo-Information},
	author = {Pádua, Luís and Guimarães, Nathalie and Adão, Telmo and Sousa, António and Peres, Emanuel and Sousa, Joaquim J.},
	month = apr,
	year = {2020},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Parrot SEQUOIA, Sentinel-2A, fire severity mapping, forest regeneration, multispectral imagery, post-fire management, unmanned aerial vehicles},
	pages = {225},
}

@article{li_above-ground_2020,
	title = {Above-ground biomass estimation and yield prediction in potato by using {UAV}-based {RGB} and hyperspectral imaging},
	volume = {162},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271620300538},
	doi = {10.1016/j.isprsjprs.2020.02.013},
	abstract = {Rapid and accurate biomass and yield estimation facilitates efficient plant phenotyping and site-specific crop management. A low altitude unmanned aerial vehicle (UAV) was used to acquire RGB and hyperspectral imaging data for a potato crop canopy at two growth stages to estimate the above-ground biomass and predict crop yield. Field experiments included six cultivars and multiple treatments of nitrogen, potassium, and mixed compound fertilisers. Crop height was estimated using the difference between digital surface model and digital elevation models derived from RGB imagery. Combining with two narrow-band vegetation indices selected by the RReliefF feature selection algorithm. Random Forest regression models demonstrated high prediction accuracy for both fresh and dry above-ground biomass, with a coefficient of determination (r2) {\textgreater} 0.90. Crop yield was predicted using four narrow-band vegetation indices and crop height (r2 = 0.63) with imagery data obtained 90 days after planting. A Partial Least Squares regression model based on the full wavelength spectra demonstrated improved yield prediction (r2 = 0.81). This study demonstrated the merits of UAV-based RGB and hyperspectral imaging for estimating the above-ground biomass and yield of potato crops, which can be used to assist in site-specific crop management.},
	language = {en},
	urldate = {2023-03-01},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Li, Bo and Xu, Xiangming and Zhang, Li and Han, Jiwan and Bian, Chunsong and Li, Guangcun and Liu, Jiangang and Jin, Liping},
	month = apr,
	year = {2020},
	keywords = {Above-ground biomass, Hyperspectral imaging, Potato, Unmanned aerial vehicle, Yield prediction},
	pages = {161--172},
}

@article{torresan_forestry_2017,
	title = {Forestry applications of {UAVs} in {Europe}: a review},
	volume = {38},
	issn = {0143-1161},
	shorttitle = {Forestry applications of {UAVs} in {Europe}},
	url = {https://doi.org/10.1080/01431161.2016.1252477},
	doi = {10.1080/01431161.2016.1252477},
	abstract = {Unmanned aerial vehicles (UAVs) or remotely piloted aircraft systems are new platforms that have been increasingly used over the last decade in Europe to collect data for forest research, thanks to the miniaturization and cost reduction of GPS receivers, inertial navigation system, computers, and, most of all, sensors for remote sensing.In this review, after describing the regulatory framework for the operation of UAVs in the European Union (EU), an overview of applications in forest research is presented, followed by a discussion of the results obtained from the analysis of different case studies.Rotary-wing and fixed-wing UAVs are equally distributed among the case studies, while ready-to-fly solutions are preferred over self-designed and developed UAVs. Most adopted technologies are visible-red, green, and blue, multispectral in visible and near-infrared, middle-infrared, thermal infrared imagery, and lidar.The majority of current UAV-based applications for forest research aim to inventory resources, map diseases, classify species, monitor fire and its effects, quantify spatial gaps, and estimate post-harvest soil displacement.Successful implementation of UAVs in forestry depends on UAV features, such as flexibility of use in flight planning, low cost, reliability and autonomy, and capability of timely provision of high-resolution data.Unfortunately, the fragmented regulations among EU countries, a result of the lack of common rules for operating UAVs in Europe, limit the chance to operate within Europe’s boundaries and prevent research mobility and exchange opportunities. Nevertheless, the applications of UAVs are expanding in different domains, and the use of UAVs in forestry will increase, possibly leading to a regular utilization for small-scale monitoring purposes in Europe when recent technologies (i.e. hyperspectral imagery and lidar) and methodological approaches will be consolidated.},
	number = {8-10},
	urldate = {2023-03-01},
	journal = {International Journal of Remote Sensing},
	author = {Torresan, Chiara and Berton, Andrea and Carotenuto, Federico and Di Gennaro, Salvatore Filippo and Gioli, Beniamino and Matese, Alessandro and Miglietta, Franco and Vagnoli, Carolina and Zaldei, Alessandro and Wallace, Luke},
	month = may,
	year = {2017},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01431161.2016.1252477},
	pages = {2427--2447},
}

@article{aasen_generating_2015,
	title = {Generating {3D} hyperspectral information with lightweight {UAV} snapshot cameras for vegetation monitoring: {From} camera calibration to quality assurance},
	volume = {108},
	issn = {0924-2716},
	shorttitle = {Generating {3D} hyperspectral information with lightweight {UAV} snapshot cameras for vegetation monitoring},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271615001938},
	doi = {10.1016/j.isprsjprs.2015.08.002},
	abstract = {This paper describes a novel method to derive 3D hyperspectral information from lightweight snapshot cameras for unmanned aerial vehicles for vegetation monitoring. Snapshot cameras record an image cube with one spectral and two spatial dimensions with every exposure. First, we describe and apply methods to radiometrically characterize and calibrate these cameras. Then, we introduce our processing chain to derive 3D hyperspectral information from the calibrated image cubes based on structure from motion. The approach includes a novel way for quality assurance of the data which is used to assess the quality of the hyperspectral data for every single pixel in the final data product. The result is a hyperspectral digital surface model as a representation of the surface in 3D space linked with the hyperspectral information emitted and reflected by the objects covered by the surface. In this study we use the hyperspectral camera Cubert UHD 185-Firefly, which collects 125 bands from 450 to 950nm. The obtained data product has a spatial resolution of approximately 1cm for the spatial and 21cm for the hyperspectral information. The radiometric calibration yields good results with less than 1\% offset in reflectance compared to an ASD FieldSpec 3 for most of the spectral range. The quality assurance information shows that the radiometric precision is better than 0.13\% for the derived data product. We apply the approach to data from a flight campaign in a barley experiment with different varieties during the growth stage heading (BBCH 52 – 59) to demonstrate the feasibility for vegetation monitoring in the context of precision agriculture. The plant parameters retrieved from the data product correspond to in-field measurements of a single date field campaign for plant height (R2=0.7), chlorophyll (BGI2, R2=0.52), LAI (RDVI, R2=0.32) and biomass (RDVI, R2=0.29). Our approach can also be applied for other image-frame cameras as long as the individual bands of the image cube are spatially co-registered beforehand.},
	language = {en},
	urldate = {2023-03-01},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Aasen, Helge and Burkart, Andreas and Bolten, Andreas and Bareth, Georg},
	month = oct,
	year = {2015},
	keywords = {Barley, Hyperspectral digital surface model, Image-frame camera, Precision agriculture, Quality assurance, Radiometric calibration},
	pages = {245--259},
}

@article{yue_comparison_2018,
	title = {A {Comparison} of {Crop} {Parameters} {Estimation} {Using} {Images} from {UAV}-{Mounted} {Snapshot} {Hyperspectral} {Sensor} and {High}-{Definition} {Digital} {Camera}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/10/7/1138},
	doi = {10.3390/rs10071138},
	abstract = {Timely and accurate estimates of crop parameters are crucial for agriculture management. Unmanned aerial vehicles (UAVs) carrying sophisticated cameras are very pertinent for this work because they can obtain remote-sensing images with higher temporal, spatial, and ground resolution than satellites. In this study, we evaluated (i) the performance of crop parameters estimates using a near-surface spectroscopy (350{\textasciitilde}2500 nm, 3 nm at 700 nm, 8.5 nm at 1400 nm, 6.5 nm at 2100 nm), a UAV-mounted snapshot hyperspectral sensor (450{\textasciitilde}950 nm, 8 nm at 532 nm) and a high-definition digital camera (Visible, R, G, B); (ii) the crop surface models (CSMs), RGB-based vegetation indices (VIs), hyperspectral-based VIs, and methods combined therefrom to make multi-temporal estimates of crop parameters and to map the parameters. The estimated leaf area index (LAI) and above-ground biomass (AGB) are obtained by using linear and exponential equations, random forest (RF) regression, and partial least squares regression (PLSR) to combine the UAV based spectral VIs and crop heights (from the CSMs). The results show that: (i) spectral VIs correlate strongly with LAI and AGB over single growing stages when crop height correlates positively with AGB over multiple growth stages; (ii) the correlation between the VIs multiplying crop height and AGB is greater than that between a single VI and crop height; (iii) the AGB estimate from the UAV-mounted snapshot hyperspectral sensor and high-definition digital camera is similar to the results from the ground spectrometer when using the combined methods (i.e., using VIs multiplying crop height, RF and PLSR to combine VIs and crop heights); and (iv) the spectral performance of the sensors is crucial in LAI estimates (the wheat LAI cannot be accurately estimated over multiple growing stages when using only crop height). The LAI estimates ranked from best to worst are ground spectrometer, UAV snapshot hyperspectral sensor, and UAV high-definition digital camera.},
	language = {en},
	number = {7},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Yue, Jibo and Feng, Haikuan and Jin, Xiuliang and Yuan, Huanhuan and Li, Zhenhai and Zhou, Chengquan and Yang, Guijun and Tian, Qingjiu},
	month = jul,
	year = {2018},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {LAI, aboveground biomass, crop height, crop surface model, partial least squares regression, random forest regression},
	pages = {1138},
}

@article{deng_effect_2018,
	title = {The effect of spatial resolution on radiometric and geometric performances of a {UAV}-mounted hyperspectral {2D} imager},
	volume = {144},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271618302144},
	doi = {10.1016/j.isprsjprs.2018.08.002},
	abstract = {The effect of spatial resolution on the radiometric and geometric performances of hyperspectral sensors is an essential issue in remote sensing that urgently needs to be investigated, especially for low-altitude remote sensing principles and applications. Using an unmanned aerial vehicle (UAV)-mounted miniature hyperspectral 2D imager (Cubert UHD 185) system, a series of hyperspectral images of several reflectance targets (5\%, 20\%, 30\%, 40\%, 60\% and 65\%) were imaged in hovering flight at various spatial resolutions (ground sampling distances (GSDs)) from 1.2 cm to 4.8 cm, with intervals of 0.4 cm, which correspond to flight altitudes from 30 m to 120 m in increments of 10 m. Subsequently, the effect of spatial resolution on radiometric and geometric performances was evaluated in terms of the change in reflectance and geometric recognition ability of the shape of targets at visible to near-infrared wavelengths. This paper provides a set of methods for assessing the effect of spatial resolution on radiometric and geometric performance, including a radiative transfer model simulation for imaging quality performance, the geometric recognition loss degree (GRLD) for measuring image geometry recognition ability, and a trend projection analysis for developing continuous distribution images of radiometric and geometric performances. The results show that when the size of the target is not less than 50 (row) × 50 (column) pixels in a Cubert hyperspectral image, the absolute error (AE) and the root mean square error (RMSE) of the reflectances of its central pixel are both less than 0.05. Additionally, as the spatial resolution decreased, the AEs of the target reflectances in visible bands increased and then stabilized, and those in the red-edge band and near-infrared bands first increased slowly and then decreased rapidly because an increasing number of pixels were influenced by the surrounding area; thus, the shapes of the spectral curves of the sample area became increasingly similar to those of the surrounding area. This study provides a guide for selecting an appropriate spatial resolution for UAV remote sensing to improve operational efficiency. The reflectance and geometric quantitative losses at different spatial resolutions are conducive to parameter inversion in quantitative remote sensing and spatial resolution transformation and enrich the knowledge of low-altitude UAV hyperspectral remote sensing.},
	language = {en},
	urldate = {2023-03-01},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Deng, Lei and Yan, Yanan and Gong, Huili and Duan, Fuzhou and Zhong, Ruofei},
	month = oct,
	year = {2018},
	keywords = {Geometric performance, High spatial resolution, Hyperspectral imaging, Radiometry, Unmanned aerial vehicles (UAVs)},
	pages = {298--314},
}

@article{vanegas_novel_2018,
	title = {A {Novel} {Methodology} for {Improving} {Plant} {Pest} {Surveillance} in {Vineyards} and {Crops} {Using} {UAV}-{Based} {Hyperspectral} and {Spatial} {Data}},
	volume = {18},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/18/1/260},
	doi = {10.3390/s18010260},
	abstract = {Recent advances in remote sensed imagery and geospatial image processing using unmanned aerial vehicles (UAVs) have enabled the rapid and ongoing development of monitoring tools for crop management and the detection/surveillance of insect pests. This paper describes a (UAV) remote sensing-based methodology to increase the efficiency of existing surveillance practices (human inspectors and insect traps) for detecting pest infestations (e.g., grape phylloxera in vineyards). The methodology uses a UAV integrated with advanced digital hyperspectral, multispectral, and RGB sensors. We implemented the methodology for the development of a predictive model for phylloxera detection. In this method, we explore the combination of airborne RGB, multispectral, and hyperspectral imagery with ground-based data at two separate time periods and under different levels of phylloxera infestation. We describe the technology used—the sensors, the UAV, and the flight operations—the processing workflow of the datasets from each imagery type, and the methods for combining multiple airborne with ground-based datasets. Finally, we present relevant results of correlation between the different processed datasets. The objective of this research is to develop a novel methodology for collecting, processing, analising and integrating multispectral, hyperspectral, ground and spatial data to remote sense different variables in different applications, such as, in this case, plant pest surveillance. The development of such methodology would provide researchers, agronomists, and UAV practitioners reliable data collection protocols and methods to achieve faster processing techniques and integrate multiple sources of data in diverse remote sensing applications.},
	language = {en},
	number = {1},
	urldate = {2023-03-01},
	journal = {Sensors},
	author = {Vanegas, Fernando and Bratanov, Dmitry and Powell, Kevin and Weiss, John and Gonzalez, Felipe},
	month = jan,
	year = {2018},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {RGB, digital elevation model, digital vigour assessment, hyperspectral, multispectral, phylloxera, remote sensing, unmanned aerial vehicle},
	pages = {260},
}

@misc{mohandoss_generating_2020,
	title = {Generating {Synthetic} {Multispectral} {Satellite} {Imagery} from {Sentinel}-2},
	url = {http://arxiv.org/abs/2012.03108},
	doi = {10.48550/arXiv.2012.03108},
	abstract = {Multi-spectral satellite imagery provides valuable data at global scale for many environmental and socio-economic applications. Building supervised machine learning models based on these imagery, however, may require ground reference labels which are not available at global scale. Here, we propose a generative model to produce multi-resolution multi-spectral imagery based on Sentinel-2 data. The resulting synthetic images are indistinguishable from real ones by humans. This technique paves the road for future work to generate labeled synthetic imagery that can be used for data augmentation in data scarce regions and applications.},
	urldate = {2023-03-01},
	publisher = {arXiv},
	author = {Mohandoss, Tharun and Kulkarni, Aditya and Northrup, Daniel and Mwebaze, Ernest and Alemohammad, Hamed},
	month = dec,
	year = {2020},
	note = {arXiv:2012.03108 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{nasi_estimating_2018,
	title = {Estimating {Biomass} and {Nitrogen} {Amount} of {Barley} and {Grass} {Using} {UAV} and {Aircraft} {Based} {Spectral} and {Photogrammetric} {3D} {Features}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/10/7/1082},
	doi = {10.3390/rs10071082},
	abstract = {The timely estimation of crop biomass and nitrogen content is a crucial step in various tasks in precision agriculture, for example in fertilization optimization. Remote sensing using drones and aircrafts offers a feasible tool to carry out this task. Our objective was to develop and assess a methodology for crop biomass and nitrogen estimation, integrating spectral and 3D features that can be extracted using airborne miniaturized multispectral, hyperspectral and colour (RGB) cameras. We used the Random Forest (RF) as the estimator, and in addition Simple Linear Regression (SLR) was used to validate the consistency of the RF results. The method was assessed with empirical datasets captured of a barley field and a grass silage trial site using a hyperspectral camera based on the Fabry-Pérot interferometer (FPI) and a regular RGB camera onboard a drone and an aircraft. Agricultural reference measurements included fresh yield (FY), dry matter yield (DMY) and amount of nitrogen. In DMY estimation of barley, the Pearson Correlation Coefficient (PCC) and the normalized Root Mean Square Error (RMSE\%) were at best 0.95\% and 33.2\%, respectively; and in the grass DMY estimation, the best results were 0.79\% and 1.9\%, respectively. In the nitrogen amount estimations of barley, the PCC and RMSE\% were at best 0.97\% and 21.6\%, respectively. In the biomass estimation, the best results were obtained when integrating hyperspectral and 3D features, but the integration of RGB images and 3D features also provided results that were almost as good. In nitrogen content estimation, the hyperspectral camera gave the best results. We concluded that the integration of spectral and high spatial resolution 3D features and radiometric calibration was necessary to optimize the accuracy.},
	language = {en},
	number = {7},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Näsi, Roope and Viljanen, Niko and Kaivosoja, Jere and Alhonoja, Katja and Hakala, Teemu and Markelin, Lauri and Honkavaara, Eija},
	month = jul,
	year = {2018},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {UAV, biomass, drone, hyperspectral, machine learning, nitrogen, photogrammetry, precision agriculture, random forest, regression},
	pages = {1082},
}

@article{honkavaara_hyperspectral_2012,
	title = {Hyperspectral {Reflectance} {Signatures} and {Point} {Clouds} for {Precision} {Agriculture} by {Light} {Weight} {Uav} {Imaging} {System}},
	volume = {I7},
	issn = {2194-9050       ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	url = {https://ui.adsabs.harvard.edu/abs/2012ISPAn..I7..353H},
	doi = {10.5194/isprsannals-I-7-353-2012},
	abstract = {The objective of this investigation was to study the use of a new type of a low-weight unmanned aerial vehicle (UAV) imaging system in the precision agriculture. The system consists of a novel Fabry-Perot interferometer based hyperspectral camera and a high-resolution small-format consumer camera. The sensors provide stereoscopic imagery in a 2D frame-format and they both weigh less than 500 g. A processing chain was developed for the production of high density point clouds and hyperspectral reflectance image mosaics (reflectance signatures), which are used as inputs in the agricultural application. We demonstrate the use of this new technology in the biomass estimation process, which is based on support vector regression machine. It was concluded that the central factors influencing on the accuracy of the estimation process were the quality of the image data, the quality of the image processing and digital surface model generation, and the performance of the regressor. In the wider perspective, our investigation showed that very low-weight, low-cost, hyperspectral, stereoscopic and spectrodirectional 3D UAV-remote sensing is now possible. This cutting edge technology is powerful and cost efficient in time-critical, repetitive and locally operated remote sensing applications.},
	urldate = {2023-03-01},
	journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Honkavaara, E. and Kaivosoja, J. and Mäkynen, J. and Pellikka, I. and Pesonen, L. and Saari, H. and Salo, H. and Hakala, T. and Marklelin, L. and Rosnell, T.},
	month = jul,
	year = {2012},
	note = {ADS Bibcode: 2012ISPAn..I7..353H},
	pages = {353--358},
}

@article{sankey_uav_2017,
	title = {{UAV} lidar and hyperspectral fusion for forest monitoring in the southwestern {USA}},
	volume = {195},
	issn = {0034-4257},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425717301578},
	doi = {10.1016/j.rse.2017.04.007},
	abstract = {Forest vegetation classification and structure measurements are fundamental steps for planning, monitoring, and evaluating large-scale forest changes including restoration treatments. High spatial and spectral resolution remote sensing data are critically needed to classify vegetation and measure their 3-dimensional (3D) canopy structure at the level of individual species. Here we test high-resolution lidar, hyperspectral, and multispectral data collected from unmanned aerial vehicles (UAV) and demonstrate a lidar-hyperspectral image fusion method in treated and control forests with varying tree density and canopy cover as well as in an ecotone environment to represent a gradient of vegetation and topography in northern Arizona, U.S.A. The fusion performs better (88\% overall accuracy) than either data type alone, particularly for species with similar spectral signatures, but different canopy sizes. The lidar data provides estimates of individual tree height (R2=0.90; RMSE=2.3m) and crown diameter (R2=0.72; RMSE=0.71m) as well as total tree canopy cover (R2=0.87; RMSE=9.5\%) and tree density (R2=0.77; RMSE=0.69 trees/cell) in 10m cells across thin only, burn only, thin-and-burn, and control treatments, where tree cover and density ranged between 22 and 50\% and 1–3.5 trees/cell, respectively. The lidar data also produces highly accurate digital elevation model (DEM) (R2=0.92; RMSE=0.75m). In comparison, 3D data derived from the multispectral data via structure-from-motion produced lower correlations with field-measured variables, especially in dense and structurally complex forests. The lidar, hyperspectral, and multispectral sensors, and the methods demonstrated here can be widely applied across a gradient of vegetation and topography for monitoring landscapes undergoing large-scale changes such as the forests in the southwestern U.S.A.},
	language = {en},
	urldate = {2023-03-01},
	journal = {Remote Sensing of Environment},
	author = {Sankey, Temuulen and Donager, Jonathon and McVay, Jason and Sankey, Joel B.},
	month = jun,
	year = {2017},
	keywords = {3D modelling, Airborne data, Crown diameter, DEM, Drone, Structure from motion (SFM), Tree delineation, UAS},
	pages = {30--43},
}

@article{sothe_tree_2019,
	title = {Tree {Species} {Classification} in a {Highly} {Diverse} {Subtropical} {Forest} {Integrating} {UAV}-{Based} {Photogrammetric} {Point} {Cloud} and {Hyperspectral} {Data}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/11/11/1338},
	doi = {10.3390/rs11111338},
	abstract = {The use of remote sensing data for tree species classification in tropical forests is still a challenging task, due to their high floristic and spectral diversity. In this sense, novel sensors on board of unmanned aerial vehicle (UAV) platforms are a rapidly evolving technology that provides new possibilities for tropical tree species mapping. Besides the acquisition of high spatial and spectral resolution images, UAV-hyperspectral cameras operating in frame format enable to produce 3D hyperspectral point clouds. This study investigated the use of UAV-acquired hyperspectral images and UAV-photogrammetric point cloud (PPC) for classification of 12 major tree species in a subtropical forest fragment in Southern Brazil. Different datasets containing hyperspectral visible/near-infrared (VNIR) bands, PPC features, canopy height model (CHM), and other features extracted from hyperspectral data (i.e., texture, vegetation indices-VIs, and minimum noise fraction-MNF) were tested using a support vector machine (SVM) classifier. The results showed that the use of VNIR hyperspectral bands alone reached an overall accuracy (OA) of 57\% (Kappa index of 0.53). Adding PPC features to the VNIR hyperspectral bands increased the OA by 11\%. The best result was achieved combining VNIR bands, PPC features, CHM, and VIs (OA of 72.4\% and Kappa index of 0.70). When only the CHM was added to VNIR bands, the OA increased by 4.2\%. Among the hyperspectral features, besides all the VNIR bands and the two VIs (NDVI and PSSR), the first four MNF features and the textural mean of 565 and 679 nm spectral bands were pointed out as more important to discriminate the tree species according to Jeffries–Matusita (JM) distance. The SVM method proved to be a good classifier for the tree species recognition task, even in the presence of a high number of classes and a small dataset.},
	language = {en},
	number = {11},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Sothe, Camile and Dalponte, Michele and Almeida, Cláudia Maria de and Schimalski, Marcos Benedito and Lima, Carla Luciane and Liesenberg, Veraldo and Miyoshi, Gabriela Takahashi and Tommaselli, Antonio Maria Garcia},
	month = jan,
	year = {2019},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {imaging spectroscopy, photogrammetry, support vector machine, tree species mapping, tropical biodiversity},
	pages = {1338},
}

@article{nasi_using_2015,
	title = {Using {UAV}-{Based} {Photogrammetry} and {Hyperspectral} {Imaging} for {Mapping} {Bark} {Beetle} {Damage} at {Tree}-{Level}},
	volume = {7},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/7/11/15467},
	doi = {10.3390/rs71115467},
	abstract = {Low-cost, miniaturized hyperspectral imaging technology is becoming available for small unmanned aerial vehicle (UAV) platforms. This technology can be efficient in carrying out small-area inspections of anomalous reflectance characteristics of trees at a very high level of detail. Increased frequency and intensity of insect induced forest disturbance has established a new demand for effective methods suitable in mapping and monitoring tasks. In this investigation, a novel miniaturized hyperspectral frame imaging sensor operating in the wavelength range of 500–900 nm was used to identify mature Norway spruce (Picea abies L. Karst.) trees suffering from infestation, representing a different outbreak phase, by the European spruce bark beetle (Ips typographus L.). We developed a new processing method for analyzing spectral characteristic for high spatial resolution photogrammetric and hyperspectral images in forested environments, as well as for identifying individual anomalous trees. The dense point clouds, measured using image matching, enabled detection of single trees with an accuracy of 74.7\%. We classified the trees into classes of healthy, infested and dead, and the results were promising. The best results for the overall accuracy were 76\% (Cohen’s kappa 0.60), when using three color classes (healthy, infested, dead). For two color classes (healthy, dead), the best overall accuracy was 90\% (kappa 0.80). The survey methodology based on high-resolution hyperspectral imaging will be of a high practical value for forest health management, indicating a status of bark beetle outbreak in time.},
	language = {en},
	number = {11},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Näsi, Roope and Honkavaara, Eija and Lyytikäinen-Saarenmaa, Päivi and Blomqvist, Minna and Litkey, Paula and Hakala, Teemu and Viljanen, Niko and Kantola, Tuula and Tanhuanpää, Topi and Holopainen, Markus},
	month = nov,
	year = {2015},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {UAV, bark beetle, classification, dense matching, digital surface model, hyperspectral, insect outbreak, photogrammetry, radiometry},
	pages = {15467--15493},
}

@article{saari_visible_2017,
	series = {International {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	title = {Visible, very near {IR} and short wave {IR} hyperspectral drone imaging system for agriculture and natural water {applicationS}: {Frontiers} in {Spectral} imaging and {3D} {Technologies} for {Geospatial} {Solutions}, {ISPRS} {SPEC3D}},
	shorttitle = {Visible, very near {IR} and short wave {IR} hyperspectral drone imaging system for agriculture and natural water {applicationS}},
	url = {http://www.scopus.com/inward/record.url?scp=85033695775&partnerID=8YFLogxK},
	doi = {10.5194/isprs-archives-XLII-3-W3-165-2017},
	abstract = {The accurate determination of the quality parameters of crops requires a spectral range from 400 nm to 2500 nm (Kawamura et al., 2010, Thenkabail et al., 2002). Presently the hyperspectral imaging systems that cover this wavelength range consist of several separate hyperspectral imagers and the system weight is from 5 to 15 kg. In addition the cost of the Short Wave Infrared (SWIR) cameras is high ({\textasciitilde}50 k€). VTT has previously developed compact hyperspectral imagers for drones and Cubesats for Visible and Very near Infrared (VNIR) spectral ranges (Saari et al., 2013, Mannila et al., 2013, Näsilä et al., 2016). Recently VTT has started to develop a hyperspectral imaging system that will enable imaging simultaneously in the Visible, VNIR, and SWIR spectral bands. The system can be operated from a drone, on a camera stand, or attached to a tractor. The targeted main applications of the DroneKnowledge hyperspectral system are grass, peas, and cereals. In this paper the characteristics of the built system are shortly described. The system was used for spectral measurements of wheat, several grass species and pea plants fixed to the camera mount in the test fields in Southern Finland and in the green house. The wheat, grass and pea field measurements were also carried out using the system mounted on the tractor. The work is part of the Finnish nationally funded “DroneKnowledge - Towards knowledge based export of small UAS remote sensing technology” project.},
	urldate = {2023-03-01},
	journal = {Frontiers in Spectral imaging and 3D Technologies for Geospatial Solutions},
	author = {Saari, Heikki and Akujärvi, Altti and Holmlund, Christer and Ojanen, Harri and Kaivosoja, J. and Nissinen, A. and Niemeläinen, O.},
	editor = {Honkavaara, E. and Hu, B. and Karantzalos, K. and Liang, X. and Müller, R. and Nocerino, E. and Pölönen, I. and Rönnholm, P.},
	month = oct,
	year = {2017},
	note = {Publisher: International Society for Photogrammetry and Remote Sensing ISPRS},
	keywords = {Drone, Fabry-Perot, Forage quality estimation, Hyperspectral imaging, Microspectrometers, OtaNano, Water quality monitoring},
	pages = {165--170},
}

@article{torabzadeh_fusion_2014,
	title = {Fusion of imaging spectroscopy and airborne laser scanning data for characterization of forest ecosystems – {A} review},
	volume = {97},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S092427161400197X},
	doi = {10.1016/j.isprsjprs.2014.08.001},
	abstract = {Forest ecosystems play an important role in the global carbon cycle and it is largely unknown how this role might be altered by transients imposed by global change and deforestation. Remote sensing can provide information on ecosystem state and functioning and, among others, two remote sensing techniques, airborne laser scanning (ALS) and imaging spectroscopy (IS), have been used to characterize forest ecosystems, both independently and combined in fusion approaches. However, the fusion of these datasetsshould make the best use of the complementarity of both sensors and provide better and more robust vegetation products in forested ecosystems. Similar to other data fusion approaches, satisfying results depend on choosing appropriate fusion levels and methods. In this review paper, we summarize and classify relevant studies that focused on forest characterization using combined ALS and IS data, limited to the last decade. We classified the approaches by fusion level (data or product level) and by choice of methods (physical or empirical methods). Five different categories of products (landcover maps, aboveground biomass, biophysical parameters, gross/net primary productivity and biochemical parameters), have been found as the main aspects of forest ecosystems studied so far. A qualitative accuracy analysis of the products exposed that currently landcover maps are profiting the most from ALS and IS data fusion, while there is room for improvements in respect to the other products, such as biophysical parameters. Only few studies using physical approaches were found, but we expect the use of such approaches will increase with the growing availability of physically based radiative transfer models that can simulate both, ALS and IS data.},
	language = {en},
	urldate = {2023-03-01},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Torabzadeh, Hossein and Morsdorf, Felix and Schaepman, Michael E.},
	month = nov,
	year = {2014},
	keywords = {Airborne laser scanning, Data fusion, Forest ecosystems, Imaging spectroscopy},
	pages = {25--35},
}

@article{saarinen_assessing_2018,
	title = {Assessing {Biodiversity} in {Boreal} {Forests} with {UAV}-{Based} {Photogrammetric} {Point} {Clouds} and {Hyperspectral} {Imaging}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/10/2/338},
	doi = {10.3390/rs10020338},
	abstract = {Forests are the most diverse terrestrial ecosystems and their biological diversity includes trees, but also other plants, animals, and micro-organisms. One-third of the forested land is in boreal zone; therefore, changes in biological diversity in boreal forests can shape biodiversity, even at global scale. Several forest attributes, including size variability, amount of dead wood, and tree species richness, can be applied in assessing biodiversity of a forest ecosystem. Remote sensing offers complimentary tool for traditional field measurements in mapping and monitoring forest biodiversity. Recent development of small unmanned aerial vehicles (UAVs) enable the detailed characterization of forest ecosystems through providing data with high spatial but also temporal resolution at reasonable costs. The objective here is to deepen the knowledge about assessment of plot-level biodiversity indicators in boreal forests with hyperspectral imagery and photogrammetric point clouds from a UAV. We applied individual tree crown approach (ITC) and semi-individual tree crown approach (semi-ITC) in estimating plot-level biodiversity indicators. Structural metrics from the photogrammetric point clouds were used together with either spectral features or vegetation indices derived from hyperspectral imagery. Biodiversity indicators like the amount of dead wood and species richness were mainly underestimated with UAV-based hyperspectral imagery and photogrammetric point clouds. Indicators of structural variability (i.e., standard deviation in diameter-at-breast height and tree height) were the most accurately estimated biodiversity indicators with relative RMSE between 24.4\% and 29.3\% with semi-ITC. The largest relative errors occurred for predicting deciduous trees (especially aspen and alder), partly due to their small amount within the study area. Thus, especially the structural diversity was reliably predicted by integrating the three-dimensional and spectral datasets of UAV-based point clouds and hyperspectral imaging, and can therefore be further utilized in ecological studies, such as biodiversity monitoring.},
	language = {en},
	number = {2},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Saarinen, Ninni and Vastaranta, Mikko and Näsi, Roope and Rosnell, Tomi and Hakala, Teemu and Honkavaara, Eija and Wulder, Michael A. and Luoma, Ville and Tommaselli, Antonio M. G. and Imai, Nilton N. and Ribeiro, Eduardo A. W. and Guimarães, Raul B. and Holopainen, Markus and Hyyppä, Juha},
	month = feb,
	year = {2018},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D, UAS, dead wood, old growth, photogrammetry, remote sensing, size variability, spectral, structural diversity, tree species},
	pages = {338},
}

@article{mitchell_combining_2015,
	series = {Special {Issue} on the {Hyperspectral} {Infrared} {Imager} ({HyspIRI})},
	title = {Combining airborne hyperspectral and {LiDAR} data across local sites for upscaling shrubland structural information: {Lessons} for {HyspIRI}},
	volume = {167},
	issn = {0034-4257},
	shorttitle = {Combining airborne hyperspectral and {LiDAR} data across local sites for upscaling shrubland structural information},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425715001443},
	doi = {10.1016/j.rse.2015.04.015},
	abstract = {Fine-scale variation of vegetation structure in dryland systems, such as the Great Basin in the western US, is critical to understanding ecosystem responses to changing land-use conditions. High resolution airborne hyperspectral (HyMap) and LiDAR datasets acquired across independent collection sites can reduce uncertainty in predictive ecosystem modeling and provide a basis for regional upscaling to satellite observations of structural metrics such as cover and height. In the first part of our study, we combined ground reference and airborne data collected at three sagebrush-steppe locations and used the statistical data mining tool random forests to identify remote sensing variables most relevant to estimating shrub cover. In the second part of our study, we hypothesized that vegetation indices derived from hyperspectral satellite observations would not only reliably predict shrub cover but also be relatable to shrub height; thereby augmenting the collection of vertical structure estimates from future satellite platforms such as ICESAT-2. To test this hypothesis, we simulated HyspIRI observations to derive variables to relate to LiDAR-based estimates of shrub cover and height. We generated the same hyperspectral variables as in the first part of this study but at coarser resolution (60m) and we again used random forests to model shrub cover and height and identify predictors of greatest importance. Overall, combining LiDAR and HyMap datasets at the airborne scale improved shrub cover model results (r2=0.58) compared to LiDAR alone (r2=0.49). Primary shrub cover variables of importance were HIQR (the interquartile range of height of all LiDAR vegetation returns), HMAD (median absolute deviation from median height of all LiDAR vegetation returns), a narrowband index sensitive to anthocyanins, the ratio of LiDAR vegetation returns to total returns, and a red to green ratio. In addition, HyspIRI-simulated narrowband vegetation indices were relatable to LiDAR-derived shrub cover and height variables (r2 ranging from 0.63 to 0.71) with relatively low root mean square error.},
	language = {en},
	urldate = {2023-03-01},
	journal = {Remote Sensing of Environment},
	author = {Mitchell, Jessica J. and Shrestha, Rupesh and Spaete, Lucas P. and Glenn, Nancy F.},
	month = sep,
	year = {2015},
	keywords = {HsypIRI, Hyperspectral, LiDAR, Sagebrush, Vegetation structure},
	pages = {98--110},
}

@article{garzonio_surface_2017,
	title = {Surface {Reflectance} and {Sun}-{Induced} {Fluorescence} {Spectroscopy} {Measurements} {Using} a {Small} {Hyperspectral} {UAS}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/9/5/472},
	doi = {10.3390/rs9050472},
	abstract = {This study describes the development of a small hyperspectral Unmanned Aircraft System (HyUAS) for measuring Visible and Near-Infrared (VNIR) surface reflectance and sun-induced fluorescence, co-registered with high-resolution RGB imagery, to support field spectroscopy surveys and calibration and validation of remote sensing products. The system, namely HyUAS, is based on a multirotor platform equipped with a cost-effective payload composed of a VNIR non-imaging spectrometer and an RGB camera. The spectrometer is connected to a custom entrance optics receptor developed to tune the instrument field-of-view and to obtain systematic measurements of instrument dark-current. The geometric, radiometric and spectral characteristics of the instruments were characterized and calibrated through dedicated laboratory tests. The overall accuracy of HyUAS data was evaluated during a flight campaign in which surface reflectance was compared with ground-based reference measurements. HyUAS data were used to estimate spectral indices and far-red fluorescence for different land covers. RGB images were processed as a high-resolution 3D surface model using structure from motion algorithms. The spectral measurements were accurately geo-located and projected on the digital surface model. The overall results show that: (i) rigorous calibration enabled radiance and reflectance spectra from HyUAS with RRMSE {\textless} 10\% compared with ground measurements; (ii) the low-flying UAS setup allows retrieving fluorescence in absolute units; (iii) the accurate geo-location of spectra on the digital surface model greatly improves the overall interpretation of reflectance and fluorescence data. In general, the HyUAS was demonstrated to be a reliable system for supporting high-resolution field spectroscopy surveys allowing one to collect systematic measurements at very detailed spatial resolution with a valuable potential for vegetation monitoring studies. Furthermore, it can be considered a useful tool for collecting spatially-distributed observations of reflectance and fluorescence that can be further used for calibration and validation activities of airborne and satellite optical images in the context of the upcoming FLEX mission and the VNIR spectral bands of optical Earth observation missions (i.e., Landsat, Sentinel-2 and Sentinel-3).},
	language = {en},
	number = {5},
	urldate = {2023-03-01},
	journal = {Remote Sensing},
	author = {Garzonio, Roberto and Di Mauro, Biagio and Colombo, Roberto and Cogliati, Sergio},
	month = may,
	year = {2017},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D surface model, FLEX, UAS, calibration and validation, hyperspectral, reflectance signature, spectroscopy, sun-induced fluorescence},
	pages = {472},
}

@article{behmann_generation_2016,
	title = {Generation and application of hyperspectral {3D} plant models: methods and challenges},
	volume = {27},
	issn = {1432-1769},
	shorttitle = {Generation and application of hyperspectral {3D} plant models},
	url = {https://doi.org/10.1007/s00138-015-0716-8},
	doi = {10.1007/s00138-015-0716-8},
	abstract = {Hyperspectral imaging sensors have been introduced for measuring the health status of plants. Recently, they also have been used for close-range sensing of plant canopies with a highly complex architecture. However, the complex geometry of plants and their interaction with the illumination setting severely affect the spectral information obtained. Furthermore, the spatial component of analysis results gain in importance as higher plants are represented by multiple plant organs as leaves, stems and seed pods. The combination of hyperspectral images and 3D point clouds is a promising approach to face these problems. We present the generation and application of hyperspectral 3D plant models as a new, interesting application field for computer vision with a variety of challenging tasks. We sum up a geometric calibration method for hyperspectral pushbroom cameras using a reference object for the combination of spectral and spatial information. Furthermore, we show exemplarily new calibration and analysis methods enabled by the hyperspectral 3D models in an experiment with sugar beet plants. An improved normalization, a comparison of image and 3D analysis and the density estimation of infected surface points underline some of the new capabilities gained using this new data type. Based on such hyperspectral 3D models the effects of plant geometry and sensor configuration can be quantified and modeled. In future, reflectance models can be used to remove or weaken the geometry-related effects in hyperspectral images and, therefore, have the potential to improve automated plant phenotyping significantly.},
	language = {en},
	number = {5},
	urldate = {2023-03-01},
	journal = {Machine Vision and Applications},
	author = {Behmann, Jan and Mahlein, Anne-Katrin and Paulus, Stefan and Dupuis, Jan and Kuhlmann, Heiner and Oerke, Erich-Christian and Plümer, Lutz},
	month = jul,
	year = {2016},
	keywords = {3D scanning, Close range, Hyperspectral, Modeling, Plant phenotyping, Sensor fusion},
	pages = {611--624},
}

@article{astor_vegetable_2020,
	title = {Vegetable {Crop} {Biomass} {Estimation} {Using} {Hyperspectral} and {RGB} {3D} {UAV} {Data}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-4395},
	url = {https://www.mdpi.com/2073-4395/10/10/1600},
	doi = {10.3390/agronomy10101600},
	abstract = {Remote sensing (RS) has been an effective tool to monitor agricultural production systems, but for vegetable crops, precision agriculture has received less interest to date. The objective of this study was to test the predictive performance of two types of RS data—crop height information derived from point clouds based on RGB UAV data, and reflectance information from terrestrial hyperspectral imagery—to predict fresh matter yield (FMY) for three vegetable crops (eggplant, tomato, and cabbage). The study was conducted in an experimental layout in Bengaluru, India, at five dates in summer 2017. The prediction accuracy varied strongly depending on the RS dataset used. For all crops, a good predictive performance with cross-validated prediction error {\textless} 10\% was achieved. The growth stage of the crops had no significant effect on the prediction accuracy, although increasing trends of an underestimation of FMY with later sampling dates for eggplant and tomato were found. The study proves that an estimation of vegetable FMY using RS data is successful throughout the growing season. Different RS datasets were best for biomass prediction of the three vegetables, indicating that multi-sensory data collection should be preferred to single sensor use, as no one sensor system is superior.},
	language = {en},
	number = {10},
	urldate = {2023-03-01},
	journal = {Agronomy},
	author = {Astor, Thomas and Dayananda, Supriya and Nautiyal, Sunil and Wachendorf, Michael},
	month = oct,
	year = {2020},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {hyperspectral, multi-source data combination, point cloud analysis, vegetable biomass},
	pages = {1600},
}

@article{bruning_approaches_2020,
	title = {Approaches, applications, and future directions for hyperspectral vegetation studies: {An} emphasis on yield-limiting factors in wheat},
	volume = {3},
	issn = {2578-2703},
	shorttitle = {Approaches, applications, and future directions for hyperspectral vegetation studies},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ppj2.20007},
	doi = {10.1002/ppj2.20007},
	abstract = {Hyperspectral instruments acquire spectral information in many narrow, contiguous bands throughout the visible, near-infrared and shortwave regions of the electromagnetic spectrum. Hyperspectral techniques are becoming very powerful tools for characterizing plants and nondestructively quantifying their chemical and physical properties because of their ability to provide layered trait information within the same spectral region. However, to effectively make use of hyperspectral sensing, an understanding of the theory behind these techniques, the power, and the limitations of the resulting data is required. This article presents an overview of hyperspectral sensing in regard to principles, instrumentation, processing methods, and current applications, specifically focusing on the quantification of yield-limiting factors in wheat (Triticum aestivum L.). The spectral properties of plants across the electromagnetic spectrum are first described to achieve a better understanding of plant–light interactions. Basic information about different imaging approaches is provided as are the necessary considerations for the analysis of hyperspectral data. Some of the major technical challenges associated with hyperspectral imaging as well as future directions are discussed. Finally, as an example crop, the use of hyperspectral techniques for quantifying yield-limiting factors in wheat is presented.},
	language = {en},
	number = {1},
	urldate = {2023-03-01},
	journal = {The Plant Phenome Journal},
	author = {Bruning, Brooke and Berger, Bettina and Lewis, Megan and Liu, Huajian and Garnett, Trevor},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ppj2.20007},
	pages = {e20007},
}

@article{behmann_calibration_2015,
	title = {Calibration of hyperspectral close-range pushbroom cameras for plant phenotyping},
	volume = {106},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271615001471},
	doi = {10.1016/j.isprsjprs.2015.05.010},
	abstract = {Hyperspectral sensors are able to detect biological processes of plants which are invisible to the naked eye. Close-range cameras in particular support the identification of biotic and abiotic stress reactions at an early stage. Up to now, their full potential is only partially realized because geometrical factors as leaf angle, curvature and self-shading, overlay the signal of biological processes. Suitable 3D plant models constitutes an important step to removing these factors from the data. The matching of these 3D model and the hyperspectral image with sufficient accuracy even for small leaf veins is required but relies on an adequate geometric calibration of hyperspectral cameras. We present a method for the geometric calibration of hyperspectral pushbroom cameras in the close-range, which enables reliable and reproducible results at sub-pixel scale. This approach extends the linear pushbroom camera by the ability to model non-linear fractions. Accuracy and reproducibility of the method is validated using a hyperspectral senor system with two line cameras observing the reflected radiation in the spectral range from 400 to 2500nm. We point out new potentials arising from with the proposed camera calibration, e.g. hyperspectral 3D plant models, which have high potential for crop plant phenotyping.},
	language = {en},
	urldate = {2023-03-01},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Behmann, Jan and Mahlein, Anne-Katrin and Paulus, Stefan and Kuhlmann, Heiner and Oerke, Erich-Christian and Plümer, Lutz},
	month = aug,
	year = {2015},
	keywords = {Calibration, Close range, Fusion, Hyper spectral, Orthorectification, Pushbroom},
	pages = {172--182},
}

@article{horstrand_uav_2019,
	title = {A {UAV} {Platform} {Based} on a {Hyperspectral} {Sensor} for {Image} {Capturing} and {On}-{Board} {Processing}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2913957},
	abstract = {Application-oriented solutions based on the combination of different technologies such as unmanned aerial vehicles (UAVs), advanced sensors, precise GPS, and embedded devices have led to important improvements in the field of cyber-physical systems. Agriculture, due to its economic and social impact on the global population, arises as a potential domain which could enormously benefit from this paradigm in terms of savings in time, resources and human labor, not to mention aspects related to sustainability and environment respect. This has led to a new revolution named precision agriculture (or precision farming), based on observing and measuring inter and intra-field variability in crops. A key technology in this scenario is the use of hyperspectral imaging, firstly used in satellites and later in manned aircraft, composed by hundreds of spectral bands which facilitate hidden data to be converted into useful information. In this paper, a hyperspectral flying platform is presented and the construction of the whole system is detailed. The proposed solution is based on a commercial DJI Matrice 600 drone and a Specim FX10 hyperspectral camera. The challenge in this work has been to adopt this latter device, mainly conceived for industrial applications, into a flying platform in which weight, power budget, and connectivity are paramount. Additionally, an embedded board with advanced processing capabilities has been mounted on the drone in order to control its trajectory, manage the data acquisition, and allow on-board processing, such as the evaluation of different vegetation indices (the normalized difference vegetation index, NDVI, the modified chlorophyll absorption ratio index, MCARI, and the modified soil-adjusted vegetation index, MSAVI), which are numerical and/or graphical indicators of the vegetation properties and compression, which is of crucial relevance due to the huge amounts of data captured. The whole system was successfully tested in a real scenario located on the island of Gran Canaria, Spain, where a vineyard area was inspected between May and August of the year 2018.},
	journal = {IEEE Access},
	author = {Horstrand, Pablo and Guerra, Raúl and Rodríguez, Aythami and Díaz, María and López, Sebastián and López, José Fco.},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Agriculture, Cameras, Hyperspectral imaging, Indexes, Sensors, Unmanned aerial vehicle, Vegetation mapping, hyperspectral, on-board processing, pushbroom sensor, vegetation index},
	pages = {66919--66938},
}

@inproceedings{schutz_rendering_2019,
	address = {New York, NY, USA},
	series = {{SA} '19},
	title = {Rendering {Point} {Clouds} with {Compute} {Shaders}},
	isbn = {978-1-4503-6943-5},
	url = {https://doi.org/10.1145/3355056.3364554},
	doi = {10.1145/3355056.3364554},
	abstract = {We propose a compute shader based point cloud rasterizer with up to 10 times higher performance than classic point-based rendering with the GL\_POINT primitive. In addition to that, our rasterizer offers 5 byte depth-buffer precision with uniform or customizable distribution, and we show that it is possible to implement a high-quality splatting method that blends together overlapping fragments while still maintaining higher frame-rates than the traditional approach.},
	urldate = {2023-02-28},
	booktitle = {{SIGGRAPH} {Asia} 2019 {Posters}},
	publisher = {Association for Computing Machinery},
	author = {Schutz, Markus and Wimmer, Michael},
	month = nov,
	year = {2019},
	keywords = {GPGPU, LIDAR, compute shader, point cloud, point-based rendering},
	pages = {1--2},
}

@inproceedings{que_voxelcontext-net_2021,
	title = {{VoxelContext}-{Net}: {An} {Octree} {Based} {Framework} for {Point} {Cloud} {Compression}},
	shorttitle = {{VoxelContext}-{Net}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Que_VoxelContext-Net_An_Octree_Based_Framework_for_Point_Cloud_Compression_CVPR_2021_paper.html},
	language = {en},
	urldate = {2023-02-28},
	author = {Que, Zizheng and Lu, Guo and Xu, Dong},
	year = {2021},
	pages = {6042--6051},
}

@inproceedings{bui_comparative_2021,
	title = {Comparative {Study} of {3D} {Point} {Cloud} {Compression} {Methods}},
	doi = {10.1109/BigData52589.2021.9671822},
	abstract = {3D sensors such as LiDAR, stereo cameras, and radar have been used in many applications, for instance, virtual or augmented reality, real-time immersive communications, and autonomous driving systems. The output of 3D sensors is often represented in the form of point clouds. However, the massive amount of point cloud data generated from 3D sensors poses big challenges in data storage and transmission. Therefore, effective compression schemes are needed for reducing the bandwidth of wireless networks or storage space of 3D point cloud data. Several point cloud compression (PCC) algorithms have been proposed using signal processing or neural network techniques. In this study, we investigate four state-of-the-art PCC methods using two different datasets with various configurations. The objective of this study is to provide a comprehensive understanding of various approaches in PCC. The results of this paper will be helpful in developing an adaptive 3D point cloud stream compression benchmark that is efficient and benefited from different PCC techniques.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Bui, Mai and Chang, Lin-Ching and Liu, Hang and Zhao, Qi and Chen, Genshe},
	month = dec,
	year = {2021},
	keywords = {Bandwidth, Big Data, Bit rate, Point cloud compression, Signal processing algorithms, Three-dimensional displays, Wireless networks, data compression, evaluation metrics, performance assessment, point cloud},
	pages = {5859--5861},
}

@misc{nvidia_nvidia_nodate,
	title = {{NVIDIA} {GeForce} {RTX} 30 {Series}},
	url = {https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/},
	abstract = {Experience The Ultimate Performance For Gamers \& Creators With Unrealistic Advanced Features.},
	language = {en-us},
	urldate = {2023-02-28},
	journal = {NVIDIA},
	author = {{NVIDIA}},
}

@misc{nvidia_nvidia_nodate-1,
	title = {{NVIDIA} {GeForce} {RTX} 40 {Series}},
	url = {https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/},
	abstract = {Beyond fast for gamers and creators.},
	language = {en-us},
	urldate = {2023-02-28},
	journal = {NVIDIA},
	author = {{NVIDIA}},
}

@misc{nvidia_nvidia_nodate-2,
	title = {{NVIDIA} {RTX} {A6000}},
	url = {https://www.nvidia.com/es-es/design-visualization/rtx-a6000/},
	abstract = {Descubre la nueva generación de diseños revolucionarios y experiencias de entretenimiento inmersivo.},
	language = {es-es},
	urldate = {2023-02-28},
	journal = {NVIDIA},
	author = {{NVIDIA}},
}

@misc{noauthor_gpu_nodate,
	title = {{GPU} {NVIDIA} {GeForce} {RTX} serie 30 con tecnología de arquitectura {Ampere}},
	url = {https://www.nvidia.com/es-es/geforce/graphics-cards/30-series/},
	abstract = {Experimente el máximo rendimiento para jugadores y creadores con funciones avanzadas poco realistas.},
	language = {es-es},
	urldate = {2023-02-28},
	journal = {NVIDIA},
}

@article{van_der_meer_effectiveness_2006,
	title = {The effectiveness of spectral similarity measures for the analysis of hyperspectral imagery},
	volume = {8},
	issn = {1569-8432},
	url = {https://www.sciencedirect.com/science/article/pii/S030324340500053X},
	doi = {10.1016/j.jag.2005.06.001},
	abstract = {In geological imaging spectrometry (i.e., hyperspectral remote sensing), surface compositional information (e.g., mineralogy and subsequently chemistry) is obtained by statistical comparison (by means of spectral matching algorithms) of known field- or library spectra to unknown image spectra. Though these algorithms are readily used, little emphasis has been given to comparison of the performance of the various spectral matching algorithms. Four spectral measures are presented: three that calculate the angle (spectral angle measure, SAM), the vector distance (Euclidean distance measure, ED) or the vector cross-correlation (spectral correlation measure, SCM), between a known reference and unknown target spectrum and a fourth measure that measures the discrepancy of probability distributions between two pixel vectors (the spectral information divergence, SID). The performance of these spectral similarity measures is compared using synthetic hyperspectral and real (i.e., Airborne Visible Infrared Imaging Spectrometer, AVIRIS) hyperspectral data of a (artificial or real) hydrothermal alteration system characterised by the minerals alunite, kaolinite, montmorillonite and quartz. Two statistics are used to assess the performance of the spectral similarity measures: the probability of spectral discrimination (PSD) and the power of spectral discrimination (PWSD). The first relates to the ability of the selected set of spectral endmembers to map a target spectrum, whereas the second expresses the capability of a spectral measure to separate two classes relative to a reference class. Analysis of the synthetic data set (i.e., simulated alteration zones with crisp boundaries at 1–2nm spectral resolution) shows that (1) the SID outperforms the classical empirical spectral matching techniques (SAM, SCM and ED), (2) that SCM (SID, SAM and ED do not) exploits the overall shape of the reflectance curve and hence its outcomes are (positively and negatively) affected by the spectral range selected, (3) SAM and ED give nearly similar results and (4) for the same reason as in (2), the SCM is also more sensitive (again in positive and negative sense) to the spectral noise added. Results from the study of AVIRIS data show that SAM yields more spectral confusion (i.e., class overlap) than SID and SCM. In turn, SID is more effective in mapping the four target minerals than SCM as it clearly outperforms SCM when the target mineral coincides with the mineral phase on the ground.},
	language = {en},
	number = {1},
	urldate = {2023-02-26},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {van der Meer, Freek},
	month = jan,
	year = {2006},
	keywords = {Absorption features, Hyperspectral remote sensing, Performance indicators, Spectral matching, Spectroscopy},
	pages = {3--17},
}

@article{cardenas_reconstruction_2022,
	title = {Reconstruction of tree branching structures from {UAV}-{LiDAR} data},
	volume = {10},
	issn = {2296-665X},
	url = {https://www.frontiersin.org/articles/10.3389/fenvs.2022.960083},
	abstract = {The reconstruction of tree branching structures is a longstanding problem in Computer Graphics which has been studied over several data sources, from photogrammetry point clouds to Terrestrial and Aerial Laser Imaging Detection and Ranging technology. However, most data sources present acquisition errors that make the reconstruction more challenging. Among them, the main challenge is the partial or complete occlusion of branch segments, thus leading to disconnected components whether the reconstruction is resolved using graph-based approaches. In this work, we propose a hybrid method based on radius-based search and Minimum Spanning Tree for the tree branching reconstruction by handling occlusion and disconnected branches. Furthermore, we simplify previous work evaluating the similarity between ground-truth and reconstructed skeletons. Using this approach, our method is proved to be more effective than the baseline methods, regarding reconstruction results and response time. Our method yields better results on the complete explored radii interval, though the improvement is especially significant on the Ground Sampling Distance In terms of latency, an outstanding performance is achieved in comparison with the baseline method.},
	urldate = {2023-02-24},
	journal = {Frontiers in Environmental Science},
	author = {Cárdenas, José L. and López, Alfonso and Ogayar, Carlos J. and Feito, Francisco R. and Jurado, Juan M.},
	year = {2022},
}

@article{gao_rate-distortion_2022,
	title = {Rate-{Distortion} {Modeling} for {Bit} {Rate} {Constrained} {Point} {Cloud} {Compression}},
	issn = {1051-8215, 1558-2205},
	url = {http://arxiv.org/abs/2211.10646},
	doi = {10.1109/TCSVT.2022.3223898},
	abstract = {As being one of the main representation formats of 3D real world and well-suited for virtual reality and augmented reality applications, point clouds have gained a lot of popularity. In order to reduce the huge amount of data, a considerable amount of research on point cloud compression has been done. However, given a target bit rate, how to properly choose the color and geometry quantization parameters for compressing point clouds is still an open issue. In this paper, we propose a rate-distortion model based quantization parameter selection scheme for bit rate constrained point cloud compression. Firstly, to overcome the measurement uncertainty in evaluating the distortion of the point clouds, we propose a unified model to combine the geometry distortion and color distortion. In this model, we take into account the correlation between geometry and color variables of point clouds and derive a dimensionless quantity to represent the overall quality degradation. Then, we derive the relationships of overall distortion and bit rate with the quantization parameters. Finally, we formulate the bit rate constrained point cloud compression as a constrained minimization problem using the derived polynomial models and deduce the solution via an iterative numerical method. Experimental results show that the proposed algorithm can achieve optimal decoded point cloud quality at various target bit rates, and substantially outperform the video-rate-distortion model based point cloud compression scheme.},
	urldate = {2023-02-23},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Gao, Pan and Luo, Shengzhou and Paul, Manoranjan},
	year = {2022},
	note = {arXiv:2211.10646 [cs, eess, math]},
	keywords = {Computer Science - Information Theory, Computer Science - Multimedia, Electrical Engineering and Systems Science - Image and Video Processing},
	pages = {1--1},
}

@article{candiago_evaluating_2015,
	title = {Evaluating {Multispectral} {Images} and {Vegetation} {Indices} for {Precision} {Farming} {Applications} from {UAV} {Images}},
	volume = {7},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/7/4/4026},
	doi = {10.3390/rs70404026},
	abstract = {Unmanned Aerial Vehicles (UAV)-based remote sensing offers great possibilities to acquire in a fast and easy way field data for precision agriculture applications. This field of study is rapidly increasing due to the benefits and advantages for farm resources management, particularly for studying crop health. This paper reports some experiences related to the analysis of cultivations (vineyards and tomatoes) with Tetracam multispectral data. The Tetracam camera was mounted on a multi-rotor hexacopter. The multispectral data were processed with a photogrammetric pipeline to create triband orthoimages of the surveyed sites. Those orthoimages were employed to extract some Vegetation Indices (VI) such as the Normalized Difference Vegetation Index (NDVI), the Green Normalized Difference Vegetation Index (GNDVI), and the Soil Adjusted Vegetation Index (SAVI), examining the vegetation vigor for each crop. The paper demonstrates the great potential of high-resolution UAV data and photogrammetric techniques applied in the agriculture framework to collect multispectral images and evaluate different VI, suggesting that these instruments represent a fast, reliable, and  cost-effective resource in crop assessment for precision farming applications.},
	language = {en},
	number = {4},
	urldate = {2023-02-17},
	journal = {Remote Sensing},
	author = {Candiago, Sebastian and Remondino, Fabio and De Giglio, Michaela and Dubbini, Marco and Gattelli, Mario},
	month = apr,
	year = {2015},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {agriculture, crops, multispectral, photogrammetry, unmanned aerial vehicles, vegetation, vegetation indices},
	pages = {4026--4047},
}

@book{navulur_multispectral_2006,
	address = {Boca Raton},
	title = {Multispectral {Image} {Analysis} {Using} the {Object}-{Oriented} {Paradigm}},
	isbn = {978-0-429-14630-5},
	abstract = {Bringing a fresh new perspective to remote sensing, object-based image analysis is a paradigm shift from the traditional pixel-based approach. Featuring various practical examples to provide understanding of this new modus operandi, Multispectral Image Analysis Using the Object-Oriented Paradigm reviews the current image analysis methods and demonstrates advantages to improve information extraction from imagery.This reference describes traditional image analysis techniques, introduces object-oriented technology, and discusses the benefits of object-based versus pixel-based classification. It examines the creation of object primitives using image segmentation approaches and the use of various techniques for object classification. The author covers image enhancement methods, how to use ancillary data to constrain image segmentation, and concepts of semantic grouping of objects. He concludes by addressing accuracy assessment approaches. The accompanying downloadable resources present sample data that enable the use of different approaches to problem solving.Integrating remote sensing techniques and GIS analysis, Multispectral Image Analysis Using the Object-Oriented Paradigm distills new tools to extract information from remotely sensed data.},
	publisher = {CRC Press},
	author = {Navulur, Kumar},
	month = dec,
	year = {2006},
	doi = {10.1201/9781420043075},
}

@article{ring_discovery_2000,
	title = {The discovery of infrared radiation in 1800},
	volume = {48},
	issn = {1368-2199},
	url = {https://doi.org/10.1080/13682199.2000.11784339},
	doi = {10.1080/13682199.2000.11784339},
	abstract = {The foundation for thermal imaging from the infrared part of the electromagnetic spectrum was laid by two members of the same distinguished family. William Herschel, a talented musician, came to England from Hannover in the eighteenth century. He became famous through his discovery in 1781 of a new planet, Uranus, while living in Bath. In 1782 he became the King's Astronomer and while at Slough in 1800 discovered the presence of invisible heating rays, now known as infrared radiation. Forty years later, William's only son John made an image of solar heat using a simple evaporagraph, where heat was used to disperse soot (carbon) particles in an alcoholic suspension. William carried out many experiments to study the relationship between visible and invisible rays. These were simple but yet elegant methods of examining absorption and reflection.},
	number = {1},
	urldate = {2023-02-15},
	journal = {The Imaging Science Journal},
	author = {Ring, E F J},
	month = jan,
	year = {2000},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/13682199.2000.11784339},
	keywords = {discovery, infrared radiation, thermal imaging},
	pages = {1--8},
}

@article{minkina_how_2021,
	title = {How {Infrared} {Radiation} {Was} {Discovered}—{Range} of {This} {Discovery} and {Detailed}, {Unknown} {Information}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/21/9824},
	doi = {10.3390/app11219824},
	abstract = {The reason for writing this article is that the details and mainly the scope of the fundamental discovery of infrared radiation are not widely known, and different accounts of this story are found in the literature. For example, not everyone knows that the discoverer of infrared radiation, F. W. Herschel, simultaneously studied its properties, which he, then, described in detail in his publications. It can be concluded that the history of the discovery of infrared radiation is treated marginally in the literature. This is not fair, considering the fact that infrared radiation is of fundamental importance to modern man. On the other hand, the history of the discovery of, for example, X-rays or Maxwell’s electromagnetic radiation is well known—this information is passed on to students of electrical faculties during lectures on “Fundamentals of Physics” or “Fundamentals of Electrical Engineering”. Although it is currently believed that the significance of infrared radiation for modern man is comparable to that of X-rays, when I ask the students during lectures who discovered infrared radiation and how, there is usually a deafening silence.},
	language = {en},
	number = {21},
	urldate = {2023-02-15},
	journal = {Applied Sciences},
	author = {Minkina, Waldemar},
	month = jan,
	year = {2021},
	note = {Number: 21
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {F. W. Herschel’s experiments, history of the discovery of infrared, infrared radiation, infrared thermography, radiant heat transfer, thermal imaging, thermovision measurements},
	pages = {9824},
}

@article{jiang_diurnal_2022,
	title = {Diurnal variations in directional brightness temperature over urban areas through a multi-angle {UAV} experiment},
	volume = {222},
	issn = {0360-1323},
	url = {https://www.sciencedirect.com/science/article/pii/S0360132322006400},
	doi = {10.1016/j.buildenv.2022.109408},
	abstract = {Quasi-synchronous multi-angle observations of urban surface temperature (UST) are critical for understanding the urban thermal anisotropy (UTA) regime and validating UTA models. Such observations can be performed by thermal sensors onboard lightweight unmanned aerial vehicles (UAVs), due to their flexibility in sampling from different directions. This study designed a straightforward and efficient protocol to obtain quasi-synchronous multi-angle USTs based on a lightweight UAV for a typical residential area in Nanjing (China). By using this protocol, we retrieved a directional brightness temperature (DBT) dataset with 73 DBTs 12 times throughout the diurnal cycle. Our results exhibited that the diurnal UTA intensity (UTAI) well corresponds to that of air temperature: the maximum UTAI (14.0 °C) occurred around 14:00 to 15:00 local time, and the minimum (3.5 °C) occurred at night. Interestingly, the nighttime DBT variations were found to depend both on viewing zenith and azimuth angles, and a slight hotspot effect was identified. These findings notably differ from previous studies based on modeling, which often assumed the independence of the nighttime DBT of the viewing azimuth angle and the absence of the hotspot effect. The analogous pattern of UTA between the daytime and nighttime (especially before midnight) was seemingly driven by a remarkable temperature contrast among urban surface components that can persist for many hours after sunset. Thus, the designed observation protocol can be applied to retrieve quasi-synchronous multi-angle USTs for other types of urban surfaces, thereby facilitating a complete understanding of diurnal UTA variations and assisting the validations of UTA models.},
	language = {en},
	urldate = {2023-02-15},
	journal = {Building and Environment},
	author = {Jiang, Lu and Zhan, Wenfeng and Tu, Lili and Dong, Pan and Wang, Shasha and Li, Long and Wang, Chunli and Wang, Chenguang},
	month = aug,
	year = {2022},
	keywords = {Directional brightness temperature, Land surface temperature, Thermal remote sensing, Unmanned aerial vehicle, Urban thermal anisotropy},
	pages = {109408},
}

@article{paziewska_integration_2022,
	title = {Integration of {Thermal} and {RGB} {Data} {Obtained} by {Means} of a {Drone} for {Interdisciplinary} {Inventory}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1996-1073},
	url = {https://www.mdpi.com/1996-1073/15/14/4971},
	doi = {10.3390/en15144971},
	abstract = {Thermal infrared imagery is very much gaining in importance in the diagnosis of energy losses in cultural heritage through non-destructive measurement methods. Hence, owing to the fact that it is a very innovative and, above all, safe solution, it is possible to determine the condition of the building, locate places exposed to thermal escape, and plan actions to improve the condition of the facility. The presented work is devoted to the technology of creating a dense point cloud and a 3D model, based on data obtained from UAV. It has been shown that it is possible to build a 3D point model based on thermograms with the specified accuracy by using thermal measurement marks and the dense matching method. The results achieved in this way were compared and, as the result of this work, the model obtained from color photos was integrated with the point cloud created on the basis of the thermal images. The discussed approach exploits measurement data obtained with three independent devices (tools/appliances): a Matrice 300 RTK drone (courtesy of NaviGate); a Phantom 4 PRO drone; and a KT-165 thermal imaging camera. A stone church located in the southern part of Poland was chosen as the measuring object.},
	language = {en},
	number = {14},
	urldate = {2023-02-15},
	journal = {Energies},
	author = {Paziewska, Joanna and Rzonca, Antoni},
	month = jan,
	year = {2022},
	note = {Number: 14
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {UAV, data integration, dense matching, thermal imagery},
	pages = {4971},
}

@misc{cambridge_english_dictionary_cambridge_2023,
	title = {Cambridge {University} {Press}},
	shorttitle = {Cambridge {English} {Dictionary}},
	url = {https://dictionary.cambridge.org/dictionary/english/},
	abstract = {The most popular dictionary and thesaurus. Meanings \& definitions of words in English with examples, synonyms, pronunciations and translations.},
	language = {en},
	urldate = {2023-02-12},
	author = {{Cambridge English Dictionary}},
	month = feb,
	year = {2023},
}

@misc{cambridge_dictionary_image_2023,
	title = {Image},
	url = {https://dictionary.cambridge.org/es/diccionario/ingles/image},
	language = {en},
	urldate = {2023-02-12},
	author = {{Cambridge Dictionary}},
	month = feb,
	year = {2023},
}

@book{singh_application_2023,
	title = {Application of {Remote} {Sensing} and {GIS} in {Natural} {Resources} and {Built} {Infrastructure} {Management}: 105},
	isbn = {978-3-031-14095-2},
	shorttitle = {Application of {Remote} {Sensing} and {GIS} in {Natural} {Resources} and {Built} {Infrastructure} {Management}},
	abstract = {This book discusses the problems in planning, building, and management strategies in the wake of application and expansion of remote sensing and GIS products in natural resources and infrastructure management. The book suggests proactive solutions to problems of natural resources and infrastructure management, providing alternatives for strategic planning, effective delivery, and growth perspectives. The uniqueness of the book is its broader spectrum of coverage with related interconnections and interdependences across science, engineering, and innovation.  The book contains information that can be downscaled to the local level. Presenting a wide spectrum of viewpoints and approaches, the book is a collective of topics such as application to agriculture and forestry (land and landscape, agriculture, forestry management and deforestation), water resources and ecology (hydro-meteorological, climate diagnostics, and prognostics, water resources management, environment management, cross-scale ecology and resilience), urban management (urban planning, design, construction and operations of infrastructure, natural disasters, novel approaches to upgrade old infrastructure), hydro informatics, predictive and geospatial data analytics, synthesis, and management through the various processes, tools, and technologies.},
	language = {Inglés},
	author = {Singh, Vijay P. and Yadav, Shalini and Yadava, Ram Narayan and Perez, Gerald Augusto Corzo and Muñoz-Arriola, Francisco and Yadav, Krishna Kumar},
	year = {2023},
}

@book{lawhead_learning_2019,
	address = {Birmingham Mumbai},
	title = {Learning {Geospatial} {Analysis} with {Python}: {Understand} {GIS} fundamentals and perform remote sensing data analysis using {Python} 3.7, 3rd {Edition}},
	isbn = {978-1-78995-927-7},
	shorttitle = {Learning {Geospatial} {Analysis} with {Python}},
	abstract = {Learn the core concepts of geospatial data analysis for building actionable and insightful GIS applicationsKey FeaturesCreate GIS solutions using the new features introduced in Python 3.7 Explore a range of GIS tools and libraries such as PostGIS, QGIS, and PROJ Learn to automate geospatial analysis workflows using Python and JupyterBook DescriptionGeospatial analysis is used in almost every domain you can think of, including defense, farming, and even medicine. With this systematic guide, you'll get started with geographic information system (GIS) and remote sensing analysis using the latest features in Python. This book will take you through GIS techniques, geodatabases, geospatial raster data, and much more using the latest built-in tools and libraries in Python 3.7. You'll learn everything you need to know about using software packages or APIs and generic algorithms that can be used for different situations. Furthermore, you'll learn how to apply simple Python GIS geospatial processes to a variety of problems, and work with remote sensing data. By the end of the book, you'll be able to build a generic corporate system, which can be implemented in any organization to manage customer support requests and field support personnel.What you will learnAutomate geospatial analysis workflows using Python Code the simplest possible GIS in just 60 lines of Python Create thematic maps with Python tools such as PyShp, OGR, and the Python Imaging Library Understand the different formats that geospatial data comes in Produce elevation contours using Python tools Create flood inundation models Apply geospatial analysis to real-time data tracking and storm chasingWho this book is forThis book is for Python developers, researchers, or analysts who want to perform geospatial modeling and GIS analysis with Python. Basic knowledge of digital mapping and analysis using Python or other scripting languages will be helpful.Table of ContentsLearning about Geospatial Analysis with PythonLearning Geospatial DataThe Geospatial Technology LandscapeGeospatial Python ToolboxPython and Geographic Information SystemsPython and Remote SensingPython and Elevation DataAdvanced Geospatial Python ModelingReal-Time DataPutting It All Together},
	language = {Inglés},
	author = {Lawhead, Joel},
	month = sep,
	year = {2019},
}

@book{quattrochi_integrating_2017,
	address = {Boca Raton},
	title = {Integrating {Scale} in {Remote} {Sensing} and {GIS}},
	isbn = {978-1-315-37372-0},
	abstract = {Integrating Scale in Remote Sensing and GIS serves as the most comprehensive documentation of the scientific and methodological advances that have taken place in integrating scale and remote sensing data. This work addresses the invariants of scale, the ability to change scale, measures of the impact of scale, scale as a parameter in process models, and the implementation of multiscale approaches as methods and techniques for integrating multiple kinds of remote sensing data collected at varying spatial, temporal, and radiometric scales. Researchers, instructors, and students alike will benefit from a guide that has been pragmatically divided into four thematic groups: scale issues and multiple scaling; physical scale as applied to natural resources; urban scale; and human health/social scale. Teeming with insights that elucidate the significance of scale as a foundation for geographic analysis, this book is a vital resource to those seriously involved in the field of GIScience.},
	publisher = {CRC Press},
	editor = {Quattrochi, Dale A. and Wentz, Elizabeth and Lam, Nina Siu-Ngan and Emerson, Charles W.},
	month = jan,
	year = {2017},
	doi = {10.1201/9781315373720},
}

@article{jia_practical_2022,
	title = {A {Practical} {Algorithm} for the {Viewpoint} {Planning} of {Terrestrial} {Laser} {Scanners}},
	volume = {2},
	doi = {10.3390/geomatics2020011},
	abstract = {Applications using terrestrial laser scanners (TLS) have been skyrocketing in the past two decades. In a scanning project, the configuration of scans is a critical issue as it has significant effects on the project cost and the quality of the product. In this paper, a practical strategy is proposed to resolve the problem of the optimal placement of the terrestrial laser scanner. The method attempts to reduce the number of viewpoints under the premise that the scenes are fully covered. In addition, the approach is designed in a way that the solutions can be efficiently explored. The method has been tested on 540 polygons simulated with different sizes and complexities. The results have also been compared with a benchmark strategy in terms of the optimality of the solutions and runtime. It is concluded that our proposed algorithm ties or reduces the number of viewpoints in the benchmark paper in 85.6\% of the 540 tests. For complex environments, the method can potentially reduce the project cost by 10\%. Although with relatively lower efficiency, our method can still reach the solution within a few minutes for a polygon with up to 500 vertices.},
	journal = {Geomatics},
	author = {Jia, Fengman and Lichti, Derek},
	month = apr,
	year = {2022},
	pages = {181--196},
}

@article{li_image_2021,
	title = {Image retrieval from remote sensing big data: {A} survey},
	volume = {67},
	issn = {1566-2535},
	shorttitle = {Image retrieval from remote sensing big data},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253520303778},
	doi = {10.1016/j.inffus.2020.10.008},
	abstract = {The blooming proliferation of aeronautics and astronautics platforms, together with the ever-increasing remote sensing imaging sensors on these platforms, has led to the formation of rapidly-growing earth observation data with the characteristics of large volume, large variety, large velocity, large veracity and large value, which raises awareness about the importance of large-scale image processing, fusion and mining. Unconsciously, we have entered an era of big earth data, also called remote sensing (RS) big data. Although RS big data provides great opportunities for a broad range of applications such as disaster rescue, global security, and so forth, it inevitably poses many additional processing challenges. As one of the most fundamental and important tasks in RS big data mining, image retrieval (i.e., image information mining) from RS big data has attracted continuous research interests in the last several decades. This paper mainly works for systematically reviewing the emerging achievements for image retrieval from RS big data. And then this paper further discusses the RS image retrieval based applications including fusion-oriented RS image processing, geo-localization and disaster rescue. To facilitate the quantitative evaluation of the RS image retrieval technique, this paper gives a list of publicly open datasets and evaluation metrics, and briefly recalls the mainstream methods on two representative benchmarks of RS image retrieval. Considering the latest advances from multiple domains including computer vision, machine learning and knowledge engineering, this paper points out some promising research directions towards RS big data mining. From this survey, engineers from industry may find skills to improve their RS image retrieval systems and researchers from academia may find ideas to conduct some innovative work.},
	language = {en},
	urldate = {2023-02-06},
	journal = {Information Fusion},
	author = {Li, Yansheng and Ma, Jiayi and Zhang, Yongjun},
	month = mar,
	year = {2021},
	keywords = {Evaluation datasets and performance discussion, Future research directions, Remote sensing (rs) big data, Rs image retrieval applications, Rs image retrieval methods},
	pages = {94--115},
}

@inproceedings{basu_deepsat_2015,
	address = {New York, NY, USA},
	series = {{SIGSPATIAL} '15},
	title = {{DeepSat}: a learning framework for satellite imagery},
	isbn = {978-1-4503-3967-4},
	shorttitle = {{DeepSat}},
	url = {https://doi.org/10.1145/2820783.2820816},
	doi = {10.1145/2820783.2820816},
	abstract = {Satellite image classification is a challenging problem that lies at the crossroads of remote sensing, computer vision, and machine learning. Due to the high variability inherent in satellite data, most of the current object classification approaches are not suitable for handling satellite datasets. The progress of satellite image analytics has also been inhibited by the lack of a single labeled high-resolution dataset with multiple class labels. The contributions of this paper are twofold -- (1) first, we present two new satellite datasets called SAT-4 and SAT-6, and (2) then, we propose a classification framework that extracts features from an input image, normalizes them and feeds the normalized feature vectors to a Deep Belief Network for classification. On the SAT-4 dataset, our best network produces a classification accuracy of 97.95\% and outperforms three state-of-the-art object recognition algorithms, namely - Deep Belief Networks, Convolutional Neural Networks and Stacked Denoising Autoencoders by {\textasciitilde}11\%. On SAT-6, it produces a classification accuracy of 93.9\% and outperforms the other algorithms by {\textasciitilde}15\%. Comparative studies with a Random Forest classifier show the advantage of an unsupervised learning approach over traditional supervised learning techniques. A statistical analysis based on Distribution Separability Criterion and Intrinsic Dimensionality Estimation substantiates the effectiveness of our approach in learning better representations for satellite imagery.},
	urldate = {2023-02-06},
	booktitle = {Proceedings of the 23rd {SIGSPATIAL} {International} {Conference} on {Advances} in {Geographic} {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Basu, Saikat and Ganguly, Sangram and Mukhopadhyay, Supratik and DiBiano, Robert and Karki, Manohar and Nemani, Ramakrishna},
	month = nov,
	year = {2015},
	keywords = {deep learning, high resolution, satellite imagery},
	pages = {1--10},
}

@article{huang_agricultural_2018,
	title = {Agricultural remote sensing big data: {Management} and applications},
	volume = {17},
	issn = {2095-3119},
	shorttitle = {Agricultural remote sensing big data},
	url = {https://www.sciencedirect.com/science/article/pii/S2095311917618598},
	doi = {10.1016/S2095-3119(17)61859-8},
	abstract = {Big data with its vast volume and complexity is increasingly concerned, developed and used for all professions and trades. Remote sensing, as one of the sources for big data, is generating earth-observation data and analysis results daily from the platforms of satellites, manned/unmanned aircrafts, and ground-based structures. Agricultural remote sensing is one of the backbone technologies for precision agriculture, which considers within-field variability for site-specific management instead of uniform management as in traditional agriculture. The key of agricultural remote sensing is, with global positioning data and geographic information, to produce spatially-varied data for subsequent precision agricultural operations. Agricultural remote sensing data, as general remote sensing data, have all characteristics of big data. The acquisition, processing, storage, analysis and visualization of agricultural remote sensing big data are critical to the success of precision agriculture. This paper overviews available remote sensing data resources, recent development of technologies for remote sensing big data management, and remote sensing data processing and management for precision agriculture. A five-layer-fifteen-level (FLFL) satellite remote sensing data management structure is described and adapted to create a more appropriate four-layer-twelve-level (FLTL) remote sensing data management structure for management and applications of agricultural remote sensing big data for precision agriculture where the sensors are typically on high-resolution satellites, manned aircrafts, unmanned aerial vehicles and ground-based structures. The FLTL structure is the management and application framework of agricultural remote sensing big data for precision agriculture and local farm studies, which outlooks the future coordination of remote sensing big data management and applications at local regional and farm scale.},
	language = {en},
	number = {9},
	urldate = {2023-02-06},
	journal = {Journal of Integrative Agriculture},
	author = {Huang, Yanbo and Chen, Zhong-xin and Yu, Tao and Huang, Xiang-zhi and Gu, Xing-fa},
	month = sep,
	year = {2018},
	keywords = {agricultural information, big data, precision agriculture, remote sensing},
	pages = {1915--1931},
}

@misc{earth_observation_portal_earth_nodate,
	title = {Earth {Observation} {Missions} - {eoPortal}},
	url = {https://www.eoportal.org/},
	urldate = {2023-02-06},
	author = {{Earth Observation Portal}},
}

@article{bejar-martos_strategies_2022,
	title = {Strategies for the {Storage} of {Large} {LiDAR} {Datasets}—{A} {Performance} {Comparison}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/11/2623},
	doi = {10.3390/rs14112623},
	abstract = {The widespread use of LiDAR technologies has led to an ever-increasing volume of captured data that pose a continuous challenge for its storage and organization, so that it can be efficiently processed and analyzed. Although the use of system files in formats such as LAS/LAZ is the most common solution for LiDAR data storage, databases are gaining in popularity due to their evident advantages: centralized and uniform access to a collection of datasets; better support for concurrent retrieval; distributed storage in database engines that allows sharding; and support for metadata or spatial queries by adequately indexing or organizing the data. The present work evaluates the performance of four popular NoSQL and relational database management systems with large LiDAR datasets: Cassandra, MongoDB, MySQL and PostgreSQL. To perform a realistic assessment, we integrate these database engines in a repository implementation with an elaborate data model that enables metadata and spatial queries and progressive/partial data retrieval. Our experimentation concludes that, as expected, NoSQL databases show a modest but significant performance difference in favor of NoSQL databases, and that Cassandra provides the best overall database solution for LiDAR data.},
	language = {en},
	number = {11},
	urldate = {2023-02-03},
	journal = {Remote Sensing},
	author = {Béjar-Martos, Juan A. and Rueda-Ruiz, Antonio J. and Ogayar-Anguita, Carlos J. and Segura-Sánchez, Rafael J. and López-Ruiz, Alfonso},
	month = jan,
	year = {2022},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {LiDAR, NoSQL, databases, point clouds},
	pages = {2623},
}

@article{ogayar-anguita_nested_2023,
	title = {Nested spatial data structures for optimal indexing of {LiDAR} data},
	volume = {195},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271622003112},
	doi = {10.1016/j.isprsjprs.2022.11.018},
	abstract = {In this paper we present a flexible framework for creating spatial data structures to manage LiDAR point clouds in the context of spatial big data. For this purpose, standard approaches typically include the use of a single data structure to index point clouds. Some of them use a hybrid two-tier solution to optimize specific application purposes such as storage or rendering. In this article we introduce a meta-structure that can have unlimited depth and a custom, user-defined combination of nested structures, such as grids, quadtrees, octrees, or kd-trees. With our approach, the out-of-core indexing of point clouds can be adapted to different types of datasets, taking into account the spatial distribution of the data. Therefore, the most suitable spatial indexing can be achieved for any type of dataset, from small TLS-based scenes to planetary-scale ALS-based scenes. This approach allows us to work with overlapping datasets of different resolutions from different acquisition technologies in the same structure.},
	language = {en},
	urldate = {2023-02-03},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Ogayar-Anguita, Carlos J. and López-Ruiz, Alfonso and Rueda-Ruiz, Antonio J. and Segura-Sánchez, Rafael J.},
	month = jan,
	year = {2023},
	keywords = {LiDAR, Spatial big data, Spatial data structure, Ubiquitous Point Cloud},
	pages = {287--297},
}

@article{lopez_metaheuristics_2023,
	title = {Metaheuristics for the optimization of {Terrestrial} {LiDAR} set-up},
	volume = {146},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580522005453},
	doi = {10.1016/j.autcon.2022.104675},
	abstract = {3D point clouds have a significant impact on a wide range of applications, although their acquisition is frequently conditioned by the occlusion of the objects in the scene. To address this problem, this paper describes an approach for optimizing LiDAR (Light Detection and Ranging) surveys using metaheuristics such as local searches and genetic algorithms. The method generates a set of optimal scanning locations to densely cover the real-world environment represented through 3D synthetic models. Compared to previous research, this paper handles 3D occlusion by varying the height of the sensor. Also, previously used metrics are compressed into three functions to avoid multi-objective optimization. Regarding performance, a LiDAR scanning solution based on GPU (Graphics Processing Unit) hardware is used. Several tests were conducted to show that the combination of local searches and genetic algorithms generates a reduced set of locations capable of optimizing the scanning of buildings.},
	language = {en},
	urldate = {2023-02-03},
	journal = {Automation in Construction},
	author = {López, Alfonso and Ogayar, Carlos J. and Jurado, Juan M. and Feito, Francisco R.},
	month = feb,
	year = {2023},
	keywords = {GPGPU, Genetic algorithm, LiDAR, Metaheuristic, Planning for Scanning},
	pages = {104675},
}

@article{stolarski_synergistic_2022,
	title = {Synergistic {Use} of {Sentinel}-2 and {UAV} {Multispectral} {Data} to {Improve} and {Optimize} {Viticulture} {Management}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-446X},
	url = {https://www.mdpi.com/2504-446X/6/11/366},
	doi = {10.3390/drones6110366},
	abstract = {The increasing use of geospatial information from satellites and unmanned aerial vehicles (UAVs) has been contributing to significant growth in the availability of instruments and methodologies for data acquisition and analysis. For better management of vineyards (and most crops), it is crucial to access the spatial-temporal variability. This knowledge throughout the vegetative cycle of any crop is crucial for more efficient management, but in the specific case of viticulture, this knowledge is even more relevant. Some research studies have been carried out in recent years, exploiting the advantage of satellite and UAV data, used individually or in combination, for crop management purposes. However, only a few studies explore the multi-temporal use of these two types of data, isolated or synergistically. This research aims to clearly identify the most suitable data and strategies to be adopted in specific stages of the vineyard phenological cycle. Sentinel-2 data from two vineyard plots, located in the Douro Demarcated Region (Portugal), are compared with UAV multispectral data under three distinct conditions: considering the whole vineyard plot; considering only the grapevine canopy; and considering inter-row areas (excluding all grapevine vegetation). The results show that data from both platforms are able to describe the vineyards’ variability throughout the vegetative growth but at different levels of detail. Sentinel-2 data can be used to map vineyard soil variability, whilst the higher spatial resolution of UAV-based data allows diverse types of applications. In conclusion, it should be noted that, depending on the intended use, each type of data, individually, is capable of providing important information for vineyard management.},
	language = {en},
	number = {11},
	urldate = {2022-11-22},
	journal = {Drones},
	author = {Stolarski, Oiliam and Fraga, Hélder and Sousa, Joaquim J. and Pádua, Luís},
	month = nov,
	year = {2022},
	keywords = {\textit{Vitis vinifera}, Douro Demarcated Region, Sentinel-2, grapevine phenology, multi-temporal monitoring, unmanned aerial vehicles},
	pages = {366},
}

@article{buscombe_12_2023,
	title = {A 1.2 {Billion} {Pixel} {Human}-{Labeled} {Dataset} for {Data}-{Driven} {Classification} of {Coastal} {Environments}},
	volume = {10},
	copyright = {2023 This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-023-01929-2},
	doi = {10.1038/s41597-023-01929-2},
	abstract = {The world’s coastlines are spatially highly variable, coupled-human-natural systems that comprise a nested hierarchy of component landforms, ecosystems, and human interventions, each interacting over a range of space and time scales. Understanding and predicting coastline dynamics necessitates frequent observation from imaging sensors on remote sensing platforms. Machine Learning models that carry out supervised (i.e., human-guided) pixel-based classification, or image segmentation, have transformative applications in spatio-temporal mapping of dynamic environments, including transient coastal landforms, sediments, habitats, waterbodies, and water flows. However, these models require large and well-documented training and testing datasets consisting of labeled imagery. We describe “Coast Train,” a multi-labeler dataset of orthomosaic and satellite images of coastal environments and corresponding labels. These data include imagery that are diverse in space and time, and contain 1.2 billion labeled pixels, representing over 3.6 million hectares. We use a human-in-the-loop tool especially designed for rapid and reproducible Earth surface image segmentation. Our approach permits image labeling by multiple labelers, in turn enabling quantification of pixel-level agreement over individual and collections of images.},
	language = {en},
	number = {1},
	urldate = {2023-02-03},
	journal = {Scientific Data},
	author = {Buscombe, Daniel and Wernette, Phillipe and Fitzpatrick, Sharon and Favela, Jaycee and Goldstein, Evan B. and Enwright, Nicholas M.},
	month = jan,
	year = {2023},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Databases, Physical oceanography},
	pages = {46},
}

@article{tezza_state---art_2019,
	title = {The {State}-of-the-{Art} of {Human}–{Drone} {Interaction}: {A} {Survey}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {The {State}-of-the-{Art} of {Human}–{Drone} {Interaction}},
	doi = {10.1109/ACCESS.2019.2953900},
	abstract = {Drones have expanded from military operations to performing a broad range of civilian applications. As drone usage increases, humans will interact with such systems more often, therefore, it is important to achieve a natural human-drone interaction. Although some knowledge can be derived from the field of human-robot interaction, drones can fly in a 3D space, which essentially changes how humans can interact with them, making human-drone interaction a field of its own. This paper is the first survey on the emerging field of human-drone interaction focusing on multi-rotor systems, providing an overview of existing literature and the current state of the art in the field. This work begins with an analysis and comparison of the drone models that are commonly used by end-users and researchers in the field of human-drone interaction. Following, the current state of the field is discussed, including the roles of humans in HDI, innovative control methods, remaining aspects of interaction, and novelty drone prototypes and applications. This paper concludes by presenting a discussion of current challenges and future work in the field of human-drone interaction.},
	journal = {IEEE Access},
	author = {Tezza, Dante and Andujar, Marvin},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Aircraft, Analytical models, Cognitive science, Drone, Drones, FAA, Prototypes, User interfaces, human-computer interaction, human-drone interaction, human-in-the-loop, human-robot interaction, unmanned aerial vehicle},
	pages = {167438--167454},
}

@misc{airbus_pleiades_2021,
	title = {Pléiades {Neo}},
	url = {https://www.airbus.com/en/products-services/space/earth-observation/earth-observation-portfolio/pleiades-neo},
	abstract = {The Pléiades Neo satellites – which are entirely funded, manufactured, owned, and operated by Airbus – represent a breakthrough in Earth observation, creating the company’s most advanced optical constellation for commercial, institutional and governmental customers.},
	language = {en},
	urldate = {2023-02-01},
	journal = {Pléiades Neo},
	author = {{Airbus}},
	month = oct,
	year = {2021},
	note = {Section: Space},
}

@misc{nasa_earth_observatory_lava_2021,
	type = {Text.{Article}},
	title = {Lava {Burns} a {Path} {Through} {La} {Palma}},
	url = {https://earthobservatory.nasa.gov/images/148880/lava-burns-a-path-through-la-palma},
	abstract = {A slow-moving wall of basaltic lava is bulldozing its way through communities on one of the Canary Islands.},
	language = {en},
	urldate = {2023-02-01},
	author = {{NASA Earth Observatory}},
	month = sep,
	year = {2021},
	note = {Publisher: NASA Earth Observatory},
}

@misc{instituto_nacional_de_pesquisas_espaciais_inpecbers_2019,
	title = {{INPE}/{CBERS}},
	url = {http://www.cbers.inpe.br/sobre/cbers04a.php},
	urldate = {2023-02-01},
	author = {{Instituto Nacional de Pesquisas Espaciais}},
	month = jun,
	year = {2019},
}

@misc{the_european_space_agency_sentinel-2_nodate,
	title = {Sentinel-2},
	url = {https://sentinels.copernicus.eu/web/sentinel/missions/sentinel-2},
	urldate = {2023-02-01},
	journal = {Sentinel-2},
	author = {{The European Space Agency}},
}

@misc{noauthor_landsat_2021,
	title = {Landsat 8},
	url = {https://landsat.gsfc.nasa.gov/satellites/landsat-8/},
	abstract = {Landsat 8 launched on February 11, 2013, from Vandenberg Air Force Base, California, on an Atlas-V 401 rocket, with the extended payload fairing (EPF) from United Launch Alliance, LLC. (The Landsat 8 Launch in Quotes.) The Landsat 8 satellite payload consists of two science instruments—the Operational Land Imager (OLI) and the Thermal Infrared Sensor (TIRS).},
	language = {en-US},
	urldate = {2023-02-01},
	journal = {Lansat 8 Mission Details},
	month = nov,
	year = {2021},
}

@misc{noauthor_landsat_2021-1,
	title = {Landsat 9},
	url = {https://landsat.gsfc.nasa.gov/satellites/landsat-9/landsat-9-overview/},
	abstract = {Landsat 9 is the latest satellite in the Landsat series—it continues Landsat’s irreplaceable record of Earth’s land surface. It launched from Vandenberg Space Force Base on September 27, 2021. Landsat 9 largely replicates its predecessor Landsat 8. Landsat 9 carries two science instruments Both instruments have sensors with moderate spatial resolution—15 m (49 ft), 30 m},
	language = {en-US},
	urldate = {2023-02-01},
	journal = {Landsat 9 Overview},
	month = nov,
	year = {2021},
}

@article{masek_landsat_2020,
	title = {Landsat 9: {Empowering} open science and applications through continuity},
	volume = {248},
	issn = {0034-4257},
	shorttitle = {Landsat 9},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425720303382},
	doi = {10.1016/j.rse.2020.111968},
	abstract = {The history of Earth observation from space is well reflected through the Landsat program. With data collection beginning with Landsat-1 in 1972, the program has evolved technical capabilities while maintaining continuity of land observations. In so doing, Landsat has provided a critical reference for assessing long-term changes to Earth's land environment due to both natural and human forcing. Poised for launch in mid-2021, the joint NASA-USGS Landsat 9 mission will continue this important data record. In many respects Landsat 9 is a clone of Landsat-8. The Operational Land Imager-2 (OLI-2) is largely identical to Landsat 8 OLI, providing calibrated imagery covering the solar reflected wavelengths. The Thermal Infrared Sensor-2 (TIRS-2) improves upon Landsat 8 TIRS, addressing known issues including stray light incursion and a malfunction of the instrument scene select mirror. In addition, Landsat 9 adds redundancy to TIRS-2, thus upgrading the instrument to a 5-year design life commensurate with other elements of the mission. Initial performance testing of OLI-2 and TIRS-2 indicate that the instruments are of excellent quality and expected to match or improve on Landsat 8 data quality. Landsat-9 will maintain the current data acquisition rate of up to 740 scenes per day, with these scenes available from the Landsat archive at no cost to users. In this communication, we provide background and rationale for the Landsat 9 mission, describe the instrument payloads and ground system, and discuss data products available from the Landsat 9 mission through USGS.},
	language = {en},
	urldate = {2023-02-01},
	journal = {Remote Sensing of Environment},
	author = {Masek, Jeffrey G. and Wulder, Michael A. and Markham, Brian and McCorkel, Joel and Crawford, Christopher J. and Storey, James and Jenstrom, Del T.},
	month = oct,
	year = {2020},
	pages = {111968},
}

@misc{national_oceanic_and_atmospheric_administration_avhrr3_nodate,
	title = {{AVHRR}/3},
	url = {https://www.esa.int/Applications/Observing_the_Earth/Meteorological_missions/MetOp/About_AVHRR_3},
	abstract = {The Advanced Very High Resolution Radiometer (AVHRR/3) is one of the complement of American instruments provided by the National Oceanic and Atmospheric Administration (NOAA) to fly on MetOp-A, B and C.The AVHRR/3 scans the Earth surface in six spectral bands in the range of 0.58\&nbsp;-\&nbsp;12.5 microns. It provides day and night imaging of land, water and clouds, measures sea surface temperature, ice, snow and vegetation cover.},
	language = {en},
	urldate = {2023-02-01},
	journal = {About AVHRR/3},
	author = {{National Oceanic and Atmospheric Administration}},
}

@book{emery_introduction_2017,
	title = {Introduction to {Satellite} {Remote} {Sensing}: {Atmosphere}, {Ocean}, {Land} and {Cryosphere} {Applications}},
	isbn = {978-0-12-809259-0},
	shorttitle = {Introduction to {Satellite} {Remote} {Sensing}},
	abstract = {Introduction to Satellite Remote Sensing: Atmosphere, Ocean and Land Applications is the first reference book to cover ocean applications, atmospheric applications, and land applications of remote sensing.  Applications of remote sensing data are finding increasing application in fields as diverse as wildlife ecology and coastal recreation management. The technology engages electromagnetic sensors to measure and monitor changes in the earth’s surface and atmosphere. The book opens with an introduction to the history of remote sensing, starting from when the phrase was first coined. It goes on to discuss the basic concepts of the various systems, including atmospheric and ocean, then closes with a detailed section on land applications.  Due to the cross disciplinary nature of the authors’ experience and the content covered, this is a must have reference book for all practitioners and students requiring an introduction to the field of remote sensing. Provides study questions at the end of each chapter to aid learning Covers all satellite remote sensing technologies, allowing readers to use the text as instructional material Includes the most recent technologies and their applications, allowing the reader to stay up-to-date Delves into laser sensing (LIDAR) and commercial satellites (DigitalGlobe) Presents examples of specific satellite missions, including those in which new technology has been introduced},
	language = {en},
	publisher = {Elsevier},
	author = {Emery, Bill and Camps, Adriano},
	month = aug,
	year = {2017},
	note = {Google-Books-ID: sZLUDQAAQBAJ},
	keywords = {Science / Earth Sciences / Geology, Science / Physics / Geophysics},
}

@book{lillesand_remote_2015,
	title = {Remote {Sensing} and {Image} {Interpretation}, 7th {Edition}},
	isbn = {978-1-118-91945-3},
	abstract = {Remote sensing and its kindred technologies, such as geographic information systems (GIS) and the Global Positioning System (GPS), are having a pervasive impact on the conduct of sciences, government, and business alike. This book is designed to be primarily used in two ways: as a textbook in the introductory courses in remote sensing and image interpretation, and as a reference for the burgeoning number of practitioners who use geospatial information and analysis in their work. Because of the wide range of academic and professional settings in which this book might be used, we have made the discussion discipline neutral. In short, anyone involved in geospatial data acquisition and analysis should find this book to be a valuable text and reference.},
	language = {en},
	publisher = {Wiley},
	author = {Lillesand, Thomas and Kiefer, Ralph W. and Chipman, Jonathan},
	month = feb,
	year = {2015},
	note = {Google-Books-ID: eQXYBgAAQBAJ},
	keywords = {Technology \& Engineering / Electrical, Technology \& Engineering / Remote Sensing \& Geographic Information Systems},
}

@article{jurado_remote_2022,
	title = {Remote sensing image fusion on {3D} scenarios: {A} review of applications for agriculture and forestry},
	volume = {112},
	issn = {1569-8432},
	shorttitle = {Remote sensing image fusion on {3D} scenarios},
	url = {https://www.sciencedirect.com/science/article/pii/S1569843222000589},
	doi = {10.1016/j.jag.2022.102856},
	abstract = {Three-dimensional (3D) image mapping of real-world scenarios has a great potential to provide the user with a more accurate scene understanding. This will enable, among others, unsupervised automatic sampling of meaningful material classes from the target area for adaptive semi-supervised deep learning techniques. This path is already being taken by the recent and fast-developing research in computational fields, however, some issues related to computationally expensive processes in the integration of multi-source sensing data remain. Recent studies focused on Earth observation and characterization are enhanced by the proliferation of Unmanned Aerial Vehicles (UAV) and sensors able to capture massive datasets with a high spatial resolution. In this scope, many approaches have been presented for 3D modeling, remote sensing, image processing and mapping, and multi-source data fusion. This survey aims to present a summary of previous work according to the most relevant contributions for the reconstruction and analysis of 3D models of real scenarios using multispectral, thermal and hyperspectral imagery. Surveyed applications are focused on agriculture and forestry since these fields concentrate most applications and are widely studied. Many challenges are currently being overcome by recent methods based on the reconstruction of multi-sensorial 3D scenarios. In parallel, the processing of large image datasets has recently been accelerated by General-Purpose Graphics Processing Unit (GPGPU) approaches that are also summarized in this work. Finally, as a conclusion, some open issues and future research directions are presented.},
	language = {en},
	urldate = {2023-01-30},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Jurado, Juan M. and López, Alfonso and Pádua, Luís and Sousa, Joaquim J.},
	month = aug,
	year = {2022},
	keywords = {3D Modeling, Data Fusion, Image Mapping, Survey},
	pages = {102856},
}

@article{morais_versatile_2021,
	title = {A {Versatile}, {Low}-{Power} and {Low}-{Cost} {IoT} {Device} for {Field} {Data} {Gathering} in {Precision} {Agriculture} {Practices}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2077-0472},
	url = {https://www.mdpi.com/2077-0472/11/7/619},
	doi = {10.3390/agriculture11070619},
	abstract = {Spatial and temporal variability characterization in Precision Agriculture (PA) practices is often accomplished by proximity data gathering devices, which acquire data from a wide variety of sensors installed within the vicinity of crops. Proximity data acquisition usually depends on a hardware solution to which some sensors can be coupled, managed by a software that may (or may not) store, process and send acquired data to a back-end using some communication protocol. The sheer number of both proprietary and open hardware solutions, together with the diversity and characteristics of available sensors, is enough to deem the task of designing a data acquisition device complex. Factoring in the harsh operational context, the multiple DIY solutions presented by an active online community, available in-field power approaches and the different communication protocols, each proximity monitoring solution can be regarded as singular. Data acquisition devices should be increasingly flexible, not only by supporting a large number of heterogeneous sensors, but also by being able to resort to different communication protocols, depending on both the operational and functional contexts in which they are deployed. Furthermore, these small and unattended devices need to be sufficiently robust and cost-effective to allow greater in-field measurement granularity 365 days/year. This paper presents a low-cost, flexible and robust data acquisition device that can be deployed in different operational contexts, as it also supports three different communication technologies: IEEE 802.15.4/ZigBee, LoRa/LoRaWAN and GRPS. Software and hardware features, suitable for using heat pulse methods to measure sap flow, leaf wetness sensors and others are embedded. Its power consumption is of only 83 μA during sleep mode and the cost of the basic unit was kept below the EUR 100 limit. In-field continuous evaluation over the past three years prove that the proposed solution—SPWAS’21—is not only reliable but also represents a robust and low-cost data acquisition device capable of gathering different parameters of interest in PA practices.},
	language = {en},
	number = {7},
	urldate = {2023-01-30},
	journal = {Agriculture},
	author = {Morais, Raul and Mendes, Jorge and Silva, Renato and Silva, Nuno and Sousa, Joaquim J. and Peres, Emanuel},
	month = jul,
	year = {2021},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {IoT, data acquisition, field devices, precision agriculture, precision viticulture},
	pages = {619},
}

@inproceedings{silva_low-cost_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Low-{Cost} {IoT} {LoRa}®{Solutions} for {Precision} {Agriculture} {Monitoring} {Practices}},
	isbn = {978-3-030-30241-2},
	doi = {10.1007/978-3-030-30241-2_20},
	abstract = {Emergent and established paradigms, such as the Internet of Things (IoT), cloud and fog/edge computing, together with increasingly cheaper computing technologies – with very low power requirements, available to exchange data with increased efficiency – and intelligent systems, have evolved to a level where it is virtually possible to create and deploy monitoring solutions, even in Precision Agriculture (PA) practices. In this work, LoRa®(Long Range) technology and LoRaWAN™protocol, are tested in a Precision Viticulture (PV) scenario, using low-power data acquisition devices deployed in a vineyard in the UTAD University Campus, distanced 400 m away from the nearest gateway. The main goal of this work is to evaluate sensor data integration in the mySense environment, a framework aimed to systematize data acquisition procedures to address common PA/PV issues, using LoRa®technology. mySense builds over a 4-layer technological structure: sensor and sensor nodes, crop field and sensor networks, cloud services and front-end applications. It makes available a set of free tools based on the Do-It-Yourself (DIY) concept and enables the use of low-cost platforms to quickly prototype a complete PA/PV monitoring application.},
	language = {en},
	booktitle = {Progress in {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Silva, Nuno and Mendes, Jorge and Silva, Renato and dos Santos, Filipe Neves and Mestre, Pedro and Serôdio, Carlos and Morais, Raul},
	editor = {Moura Oliveira, Paulo and Novais, Paulo and Reis, Luís Paulo},
	year = {2019},
	keywords = {Data integration, Internet-of-Things, LoRa, LoRaWAN, Precision Viticulture},
	pages = {224--235},
}

@article{li_object_2020,
	title = {Object detection in optical remote sensing images: {A} survey and a new benchmark},
	volume = {159},
	issn = {0924-2716},
	shorttitle = {Object detection in optical remote sensing images},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271619302825},
	doi = {10.1016/j.isprsjprs.2019.11.023},
	abstract = {Substantial efforts have been devoted more recently to presenting various methods for object detection in optical remote sensing images. However, the current survey of datasets and deep learning based methods for object detection in optical remote sensing images is not adequate. Moreover, most of the existing datasets have some shortcomings, for example, the numbers of images and object categories are small scale, and the image diversity and variations are insufficient. These limitations greatly affect the development of deep learning based object detection methods. In the paper, we provide a comprehensive review of the recent deep learning based object detection progress in both the computer vision and earth observation communities. Then, we propose a large-scale, publicly available benchmark for object DetectIon in Optical Remote sensing images, which we name as DIOR. The dataset contains 23,463 images and 192,472 instances, covering 20 object classes. The proposed DIOR dataset (1) is large-scale on the object categories, on the object instance number, and on the total image number; (2) has a large range of object size variations, not only in terms of spatial resolutions, but also in the aspect of inter- and intra-class size variability across objects; (3) holds big variations as the images are obtained with different imaging conditions, weathers, seasons, and image quality; and (4) has high inter-class similarity and intra-class diversity. The proposed benchmark can help the researchers to develop and validate their data-driven methods. Finally, we evaluate several state-of-the-art approaches on our DIOR dataset to establish a baseline for future research.},
	language = {en},
	urldate = {2023-01-30},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Li, Ke and Wan, Gang and Cheng, Gong and Meng, Liqiu and Han, Junwei},
	month = jan,
	year = {2020},
	keywords = {Benchmark dataset, Convolutional Neural Network (CNN), Deep learning, Object detection, Optical remote sensing images},
	pages = {296--307},
}

@article{asokan_change_2019,
	title = {Change detection techniques for remote sensing applications: a survey},
	volume = {12},
	issn = {1865-0481},
	shorttitle = {Change detection techniques for remote sensing applications},
	url = {https://doi.org/10.1007/s12145-019-00380-5},
	doi = {10.1007/s12145-019-00380-5},
	abstract = {Change detection captures the spatial changes from multi temporal satellite images due to manmade or natural phenomenon. It is of great importance in remote sensing, monitoring environmental changes and land use –land cover change detection. Remote sensing satellites acquire satellite images at varying resolutions and use these for change detection. This paper briefly analyses various change detection methods and the challenges and issues faced as part of change detection. Over the years, a wide range of methods have been developed for analyzing remote sensing data and newer methods are still being developed. Timely and accurate change detection of Earth’s surface features provides the basis for evaluating the relationships and interactions between human and natural phenomena for the better management of resources. In general, change detection applies multi-temporal datasets to quantitatively analyse the temporal effects of the phenomenon. As such, this study attempts to provide a comprehensive review of the fundamental processes required for change detection. The study also gives a brief account of the main techniques of change detection and discusses the need for development of enhanced change detection methods.},
	language = {en},
	number = {2},
	urldate = {2023-01-30},
	journal = {Earth Science Informatics},
	author = {Asokan, Anju and Anitha, J.},
	month = jun,
	year = {2019},
	keywords = {Change detection, Change map, Image segmentation, Multi temporal, REMOTE sensing},
	pages = {143--160},
}

@article{li_deep_2018,
	title = {Deep learning for remote sensing image classification: {A} survey},
	volume = {8},
	issn = {1942-4795},
	shorttitle = {Deep learning for remote sensing image classification},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1264},
	doi = {10.1002/widm.1264},
	abstract = {Remote sensing (RS) image classification plays an important role in the earth observation technology using RS data, having been widely exploited in both military and civil fields. However, due to the characteristics of RS data such as high dimensionality and relatively small amounts of labeled samples available, performing RS image classification faces great scientific and practical challenges. In recent years, as new deep learning (DL) techniques emerge, approaches to RS image classification with DL have achieved significant breakthroughs, offering novel opportunities for the research and development of RS image classification. In this paper, a brief overview of typical DL models is presented first. This is followed by a systematic review of pixel-wise and scene-wise RS image classification approaches that are based on the use of DL. A comparative analysis regarding the performances of typical DL-based RS methods is also provided. Finally, the challenges and potential directions for further research are discussed. This article is categorized under: Application Areas {\textgreater} Science and Technology Technologies {\textgreater} Classification},
	language = {en},
	number = {6},
	urldate = {2023-01-30},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Li, Ying and Zhang, Haokui and Xue, Xizhe and Jiang, Yenan and Shen, Qiang},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1264},
	keywords = {convolutional neural network, deep belief network, deep learning, pixel-wise classification, remote sensing image, scene classification, stacked auto-encoder},
	pages = {e1264},
}

@article{toth_remote_2016,
	series = {Theme issue '{State}-of-the-art in photogrammetry, remote sensing and spatial information science'},
	title = {Remote sensing platforms and sensors: {A} survey},
	volume = {115},
	issn = {0924-2716},
	shorttitle = {Remote sensing platforms and sensors},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271615002270},
	doi = {10.1016/j.isprsjprs.2015.10.004},
	abstract = {The objective of this article is to review the state-of-the-art remote sensing technologies, including platforms and sensors, the topics representing the primary research interest in the ISPRS Technical Commission I activities. Due to ever advancing technologies, the remote sensing field is experiencing unprecedented developments recently, fueled by sensor advancements and continuously increasing information infrastructure. The scope and performance potential of sensors in terms of spatial, spectral and temporal sensing abilities have expanded far beyond the traditional boundaries of remote sensing, resulting in significantly better observation capabilities. First, platform developments are reviewed with the main focus on emerging new remote sensing satellite constellations and UAS (Unmanned Aerial System) platforms. Next, sensor georeferencing and supporting navigation infrastructure, an enabling technology for remote sensing, are discussed. Finally, we group sensors based on their spatial, spectral and temporal characteristics, and classify them by their platform deployment competencies. In addition, we identify current trends, including the convergence between the remote sensing and navigation field, and the emergence of cooperative sensing, and the potential of crowdsensing.},
	language = {en},
	urldate = {2023-01-30},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Toth, Charles and Jóźków, Grzegorz},
	month = may,
	year = {2016},
	keywords = {Cameras, Georeferencing, Imaging sensors, Platforms, Remote sensing, Satellites, UAS},
	pages = {22--36},
}

@article{mcmanus_infrared_2016,
	title = {Infrared thermography in animal production: {An} overview},
	volume = {123},
	issn = {0168-1699},
	shorttitle = {Infrared thermography in animal production},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169916000326},
	doi = {10.1016/j.compag.2016.01.027},
	abstract = {Infrared thermography technology is a noninvasive method that has been used to indicate thermal biometric changes in animal metabolism resulting from increased body temperature and changes in blood flow in response to environmental or physiological conditions. Thus, this technology can be a useful tool and general stress indicator as well as indicate inflammatory processes, pain and disease. Therefore, this manuscript aims to review the use of this technology in animal production, addressing aspects of heat and physiological stress, metabolism, nutrition, inflammatory processes, diseases, ectoparasite detection and reproduction.},
	language = {en},
	urldate = {2021-09-03},
	journal = {Computers and Electronics in Agriculture},
	author = {McManus, Concepta and Tanure, Candice B. and Peripolli, Vanessa and Seixas, Luiza and Fischer, Vivian and Gabbi, Alexandre M. and Menegassi, Silvio R. O. and Stumpf, Marcelo T. and Kolling, Giovani J. and Dias, Eduardo and Costa, João Batista G.},
	month = apr,
	year = {2016},
	keywords = {Disease, Ectoparasites, Mastitis, Pain, Reproduction, Stress},
	pages = {10--16},
}

@inproceedings{macher_combination_2019,
	title = {Combination of thermal and geometric information for {BIM} enrichment},
	volume = {XLII-2-W15},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2-W15/719/2019/isprs-archives-XLII-2-W15-719-2019-metrics.html},
	doi = {10.5194/isprs-archives-XLII-2-W15-719-2019},
	abstract = {{\textbackslash}textlessp{\textbackslash}textgreater{\textbackslash}textlessstrong class="journal-contentHeaderColor"{\textbackslash}textgreaterAbstract.{\textbackslash}textless/strong{\textbackslash}textgreater In the context of building renovation, infrared (IR) cameras are widely used to perform the energy audit of buildings. They allow analysing precisely the energetic performances of existing buildings and thermal analyses represent a key step for the reduction of energy consumption. They are also used to assess the thermal comfort of people living or working in a building. Building Information Models (BIM) are widespread to plan the rehabilitation of existing buildings and laser scanning is now commonly used to capture the geometry of buildings for as-built BIM creation. The combination of thermographic and geometric data presents a high number and variety of applications (Lagüela and Díaz-Vilariño, 2016). However, geometric and thermal information are generally acquired separately by different building stakeholders and thermal analyses are performed with independence of geometry. In this paper, the combination of thermal and geometric information is investigated for indoor of buildings. The aim of the project is to create 3D thermographic point clouds based on data acquired by a laser scanner and a thermal camera. Based on these point clouds, BIM models might be enriched with thermal information through the scan-to-BIM process.{\textbackslash}textless/p{\textbackslash}textgreater},
	language = {English},
	urldate = {2022-09-01},
	booktitle = {The {International} {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Macher, H. and Boudhaim, M. and Grussenmeyer, P. and Siroux, M. and Landes, T.},
	month = aug,
	year = {2019},
	keywords = {BIM, RGB camera, data fusion, energy efficiency, infrared camera, laser scanning, photogrammetry},
	pages = {719--725},
}

@article{juszczyk_wound_2021,
	title = {Wound {3D} {Geometrical} {Feature} {Estimation} {Using} {Poisson} {Reconstruction}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3035125},
	abstract = {With the aging population of the developed countries, the diseases of affluence become common. One of the consequences of their complications are chronic wounds that affect more and more people. Because the healing of chronic wounds is a lengthy process, its monitoring should be as simple as possible, yet providing accurate and reliable documentation. In this article, we present an image acquisition system and wound surface reconstruction method using several imaging modalities: color photography, thermal imaging, and depth perception. The proposed method is dedicated mainly to wounds located on the limbs where body curvature is large, and its influence on 2D results cannot be neglected. Our approach minimizes the extra effort taken by the medical staff to prepare the wound outline as it is still performed using 2D color photo and then mapped into the 3D space. The method was validated on 29 data sets containing two 3D point clouds from two depth imaging devices (depth camera and stereo camera) as well as 2D color photos and thermal maps. The approach was compared with expert delineations as well as other contemporary methods for wound surface reconstruction presented in the literature. Performed experiments and the obtained results show that the proposed method is statistically concordant with expert delineations performed in 3D.},
	journal = {IEEE Access},
	author = {Juszczyk, Jan Maria and Wijata, Agata and Czajkowska, Joanna and Krecichwost, Michal and Rudzki, Marcin and Biesok, Marta and Pyciński, Bartłomiej and Majewski, Jakub and Kostecki, Jacek and Pietka, Ewa},
	year = {2021},
	keywords = {3D model, 3D surface reconstruction, Cameras, Image reconstruction, Monitoring, Surface reconstruction, Three-dimensional displays, Two dimensional displays, Wounds, chronic wounds, image registration, multimodal image processing, photography, point clouds, stereovision, thermography, wound assessment, wound geometrical parameters},
	pages = {7894--7907},
}

@inproceedings{zia_3d_2015,
	title = {{3D} {Reconstruction} from {Hyperspectral} {Images}},
	doi = {10.1109/WACV.2015.49},
	abstract = {3D reconstruction from hyper spectral images has seldom been addressed in the literature. This is a challenging problem because 3D models reconstructed from different spectral bands demonstrate different properties. If we use a single band or covert the hyper spectral image to gray scale image for the reconstruction, fine structural information may be lost. In this paper, we present a novel method to reconstruct a 3D model from hyper spectral images. Our proposed method first generates 3D point sets from images at each wavelength using the typical structure from motion approach. A structural descriptor is developed to characterize the spatial relationship between the points, which allows robust point matching between two 3D models at different wavelength. Then a 3D registration method is introduced to combine all band-level models into a single and complete hyper spectral 3D model. As far as we know, this is the first attempt in reconstructing a complete 3D model from hyper spectral images. This work allows fine structural-spectral information of an object be captured and integrated into the 3D model, which can be used to support further research and applications.},
	booktitle = {2015 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision}},
	author = {Zia, Ali and Liang, Jie and Zhou, Jun and Gao, Yongsheng},
	month = jan,
	year = {2015},
	keywords = {Histograms, Hyperspectral imaging, Image reconstruction, Mathematical model, Solid modeling, Three-dimensional displays},
	pages = {318--325},
}

@article{yu_early_2021,
	title = {Early detection of pine wilt disease using deep learning algorithms and {UAV}-based multispectral imagery},
	volume = {497},
	journal = {Forest Ecology and Management},
	author = {Yu, Run and Luo, Youqing and Zhou, Quan and Zhang, Xudong and Wu, Dewei and Ren, Lili},
	year = {2021},
	pages = {119493},
}

@article{cohen-steiner_greedy_2004,
	title = {A {Greedy} {Delaunay}-{Based} {Surface} {Reconstruction} {Algorithm}},
	volume = {20},
	issn = {0178-2789},
	doi = {10.1007/s00371-003-0217-z},
	number = {1},
	journal = {Vis. Comput.},
	author = {Cohen-Steiner, David and Da, Frank},
	month = apr,
	year = {2004},
	note = {Place: Berlin, Heidelberg
Publisher: Springer-Verlag},
	pages = {4--16},
}

@article{hou_fusing_2021,
	title = {Fusing tie points' {RGB} and thermal information for mapping large areas based on aerial images: {A} study of fusion performance under different flight configurations and experimental conditions},
	volume = {124},
	issn = {0926-5805},
	shorttitle = {Fusing tie points' {RGB} and thermal information for mapping large areas based on aerial images},
	url = {https://publikationen.bibliothek.kit.edu/1000129014},
	doi = {10.1016/j.autcon.2021.103554},
	abstract = {Three-dimensional thermal mapping from aerial images can be used in energy audits. Tie points that define the location of object points in a 3D space for reconstructing a 3D model, also include thermal information, which plays an important role in energy audits. However, it is often harder and less accurate to extract common features and determine tie points from low-resolution thermal images. It is more effective and accurate to use high-definition RGB images to determine tie points and fuse the RGB and thermal information. In this study, we investigate how to utilize high-definition RGB images that allow for more accurate tie point detection, how different flight configurations affect tie point data fusion, and how tie point data fusion performance can be improved. We propose a tie points' thermal and RGB data-fusion framework to create district-level thermal mapping to solve such problems. This paper aims to evaluate how different flight configurations affect the results of the proposed data fusion approach. Flight configurations include different camera altitudes (60 m and 35 m), distinct camera angles (45 degrees and 30 degrees), diverse flight path designs (mesh grid and Y path), and various building styles (campus buildings and city buildings). We find the following results in this paper: (1) higher flight altitude is not suggested for our data fusion approach; (2) a 30-degree thermal camera angle is suggested for roof inspection, while a 45-degree thermal camera angle is suggested for façade inspection when using the tie point data fusion approach; (3) a Y flight path performs better than a mesh grid path; and (4) our tie point data fusion approach performs better in traditional European city buildings than in modern campus buildings. We also demonstrate that pixels in the thermal images' central area can more accurately represent thermal information than pixels around the image edges for tie point data fusion. Additionally, our studies show that images taken at the edges of mapping areas have more errors. Thus, it is crucial to enlarge the survey area to obtain more accurate possible results.},
	language = {de},
	urldate = {2021-09-03},
	journal = {Automation in construction},
	author = {Hou, Yu and Volk, Rebekka and Chen, Meida and Soibelman, Lucio},
	year = {2021},
	pages = {103554},
}

@article{jo_dense_2021,
	title = {Dense {Thermal} {3D} {Point} {Cloud} {Generation} of {Building} {Envelope} by {Drone}-based {Photogrammetry}},
	volume = {39},
	issn = {1598-4850},
	url = {http://koreascience.or.kr/article/JAKO202116047190036.page},
	doi = {10.7848/ksgpc.2021.39.2.73},
	abstract = {Recently there are growing interests on the energy conservation and emission reduction. In the fields of architecture and civil engineering, the energy monitoring of structures is required to response the energy issues. In perspective of thermal monitoring, thermal images gains popularity for their rich visual information. With the rapid development of the drone platform, aerial thermal images acquired using drone can be used to monitor not only a part of structure, but wider coverage. In addition, the stereo photogrammetric process is expected to generate 3D point cloud with thermal information. However thermal images show very poor in resolution with narrow field of view that limit the use of drone-based thermal photogrammety. In the study, we aimed to generate 3D thermal point cloud using visible and thermal images. The visible images show high spatial resolution being able to generate precise and dense point clouds. Then we extract thermal information from thermal images to assign them onto the point clouds by precisely establishing photogrammetric collinearity between the point clouds and thermal images. From the experiment, we successfully generate dense 3D thermal point cloud showing 3D thermal distribution over the building structure.},
	language = {eng},
	number = {2},
	urldate = {2022-09-01},
	journal = {Journal of the Korean Society of Surveying, Geodesy, Photogrammetry and Cartography},
	author = {Jo, Hyeon Jeong and Jang, Yeong Jae and Lee, Jae Wang and Oh, Jae Hong},
	year = {2021},
	keywords = {3d thermal information, Bundle adjustment, Drone photogrammetry, Point cloud, Thermal camera},
	pages = {73--79},
}

@article{dahaghin_precise_2021,
	title = {Precise {3D} extraction of building roofs by fusion of {UAV}-based thermal and visible images},
	volume = {42},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431161.2021.1951875},
	doi = {10.1080/01431161.2021.1951875},
	abstract = {Thermography is an efficient way of detecting the thermal problems of the roof as a major part of a building’s energy dissipation. Thermal images have a low spatial resolution, making it a challenge to produce a three-dimensional thermal model using aerial images. This paper proposes a combination of thermal and visible point clouds to generate a higher-resolution thermal point cloud from roofs of buildings. For this purpose, after obtaining the internal orientation parameters through camera calibration, visible and thermal point clouds were generated and then registered to each other using ground control points. The roofs of buildings were then extracted from the visible point cloud in four steps. First ground points were removed using cloth simulation filter (CSF), and then vegetation points were eliminated by applying entropy and red-green-blue vegetation index (RGBVI). Geometric features and a segmentation step were considered to filter roofs from other parts. Finally, by combining visible and thermal point clouds, the generated point had a high spatial resolution along with thermal information. In the achieved results, the thermal camera calibration was performed with an accuracy of 0.31 pixels, and the thermal and visible point clouds were registered with an absolute distance of {\textbackslash}textless 0.3 m. The experimental results showed an accuracy of 18 cm for automated extraction of building roofs and 0.6 pixel for production of a high-resolution thermal point cloud, which was five times the density of the primary thermal point cloud and free from distortions.},
	number = {18},
	urldate = {2022-09-01},
	journal = {International Journal of Remote Sensing},
	author = {Dahaghin, Mitra and Samadzadegan, Farhad and Dadrass Javan, Farzaneh},
	month = sep,
	year = {2021},
	pages = {7002--7030},
}

@inproceedings{gonzalez_thermal_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Thermal {Radiation} {Dynamics} of {Soil} {Surfaces} with {Unmanned} {Aerial} {Systems}},
	isbn = {978-3-030-21077-9},
	doi = {10.1007/978-3-030-21077-9_17},
	abstract = {Thermographies are a source of abundant and rapid information, valuable in precision agriculture tasks such as crop stress assessment, plant disease analysis, and soil moisture evaluation. Traditionally, practitioners obtain soil temperature directly from the ground or using satellites and other airborne methods, which are costly and have a low spatial and temporal resolution. In this paper, we introduce a method for short term tracking of thermal radiance inertia with the use of an unmanned aerial system (UAS). In our approach, we retro-project the spatial reconstruction obtained with structure from motion (SfM) to estimate the thermal radiation corresponding to three-dimensional structures. Then, we register the resulting orthomosaics using a pyramidal scheme. We use the first cloud of points as the fixed reference as new orthomosaics become available. Finally, we estimate the dynamics of the thermal radiation using the difference of the registered orthomosaic radiation intensity measurements.},
	language = {en},
	booktitle = {Pattern {Recognition}},
	publisher = {Springer International Publishing},
	author = {González, Othón and Lizarraga, Mariano I. and Karaman, Sertac and Salas, Joaquín},
	editor = {Carrasco-Ochoa, Jesús Ariel and Martínez-Trinidad, José Francisco and Olvera-López, José Arturo and Salas, Joaquín},
	year = {2019},
	keywords = {Remote sensing, Soil surface temperature, Thermographic imaging, Unmanned aerial systems},
	pages = {183--192},
}

@article{zhu_fusion_2021,
	title = {Fusion of urban {3D} point clouds with thermal attributes using {MLS} data and {TIR} image sequences},
	volume = {113},
	issn = {1350-4495},
	url = {https://www.sciencedirect.com/science/article/pii/S1350449520306708},
	doi = {10.1016/j.infrared.2020.103622},
	abstract = {Buildings take a large proportion of the total energy consumption in the city area in winter, therefore Thermal Infrared (TIR) images are widely used to evaluate the energy consumption and leakage of the building. To overcome the difficulties of image interpretation and occlusion in TIR images, utilizing thermal information in 3D structures that fuse TIR images with usable georeferenced 3D coordinates of buildings, is mandatory. To accomplish this mission, we propose a strategy to generate a thermal point cloud combining Mobile Laser Scanning (MLS) point clouds and TIR images. At first, a key-points extraction method based on line-intersection is presented, which helps to detect key-points from highly distorted TIR images. Then, the semi-automatic and the automatic correspondence determination algorithm based on restricted RANSAC for 6DOF pose estimation is proposed and tested. Finally, a non-local mean strategy for data fusion is applied, which integrates the thermal attributes from image sequences and the geometric property from the point clouds, resulting in a smooth representation of thermal point clouds with comprehensive radiance property for thermal analysis. The fusion result qualitatively presents thermal radiation of elements of buildings with detailed geometric information. The generated thermal point clouds contribute to evaluating the energy efficiency of buildings, or building blocks, analyzing the invisible building structure or pipelines for retrofit, and monitoring the energy usage of buildings for long-term development.},
	language = {en},
	urldate = {2022-09-01},
	journal = {Infrared Physics \& Technology},
	author = {Zhu, Jingwei and Xu, Yusheng and Ye, Zhen and Hoegner, Ludwig and Stilla, Uwe},
	month = mar,
	year = {2021},
	keywords = {3D thermal model, Buildings, Fusion, Point clouds, Thermal infrared images},
	pages = {103622},
}

@article{zheng_thermal_2020,
	title = {A {Thermal} {Performance} {Detection} {Method} for {Building} {Envelope} {Based} on {3D} {Model} {Generated} by {UAV} {Thermal} {Imagery}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1996-1073},
	url = {https://www.mdpi.com/1996-1073/13/24/6677},
	doi = {10.3390/en13246677},
	abstract = {The evaluation and renovation of existing building envelope has important practical significance for energy conservation and emission reduction in the field of architecture. With the development of digital cities, 3D models with rich temperature information can realize the comprehensive and accurate detection and evaluation of the existing building envelope. However, the 3D model reconstructed from thermal infrared images has only relative temperature distribution and no temperature value of each location, so it is impossible to quantify the extent of the defect from it. To solve this issue, this paper develops a method to establish a 3D point cloud model with temperature information at selected points. The proposed 3D model is generated based on the thermal infrared images acquired by an unmanned aerial vehicle (UAV) equipped with an infrared camera. In the generated 3D thermal infrared model, we can not only get the relative temperature distribution of the building’s full envelope structure, but also obtain the exact temperature value of any selected point. This method has been verified by field measurements and the result shows that the deviation is within 5 °C. In addition to temperature information, the generated 3D model also has spatial and depth information, which can reflect the appearance information and 3D structure of the monitoring target more realistically. Thus, by using this method, it is possible to achieve a comprehensive, accurate, and efficient on-site assessment of the building envelope in the urban area.},
	language = {en},
	number = {24},
	urldate = {2022-09-01},
	journal = {Energies},
	author = {Zheng, Haichao and Zhong, Xue and Yan, Junru and Zhao, Lihua and Wang, Xintian},
	month = jan,
	year = {2020},
	keywords = {3D reconstruction, 3d reconstruction, Building envelope, Thermal infrared (tir) image, Thermal performance detection, Unmanned aerial vehicle (uav), building envelope, thermal infrared (TIR) image, thermal performance detection, unmanned aerial vehicle (UAV)},
	pages = {6677},
}

@article{ham_automated_2013,
	title = {An automated vision-based method for rapid {3D} energy performance modeling of existing buildings using thermal and digital imagery},
	volume = {27},
	issn = {1474-0346},
	url = {https://www.sciencedirect.com/science/article/pii/S147403461300027X},
	doi = {10.1016/j.aei.2013.03.005},
	abstract = {Modeling the energy performance of existing buildings enables quick identification and reporting of potential areas for building retrofit. However, current modeling practices of using energy simulation tools do not model the energy performance of buildings at their element level. As a result, potential retrofit candidates caused by construction defects and degradations are not represented. Furthermore, due to manual modeling and calibration processes, their application is often time-consuming. Current application of 2D thermography for building diagnostics is also facing several challenges due to a large number of unordered and non-geo-tagged images. To address these limitations, this paper presents a new computer vision-based method for automated 3D energy performance modeling of existing buildings using thermal and digital imagery captured by a single thermal camera. First, using a new image-based 3D reconstruction pipeline which consists of Graphic Processing Unit (GPU)-based Structure-from-Motion (SfM) and Multi-View Stereo (MVS) algorithms, the geometrical conditions of an existing building is reconstructed in 3D. Next, a 3D thermal point cloud model of the building is generated by using a new 3D thermal modeling algorithm. This algorithm involves a one-time thermal camera calibration, deriving the relative transformation by forming the Epipolar geometry between thermal and digital images, and the MVS algorithm for dense reconstruction. By automatically superimposing the 3D building and thermal point cloud models, 3D spatio-thermal models are formed, which enable the users to visualize, query, and analyze temperatures at the level of 3D points. The underlying algorithms for generating and visualizing the 3D spatio-thermal models and the 3D-registered digital and thermal images are presented in detail. The proposed method is validated for several interior and exterior locations of a typical residential building and an instructional facility. The experimental results show that inexpensive digital and thermal imagery can be converted into ubiquitous reporters of the actual energy performance of existing buildings. The proposed method expedites the modeling process and has the potential to be used as a rapid and robust building diagnostic tool.},
	language = {en},
	number = {3},
	urldate = {2021-09-03},
	journal = {Advanced Engineering Informatics},
	author = {Ham, Youngjib and Golparvar-Fard, Mani},
	month = aug,
	year = {2013},
	keywords = {3D reconstruction, Building retrofit, Energy performance modeling, Structure-from-Motion, Thermography},
	pages = {395--409},
}

@article{alfredo_osornio-rios_recent_2019,
	title = {Recent {Industrial} {Applications} of {Infrared} {Thermography}: {A} {Review}},
	volume = {15},
	issn = {1941-0050},
	shorttitle = {Recent {Industrial} {Applications} of {Infrared} {Thermography}},
	doi = {10.1109/TII.2018.2884738},
	abstract = {Infrared thermography (IRT) is a noninvasive technique that is drawing an increasing attention in industry. The spectacular advancement in the features of the infrared cameras that has come together with their progressive cost reduction has expanded the use of this technique to many industrial applications that were unfeasible just a few years ago. This paper compiles and comments the most recent scientific contributions related to the application of this technique in the industrial context. The paper classifies the analyzed references into three main groups: electrical, mechanical, and other applications. Especial emphasis is made on induction-motor-related applications of the IRT due to the extensive participation of these machines in the industrial context. The paper provides a critical review of most of the analyzed references, emphasizes the way in which the infrared technique is applied to the specific application and presents the limitations and pending issues as well as future challenges regarding the application of the technique.},
	number = {2},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Alfredo Osornio-Rios, Roque and Antonino-Daviu, Jose Alfonso and de Jesus Romero-Troncoso, Rene},
	month = feb,
	year = {2019},
	keywords = {Batteries, Cameras, Fault diagnosis, Industries, Insulators, Monitoring, Switchgear, Visualization, induction motors (IMs), industry, infrared thermography (IRT), mechanical faults, transformers},
	pages = {615--625},
}

@inproceedings{metcalf_evaluation_2016,
	title = {Evaluation of terrestrial photogrammetric point clouds derived from thermal imagery},
	volume = {9861},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9861/986119/Evaluation-of-terrestrial-photogrammetric-point-clouds-derived-from-thermal-imagery/.full},
	doi = {10.1117/12.2224406},
	abstract = {Computer vision and photogrammetric techniques have been widely applied to digital imagery producing high density 3D point clouds. Using thermal imagery as input, the same techniques can be applied to infrared data to produce point clouds in 3D space, providing surface temperature information. The work presented here is an evaluation of the accuracy of 3D reconstruction of point clouds produced using thermal imagery. An urban scene was imaged over an area at the Naval Postgraduate School, Monterey, CA, viewing from above as with an airborne system. Terrestrial thermal and RGB imagery were collected from a rooftop overlooking the site using a FLIR SC8200 MWIR camera and a Canon T1i DSLR. In order to spatially align each dataset, ground control points were placed throughout the study area using Trimble R10 GNSS receivers operating in RTK mode. Each image dataset is processed to produce a dense point cloud for 3D evaluation.},
	urldate = {2021-10-06},
	booktitle = {Thermosense: {Thermal} {Infrared} {Applications} {XXXVIII}},
	publisher = {SPIE},
	author = {Metcalf, Jeremy P. and Olsen, Richard C.},
	month = may,
	year = {2016},
	pages = {323--329},
}

@article{dino_image-based_2020,
	title = {Image-based construction of building energy models using computer vision},
	volume = {116},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580519313287},
	doi = {10.1016/j.autcon.2020.103231},
	abstract = {Improving existing buildings' energy performance requires energy models that accurately represent the building. Computer vision methods, particularly image-based 3D reconstruction, can effectively support the creation of 3D building models. In this paper, we present an image-based 3D reconstruction pipeline that supports the semi-automated modeling of existing buildings. We developed two methods for the robust estimation of the building planes from a 3D point cloud that (i) independently estimate each plane and (ii) impose a perpendicularity constraint to plane estimation. We also estimate external walls' thermal transmittance values using an infrared thermography-based method, with the surface temperatures measured by a thermal camera. We validate our approach (i) by testing the pipeline's ability in constructing accurate surface models subject to different image sets with varying sizes and levels of image quality, and (ii) through a comparative analysis between the calculated energy performance metrics of a ground truth and calculated energy simulation model.},
	language = {en},
	urldate = {2022-09-01},
	journal = {Automation in Construction},
	author = {Dino, Ipek Gursel and Sari, Alp Eren and Iseri, Orcun Koral and Akin, Sahin and Kalfaoglu, Esat and Erdogan, Bilge and Kalkan, Sinan and Alatan, A. Aydin},
	month = aug,
	year = {2020},
	keywords = {3D modeling, 3D reconstruction, Building energy modeling, Computer vision, Infrared imaging},
	pages = {103231},
}

@inproceedings{hoegner_3d_2016,
	title = {{3D} building reconstruction and construction site monitoring from {RGB} and {TIR} image sets},
	doi = {10.1109/ISETC.2016.7781118},
	abstract = {This paper discusses methods on 3D reconstruction from ordered and unordered image sets in the visible RGB spectrum and the thermal infrared (TIR) spectrum. The 3D reconstruction is done detecting feature points in images and assigning the corresponding homologoues point. A bundle block adjustment with and without preknowledge of the camera orientations is used to coregister images of one image set. Next, dense matching is performed to calculate 3D object points for most of the image points. Resulting dense 3D point clouds are coregistered to other point clouds and polygonal building models. Facade textures are extracted from the images using corrected camera orientations of the 3D reconstruction. Change detection can be done based on 3D geometry and on radiometric changes like detected temperature changes in TIR based textures.},
	booktitle = {2016 12th {IEEE} {International} {Symposium} on {Electronics} and {Telecommunications} ({ISETC})},
	author = {Hoegner, Ludwig and Tuttas, Sebastian and Stilla, Uwe},
	month = oct,
	year = {2016},
	keywords = {Buildings, Cameras, Data models, Global Positioning System, Image sequences, Solid modeling, Three-dimensional displays},
	pages = {305--308},
}

@inproceedings{neale_estimating_2011,
	title = {Estimating evapotranspiration of riparian vegetation using high resolution multispectral, thermal infrared and lidar data},
	volume = {8174},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8174/81740P/Estimating-evapotranspiration-of-riparian-vegetation-using-high-resolution-multispectral-thermal/.full},
	doi = {10.1117/12.903246},
	abstract = {High resolution airborne multispectral and thermal infrared imagery was acquired over the Mojave River, California with the Utah State University airborne remote sensing system integrated with the LASSI imaging Lidar also built and operated at USU. The data were acquired in pre-established mapping blocks over a 2 day period covering approximately 144 Km of the Mojave River floodplain and riparian zone, approximately 1500 meters in width. The multispectral imagery (green, red and near-infrared bands) was ortho-rectified using the Lidar point cloud data through a direct geo-referencing technique. Thermal Infrared imagery was rectified to the multispectral ortho-mosaics. The lidar point cloud data was classified to separate ground surface returns from vegetation returns as well as structures such as buildings, bridges etc. One-meter DEM's were produced from the surface returns along with vegetation canopy height also at 1-meter grids. Two surface energy balance models that use remote sensing inputs were applied to the high resolution imagery, namely the SEBAL and the Two Source Model. The model parameterizations were slightly modified to accept high resolution imagery (1-meter) as well as the lidar-based vegetation height product, which was used to estimate the aerodynamic roughness length. Both models produced very similar results in terms of latent heat fluxes (LE). Instantaneous LE values were extrapolated to daily evapotranspiration rates (ET) using the reference ET fraction, with data obtained from a local weather station. Seasonal rates were obtained by extrapolating the reference ET fraction according to the seasonal growth habits of the different species. Vegetation species distribution and area were obtained from classification of the multispectral imagery. Results indicate that cottonwood and salt cedar (tamarisk) had the highest evapotranspiration rates followed by mesophytes, arundo, mesquite and desert shrubs. This research showed that high-resolution airborne multispectral and thermal infrared imagery integrated with precise full-waveform lidar data can be used to estimate evapotranspiration and water use by riparian vegetation.},
	urldate = {2021-09-09},
	booktitle = {Remote {Sensing} for {Agriculture}, {Ecosystems}, and {Hydrology} {XIII}},
	publisher = {SPIE},
	author = {Neale, Christopher M. U. and Geli, Hatim and Taghvaeian, Saleh and Masih, Ashish and Pack, Robert T. and Simms, Ronald D. and Baker, Michael and Milliken, Jeff A. and O'Meara, Scott and Witherall, Amy J.},
	month = oct,
	year = {2011},
	pages = {254--262},
}

@article{westfeld_generation_2015,
	title = {Generation of {TIR}-attributed {3D} {Point} {Clouds} from {UAV}-based {Thermal} {Imagery}},
	volume = {2015},
	doi = {10.1127/1432-8364/2015/0274},
	number = {5},
	journal = {Photogrammetrie - Fernerkundung - Geoinformation},
	author = {Westfeld, Patrick and Mader, David and Maas, Hans-Gerd},
	month = nov,
	year = {2015},
	note = {Place: Stuttgart, Germany
Publisher: Schweizerbart Science Publishers},
	pages = {381--393},
}

@article{zhang_fusion_2021,
	title = {Fusion of {Multispectral} {Aerial} {Imagery} and {Vegetation} {Indices} for {Machine} {Learning}-{Based} {Ground} {Classification}},
	volume = {13},
	number = {8},
	journal = {Remote Sensing},
	author = {Zhang, Yanchao and Yang, Wen and Sun, Ying and Chang, Christine and Yu, Jiya and Zhang, Wenbo},
	year = {2021},
	pages = {1411},
}

@article{yandun_narvaez_survey_2017,
	title = {A {Survey} of {Ranging} and {Imaging} {Techniques} for {Precision} {Agriculture} {Phenotyping}},
	volume = {22},
	issn = {1941-014X},
	doi = {10.1109/TMECH.2017.2760866},
	abstract = {Agricultural production must double by 2050 in order to meet the expected food demand due to population growth. Precision agriculture is the key to improve productivity and efficiency in the use of resources, thus helping to achieve this goal under the diverse challenges currently faced by agriculture mainly due to climate changes, land degradation, availability of farmable land, labor force shortage, and increasing costs. To face these challenges, precision agriculture uses and develops sensing methodologies that provide information about crop growth and health indicators. This paper presents a survey of the state-of-the-art in optical visible and near-visible spectrum sensors and techniques to estimate phenotyping variables from intensity, spectral, and volumetric measurements. The sensing methodologies are classified into three areas according to the purpose of the measurements: 1) plant structural characterization; 2) plant/fruit detection; and 3) plant physiology assessment. This paper also discusses the progress in data processing methods and the current open challenges in agricultural tasks in which the development of innovative sensing methodologies is required, such as pruning, fertilizer and pesticide management, crop monitoring, and automated harvesting.},
	number = {6},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Yandun Narvaez, Francisco and Reina, Giulio and Torres-Torriti, Miguel and Kantor, George and Cheein, Fernando Auat},
	month = dec,
	year = {2017},
	keywords = {Advanced sensing in agriculture, Agriculture, Food production, Laser radar, Robot sensing systems, Sensor phenomena and characterization, Three-dimensional displays, Vegetation, fruit detection, morphology characterization, phenotyping, physiology assessment, precision agriculture},
	pages = {2428--2439},
}

@book{vollmer_infrared_2017,
	title = {Infrared {Thermal} {Imaging}: {Fundamentals}, {Research} and {Applications}},
	isbn = {978-3-527-41351-5},
	publisher = {Wiley},
	author = {Vollmer, Michael and Möllmann, Klaus‐Peter},
	month = nov,
	year = {2017},
	doi = {10.1002/9783527693306.ch3},
}

@article{lorinczy_thermal_2017,
	title = {Thermal analysis in biological and medical applications},
	volume = {130},
	issn = {1588-2926},
	url = {https://doi.org/10.1007/s10973-017-6308-2},
	doi = {10.1007/s10973-017-6308-2},
	abstract = {In this paper, I will sum up our research activity from this field performed in the last about 25 years. I will focus on three main points: basic muscle research in the different intermediate states of ATP hydrolysis cycle during muscle contraction, R\&D activities to develop and test different dairy products and TA application in some surgical and diagnostic problem. Our initial research concerned the investigation of thermal stability of main muscle proteins clarifies their basic unfolding characteristics. Later we extended the thermal stability investigation from protein solution to the myosin myofibrils and to the higher organization of muscle proteins, the muscle fibres, checking the effect of nucleotides. At that time, it became possible to stabilize the different intermediate states of ATP hydrolysis up to the time of DSC measurement, using different Pi analogues (Vi, AlFx and BeFx) and non-hydrolysable ATP analogue (AMP.PNP). This way the targets were AM.ADP.Vi (and with AlFx or BeFx) so-called weak binding state, AM.ADP the “strong” binding state as well as the “rigor” AM complex state. AM.AMP.PNP state was used to mimic the AM.ATP state. With our R\&D cooperation, a cold spreadable butter was successfully developed. We were a partner in the development of Ca-enriched cheese, in its spreadable form too as well as in the development and testing of different dairy products using probiotic cultures. Our TA activity covers a wide range of medical applications, e.g. investigation of the different abnormalities of human leg skeletal muscle, different stages of degeneration of human vertebral disc, to judge the goodness/applicability of different suture technique on tracheal cartilage in primary airway reconstruction or the characterization of different self-expandable stents implantation in the oesophagus treatment. We have joined those groups who try to use DSC in the diagnosis of different diseases from blood plasma, e.g. in the case of breast cancer, melanoma, in psoriasis.},
	language = {en},
	number = {3},
	urldate = {2021-09-03},
	journal = {Journal of Thermal Analysis and Calorimetry},
	author = {Lőrinczy, D.},
	month = dec,
	year = {2017},
	pages = {1263--1280},
}

@article{kylili_infrared_2014,
	title = {Infrared thermography ({IRT}) applications for building diagnostics: {A} review},
	volume = {134},
	issn = {0306-2619},
	shorttitle = {Infrared thermography ({IRT}) applications for building diagnostics},
	url = {https://www.sciencedirect.com/science/article/pii/S0306261914008083},
	doi = {10.1016/j.apenergy.2014.08.005},
	abstract = {Infrared thermography (IRT) has met an extensive popularity among the non-destructive technologies for building diagnostics, especially with the increasing concerns of energy minimisation and low energy consumption of the building sector. Its popularity for a broad range of applications can be attributed to its non-contact safe nature, its usefulness and effectiveness, as well as the energy and cost savings it can achieve. This paper reviews the state-of-the-art literature and research regarding the passive and active infrared thermography. The fundamentals of IRT are thoroughly explained and the thermographic process for building diagnostics is presented. This work also presents the fields of applicability of IRT with a focus on the building sector, as well as the advantages, limitations and potential sources of errors of IRT employment. Additionally previous non-destructive testing (NDT) studies that employed passive, active pulsed, and active lock-in thermographies for building diagnostics are presented. A review of the thermal image analysis methods and the future trends of thermal imaging are also included in this work. It can be concluded that while IRT is a useful tool for the characterisation of defects in the building sector, there is great prospect for the development of more advanced, effective and accurate approaches that will employ a combination of thermography approaches.},
	language = {en},
	urldate = {2021-09-03},
	journal = {Applied Energy},
	author = {Kylili, Angeliki and Fokaides, Paris A. and Christou, Petros and Kalogirou, Soteris A.},
	month = dec,
	year = {2014},
	keywords = {Active thermography, Infrared thermography, Locked in thermography, Non-destructive testing, Passive thermography, Pulsed thermography},
	pages = {531--549},
}

@article{teza_evaluation_2019,
	title = {Evaluation of the temperature pattern of a complex body from thermal imaging and {3D} information: {A} method and its {MATLAB} implementation},
	volume = {96},
	issn = {1350-4495},
	shorttitle = {Evaluation of the temperature pattern of a complex body from thermal imaging and {3D} information},
	doi = {10.1016/j.infrared.2018.11.029},
	abstract = {The standard setting of a camera used in Infrared thermography (IRT) is based on the choice of the same values of emissivity and distance for all pixels of a thermal image even if the emissivity depends on the relative position of camera and observed surface. Often this is not a problem. However, the resulting temperature pattern could be inadequate if a body having a complex shape is observed from strongly constrained positions. In order to face this issue, a procedure aimed at providing a correct temperature pattern by using 3D information related to a point cloud is proposed together with its MATLAB implementation (COMAP3 toolbox). For each pixel of a thermal image, the relative position of camera and observed surface is estimated, leading to pixel-specific values of emissivity and distance. The temperature obtained in this way is also mapped onto the point cloud. The effectiveness of the procedure in recognizing areas characterized by peculiar thermal behavior is shown in the case of a historic cylindrical masonry bell tower (Caorle's bell tower, Venice, Italy). © 2018 Elsevier B.V.},
	language = {English},
	journal = {Infrared Physics and Technology},
	author = {Teza, G. and Pesci, A.},
	year = {2019},
	keywords = {3D thermography, Damage recognition, In situ measurements, Radiometric JPEG image, Thermal imaging},
	pages = {228--237},
}

@book{lopez_comparison_2021,
	title = {Comparison of {GPU}-based {Methods} for {Handling} {Point} {Cloud} {Occlusion}},
	isbn = {978-3-03868-160-1},
	url = {https://diglib.eg.org:443/xmlui/handle/10.2312/ceig20211364},
	abstract = {Three-dimensional point clouds have conventionally been used along with several sources of information. This fusion can be performed by projecting the point cloud into the image plane and retrieving additional data for each point. Nevertheless, the raw projection omits the occlusion caused by foreground surfaces, thus assigning wrong information to 3D points. For large point clouds, testing the occlusion of each point from every viewpoint is a time-consuming task. Hence, we propose several algorithms implemented in GPU and based on the use of z-buffers. Given the size of nowadays point clouds, we also adapt our methodologies to commodity hardware by splitting the point cloud into several chunks. Finally, we compare their performance through the response time.},
	language = {en},
	urldate = {2021-10-06},
	publisher = {The Eurographics Association},
	author = {López, Alfonso and Jurado, Juan Manuel and Padrón, Emilio José and Ogayar, Carlos Javier and Feito, Francisco Ramón},
	year = {2021},
	doi = {10.2312/ceig.20211364},
}

@article{vong_how_2021,
	title = {How to {Build} a {2D} and {3D} {Aerial} {Multispectral} {Map}?—{All} {Steps} {Deeply} {Explained}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {How to {Build} a {2D} and {3D} {Aerial} {Multispectral} {Map}?},
	url = {https://www.mdpi.com/2072-4292/13/16/3227},
	doi = {10.3390/rs13163227},
	abstract = {The increased development of camera resolution, processing power, and aerial platforms helped to create more cost-efficient approaches to capture and generate point clouds to assist in scientific fields. The continuous development of methods to produce three-dimensional models based on two-dimensional images such as Structure from Motion (SfM) and Multi-View Stereopsis (MVS) allowed to improve the resolution of the produced models by a significant amount. By taking inspiration from the free and accessible workflow made available by OpenDroneMap, a detailed analysis of the processes is displayed in this paper. As of the writing of this paper, no literature was found that described in detail the necessary steps and processes that would allow the creation of digital models in two or three dimensions based on aerial images. With this, and based on the workflow of OpenDroneMap, a detailed study was performed. The digital model reconstruction process takes the initial aerial images obtained from the field survey and passes them through a series of stages. From each stage, a product is acquired and used for the following stage, for example, at the end of the initial stage a sparse reconstruction is produced, obtained by extracting features of the images and matching them, which is used in the following step, to increase its resolution. Additionally, from the analysis of the workflow, adaptations were made to the standard workflow in order to increase the compatibility of the developed system to different types of image sets. Particularly, adaptations focused on thermal imagery were made. Due to the low presence of strong features and therefore difficulty to match features across thermal images, a modification was implemented, so thermal models could be produced alongside the already implemented processes for multispectral and RGB image sets.},
	language = {en},
	number = {16},
	urldate = {2021-10-18},
	journal = {Remote Sensing},
	author = {Vong, André and Matos-Carvalho, João P. and Toffanin, Piero and Pedro, Dário and Azevedo, Fábio and Moutinho, Filipe and Garcia, Nuno Cruz and Mora, André},
	month = jan,
	year = {2021},
	keywords = {2D and 3D mapping, Aerial mapping, Multi-view stereo, Multispectral, Poisson Reconstruction, Poisson reconstruction, Stitching, Structure from motion, Thermal, aerial mapping, multi-view stereo, multispectral, stitching, structure from motion, thermal},
	pages = {3227},
}

@article{lopez_optimized_2021,
	title = {An optimized approach for generating dense thermal point clouds from {UAV}-imagery},
	volume = {182},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271621002604},
	doi = {10.1016/j.isprsjprs.2021.09.022},
	abstract = {Thermal infrared (TIR) images acquired from Unmanned Aircraft Vehicles (UAV) are gaining scientific interest in a wide variety of fields. However, the reconstruction of three-dimensional (3D) point clouds utilizing consumer-grade TIR images presents multiple drawbacks as a consequence of low-resolution and induced aberrations. Consequently, these problems may lead photogrammetric techniques, such as Structure from Motion (SfM), to generate poor results. This work proposes the use of RGB point clouds estimated from SfM as the input for building thermal point clouds. For that purpose, RGB and thermal imagery are registered using the Enhanced Correlation Coefficient (ECC) algorithm after removing acquisition errors, thus allowing us to project TIR images into an RGB point cloud. Furthermore, we consider several methods to provide accurate thermal values for each 3D point. First, the occlusion problem is solved through two different approaches, so that points that are not visible from a viewing angle do not erroneously receive values from foreground objects. Then, we propose a flexible method to aggregate multiple thermal values considering the dispersion from such aggregation to the image samples. Therefore, it minimizes error measurements. A naive classification algorithm is then applied to the thermal point clouds as a case study for evaluating the temperature of vegetation and ground points. As a result, our approach builds thermal point clouds with up to 798,69\% more point density than results from other commercial solutions. Moreover, it minimizes the build time by using parallel computing for time-consuming tasks. Despite obtaining larger point clouds, we report up to 96,73\% less processing time per 3D point.},
	language = {en},
	urldate = {2021-10-24},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {López, Alfonso and Jurado, Juan M. and Ogayar, Carlos J. and Feito, Francisco R.},
	month = dec,
	year = {2021},
	keywords = {GPU computing, Image processing, Occlusion, Point cloud, Thermal imagery, UAV imagery},
	pages = {78--95},
}

@article{sankey_quantifying_2021,
	title = {Quantifying plant-soil-nutrient dynamics in rangelands: {Fusion} of {UAV} hyperspectral-{LiDAR}, {UAV} multispectral-photogrammetry, and ground-based {LiDAR}-digital photography in a shrub-encroached desert grassland},
	volume = {253},
	issn = {0034-4257},
	shorttitle = {Quantifying plant-soil-nutrient dynamics in rangelands},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425720305964},
	doi = {10.1016/j.rse.2020.112223},
	abstract = {Rangelands cover 70\% of the world's land surface, and provide critical ecosystem services of primary production, soil carbon storage, and nutrient cycling. These ecosystem services are governed by very fine-scale spatial patterning of soil carbon, nutrients, and plant species at the centimeter-to-meter scales, a phenomenon known as “islands of fertility”. Such fine-scale dynamics are challenging to detect with most satellite and manned airborne platforms. Remote sensing from unmanned aerial vehicles (UAVs) provides an alternative option for detecting fine-scale soil nutrient and plant species changes in rangelands tn0020 smaller extents. We demonstrate that a model incorporating the fusion of UAV multispectral and structure-from-motion photogrammetry classifies plant functional types and bare soil cover with an overall accuracy of 95\% in rangelands degraded by shrub encroachment and disturbed by fire. We further demonstrate that employing UAV hyperspectral and LiDAR fusion greatly improves upon these results by classifying 9 different plant species and soil fertility microsite types (SFMT) with an overall accuracy of 87\%. Among them, creosote bush and black grama, the most important native species in the rangeland, have the highest producer's accuracies at 98\% and 94\%, respectively. The integration of UAV LiDAR-derived plant height differences was critical in these improvements. Finally, we use synthesis of the UAV datasets with ground-based LiDAR surveys and lab characterization of soils to estimate that the burned rangeland potentially lost 1474 kg/ha of C and 113 kg/ha of N owing to soil erosion processes during the first year after a prescribed fire. However, during the second-year post-fire, grass and plant-interspace SFMT functioned as net sinks for sediment and nutrients and gained approximately 175 kg/ha C and 14 kg/ha N, combined. These results provide important site-specific insight that is relevant to the 423 Mha of grasslands and shrublands that are burned globally each year. While fire, and specifically post-fire erosion, can degrade some rangelands, post-fire plant-soil-nutrient dynamics might provide a competitive advantage to grasses in rangelands degraded by shrub encroachment. These novel UAV and ground-based LiDAR remote sensing approaches thus provide important details towards more accurate accounting of the carbon and nutrients in the soil surface of rangelands.},
	language = {en},
	urldate = {2022-09-01},
	journal = {Remote Sensing of Environment},
	author = {Sankey, Joel B. and Sankey, Temuulen T. and Li, Junran and Ravi, Sujith and Wang, Guan and Caster, Joshua and Kasprak, Alan},
	month = feb,
	year = {2021},
	keywords = {Airborne data, Change detection, Digital elevation model (DEM), Digital elevation model of difference (DOD), Drone, Fire, Grass, Hyperspectral, Islands of fertility, Lidar, Machine learning, Nutrient, Photogrammetry, Rangeland, Shrub, Soil, Structure from motion (SFM), Terrestrial laser scanning, Unmanned aerial system (UAS), Unmanned aerial vehicle (UAV)},
	pages = {112223},
}

@article{lin_fusion_2019,
	title = {Fusion of thermal imagery with point clouds for building façade thermal attribute mapping},
	volume = {151},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271619300796},
	doi = {10.1016/j.isprsjprs.2019.03.010},
	abstract = {Thermal image data are widely used to assess the insulation quality of buildings and to detect thermal leakages. In our approach, we merge terrestrial thermal image data and 3D point clouds to perform thermal texture mapping for building facades. Since geo-referencing data of a hand-held thermal camera is usually not available in such applications, registration between thermal images and a 3D point cloud (for instance generated from RGB image data by structure-from-motion techniques) is essential. In our approach, thermal image data registration is conducted in four steps: First, another point cloud is generated from the thermal image data. Next, a coarse registration between thermal point cloud and RGB point cloud is performed using the fast global registration (FGR) algorithm. The best corresponding thermal-RGB image pairs are acquired by picking up the lowest Euclidean distance between the exterior orientation parameters of thermal images and transformed exterior orientation parameters of RGB images. Subsequently, radiation-invariant feature transform (RIFT), normalized barycentric coordinate system (NBCS) and random sample consensus (RANSAC) are employed to extract reliable matching features on thermal-RGB image pairs. Afterwards, a fine registration is performed by mono-plotting of the RGB image, followed by image resection of the thermal image. Finally, in terms of texture mapping algorithms, in order to remove the blur effects caused by small misalignments for different candidate images, a global image pose refinement approach, which aims to minimize the temperature disagreements provided by different images for the same object points, is proposed. In addition, in order to ensure high geometric and radiant accuracy, camera calibrations are performed. Experiments showed that the proposed method could not only achieve high geometric registration accuracy, but also provide a good radiometric accuracy with RMSE lower than 1.5 °C.},
	language = {en},
	urldate = {2022-09-01},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Lin, Dong and Jarzabek-Rychard, Malgorzata and Tong, Xiaochong and Maas, Hans-Gerd},
	month = may,
	year = {2019},
	keywords = {Image matching, Image pose refinement, Point cloud, Registration, Texture mapping, Thermal image},
	pages = {162--175},
}

@inproceedings{kong_3-d_2018,
	title = {3-{D} {Point} {Cloud} {Reconstruction} of {Infrared} {Images} {Based} on {Improved} {Structure} from {Motion}},
	doi = {10.1109/EECS.2018.00063},
	abstract = {Structure from Motion (SfM) has been proved an efficient algorithm of 3-D point cloud reconstruction derived from optical images. This paper extends it to infrared images taken by thermal cameras. To solve the absence of distinctive features and presence of thermal reflections with low contrast, this paper proposed a new TAC-RANSAC model to eliminate the mismatches using a feature detection algorithm suitable for infrared images. The experiment shows that the proposed method reduced the number of mismatches and obtain an ideal result of reconstruction.},
	booktitle = {2018 2nd {European} {Conference} on {Electrical} {Engineering} and {Computer} {Science} ({EECS})},
	author = {Kong, Yingying and Leung, Henry and Zhang, Bowen and Xing, Shiyu and Chen, Yingying},
	month = dec,
	year = {2018},
	keywords = {3-D Reconstruction, Computer science, Electrical engineering, Europe, Handheld computers, Infrared images, MVG, Mismatch eliminating, RANSAC, SfM, infrared images, mismatch eliminating},
	pages = {307--310},
}

@article{hoegner_mobile_2018,
	title = {Mobile thermal mapping for matching of infrared images with {3D} building models and {3D} point clouds},
	volume = {15},
	issn = {1768-6733},
	url = {https://doi.org/10.1080/17686733.2018.1455129},
	doi = {10.1080/17686733.2018.1455129},
	abstract = {Two workflows for mobile thermal mapping including geometric camera calibration, automatic image orientation, 3D reconstruction and extraction of textures of building façades from thermal IR image sequences are introduced. In the first method, a coregistration of a terrestrial image sequence is done with a given 3D building model with the 3D building model being included in the image sequence orientation directly. Homologue image points are tracked through the sequence and checked whether they are on the 3D model façade. This is done to remove outliers and to refine the image sequence orientation and 3D reconstruction process as additional constraint in the bundle adjustment. The second method is used to match a high resoluted RGB-image based 3D point cloud and a TIR-image based 3D point cloud to assign thermal intensities to the dense RGB point cloud. The matching is done based on given pre-orientation of the two-point clouds performing Iterative Closest Point to minimise the distance of the two-point clouds. The corrected orientation is transferred to the orientation parameters of the TIR-images. The final image orientations are used to extract thermal textures for the 3D building model. Both methods are evaluated on their geometric accuracy using experimental image sequences.},
	number = {2},
	urldate = {2022-09-01},
	journal = {Quantitative InfraRed Thermography Journal},
	author = {Hoegner, L. and Stilla, U.},
	month = jul,
	year = {2018},
	keywords = {3D reconstruction, Image sequence, building information model, texture extraction},
	pages = {252--270},
}

@article{adan_towards_2020,
	title = {Towards the {Use} of {3D} {Thermal} {Models} in {Constructions}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2071-1050},
	url = {https://www.mdpi.com/2071-1050/12/20/8521},
	doi = {10.3390/su12208521},
	abstract = {The use of point clouds in architecture and civil engineering has, to date, been limited almost exclusively to functional geometric features. Nevertheless, hardly any works have attempted to process and explore 3D thermal models for buildings. This paper presents a method for the visualisation and exploration of 3D thermal models (3D-T) of building interiors. A 3D-T model consists of a thermal point cloud, which has been generated with a 3D thermal-scanner platform. Given a 3D-T of a building at a specific time, the user can visualise and navigate through different room models and each room can, in turn, be segmented into its architectonic components (walls, ceilings and floors), from which thermal orthoimages can be generated. When the building is sensed at different times, a 3D temporal-thermal (3D-TT) model is integrated. The temporal-thermal evolution of these structural components, along with selected zones of them, can then be analysed by performing a new type of thermal characterisation. This method has successfully been tested using real building-related data.},
	language = {en},
	number = {20},
	urldate = {2022-09-01},
	journal = {Sustainability},
	author = {Adán, Antonio and Quintana, Blanca and García Aguilar, Juan and Pérez, Víctor and Castilla, Francisco Javier},
	month = jan,
	year = {2020},
	keywords = {3D data analysis, 3D thermal models of buildings, 3D visualisation},
	pages = {8521},
}

@inproceedings{gomez_experimental_2022,
	title = {An {Experimental} {Comparison} of {Multi}-{View} {Stereo} {Approaches} on {Satellite} {Images}},
	url = {https://openaccess.thecvf.com/content/WACV2022/html/Gomez_An_Experimental_Comparison_of_Multi-View_Stereo_Approaches_on_Satellite_Images_WACV_2022_paper.html},
	language = {en},
	urldate = {2022-04-29},
	author = {Gómez, Alvaro and Randall, Gregory and Facciolo, Gabriele and von Gioi, Rafael Grompone},
	year = {2022},
	pages = {844--853},
}

@article{paul_comprehensive_2021,
	title = {A comprehensive review on remote sensing image registration},
	volume = {42},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431161.2021.1906985},
	doi = {10.1080/01431161.2021.1906985},
	abstract = {Over the last two decades, there has been a significant improvement in the quality and volume of the remote-sensing images. With the extensive availability of the images, these are used in many applications such as change detection, image mosaicing, and image fusion where image registration is very necessary. However, image registration is a challenging task in remote sensing due to the following reasons: geometric differences between images, intensity differences, and noise affect. Over the years, wide variety of algorithms have come into existence to handle these issues. In this paper, a comprehensive review is presented on these remote sensing image registration methods. At first, the general information of remote sensing image registration, its classifications, and application areas are provided. Then, an exhaustive review is given on the existing methods and their merits as well as shortcomings are mentioned. Finally, a comparative analysis is provided for the state-of-the-art methods and current challenges of this research are presented.},
	number = {14},
	urldate = {2022-05-02},
	journal = {International Journal of Remote Sensing},
	author = {Paul, Sourabh and Pati, Umesh C.},
	month = jul,
	year = {2021},
	pages = {5396--5432},
}

@article{kim_3d_2012,
	title = {{3D} imaging spectroscopy for measuring hyperspectral patterns on solid objects},
	volume = {31},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2185520.2185534},
	doi = {10.1145/2185520.2185534},
	abstract = {Sophisticated methods for true spectral rendering have been developed in computer graphics to produce highly accurate images. In addition to traditional applications in visualizing appearance, such methods have potential applications in many areas of scientific study. In particular, we are motivated by the application of studying avian vision and appearance. An obstacle to using graphics in this application is the lack of reliable input data. We introduce an end-to-end measurement system for capturing spectral data on 3D objects. We present the modification of a recently developed hyperspectral imager to make it suitable for acquiring such data in a wide spectral range at high spectral and spatial resolution. We capture four megapixel images, with data at each pixel from the near-ultraviolet (359 nm) to near-infrared (1,003 nm) at 12 nm spectral resolution. We fully characterize the imaging system, and document its accuracy. This imager is integrated into a 3D scanning system to enable the measurement of the diffuse spectral reflectance and fluorescence of specimens. We demonstrate the use of this measurement system in the study of the interplay between the visual capabilities and appearance of birds. We show further the use of the system in gaining insight into artifacts from geology and cultural heritage.},
	number = {4},
	urldate = {2022-09-25},
	journal = {ACM Transactions on Graphics},
	author = {Kim, Min H. and Harvey, Todd Alan and Kittle, David S. and Rushmeier, Holly and Dorsey, Julie and Prum, Richard O. and Brady, David J.},
	month = jul,
	year = {2012},
	pages = {38:1--38:11},
}

@article{stucker_resdepth_2022,
	title = {{ResDepth}: {A} deep residual prior for {3D} reconstruction from high-resolution satellite images},
	volume = {183},
	issn = {0924-2716},
	shorttitle = {{ResDepth}},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271621003075},
	doi = {10.1016/j.isprsjprs.2021.11.009},
	abstract = {Modern optical satellite sensors enable high-resolution stereo reconstruction from space. But the challenging imaging conditions when observing the Earth from space push stereo matching to its limits. In practice, the resulting digital surface models (DSMs) are fairly noisy and often do not attain the accuracy needed for high-resolution applications such as 3D city modeling. Arguably, stereo correspondence based on low-level image similarity is insufficient and should be complemented with a priori knowledge about the expected surface geometry beyond basic local smoothness. To that end, we introduce ResDepth, a convolutional neural network that learns such an expressive geometric prior from example data. ResDepth refines an initial, raw stereo DSM while conditioning the refinement on the images. I.e., it acts as a smart, learned post-processing filter and can seamlessly complement any stereo matching pipeline. In a series of experiments, we find that the proposed method consistently improves stereo DSMs both quantitatively and qualitatively. We show that the prior encoded in the network weights captures meaningful geometric characteristics of urban design, which also generalize across different districts and even from one city to another. Moreover, we demonstrate that, by training on a variety of stereo pairs, ResDepth can acquire a sufficient degree of invariance against variations in imaging conditions and acquisition geometry.},
	language = {en},
	urldate = {2022-04-29},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Stucker, Corinne and Schindler, Konrad},
	month = jan,
	year = {2022},
	keywords = {3D reconstruction, Convolutional neural network (CNN), Digital surface model (DSM), Satellite imagery, Scene refinement},
	pages = {560--580},
}

@article{li_recent_2021,
	title = {Recent advances in image fusion technology in agriculture},
	volume = {191},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169921005081},
	doi = {10.1016/j.compag.2021.106491},
	abstract = {Crop and livestock monitoring are essential for guiding agricultural production and improving agricultural yield by facilitating the early prevention and treatment of diseases, yield prediction, and automatic harvesting. However, traditional monitoring methods have some shortcomings, including by humans and through a single sensor, manual monitoring method is time-consuming and labor-intensive, monitoring methods using a single sensor are inaccurate. Therefore, research on improving the methods of crop and livestock monitoring is essential. Image fusion technology can help provide important information from two or more images that can be used to obtain comprehensive agricultural information. After discussing the advantages and disadvantages of various of image fusion methods in agriculture, this paper reviews the application of image fusion in crop recognition and detection, planting area estimation, plant diseases and pest detection, and livestock health assessment and classification. It also highlights the main challenges to the successful application of image fusion technology to agriculture, and discusses directions of future research and development directions in the area.},
	language = {en},
	urldate = {2022-05-02},
	journal = {Computers and Electronics in Agriculture},
	author = {Li, Daoliang and Song, Zhaoyang and Quan, Chaoqun and Xu, Xianbao and Liu, Chang},
	month = dec,
	year = {2021},
	keywords = {Agriculture, Convolutional neural network, Image fusion, Monitoring, Wavelet transform},
	pages = {106491},
}

@article{padua_vineyard_2019,
	title = {Vineyard {Variability} {Analysis} through {UAV}-{Based} {Vigour} {Maps} to {Assess} {Climate} {Change} {Impacts}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2073-4395/9/10/581},
	doi = {10.3390/agronomy9100581},
	abstract = {Climate change is projected to be a key influence on crop yields across the globe. Regarding viticulture, primary climate vectors with a significant impact include temperature, moisture stress, and radiation. Within this context, it is of foremost importance to monitor soils\&rsquo; moisture levels, as well as to detect pests, diseases, and possible problems with irrigation equipment. Regular monitoring activities will enable timely measures that may trigger field interventions that are used to preserve grapevines\&rsquo; phytosanitary state, saving both time and money, while assuring a more sustainable activity. This study employs unmanned aerial vehicles (UAVs) to acquire aerial imagery, using RGB, multispectral and thermal infrared sensors in a vineyard located in the Portuguese Douro wine region. Data acquired enabled the multi-temporal characterization of the vineyard development throughout a season through the computation of the normalized difference vegetation index, crop surface models, and the crop water stress index. Moreover, vigour maps were computed in three classes (high, medium, and low) with different approaches: (1) considering the whole vineyard, including inter-row vegetation and bare soil; (2) considering only automatically detected grapevine vegetation; and (3) also considering grapevine vegetation by only applying a normalization process before creating the vigour maps. Results showed that vigour maps considering only grapevine vegetation provided an accurate representation of the vineyard variability. Furthermore, significant spatial associations can be gathered through (i) a multi-temporal analysis of vigour maps, and (ii) by comparing vigour maps with both height and water stress estimation. This type of analysis can assist, in a significant way, the decision-making processes in viticulture.},
	language = {en},
	number = {10},
	urldate = {2021-12-30},
	journal = {Agronomy},
	author = {Pádua, Luís and Marques, Pedro and Adão, Telmo and Guimarães, Nathalie and Sousa, António and Peres, Emanuel and Sousa, Joaquim João},
	month = oct,
	year = {2019},
	keywords = {climate change, crop surface model, crop water stress index, multi-temporal analysis, normalized difference vegetation index, precision viticulture, spatial variability, unmanned aerial vehicles, vigour maps},
	pages = {581},
}

@article{manzanera_fusion_2016,
	title = {Fusion of airborne {LiDAR} and multispectral sensors reveals synergic capabilities in forest structure characterization},
	volume = {53},
	issn = {1548-1603},
	url = {https://doi.org/10.1080/15481603.2016.1231605},
	doi = {10.1080/15481603.2016.1231605},
	abstract = {Forest stand structure is an important concept for ecology and planning in sustainable forest management. In this article, we consider that the incorporation of complementary multispectral information from optical sensors to Light Detection and Ranging (LiDAR) may be advantageous, especially through data fusion by back-projecting the LiDAR points onto the multispectral image. A multivariate data set of both LiDAR and multispectral metrics was related with a multivariate data set of stand structural variables measured in a Scots pine forest through canonical correlation analysis (CCA). Four statistically significant pairs of canonical variables were found, which explained 83.0\% accumulated variance. The first pair of canonical variables related indicators of stand development, i.e. height and volume, with LiDAR height metrics. CCA also found attributes describing stand density to be related to LiDAR and spectral variables determining canopy coverage. Other canonical variables pertained to Lorenz curve-derived attributes, which are measures of within-stand tree size variability and heterogeneity, able to discriminate even-sized from uneven-sized stands. The most relevant result was to find that metrics derived from the multispectral sensor showed significant explanatory potential for the prediction of these structural attributes. Therefore, we concluded that metrics derived from the optical sensor have potential for complementing the information from the LiDAR sensor in describing structural properties of forest stands. We recommend the use of back-projecting for jointly exploiting the synergies of both sensors using similar types of metrics as they are customary in forestry applications of LiDAR.},
	number = {6},
	urldate = {2021-12-29},
	journal = {GIScience \& Remote Sensing},
	author = {Manzanera, Jose A. and García-Abril, Antonio and Pascual, Cristina and Tejera, Rosario and Martín-Fernández, Susana and Tokola, Timo and Valbuena, Ruben},
	month = nov,
	year = {2016},
	keywords = {Stand structure, airborne laser scanning, data fusion, forest Structural Types, multispectral imagery},
	pages = {723--738},
}

@incollection{valbuena_integrating_2014,
	address = {Dordrecht},
	series = {Managing {Forest} {Ecosystems}},
	title = {Integrating {Airborne} {Laser} {Scanning} with {Data} from {Global} {Navigation} {Satellite} {Systems} and {Optical} {Sensors}},
	isbn = {978-94-017-8663-8},
	url = {https://doi.org/10.1007/978-94-017-8663-8_4},
	abstract = {Most forestry applications of airborne laser scanning (ALS) require simultaneous use of various data sources. This chapter covers a number of common issues that practitioners face when dealing with data fusion schemes. The first subsection points out the objectives that may be pursued when integrating different data sources, and the benefits that can be obtained from using diverse remote sensors onboard differing platforms. The next subsections are devoted to the two data sources that usually pose most problems in their spatial co-registration with ALS datasets: field inventory and aerial photographs. All data sources ultimately rely on global navigation satellite systems (GNSS) which are especially error-prone when operating under forest canopies. Positioning methods and spatial accuracy assessment applied to forest plot and individual tree surveying are presented, also including terrestrial laser scanning (TLS). Furthermore, procedures for digital elevation model (DEM) generation are reviewed in the context of their use in orthorectification, which is the most widespread method for fusion of ALS with optical sensors. Drawbacks of using orthophotos are identified, therefore suggesting alternatives: true-orthorectification, back-projecting ALS and image matching.},
	language = {en},
	urldate = {2021-12-29},
	booktitle = {Forestry {Applications} of {Airborne} {Laser} {Scanning}: {Concepts} and {Case} {Studies}},
	publisher = {Springer Netherlands},
	author = {Valbuena, Rubén},
	editor = {Maltamo, Matti and Næsset, Erik and Vauhkonen, Jari},
	year = {2014},
	doi = {10.1007/978-94-017-8663-8_4},
	keywords = {Airborne Laser Scanning, Digital Terrain Model, Global Navigation Satellite System, Terrestrial Laser Scanning},
	pages = {63--88},
}

@article{valbuena_most_2018,
	title = {Most similar neighbor imputation of forest attributes using metrics derived from combined airborne {LIDAR} and multispectral sensors},
	volume = {11},
	issn = {1753-8947},
	url = {https://doi.org/10.1080/17538947.2017.1387183},
	doi = {10.1080/17538947.2017.1387183},
	abstract = {In the context of predicting forest attributes using a combination of airborne LIDAR and multispectral (MS) sensors, we suggest the inclusion of normalized difference vegetation index (NDVI) metrics along with the more traditional LIDAR height metrics. Here the data fusion method consists of back-projecting LIDAR returns onto original MS images, avoiding co-registration errors. The prediction method is based on non-parametric imputation (the most similar neighbor). Predictor selection and accuracy assessment include hypothesis tests and over-fitting prevention methods. Results show improvements when using combinations of LIDAR and MS compared to using either of them alone. The MS sensor has little explanatory capacity for forest variables dependent on tree height, already well determined from LIDAR alone. However, there is potential for variables dependent on tree diameters and their density. The combination of LIDAR and MS sensors can be very beneficial for predicting variables describing forests structural heterogeneity, which are best described from synergies between LIDAR heights and NDVI dispersion. Results demonstrate the potential of NDVI metrics to increase prediction accuracy of forest attributes. Their inclusion in the predictor dataset may, however, in a few cases be detrimental to accuracy, and therefore we recommend to carefully assess the possible advantages of data fusion on a case-by-case basis.},
	number = {12},
	urldate = {2021-12-29},
	journal = {International Journal of Digital Earth},
	author = {Valbuena, Ruben and Hernando, Ana and Manzanera, Jose Antonio and Martínez-Falero, Eugenio and García-Abril, Antonio and Mola-Yudego, Blas},
	month = dec,
	year = {2018},
	keywords = {Airborne laser scanning, data fusion, forest attribute prediction, multispectral imagery, nearest neighbor},
	pages = {1205--1218},
}

@article{griffiths_improving_2019,
	title = {Improving public data for building segmentation from {Convolutional} {Neural} {Networks} ({CNNs}) for fused airborne lidar and image data using active contours},
	volume = {154},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271619301352},
	doi = {10.1016/j.isprsjprs.2019.05.013},
	abstract = {Robust and reliable automatic building detection and segmentation from aerial images/point clouds has been a prominent field of research in remote sensing, computer vision and point cloud processing for a number of decades. One of the largest issues associated with deep learning methods is the high quantity of data required for training. To help address this we present a method to improve public GIS building footprint labels by using Morphological Geodesic Active Contours (MorphGACs). We demonstrate by improving the quality of building footprint labels for detection and semantic segmentation, more robust and reliable models can be obtained. We evaluate these methods over a large UK-based dataset of 24556 images containing 169835 building instances. This is achieved by training several Mask/Faster R-CNN and RetinaNet deep convolutional neural networks. Networks are supplied with both RGB and fused RGB-lidar data. We offer quantitative analysis on the benefits of the inclusion of depth data for building segmentation. By employing both methods we achieve a detection accuracy of 0.92 (mAP@0.5) and segmentation f1 scores of 0.94 over a 4911 test images ranging from urban to rural scenes.},
	language = {en},
	urldate = {2021-12-29},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Griffiths, David and Boehm, Jan},
	month = aug,
	year = {2019},
	keywords = {Aerial, Convolutional neural networks, Deep learning, Image processing, Lidar, Segmentation},
	pages = {70--83},
}

@article{guo_relevance_2011,
	title = {Relevance of airborne lidar and multispectral image data for urban scene classification using {Random} {Forests}},
	volume = {66},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271610000705},
	doi = {10.1016/j.isprsjprs.2010.08.007},
	abstract = {Airborne lidar systems have become a source for the acquisition of elevation data. They provide georeferenced, irregularly distributed 3D point clouds of high altimetric accuracy. Moreover, these systems can provide for a single laser pulse, multiple returns or echoes, which correspond to different illuminated objects. In addition to multi-echo laser scanners, full-waveform systems are able to record 1D signals representing a train of echoes caused by reflections at different targets. These systems provide more information about the structure and the physical characteristics of the targets. Many approaches have been developed, for urban mapping, based on aerial lidar solely or combined with multispectral image data. However, they have not assessed the importance of input features. In this paper, we focus on a multi-source framework using aerial lidar (multi-echo and full waveform) and aerial multispectral image data. We aim to study the feature relevance for dense urban scenes. The Random Forests algorithm is chosen as a classifier: it runs efficiently on large datasets, and provides measures of feature importance for each class. The margin theory is used as a confidence measure of the classifier, and to confirm the relevance of input features for urban classification. The quantitative results confirm the importance of the joint use of optical multispectral and lidar data. Moreover, the relevance of full-waveform lidar features is demonstrated for building and vegetation area discrimination.},
	language = {en},
	number = {1},
	urldate = {2021-12-29},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Guo, Li and Chehata, Nesrine and Mallet, Clément and Boukir, Samia},
	month = jan,
	year = {2011},
	keywords = {Lidar, Multispectral image, Random forests, Urban, Variable importance},
	pages = {56--66},
}

@article{shendryk_fine-scale_2020,
	title = {Fine-scale prediction of biomass and leaf nitrogen content in sugarcane using {UAV} {LiDAR} and multispectral imaging},
	volume = {92},
	issn = {0303-2434},
	url = {https://www.sciencedirect.com/science/article/pii/S0303243420303457},
	doi = {10.1016/j.jag.2020.102177},
	abstract = {Unmanned Aerial Vehicle (UAV) platforms and associated sensing technologies are extensively utilized in precision agriculture. Using LiDAR and imaging sensors mounted on multirotor UAVs, we can observe fine-scale crop variations that can help improve fertilizer management and maximize yields. In this study we used UAV mounted LiDAR and multispectral imaging sensors to monitor two sugarcane field trials with variable nitrogen (N) fertilization inputs in the Wet Tropics region of Australia. From six surveys performed at 42-day intervals, we monitored crop growth in terms of height, density and vegetation indices. In each survey period, we estimated a set of models to predict at-harvest biomass at fine scale (2m×2m plots). We compared the predictive performance of models based on multispectral predictors only, LiDAR predictors only, a fusion of multispectral and LiDAR predictors, and a normalized difference vegetation index (NDVI) benchmark. We found that predictive performance peaked early in the season, at 100–142 days after the previous harvest (DAH), and declined closer to the harvest date. At peak performance (i.e. 142 DAH), the multispectral model performed slightly better (R¯2=0.57) than the LiDAR model (R¯2=0.52), with both outperforming NDVI benchmark (R¯2=0.34). This, however, flipped later in the season, with LiDAR performing slightly better than the multispectral imaging and NDVI benchmark. Interestingly, the fusion model did not perform significantly better than the multispectral model at 100–142 DAH, nor better than LiDAR in subsequent periods. We also estimated a model to predict contemporaneous leaf N content (\%) using multispectral imagery, which demonstrated an R¯2 of 0.57. Our results are of particular interest to nutrient management programs aiming to deliver N fertilizer guidelines for sustainable sugarcane production, as both fine-scale biomass and leaf N content predictions are feasible when management interventions are still possible (i.e. as early as at 100 DAH).},
	language = {en},
	urldate = {2021-12-29},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Shendryk, Yuri and Sofonia, Jeremy and Garrard, Robert and Rist, Yannik and Skocaj, Danielle and Thorburn, Peter},
	month = oct,
	year = {2020},
	keywords = {Biomass, Drone, Fertilizer, Fusion, LiDAR, Multispectral, NDVI, Nitrogen, PCA, Sugarcane, UAV, Yield},
	pages = {102177},
}

@inproceedings{clamens_real-time_2021,
	address = {Vienna, Austria},
	title = {Real-time {Multispectral} {Image} {Processing} and {Registration} on {3D} {Point} {Cloud} for {Vineyard} {Analysis}},
	url = {https://hal.archives-ouvertes.fr/hal-03156063},
	doi = {10.5220/0010266203880398},
	abstract = {Nowadays, precision agriculture and precision viticulture are under strong development. In order to accomplish effective actions, robots require robust perception of the culture and the surrounding environment. Computer vision systems have to identify plant parts (branches, stems, leaves, flowers, fruits, vegetables, etc.) and their respective health status. Moreover, they must merge various plant information, to measure agronomic indices, to classify them and finally to extract data to enable the agriculturist or expert to make a relevant decision. We propose a real-time method to acquire, process and register multispectral images fused to 3D. The sensors system, consisting of a multispectral camera and a RGB-D sensor, can be embedded on a ground robot or other terrestrial vehicles. Experiments conducted in the vineyard field demonstrate that agronomic analyses are allowed.},
	urldate = {2021-12-26},
	booktitle = {16th {International} {Conference} on {Computer} {Vision} {Theory} and {Applications}},
	author = {Clamens, Thibault and Alexakis, Georgios and Duverne, Raphaël and Seulin, Ralph and Fauvet, Eric and Fofi, David},
	month = feb,
	year = {2021},
	keywords = {3D Point Cloud, Agricultural Robotics, Image Registration, Multispectral Imaging, Precision Viticulture},
}

@article{gu_uav-based_2020,
	title = {{UAV}-based integrated multispectral-{LiDAR} imaging system and data processing},
	volume = {63},
	issn = {1869-1900},
	url = {https://doi.org/10.1007/s11431-019-1571-0},
	doi = {10.1007/s11431-019-1571-0},
	abstract = {In the field of remote sensing imaging, multispectral imaging can obtain an image of the observed scene in several bands, while the light detection and ranging (LiDAR) can acquire the accurate 3D geometric information of the scene. With the development of remote sensing technology, how to effectively integrate the two imaging technologies in order to collect and process simultaneous spectral and 3D geometric information has been one of the frontier problems. Most of the present researches on simultaneous spectral and geometric data acquisition focus on the design of physical multispectral LiDAR system, which inevitably lead to an imaging system of heavy weight and high power consumption and thus inconvenient in practice. Different from the present researches, a UAV-based integrated multispectral-LiDAR system is introduced in this paper. Through simultaneous multi-sensor data collection and multispectral point cloud generation, a low-cost and UAV-based portable 3D geometric and spectral information acquisition system can be achieved.},
	language = {en},
	number = {7},
	urldate = {2021-12-29},
	journal = {Science China Technological Sciences},
	author = {Gu, YanFeng and Jin, XuDong and Xiang, RunZi and Wang, QingWang and Wang, Chen and Yang, ShengXiong},
	month = jul,
	year = {2020},
	pages = {1293--1301},
}

@article{vercauteren_diffeomorphic_2009,
	series = {Mathematics in {Brain} {Imaging}},
	title = {Diffeomorphic demons: {Efficient} non-parametric image registration},
	volume = {45},
	issn = {1053-8119},
	shorttitle = {Diffeomorphic demons},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811908011683},
	doi = {10.1016/j.neuroimage.2008.10.040},
	abstract = {We propose an efficient non-parametric diffeomorphic image registration algorithm based on Thirion's demons algorithm. In the first part of this paper, we show that Thirion's demons algorithm can be seen as an optimization procedure on the entire space of displacement fields. We provide strong theoretical roots to the different variants of Thirion's demons algorithm. This analysis predicts a theoretical advantage for the symmetric forces variant of the demons algorithm. We show on controlled experiments that this advantage is confirmed in practice and yields a faster convergence. In the second part of this paper, we adapt the optimization procedure underlying the demons algorithm to a space of diffeomorphic transformations. In contrast to many diffeomorphic registration algorithms, our solution is computationally efficient since in practice it only replaces an addition of displacement fields by a few compositions. Our experiments show that in addition to being diffeomorphic, our algorithm provides results that are similar to the ones from the demons algorithm but with transformations that are much smoother and closer to the gold standard, available in controlled experiments, in terms of Jacobians.},
	language = {en},
	number = {1, Supplement 1},
	urldate = {2021-12-26},
	journal = {NeuroImage},
	author = {Vercauteren, Tom and Pennec, Xavier and Perchant, Aymeric and Ayache, Nicholas},
	month = mar,
	year = {2009},
	pages = {S61--S72},
}

@article{dai_new_2018,
	title = {A new method for {3D} individual tree extraction using multispectral airborne {LiDAR} point clouds},
	volume = {144},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271618302296},
	doi = {10.1016/j.isprsjprs.2018.08.010},
	abstract = {Characterization of individual trees is essential for many applications in forest management and ecology. Previous studies relied on single tree detection from monochromatic wavelength airborne laser scanning (ALS) systems and they focused on the use of the geometric spatial information of the point clouds (i.e., X, Y, and Z coordinates). However, there is quite often a difficulty dealing with clumped trees when only the geometric spatial information is considered. The emergence of multispectral LiDAR sensors provides a new solution for individual tree structure acquisition. The aim of this paper is to investigate the performance of multispectral ALS data for delineating individual trees which are challenging by using the monochromatic wavelength ALS system. The proposed workflow utilizes the mean shift segmentation method on different feature spaces for crown isolation. In addition, both spatial domain and multispectral domain are used to refine the under-segmentation crown segments. Ten plots (2 sets of different structural complexity) located in the dense coniferous forest area in Tobermory, Ontario, Canada are selected as experiment data. Results show that the developed method correctly detects 88\% and 82\% of the dominant trees with and without multispectral information, respectively. Compared with segmentation using geometric spatial information solely, the main improvements are achieved for clumped tree segment with the distinguished multispectral features. This study demonstrates that multispectral airborne laser scanning data is more capable for individual tree delineation than monochromatic wavelength laser scanning data in dealing with forests with clumped crowns in dense forests.},
	language = {en},
	urldate = {2021-12-26},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Dai, Wenxia and Yang, Bisheng and Dong, Zhen and Shaker, Ahmed},
	month = oct,
	year = {2018},
	keywords = {Feature extraction, Forest mapping, Multispectral LiDAR, Point clouds, Segmentation},
	pages = {400--411},
}

@article{ekhtari_classification_2018,
	title = {Classification of {Airborne} {Multispectral} {Lidar} {Point} {Clouds} for {Land} {Cover} {Mapping}},
	volume = {11},
	issn = {2151-1535},
	doi = {10.1109/JSTARS.2018.2835483},
	abstract = {Airborne light detection and ranging (lidar) data are widely used for high-resolution land cover mapping. The lidar elevation data are typically used as complementary information to passive multispectral or hyperspectral imagery to enable higher land cover classification accuracy. In this paper, we examine the capabilities of a recently developed multispectral airborne laser scanner, manufactured by Teledyne Optech, for direct classification of multispectral point clouds into ten land cover classes including grass, trees, two classes of soil, four classes of pavement, and two classes of buildings. The scanner, Titan MW, collects point clouds at three different laser wavelengths simultaneously, opening the door to new possibilities in land cover classification using only lidar data. We show that the recorded intensities of laser returns together with spatial metrics calculated from the three-dimensional (3D) locations of laser returns are sufficient for classifying the point cloud into ten distinct land cover classes. Our classification methods achieved an overall accuracy of 94.7\% with a kappa coefficient of 0.94 using the support vector machine (SVM) method to classify single-return points and an overall accuracy of 79.7\% and kappa coefficient of 0.77 using a rule-based classifier on multireturn points. A land cover map is then generated from the classified point cloud. We show that our results outperform the common approach of rasterizing the point cloud prior to classification by 4\% in overall accuracy, 0.04 in kappa coefficient, and by up to 16\% in commission and omission errors. This improvement however comes at the price of increased complexity and computational burden.},
	number = {6},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Ekhtari, Nima and Glennie, Craig and Fernandez-Diaz, Juan Carlos},
	month = jun,
	year = {2018},
	keywords = {Buildings, Classification, Distance measurement, Earth, Laser radar, Lasers, Support vector machines, Three-dimensional displays, Titan laser scanner, light detection and ranging (lidar), multispectral lidar, point cloud},
	pages = {2068--2078},
}

@article{xu_tree_2020,
	title = {Tree species classification using {UAS}-based digital aerial photogrammetry point clouds and multispectral imageries in subtropical natural forests},
	volume = {92},
	issn = {0303-2434},
	url = {https://www.sciencedirect.com/science/article/pii/S0303243420300647},
	doi = {10.1016/j.jag.2020.102173},
	abstract = {Tree species composition of forest stand is an important indicator of forest inventory attributes for assessing ecosystem health, understanding successional processes, and digitally displaying forest biodiversity. In this study, we acquired high spatial resolution multispectral and RGB imagery over a subtropical natural forest in southwest China using a fixed-wing UAV system. Digital aerial photogrammetric (DAP) technique was used to generate multi-spectral and RGB derived point clouds, upon which individual tree crown (ITC) delineation algorithms and a machine learning classifier were used to identify dominant tree species. To do so, the structure-from-motion method was used to generate RGB imagery-based DAP point clouds. Then, three ITC delineation algorithms (i.e., point cloud segmentation (PCS), image-based multiresolution segmentation (IMRS), and advanced multiresolution segmentation (AMRS)) were used and assessed for ITC detection. Finally, tree-level metrics (i.e., multispectral, texture and point cloud metrics) were used as metrics in the random forest classifier used to classify eight dominant tree species. Results indicated that the accuracy of the AMRS ITC segmentation was highest (F1-score = 82.5 \%), followed by the segmentation using PCS (F1-score = 79.6 \%), the IMRS exhibited the lowest accuracy (F1-score = 78.6 \%); forest types classification (coniferous and deciduous) had a higher accuracy than the classification of all eight tree species, and the combination of spectral, texture and structural metrics had the highest classification accuracy (overall accuracy = 80.20 \%). In the classification of both eight tree species and two forest types, the classification accuracies were lowest when only using spectral metrics, indicated that the texture metrics and point cloud structural metrics had a positive impact on the classification (the overall accuracy and kappa accuracy increased by 1.49–4.46 \% and 2.86–6.84 \%, respectively).},
	language = {en},
	urldate = {2021-12-26},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Xu, Zhong and Shen, Xin and Cao, Lin and Coops, Nicholas C. and Goodbody, Tristan R. H. and Zhong, Tai and Zhao, Weidong and Sun, Qinglei and Ba, Sang and Zhang, Zhengnan and Wu, Xiangqian},
	month = oct,
	year = {2020},
	keywords = {High-resolution, Individual tree crown delineation, Multispectral, Tree species classification, UAS},
	pages = {102173},
}

@article{shen_estimation_2019,
	title = {Estimation of {Forest} {Structural} {Attributes} {Using} {Spectral} {Indices} and {Point} {Clouds} from {UAS}-{Based} {Multispectral} and {RGB} {Imageries}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/11/7/800},
	doi = {10.3390/rs11070800},
	abstract = {Forest structural attributes are key indicators for parameterization of forest growth models, which play key roles in understanding the biophysical processes and function of the forest ecosystem. In this study, UAS-based multispectral and RGB imageries were used to estimate forest structural attributes in planted subtropical forests. The point clouds were generated from multispectral and RGB imageries using the digital aerial photogrammetry (DAP) approach. Different suits of spectral and structural metrics (i.e., wide-band spectral indices and point cloud metrics) derived from multispectral and RGB imageries were compared and assessed. The selected spectral and structural metrics were used to fit partial least squares (PLS) regression models individually and in combination to estimate forest structural attributes (i.e., Lorey\&rsquo;s mean height (HL) and volume(V)), and the capabilities of multispectral- and RGB-derived spectral and structural metrics in predicting forest structural attributes in various stem density forests were assessed and compared. The results indicated that the derived DAP point clouds had perfect visual effects and that most of the structural metrics extracted from the multispectral DAP point cloud were highly correlated with the metrics derived from the RGB DAP point cloud (R2 \&gt; 0.75). Although the models including only spectral indices had the capability to predict forest structural attributes with relatively high accuracies (R2 = 0.56\&ndash;0.69, relative Root-Mean-Square-Error (RMSE) = 10.88\&ndash;21.92\%), the models with spectral and structural metrics had higher accuracies (R2 = 0.82\&ndash;0.93, relative RMSE = 4.60\&ndash;14.17\%). Moreover, the models fitted using multispectral- and RGB-derived metrics had similar accuracies (∆R2 = 0\&ndash;0.02, ∆ relative RMSE = 0.18\&ndash;0.44\%). In addition, the combo models fitted with stratified sample plots had relatively higher accuracies than those fitted with all of the sample plots (∆R2 = 0\&ndash;0.07, ∆ relative RMSE = 0.49\&ndash;3.08\%), and the accuracies increased with increasing stem density.},
	language = {en},
	number = {7},
	urldate = {2021-12-26},
	journal = {Remote Sensing},
	author = {Shen, Xin and Cao, Lin and Yang, Bisheng and Xu, Zhong and Wang, Guibin},
	month = jan,
	year = {2019},
	keywords = {UAS platform, forest structural attributes, multispectral imagery, point cloud},
	pages = {800},
}

@article{villacres_construction_2022,
	title = {Construction of {3D} maps of vegetation indices retrieved from {UAV} multispectral imagery in forested areas},
	volume = {213},
	issn = {1537-5110},
	url = {https://www.sciencedirect.com/science/article/pii/S153751102100297X},
	doi = {10.1016/j.biosystemseng.2021.11.025},
	abstract = {The construction of fuel moisture content (FMC) maps, as well as temperature, terrain topography, and wind speed maps, are essential for the development of fire susceptibility models in forested areas. Moisture distribution in tree canopies requires exploration and a three-dimensional representation. This paper presents the construction of FMC maps expressed as vegetation indices (VIs) in a point cloud. Multispectral images were captured by a camera mounted on an unmanned aerial vehicle to create the point cloud. VIs were estimated in the points that belonged to the forest canopy. To classify the canopy points, we a combination of filtering of ground points and thresholding of VIs was evaluated. On such canopy points, random forest (RF), kernel ridge regression (KRR), and Gaussian process retrieval (GPR) regressors were investigated to estimate twelve VIs related to FMC. The input set of the models consisted of the points representing five wavelengths provided by the multispectral camera. The ground truth of VIs was obtained using a spectrometer. The study area was a 1 ha forest of Pinus radiata in the Maule Region, Chile. The results demonstrated that combining ground filtering and VIs thresholding for canopy points segmentation achieved a precision of 93.27\%, recall of 95.65\%, F1 score of 90.12\%, and accuracy of 87.82\%. Furthermore, the recovery of the VIs using GPR achieved a root mean square error of 0.175 and a coefficient of determination of 0.18. According to the correlation coefficient, GPR was able to recover eleven of the twelve VIs, KRR recovered three, and RF failed to recover any.},
	language = {en},
	urldate = {2021-12-26},
	journal = {Biosystems Engineering},
	author = {Villacrés, Juan and Auat Cheein, Fernando A.},
	month = jan,
	year = {2022},
	keywords = {Canopy segmentation, Fuel moisture content, Multispectral images, Vegetation indices estimation},
	pages = {76--88},
}

@article{isgro_unmanned_2021,
	title = {Unmanned {Aerial} {System}-{Based} {Multispectral} {Water} {Quality} {Monitoring} in the {Iberian} {Pyrite} {Belt} ({SW} {Spain})},
	journal = {Mine Water and the Environment},
	author = {Isgró, Melisa A. and Basallote, M. Dolores and Barbero, Luis},
	year = {2021},
	pages = {1--12},
}

@article{jing_multispectral_2021,
	title = {Multispectral {LiDAR} {Point} {Cloud} {Classification} {Using} {SE}-{PointNet}++},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/13/13/2516},
	doi = {10.3390/rs13132516},
	abstract = {A multispectral light detection and ranging (LiDAR) system, which simultaneously collects spatial geometric data and multi-wavelength intensity information, opens the door to three-dimensional (3-D) point cloud classification and object recognition. Because of the irregular distribution property of point clouds and the massive data volume, point cloud classification directly from multispectral LiDAR data is still challengeable and questionable. In this paper, a point-wise multispectral LiDAR point cloud classification architecture termed as SE-PointNet++ is proposed via integrating a Squeeze-and-Excitation (SE) block with an improved PointNet++ semantic segmentation network. PointNet++ extracts local features from unevenly sampled points and represents local geometrical relationships among the points through multi-scale grouping. The SE block is embedded into PointNet++ to strengthen important channels to increase feature saliency for better point cloud classification. Our SE-PointNet++ architecture has been evaluated on the Titan multispectral LiDAR test datasets and achieved an overall accuracy, a mean Intersection over Union (mIoU), an F1-score, and a Kappa coefficient of 91.16\%, 60.15\%, 73.14\%, and 0.86, respectively. Comparative studies with five established deep learning models confirmed that our proposed SE-PointNet++ achieves promising performance in multispectral LiDAR point cloud classification tasks.},
	language = {en},
	number = {13},
	urldate = {2021-12-26},
	journal = {Remote Sensing},
	author = {Jing, Zhuangwei and Guan, Haiyan and Zhao, Peiran and Li, Dilong and Yu, Yongtao and Zang, Yufu and Wang, Hanyun and Li, Jonathan},
	month = jan,
	year = {2021},
	keywords = {PointNet++, multispectral LiDAR, point cloud classification, squeeze and excitation},
	pages = {2516},
}

@article{lu_experimental_2020,
	title = {Experimental {Evaluation} and {Consistency} {Comparison} of {UAV} {Multispectral} {Minisensors}},
	volume = {12},
	number = {16},
	journal = {Remote Sensing},
	author = {Lu, Han and Fan, Tianxing and Ghimire, Prakash and Deng, Lei},
	year = {2020},
	pages = {2542},
}

@article{hutton_high_2020,
	title = {High {Accuracy} {Direct} {Georeferencing} of the {Altum} {Multi}-{Spectral} {Uav} {Camera} and its {Application} to {High} {Throughput} {Plant} {Phenotyping}},
	volume = {43},
	journal = {The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Hutton, J. J. and Lipa, G. and Baustian, D. and Sulik, J. and Bruce, R. W.},
	year = {2020},
	pages = {451--456},
}

@inproceedings{chakhvashvili_comparison_2021,
	title = {Comparison of {Reflectance} {Calibration} {Workflows} for a {UAV}-{Mounted} {Multi}-{Camera} {Array} {System}},
	doi = {10.1109/IGARSS47720.2021.9555143},
	abstract = {Well radiometrically calibrated UAV-derived reflectance maps are important when analysing time series of vegetation canopies. In this paper, we assessed the quality of reflectance calibration of a multispectral camera system, MicaSense Dual, using two different methods: a single-panel approach offered by the camera manufacturer and an empirical line method. The results show a significant discrepancy between the reference reflectance measurements, and the single-panel approach in the NIR and the red edge bands. This discrepancy is especially pronounced for dark targets. The empirical line correction method has proven to be more accurate, yet for shaded and densely vegetated areas it has led to negative reflectance values in the visible bands. Hence, we argue that users should be aware of the caveats of both reflectance calibration pipelines when working with time-series UAV data.},
	booktitle = {2021 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium} {IGARSS}},
	author = {Chakhvashvili, Erekle and Siegmann, Bastian and Bendig, Juliane and Rascher, Uwe},
	month = jul,
	year = {2021},
	keywords = {Calibration, Cameras, Pipelines, Radiometry, Reflectivity, Time series analysis, UAV, Vegetation mapping, calibration, multispectral, reflectance},
	pages = {8225--8228},
}

@article{franzini_geometric_2019,
	title = {Geometric and radiometric consistency of parrot sequoia multispectral imagery for precision agriculture applications},
	volume = {9},
	number = {24},
	journal = {Applied Sciences},
	author = {Franzini, Marica and Ronchetti, Giulia and Sona, Giovanna and Casella, Vittorio},
	year = {2019},
	pages = {5314},
}

@article{dash_early_2019,
	title = {Early detection of invasive exotic trees using {UAV} and manned aircraft multispectral and {LiDAR} {Data}},
	volume = {11},
	number = {15},
	journal = {Remote Sensing},
	author = {Dash, Jonathan P. and Watt, Michael S. and Paul, Thomas SH and Morgenroth, Justin and Pearse, Grant D.},
	year = {2019},
	pages = {1812},
}

@article{yeo_classification_2020,
	title = {Classification and mapping of saltmarsh vegetation combining multispectral images with field data},
	volume = {236},
	issn = {0272-7714},
	url = {https://www.sciencedirect.com/science/article/pii/S027277141930232X},
	doi = {10.1016/j.ecss.2020.106643},
	abstract = {Salt marshes are areas of high conservation value encompassing diverse ecological gradients responsible for creating unique vegetation structure and composition. In complement to the large body of studies developing vegetation mapping methods through the use of remote sensing data, we tested for the possibility of developing a cost-effective method to map salt marsh vegetation as a basis for monitoring a French Nature Reserve. Using classical multivariate ordination and cluster analyses, accurate and ecologically relevant vegetation groups matching existing typologies were determined from a vegetation database collected for management and conservation rather than mapping purposes. This resulted in six distinct vegetation groups, which were mapped through the combination of the NIR spectral band and radiometric indices (NDVI and NDWI) derived from multispectral 2 m-resolution satellite images (Pleiades images). The addition of a LiDAR-derived digital elevation model (DEM) layer was also tested. The final classified map of the reserve based only on optical layers had an overall accuracy of 75.5\% (Kappa coefficient of 0.71), with varying success between the different vegetation groups. The addition of the DEM slightly decreased map accuracy to 73.6\% (Kappa of 0.68). Decreasing the number of records used for map training had detectable negative effects on map accuracy. This study demonstrated that using already existing field observations combined with only a few spectral bands and radiometric indices from multi-temporal multispectral images with a high spatial resolution can be used as a basis to aid in vegetation classification and mapping of saltmarsh habitats, with the goal of monitoring their dynamics.},
	language = {en},
	urldate = {2021-12-20},
	journal = {Estuarine, Coastal and Shelf Science},
	author = {Yeo, Samantha and Lafon, Virginie and Alard, Didier and Curti, Cécile and Dehouck, Aurélie and Benot, Marie-Lise},
	month = may,
	year = {2020},
	keywords = {Arcachon bay, Multi-temporal image classification, Multispectral data, Phytosociology, Saltmarsh zonation, Southwestern France, Vegetation mapping, Wetland remote sensing},
	pages = {106643},
}

@misc{noauthor_gfkuts_nodate,
	title = {{GFkuts}: a novel multispectral image segmentation method applied to precision agriculture},
	shorttitle = {{GFkuts}},
	url = {https://ieeexplore.ieee.org/abstract/document/9535659/},
	abstract = {Image segmentation enables the precise extraction of several crop traits from multispectral aerial imagery. This paper presents a novel segmentation technique called GFKuts. The method integrates a graph-based optimization algorithm with a k-means Monte Carlo approach. Here, we evaluate the performance of the proposed method against other approaches for image segmentation found in the specialized literature. Results report an improvement on the F1-score accuracy in terms of crop canopy segmentation. These findings are promising for the precise calculation of vegetative indices and other crop trait features.},
	language = {en-US},
	urldate = {2021-12-15},
}

@inproceedings{rapaka_multispectral_2021,
	address = {Cham},
	series = {{EAI}/{Springer} {Innovations} in {Communication} and {Computing}},
	title = {Multispectral {Data} {Processing} for {Agricultural} {Applications} {Using} {Deep} {Learning} {Classification} {Methods}},
	isbn = {978-3-030-47560-4},
	doi = {10.1007/978-3-030-47560-4_6},
	abstract = {Multispectral imaging is used in agricultural systems where variations in the colour reflection of the items investigated are low. Though progress handling techniques are required for the big quantity of information produced by multispectral detectors, deep learning methods can serve a significant part in this assignment. The objective of this job is to show the position of the use of computer training to deep learning methods in multispectral information processing, focusing on apps for agriculture. However, there are not very many research accessible that introduce multispectral information for agricultural implementations to deep learning approaches. In reality, this obvious restriction was the impetus to make this study. Here we use max pooling profound reading (MPDL) technique on multispectral information, and initial findings are provided utilizing Manu’s Disease Dataset-based information, demonstrating the appropriateness of deep learning methods in distant felt information.},
	language = {en},
	booktitle = {2nd {EAI} {International} {Conference} on {Big} {Data} {Innovation} for {Sustainable} {Cognitive} {Computing}},
	publisher = {Springer International Publishing},
	author = {Rapaka, Anuj and Ramu, Arulmurugan},
	editor = {Haldorai, Anandakumar and Ramu, Arulmurugan and Mohanram, Sudha and Chen, Mu-Yen},
	year = {2021},
	keywords = {Deep learning, Max pooling deep learning (MPDL), Multispectral data, Remote sensing},
	pages = {63--82},
}

@article{saha_unsupervised_2020,
	title = {Unsupervised deep transfer learning-based change detection for hr multispectral images},
	volume = {18},
	number = {5},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Saha, Sudipan and Solano-Correa, Yady Tatiana and Bovolo, Francesca and Bruzzone, Lorenzo},
	year = {2020},
	pages = {856--860},
}

@article{dalponte_mapping_2020,
	title = {Mapping forest windthrows using high spatial resolution multispectral satellite images},
	volume = {93},
	issn = {0303-2434},
	url = {https://www.sciencedirect.com/science/article/pii/S0303243420305924},
	doi = {10.1016/j.jag.2020.102206},
	abstract = {Wind disturbances represent the main source of damage in European forests, affecting them directly (windthrows) or indirectly due to secondary damages (insect outbreaks and forest fires). The assessment of windthrows damages is very important to establish adequate management plans and remote sensing can be very useful for this purpose. Many types of optical remote sensing data are available with different spectral, spatial and temporal resolutions, and many options are possible for data acquisition, i.e. immediately after the event or after a certain time. The objective of this study is to compare the windthrows mapping capabilities of two multispectral satellite constellations (i.e. Sentinel-2 and PlanetScope) characterized by very different spectral, spatial and temporal resolutions, and to evaluate the impact of the acquisition conditions on the mapping results. The analysed area, with an extent of 732 km2, is located in the Trentino-South Tyrol region (Italy) which was affected by the Vaia storm on the 27th-30th of October 2018, causing serious forest damages. The change vector analysis technique was used to detect the windthrows. For each data source, two pairs of images were considered: 1) pre- and post- event images acquired as close as possible to the event; 2) pre- and post- event images acquired at optimal conditions, i.e. at similar phenological state and similar illumination conditions. The results obtained with the two satellite constellations are very similar despite their different resolutions. Data acquired in optimal conditions allowed having the best detection rate (accuracy above 80 \%), while data acquired just after the event showed many limitations. Improved spatial resolution (PlanetScope data) allows for a better delineation of the borders of the windthrow areas and of the detection of smaller windthrow patches.},
	language = {en},
	urldate = {2021-12-15},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Dalponte, Michele and Marzini, Sebastian and Solano-Correa, Yady Tatiana and Tonon, Giustino and Vescovo, Loris and Gianelle, Damiano},
	month = dec,
	year = {2020},
	keywords = {Change detection, Change vector analysis, Forests, PlanetScope, Satellite multispectral data, Sentinel-2, Windthrows mapping},
	pages = {102206},
}

@article{du_multi-temporal_2020,
	title = {Multi-temporal monitoring of leaf area index of rice under different nitrogen treatments using {UAV} images},
	volume = {3},
	number = {1},
	journal = {International Journal of Precision Agricultural Aviation},
	author = {Du, Xiaoyue and Wan, Liang and Cen, Haiyan and Chen, Shuobo and Zhu, Jiangpeng and Wang, Hongyan and He, Yong},
	year = {2020},
}

@article{illarionova_neural-based_2020,
	title = {Neural-{Based} {Hierarchical} {Approach} for {Detailed} {Dominant} {Forest} {Species} {Classification} by {Multispectral} {Satellite} {Imagery}},
	volume = {14},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Illarionova, Svetlana and Trekin, Alexey and Ignatiev, Vladimir and Oseledets, Ivan},
	year = {2020},
	pages = {1810--1820},
}

@article{qiu_new_2020,
	title = {A new individual tree crown delineation method for high resolution multispectral imagery},
	volume = {12},
	number = {3},
	journal = {Remote Sensing},
	author = {Qiu, Lin and Jing, Linhai and Hu, Baoxin and Li, Hui and Tang, Yunwei},
	year = {2020},
	pages = {585},
}

@article{wan_tree_2021,
	title = {Tree species classification of forest stands using multisource remote sensing data},
	volume = {13},
	number = {1},
	journal = {Remote Sensing},
	author = {Wan, Haoming and Tang, Yunwei and Jing, Linhai and Li, Hui and Qiu, Fang and Wu, Wenjin},
	year = {2021},
	pages = {144},
}

@article{pocas_remote_2020,
	title = {Remote sensing for estimating and mapping single and basal crop coefficientes: {A} review on spectral vegetation indices approaches},
	volume = {233},
	shorttitle = {Remote sensing for estimating and mapping single and basal crop coefficientes},
	journal = {Agricultural Water Management},
	author = {Pôças, I. and Calera, A. and Campos, I. and Cunha, M.},
	year = {2020},
	pages = {106081},
}

@article{mesas-carrascosa_classification_2020,
	title = {Classification of {3D} point clouds using color vegetation indices for precision viticulture and digitizing applications},
	volume = {12},
	number = {2},
	journal = {Remote Sensing},
	author = {Mesas-Carrascosa, Francisco-Javier and de Castro, Ana I. and Torres-Sánchez, Jorge and Triviño-Tarradas, Paula and Jiménez-Brenes, Francisco M. and García-Ferrer, Alfonso and López-Granados, Francisca},
	year = {2020},
	pages = {317},
}

@inproceedings{main-knorn_sen2cor_2017,
	title = {{Sen2Cor} for sentinel-2},
	volume = {10427},
	booktitle = {Image and {Signal} {Processing} for {Remote} {Sensing} {XXIII}},
	publisher = {International Society for Optics and Photonics},
	author = {Main-Knorn, Magdalena and Pflug, Bringfried and Louis, Jerome and Debaecker, Vincent and Müller-Wilm, Uwe and Gascon, Ferran},
	year = {2017},
	pages = {1042704},
}

@article{wang_fusion_2016,
	title = {Fusion of {Sentinel}-2 images},
	volume = {187},
	journal = {Remote sensing of environment},
	author = {Wang, Qunming and Shi, Wenzhong and Li, Zhongbin and Atkinson, Peter M.},
	year = {2016},
	pages = {241--252},
}

@article{wulder_current_2019,
	title = {Current status of {Landsat} program, science, and applications},
	volume = {225},
	journal = {Remote sensing of environment},
	author = {Wulder, Michael A. and Loveland, Thomas R. and Roy, David P. and Crawford, Christopher J. and Masek, Jeffrey G. and Woodcock, Curtis E. and Allen, Richard G. and Anderson, Martha C. and Belward, Alan S. and Cohen, Warren B.},
	year = {2019},
	pages = {127--147},
}

@inproceedings{keshk_satellite_2017,
	title = {Satellite super-resolution images depending on deep learning methods: {A} comparative study},
	shorttitle = {Satellite super-resolution images depending on deep learning methods},
	doi = {10.1109/ICSPCC.2017.8242625},
	abstract = {The deep learning neural network is a recent development that has become the subject of research in the computer vision and remote sensing disciplines. Super resolution (SR) images can be obtained using deep neural network methods that achieve a higher performance than all previous traditional methods. Here, in this study, the objective is to describe existing deep learning methods for SR satellite images. Different satellite data are used to predict the performance of each deep learning model. This article presents a brief overview of most deep learning techniques and compares them to obtain a more effective and efficient model. The deep network cascade model outperforms other deep learning algorithms; this algorithm is dependable in the reconstruction process for obtaining SR images and overcomes some drawbacks found in traditional reconstruction algorithms. The sparse coding network method remains valuable, and with some enhancements, further improvement in results can be achieved.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Signal} {Processing}, {Communications} and {Computing} ({ICSPCC})},
	author = {Keshk, Hatem Magdy and Yin, Xu-Cheng},
	month = oct,
	year = {2017},
	keywords = {Deep Network Cascade, Deep learning, Encoding, Image reconstruction, Image resolution, Machine learning, Remote Sensing, Remote sensing, Satellite Images, Satellites, Sparse Coding Network, Super-resolution, Training},
	pages = {1--7},
}

@inproceedings{nguyen_self-supervised_2021,
	title = {Self-{Supervised} {Multi}-{Image} {Super}-{Resolution} for {Push}-{Frame} {Satellite} {Images}},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Nguyen_Self-Supervised_Multi-Image_Super-Resolution_for_Push-Frame_Satellite_Images_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2021-12-16},
	author = {Nguyen, Ngoc Long and Anger, Jeremy and Davy, Axel and Arias, Pablo and Facciolo, Gabriele},
	year = {2021},
	pages = {1121--1131},
}

@article{effiom_combining_2019,
	title = {Combining unmanned aerial vehicle and multispectral {Pleiades} data for tree species identification, a prerequisite for accurate carbon estimation},
	volume = {13},
	number = {3},
	journal = {Journal of applied remote sensing},
	author = {Effiom, Agbor Esong and van Leeuwen, Louise M. and Nyktas, Panagiotis and Okojie, Jefferson Adetokunbo and Erdbrügger, Jana},
	year = {2019},
	pages = {034530},
}

@article{lee_determination_2020,
	title = {Determination of the {Normalized} {Difference} {Vegetation} {Index} ({NDVI}) with {Top}-of-{Canopy} ({TOC}) reflectance from a {KOMPSAT}-{3A} image using {Orfeo} {ToolBox} ({OTB}) extension},
	volume = {9},
	number = {4},
	journal = {ISPRS International Journal of Geo-Information},
	author = {Lee, Kiwon and Kim, Kwangseob and Lee, Sun-Gu and Kim, Yongseung},
	year = {2020},
	pages = {257},
}

@article{rupnik_3d_2018,
	title = {{3D} reconstruction from multi-view {VHR}-satellite images in {MicMac}},
	volume = {139},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271618300819},
	doi = {10.1016/j.isprsjprs.2018.03.016},
	abstract = {This work addresses the generation of high quality digital surface models by fusing multiple depths maps calculated with the dense image matching method. The algorithm is adapted to very high resolution multi-view satellite images, and the main contributions of this work are in the multi-view fusion. The algorithm is insensitive to outliers, takes into account the matching quality indicators, handles non-correlated zones (e.g. occlusions), and is solved with a multi-directional dynamic programming approach. No geometric constraints (e.g. surface planarity) or auxiliary data in form of ground control points are required for its operation. Prior to the fusion procedures, the RPC geolocation parameters of all images are improved in a bundle block adjustment routine. The performance of the algorithm is evaluated on two VHR (Very High Resolution)-satellite image datasets (Pléiades, WorldView-3) revealing its good performance in reconstructing non-textured areas, repetitive patterns, and surface discontinuities.},
	language = {en},
	urldate = {2021-12-16},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Rupnik, Ewelina and Pierrot-Deseilligny, Marc and Delorme, Arthur},
	month = may,
	year = {2018},
	keywords = {Bundle block adjustment, Dense image matching, Depth map fusion, Multi-view, VHR-satellite imagery},
	pages = {201--211},
}

@article{rothermel_photometric_2020,
	title = {Photometric multi-view mesh refinement for high-resolution satellite images},
	volume = {166},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271620301210},
	doi = {10.1016/j.isprsjprs.2020.05.001},
	abstract = {Modern high-resolution satellite sensors collect optical imagery with ground sampling distances (GSDs) of 30–50 cm, which has sparked a renewed interest in photogrammetric 3D surface reconstruction from satellite data. State-of-the-art reconstruction methods typically generate 2.5D elevation data. Here, we present an approach to recover full 3D surface meshes from multi-view satellite imagery. The proposed method takes as input a coarse initial mesh and refines it by iteratively updating all vertex positions to maximise the photo-consistency between images. Photo-consistency is measured in image space, by transferring texture from one image to another via the surface. We derive the equations to propagate changes in texture similarity through the rational function model (RFM), often also referred to as rational polynomial coefficient (RPC) model. Furthermore, we devise a hierarchical scheme to optimise the surface with gradient descent. In experiments with two different datasets, we show that the refinement improves the initial digital elevation models (DEMs) generated with conventional dense image matching. Moreover, we demonstrate that our method is able to reconstruct true 3D geometry, such as facade structures, if off-nadir views are available.},
	language = {en},
	urldate = {2021-12-16},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Rothermel, Mathias and Gong, Ke and Fritsch, Dieter and Schindler, Konrad and Haala, Norbert},
	month = aug,
	year = {2020},
	keywords = {3D reconstruction, DSM, Multi-view-stereo image matching, Satellite imagery},
	pages = {52--62},
}

@inproceedings{dangelo_dense_2012,
	title = {Dense multi-view stereo from satellite imagery},
	doi = {10.1109/IGARSS.2012.6352565},
	abstract = {Digital surface models can be efficiently generated with automatic image matching from optical stereo images. Modern satellites, such as WorldView-1 and 2 can acquire multiple views of an area in the same orbit. These datasets offer high redundancy and thus allow higher quality 3D reconstructions than previously possible. This paper evaluates multiple DSM generation algorithms based on dense image matching on a 25 view dataset acquired by the WorldView-2 satellite. Fusion of the images is performed at different stages of image matching and results are compared to reveal the advantages and disadvantages of each method.},
	booktitle = {2012 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {d'Angelo, Pablo and Kuschk, Georg},
	month = jul,
	year = {2012},
	keywords = {Image matching, Image resolution, Merging, Satellites, Space vehicles, Stereo vision, Surface reconstruction, digital surface model, multi-view, optical imagery, stereo matching},
	pages = {6944--6947},
}

@article{bolton_optimizing_2020,
	title = {Optimizing {Landsat} time series length for regional mapping of lidar-derived forest structure},
	volume = {239},
	issn = {0034-4257},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425720300146},
	doi = {10.1016/j.rse.2020.111645},
	abstract = {The value of combining Landsat time series and airborne laser scanning (ALS) data to produce regional maps of forest structure has been well documented. However, studies are often performed over single study areas or forest types, preventing a robust assessment of the approaches that produce the most accurate estimates. Here, we use Landsat time series data to estimate forest attributes across six Canadian study sites, which vary by forest type, productivity, management regime, and disturbance history, with the goal of investigating which spectral indices and time series lengths yield the most accurate estimates of forest attributes across a range of conditions. We use estimates of stand height, basal area, and stem volume derived from ALS data as calibration and validation data, and develop random forest models to estimate forest structure with Landsat time series data and topographic variables at each site. Landsat time series predictors, which were derived from annual gap-free image composites, included the median, interquartile range, and Theil Sen slope of vegetation indices through time. To investigate the optimal time series length for predictor variables, time series length was varied from 1 to 33 years. Across all six sites, increasing the time series length led to improved estimation accuracy, however the optimal time series length was not consistent across sites. Specifically, model accuracies plateaued at a time series length of {\textbackslash}textasciitilde15 years for two sites (R2 = 0.67–0.74), while the accuracies continued to increase until the maximum time series length was reached (24–29 years) for the remaining four sites (R2 = 0.45–0.70). Spectral indices that relied on shortwave infrared bands (Tasseled Cap Wetness and Normalized Burn Ratio) were frequently the most important spectral indices. Adding Landsat-derived disturbance variables (time since last disturbance, type of disturbance) did not meaningfully improve model results; however, this finding was largely due to the fact that most recently disturbed stands did not have predictions of forest attributes from ALS, so disturbed sites were poorly represented in the models. As model accuracies varied regionally and no optimal time series length was found, we provide an approach that can be utilized to determine the optimal time series length on a case by case basis, allowing users to extrapolate estimates of forest attributes both spatially and temporally using multispectral time series data.},
	language = {en},
	urldate = {2021-12-16},
	journal = {Remote Sensing of Environment},
	author = {Bolton, Douglas K. and Tompalski, Piotr and Coops, Nicholas C. and White, Joanne C. and Wulder, Michael A. and Hermosilla, Txomin and Queinnec, Martin and Luther, Joan E. and van Lier, Olivier R. and Fournier, Richard A. and Woods, Murray and Treitz, Paul M. and van Ewijk, Karin Y. and Graham, George and Quist, Lauren},
	month = mar,
	year = {2020},
	keywords = {Airborne laser scanning, Enhanced forest inventory, Forest structure, Landsat time series, Remote sensing},
	pages = {111645},
}

@article{lechner_applications_2020,
	title = {Applications in {Remote} {Sensing} to {Forest} {Ecology} and {Management}},
	volume = {2},
	issn = {2590-3322},
	url = {https://www.sciencedirect.com/science/article/pii/S2590332220302062},
	doi = {10.1016/j.oneear.2020.05.001},
	abstract = {Remote sensing provides valuable insights into pressing environmental challenges and is a critical tool for driving solutions. In this Primer, we briefly introduce the important role of remote sensing in forest ecology and management, which includes applications as diverse as mapping the distribution of forest ecosystems and characterizing the three-dimensional structure of forests. We describe six key reasons why remote sensing has become an important data source and introduce the different types of sensors (e.g., multispectral and synthetic aperture radar) and platforms (e.g., unmanned aerial vehicles and satellites) that have been used for mapping a diversity of forest variables. The rapid advancement in remote-sensing technology, techniques, and platforms is likely to result in a greater democratization of remote-sensing data to support forest management and conservation in parts of the world where environmental issues are the most urgent.},
	language = {en},
	number = {5},
	urldate = {2021-12-16},
	journal = {One Earth},
	author = {Lechner, Alex M. and Foody, Giles M. and Boyd, Doreen S.},
	month = may,
	year = {2020},
	pages = {405--412},
}

@article{rohith_super-resolution_2020,
	title = {Super-{Resolution} {Based} {Deep} {Learning} {Techniques} for {Panchromatic} {Satellite} {Images} in {Application} to {Pansharpening}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3020978},
	abstract = {Pansharpening is a technique that fuses the coarser resolution of multispectral imagery (MS) with high spatial resolution panchromatic (PAN) imagery. Pansharpening is prone to spectral distortions based on the nature of the panchromatic band. If the spatial features are unclear in the panchromatic image, the pan-sharpened image will not be able to produce clear images. Super-Resolution (SR) is a technique that enhances minute details of the features in the image, thereby improving spatial information in the image. By fusing the Multispectral image with the super-resolved panchromatic image, there is a chance for producing high-quality multispectral imagery (pan-sharpened image). In this paper, ten state-of-the-art super-resolution based on deep learning techniques are tested and analyzed using ten different publicly available panchromatic datasets. On analysis, a feedback network for image super-resolution (SRFBN) technique outperforms the other algorithms in terms of sharp edges and pattern clarity, which are not visible in the input image. The proposed method is the fusion of SR applied PAN image with the MS image using a benchmarked Band Depended Spatial Detail (BDSD) pansharpening algorithm. The proposed method experiments with six datasets from different sensors. On analysis, the proposed technique outperforms the other counterpart pansharpening algorithms in terms of enhanced spatial information in addition to sharp edges and pattern clarity at reduced spectral distortion. Hence, the super-resolution based pansharpening algorithm is recommended for high spatial image applications.},
	journal = {IEEE Access},
	author = {Rohith, G. and Kumar, Lakshmi Sutha},
	year = {2020},
	keywords = {Deep Learning (DL), Machine learning, Satellites, Sensors, Signal resolution, Spatial resolution, Training, convolution neural networks (CNN), pansharpening applications},
	pages = {162099--162121},
}

@article{saralioglu_semantic_2020,
	title = {Semantic segmentation of land cover from high resolution multispectral satellite images by spectral-spatial convolutional neural network},
	volume = {0},
	issn = {1010-6049},
	url = {https://doi.org/10.1080/10106049.2020.1734871},
	doi = {10.1080/10106049.2020.1734871},
	abstract = {Research to improve the accuracy of very high-resolution satellite image classification algorithms is still one of the hot topics in the field of remote sensing. Successful results of deep learning methods in areas such as image classification and object detection have led to the application of these methods to remote sensing problems. Recently, Convolutional Neural Networks (CNNs) are among the most common deep learning methods used in image classification, however, the use of CNN’s in satellite image classification is relatively new. Due to the high computational complexity of 3D CNNs, which aim to extract both spatial and spectral information, 2D CNNs focussing on the extraction of spatial information are often preferred. High-resolution satellite images, however, contain crucial spectral information as well as spatial information. In this study, a 3D-2D CNN model using both spectral and spatial information was applied to extract more accurate land cover information from very high-resolution satellite images. The model was applied on a Worldview-2 satellite image including agricultural product areas such as tea, hazelnut groves and land use classes such as buildings and roads. The results of the CNN based model were also compared against those of the Support Vector Machine (SVM) and Random Forest (RF) algorithms. The post-classification accuracies were obtained using 800 control points generated by a web interface created for crowdsourcing purposes. The classification accuracy was 95.6\% for the 3D-2D CNN model, 89.2\% for the RF and 86.4\% for the SVM.},
	number = {0},
	urldate = {2021-12-16},
	journal = {Geocarto International},
	author = {Saralioglu, Ekrem and Gungor, Oguz},
	month = mar,
	year = {2020},
	keywords = {Remote sensing, classification, deep learning, semantic segmentation},
	pages = {1--21},
}

@incollection{ballouch_toward_2022,
	address = {Cham},
	series = {Advances in {Science}, {Technology} \& {Innovation}},
	title = {Toward a {Deep} {Learning} {Approach} for {Automatic} {Semantic} {Segmentation} of {3D} {Lidar} {Point} {Clouds} in {Urban} {Areas}},
	isbn = {978-3-030-80458-9},
	url = {https://doi.org/10.1007/978-3-030-80458-9_6},
	abstract = {Semantic segmentation of Lidar data using Deep Learning (DL) is a fundamental step for a deep and rigorous understanding of large-scale urban areas. Indeed, the increasing development of Lidar technology in terms of accuracy and spatial resolution offers a best opportunity for delivering a reliable semantic segmentation in large-scale urban environments. Significant progress has been reported in this direction. However, the literature lacks a deep comparison of the existing methods and algorithms in terms of strengths and weakness. The aim of the present paper is therefore to propose an objective review about these methods by highlighting their strengths and limitations. We then propose a new approach based on the combination of Lidar data and other sources in conjunction with a Deep Learning technique whose objective is to automatically extract semantic information from airborne Lidar point clouds by enhancing both accuracy and semantic precision compared to the existing methods. We finally present the first results of our approach.},
	language = {en},
	urldate = {2021-12-16},
	booktitle = {Geospatial {Intelligence}: {Applications} and {Future} {Trends}},
	publisher = {Springer International Publishing},
	author = {Ballouch, Zouhair and Hajji, Rafika and Ettarid, Mohamed},
	editor = {Barramou, Fatimazahra and El Brirchi, El Hassan and Mansouri, Khalifa and Dehbi, Youness},
	year = {2022},
	doi = {10.1007/978-3-030-80458-9_6},
	keywords = {Deep learning, Lidar, Semantic segmentation, Urban environment},
	pages = {67--77},
}

@incollection{furukawa_corn_2020,
	address = {Cham},
	title = {Corn {Height} {Estimation} {Using} {UAV} for {Yield} {Prediction} and {Crop} {Monitoring}},
	isbn = {978-3-030-27157-2},
	url = {https://doi.org/10.1007/978-3-030-27157-2_5},
	abstract = {Precision agriculture is improving agriculture management worldwide through technologies such as global navigation satellite system (GNSS), robotics, sensors, and variable rate applications. Geographic information system (GIS) and remote sensing are fundamental techniques for precision agriculture, providing different types of information: plantation layouts, crop health, and plant growth stages; these tools can provide information to farmers quickly. The popularization of unmanned aerial vehicles (UAV) made those aircraft more affordable and easy to use, providing information with high spatial and temporal resolutions. This study aimed to predict corn crop yield through corn height estimation generated by 3D photogrammetry based on structure from motion technology. The UAV data were taken in 14-flight campaign to acquire 3D; red, green, and blue (RGB); and normalized difference vegetation index (NDVI) data for 5 months in 2017 and compared with the ground data obtained in the harvest in middle October of the same year. The methodology allows understanding the whole field, while other methods are based on sample data, showing it to be more convenient since it is less time-consuming. Considering only the UAV height estimation (UHE) variable, the prediction reached an R-squared value of 0.51 with dry grain yield at the beginning of August and allowed plant height monitoring after NDVI saturation, presenting a high potential for yield prediction and crop monitoring.},
	language = {en},
	urldate = {2021-12-16},
	booktitle = {Unmanned {Aerial} {Vehicle}: {Applications} in {Agriculture} and {Environment}},
	publisher = {Springer International Publishing},
	author = {Furukawa, Flavio and Maruyama, Kenji and Saito, Youlia Kamei and Kaneko, Masami},
	editor = {Avtar, Ram and Watanabe, Teiji},
	year = {2020},
	doi = {10.1007/978-3-030-27157-2_5},
	keywords = {Height estimation, Photogrammetry, Remote sensing, UAV, Yield prediction},
	pages = {51--69},
}

@inproceedings{gadiraju_multimodal_2020,
	address = {New York, NY, USA},
	series = {{KDD} '20},
	title = {Multimodal {Deep} {Learning} {Based} {Crop} {Classification} {Using} {Multispectral} and {Multitemporal} {Satellite} {Imagery}},
	isbn = {978-1-4503-7998-4},
	url = {https://doi.org/10.1145/3394486.3403375},
	doi = {10.1145/3394486.3403375},
	abstract = {The Food and Agriculture Organization (FAO) of the United Nations predicts that in order to meet the needs of the expected 3 billion population growth by 2050, food production has to increase by 60\%. Therefore, monitoring and mapping crops accurately is essential for estimating food production during each crop growing season across the globe. Traditionally, multispectral remote sensing imagery has been widely used for mapping crops worldwide. However, single date imagery does not capture temporal characteristics (phenology) of growing crops, leading to imprecise crop maps and food estimates. On the other hand, purely temporal classification approaches also produce inaccurate crop maps as they do not account for spatial autocorrelations. In this paper, we present a multimodal deep learning solution that jointly exploits spatial-spectral and phenological properties to identify major crop types. Using a two stream architecture, spatial characteristics are captured via a spatial stream consisting of very high resolution images (single date, 1m, 3-spectral bands, USDA NAIP) with a CNN and the phenological characteristics via a temporal stream images (biweekly, 250m, MODIS NDVI) with an LSTM. Experimental results show that the proposed multimodal solution reduces prediction error by 60\%.},
	urldate = {2021-12-16},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Gadiraju, Krishna Karthik and Ramachandra, Bharathkumar and Chen, Zexi and Vatsavai, Ranga Raju},
	month = aug,
	year = {2020},
	keywords = {crop classification, multimodal, neural networks, remote sensing},
	pages = {3234--3242},
}

@inproceedings{abady_gan_2020,
	title = {{GAN} generation of synthetic multispectral satellite images},
	volume = {11533},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11533/115330L/GAN-generation-of-synthetic-multispectral-satellite-images/10.1117/12.2575765.full},
	doi = {10.1117/12.2575765},
	abstract = {Generative Adversarial Networks (GAN) have been used for both image generation and image style translation. In this paper, we aim to apply GANs to multispectral satellite image. For the image generation, we take advantage of the progressive GAN training methodology, that is purposely modified to generate multi-band 16 bits satellite images that are similar to a Sentinel-2 level-1C product. The generated images that we obtained imitate closely the spectral signatures of the kind of terrain in the images, as it can be seen by comparing typical spectral view between synthetic and natural images. Furthermore, we consider the recent use of GAN architectures for transferring the style of the images and apply them to perform land-cover transfer of satellite images. Specifically, we used the unpaired style transfer method to modify images that are dominant in vegetation land cover into images that are dominated by bare land cover and vice versa. The land-cover transfer via GANs gives very promising results and the visual quality for the transferred images is also satisfactory, showing that the land-cover transfer is an easier task compared to the GAN generation from scratch. Especially, results are good when the target domain is bare land, in which the visual quality for the transferred images is also very good.},
	urldate = {2021-12-16},
	booktitle = {Image and {Signal} {Processing} for {Remote} {Sensing} {XXVI}},
	publisher = {SPIE},
	author = {Abady, L. and Barni, M. and Garzelli, A. and Tondi, B.},
	month = sep,
	year = {2020},
	pages = {122--133},
}

@article{wang_extraction_2020,
	title = {Extraction of urban building damage using spectral, height and corner information from {VHR} satellite images and airborne {LiDAR} data},
	volume = {159},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271619302862},
	doi = {10.1016/j.isprsjprs.2019.11.028},
	abstract = {Earth observation-based damage assessment has been widely studied in recent years. Considering that the height and spatial variability of buildings change significantly in earthquake-devastated areas, a novel multi-stage urban building damage extraction method that uses bi-temporal spectral, height and corner information is proposed in this study. The post-event height features were directly derived from airborne light detection and ranging (LiDAR) data, whereas pre-event height features were generated using pre-event stereo-paired images from different satellites. The spatial features were quantified using the density of corner points (DCP) in spectral images. The proposed method of urban building damage extraction is summarised as follows. Bi-temporal height and corner features were first generated from bi-temporal very high resolution (VHR) satellite data and post-event airborne LiDAR data. Then, vegetation, bare land (pavement and soil) and shadow were extracted from post-event VHR image and height data, and masked out. Finally, building damage was extracted from the remaining areas using the height difference and DCP difference between pre- and post-event images. A post-processing procedure was used to further refine the initial extraction results. The proposed method was evaluated using bi-temporal VHR images and post-event LiDAR data collected in Port au Prince, Haiti, which was heavily hit by an earthquake in January 2010. The results showed that the proposed method significantly outperformed the two comparative methods in the extraction of urban building damage.},
	language = {en},
	urldate = {2021-12-16},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Wang, Xue and Li, Peijun},
	month = jan,
	year = {2020},
	keywords = {Building damage, Corner, Earthquake, Height, Very high resolution (VHR)},
	pages = {322--336},
}

@article{poli_radiometric_2015,
	series = {High-{Resolution} {Earth} {Imaging} for {Geospatial} {Information}},
	title = {Radiometric and geometric evaluation of {GeoEye}-1, {WorldView}-2 and {Pléiades}-{1A} stereo images for {3D} information extraction},
	volume = {100},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271614000951},
	doi = {10.1016/j.isprsjprs.2014.04.007},
	abstract = {Today the use of spaceborne Very High Resolution (VHR) optical sensors for automatic 3D information extraction is increasing in the scientific and civil communities. The 3D Optical Metrology (3DOM) unit of the Bruno Kessler Foundation (FBK) in Trento (Italy) has collected VHR satellite imagery, as well as aerial and terrestrial data over Trento for creating a complete testfield for investigations on image radiometry, geometric accuracy, automatic digital surface model (DSM) generation, 2D/3D feature extraction, city modelling and data fusion. This paper addresses the radiometric and the geometric aspects of the VHR spaceborne imagery included in the Trento testfield and their potential for 3D information extraction. The dataset consist of two stereo-pairs acquired by WorldView-2 and by GeoEye-1 in panchromatic and multispectral mode, and a triplet from Pléiades-1A. For reference and validation, a DSM from airborne LiDAR acquisition is used. The paper gives details on the project, dataset characteristics and achieved results.},
	language = {en},
	urldate = {2021-12-16},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Poli, D. and Remondino, F. and Angiuli, E. and Agugiaro, G.},
	month = feb,
	year = {2015},
	keywords = {DSM, Geometry, Quality assessment, Quantitative analysis, Radiometry, Very High Resolution},
	pages = {35--47},
}

@article{sagan_field-scale_2021,
	title = {Field-scale crop yield prediction using multi-temporal {WorldView}-3 and {PlanetScope} satellite data and deep learning},
	volume = {174},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271621000411},
	doi = {10.1016/j.isprsjprs.2021.02.008},
	abstract = {Agricultural management at field-scale is critical for improving yield to address global food security, as providing enough food for the world’s growing population has become a wicked problem for both scientists and policymakers. County- or regional-scale data do not provide meaningful information to farmers who are interested in field-scale yield forecasting for effective and timely field management. No studies directly utilized raw satellite imagery for field-scale yield prediction using deep learning. The objectives of this paper were twofold: (1) to develop a raw imagery-based deep learning approach for field-scale yield prediction, (2) investigate the contribution of in-season multitemporal imagery for grain yield prediction with hand-crafted features and WorldView-3 (WV) and PlanetScope (PS) imagery as the direct input, respectively. Four WV-3 and 25 PS imagery collected during the growing season of soybean were utilized. Both 2-dimensional (2D) and 3-dimensional (3D) convolution neural network (CNN) architectures were developed that integrated spectral, spatial, temporal information contained in the satellite data. For comparison, hundreds of carefully selected spectral, spatial, textural, and temporal features that are optimal for crop growth monitoring were extracted and fed into the same deep learning model. Our results demonstrated that (1) deep learning was able to predict yield directly using raw satellite imagery to the extent that was comparable to feature-fed deep learning approaches; (2) both 2D and 3D CNN models were able to explain nearly 90\% variance in field-scale yield; (3) limited number of WV-3 outperformed multi-temporal PS data collected during entire growing season mainly attributed to RedEdge and SWIR bands available with WV-3; and (4) 3D CNN increased the prediction power of PS data compared to 2D CNN due to its ability to digest temporal features extracted from PS data.},
	language = {en},
	urldate = {2021-12-16},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Sagan, Vasit and Maimaitijiang, Maitiniyazi and Bhadra, Sourav and Maimaitiyiming, Matthew and Brown, Davis R. and Sidike, Paheding and Fritschi, Felix B.},
	month = apr,
	year = {2021},
	keywords = {Artificial intelligence, Convolutionneural network, Deep learning, Food security, PlanetScope, ResNet, WorldView-3},
	pages = {265--281},
}

@article{liu_registration_2018,
	title = {Registration of multispectral {3D} points for plant inspection},
	volume = {19},
	issn = {1573-1618},
	url = {https://doi.org/10.1007/s11119-017-9536-3},
	doi = {10.1007/s11119-017-9536-3},
	abstract = {Machine vision technologies have shown advantages for efficient and accurate plant inspection in precision agriculture. Regarding the balance between accuracy of inspection and compactness for infield applications, multispectral imaging systems would be more suitable than RGB colour cameras or hyperspectral imaging systems. Multispectral image registration (MIR) is a key issue for multispectral imaging systems, however, this task is challenging. First of all, in many cases, two images needing registration do not have a one-to-one linear mapping in 2D space and therefore they cannot be aligned in 2D images. Furthermore, the general MIR algorithms are limited to images with uniform intensity and are incapable of registering images with rich features. This study developed a machine vision system (MVS) and a MIR method which replaces 2D-2D image registration by 3D-3D point cloud registration. The system can register 3D point clouds of ultraviolet (UV), blue, green, red and near-infrared (NIR) spectra in 3D space. It was found that the point clouds of general plants created by images of different spectral bands have a complementary property, and therefore a combined point cloud, called multispectral 3D point cloud, is denser than any cloud created by a single spectral band. Intensity information of each spectral band is available in a multispectral 3D point cloud and therefore image fusion and 3D morphological analysis can be conducted in the cloud. The MVS could be used as a sensor of a robotic system to fulfil on-the-go infield plant inspection tasks.},
	language = {en},
	number = {3},
	urldate = {2021-12-17},
	journal = {Precision Agriculture},
	author = {Liu, Huajian and Lee, Sang-Heon and Chahl, Javaan Singh},
	month = jun,
	year = {2018},
	pages = {513--536},
}

@article{gui_automated_2021,
	title = {Automated {LoD}-2 model reconstruction from very-high-resolution satellite-derived digital surface model and orthophoto},
	volume = {181},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271621002318},
	doi = {10.1016/j.isprsjprs.2021.08.025},
	abstract = {Digital surface models (DSM) generated from multi-stereo satellite images are getting higher in quality owing to the improved data resolution and photogrammetric reconstruction algorithms. Very-high-resolution (VHR, with sub-meter level resolution) satellite images effectively act as a unique data source for 3D building modeling, because it provides a much wider data coverage with lower cost than the traditionally used LiDAR and airborne photogrammetry data. Although 3D building modeling from point clouds has been intensively investigated, most of the methods are still ad-hoc to specific types of buildings and require high-quality and high-resolution data sources as input. Therefore, when applied to satellite-based point cloud or DSMs, these developed approaches are not readily applicable and more adaptive and robust methods are needed. As a result, most of the existing work on building modeling from satellite DSM achieves LoD-1 generation. In this paper, we propose a model-driven method that reconstructs LoD-2 building models following a “decomposition-optimization-fitting” paradigm. The proposed method starts building detection results through a deep learning-based detector and vectorizes individual segments into polygons using a “three-step” polygon extraction method, followed by a novel grid-based decomposition method that decomposes the complex and irregularly shaped building polygons to tightly combined elementary building rectangles ready to fit elementary building models. We have optionally introduced OpenStreetMap (OSM) and Graph-Cut (GC) labeling to further refine the orientation of 2D building rectangle. The 3D modeling step takes building-specific parameters such as hip lines, as well as non-rigid and regularized transformations to optimize the flexibility for using a minimal set of elementary models. Finally, roof type of building models s refined and adjacent building models in one building segment are merged into the complex polygonal model. Our proposed method has addressed a few technical caveats over existing methods, resulting in practically high-quality results, based on our evaluation and comparative study on a diverse set of experimental datasets of cities with different urban patterns. (codes /binaries may be available under this GitHub page: https://github.com/GDAOSU/LOD2BuildingModel).},
	language = {en},
	urldate = {2021-12-16},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Gui, Shengxi and Qin, Rongjun},
	month = nov,
	year = {2021},
	keywords = {Data-driven, Decomposition and merging, LoD-2 Building Modeling, Multi-stereo satellite images},
	pages = {1--19},
}

@article{qin_critical_2019,
	title = {A critical analysis of satellite stereo pairs for digital surface model generation and a matching quality prediction model},
	volume = {154},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271619301467},
	doi = {10.1016/j.isprsjprs.2019.06.005},
	abstract = {The geometric analysis and data acquisition of satellite photogrammetric images are often regarded as a direct extension of traditional aerial photogrammetry, with the only difference being the sensor model (linear array vs. central perspective). The intersection angle (or base-height ratio) between two images is seen as the most important metadata of stereo pairs, which directly relates to the base-high ratio and texture distortion in the parallax direction, thus both affecting the horizontal and vertical accuracy. State-of-the-art DIM algorithms were reported to work best for narrow baseline stereos (small intersection angle), e.g. Semi-Global Matching empirically takes 15–25° as “good” intersection angles. However, our experiments found that the intersection angle is not the only determining factor, as the same DIM algorithm applied to stereo pairs of the same area with similar and good intersection angle may produce point clouds with dramatically different accuracy (demonstrated in the graphical abstract). This raises a very practical and often asked question: what factors constitute a good satellite stereo pair for DIM algorithms? In this paper, we provide a comprehensive analysis on this matter by performing stereo matching using the very typical and widely-used Semi-Global Matching (SGM) with a Census cost over 1000 satellite stereo pairs of the same region with different meta-parameters including their intersection, off-nadir, sun elevation \& azimuth angles, completeness and time differences, thus to offer a thorough answer to this question. Our conclusion has specifically outlined an important yet often ignored factor – the Sun-angle difference to be one decisive in determining good stereo pair. Based on the analytical results, we propose a simple idea by training a support vector machine model for predicting potential stereo matching quality (i.e. potential level of accuracy and completeness given a stereo pair). Experiments have shown that the model is well-suited and generalized for multi-stereo 3D reconstruction, evidenced by a comparative analysis against three other strategies: (1) pair selection based on an example patch where partial ground-truth data is available for computing a priori ranking (2) based on intersection angles and (3) based on a recent algorithm using intersection angle, off-nadir angle and time intervals. This work will potentially provide a valuable reference to researchers working on multi-view satellite image reconstruction, as well as for practitioners minimizing costs for high-quality large-scale mapping. The trained model is made available to the academic community upon request.},
	language = {en},
	urldate = {2021-12-16},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Qin, Rongjun},
	month = aug,
	year = {2019},
	keywords = {3D reconstruction, Dense image matching, Digital surface model, Matching quality indicator, Rational polynomial functions, Satellite photogrammetry},
	pages = {139--150},
}

@inproceedings{james_uav_2021,
	title = {{UAV} {Multispectral} {Optical} {Contribution} to {Coastal} {3D} {Modelling}},
	doi = {10.1109/IGARSS47720.2021.9553865},
	abstract = {Due to global changes, UAV flights are of great interest for the surveillance of coastal environments. Some UAVs are fitted with multispectral sensors to better discriminate coastal eco-geo-systems such as saltmarsh meadows and sandy dunes. This research evaluates photogrammetry-based horizontal (XY) and vertical (Z) accuracies derived from four separate channels (Green, G, Red, R, Red-Edge, RE, and Near-Infrared, NIR). The best X and Y accuracy is achieved by the NIR channel (0.07 m and 0.09 m, respectively). The best Z vertical accuracy (0.13 m) is reached with the full set of channels (R-G-RE-NIR). The individualized investigations of the saltmarsh vegetation and the sand dune show that the best vertical accuracies are attained with the NIR and RE with 0.10 m and 0.11 m, respectively. Our findings witness that the visible spectrum, represented by the G and R bands, provide lower performance than the infrared gamut.},
	booktitle = {2021 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium} {IGARSS}},
	author = {James, Dorothée and Collin, Antoine and Mury, Antoine and Letard, Mathilde and Guillot, Benoit},
	month = jul,
	year = {2021},
	keywords = {Green products, Optical sensors, Sea measurements, Surveillance, Three-dimensional displays, UAV, Vegetation mapping, coastal zone, infrared},
	pages = {7951--7954},
}

@misc{noauthor_accelerated_nodate,
	title = {An accelerated image matching technique for {UAV} orthoimage registration - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271617302277},
	urldate = {2021-12-17},
}

@article{sedaghat_high-resolution_2019,
	title = {High-resolution image registration based on improved {SURF} detector and localized {GTM}},
	volume = {40},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431161.2018.1528402},
	doi = {10.1080/01431161.2018.1528402},
	abstract = {High-resolution image registration is an important task in remote sensing image processing. In this paper, an automatic and robust local feature-based image registration approach is proposed for high-resolution remote sensing images. The proposed method consists of four main steps. In the first step, an integrated local feature-based matching method based on an improved speeded-up robust features (SURF) detector and an adaptive binning scale-invariant feature transform (AB-SIFT) descriptor is developed for fast, dense and robust tie-point extraction. In the second step, a localized graph transformation matching (LGTM) method is developed for reliable mismatch elimination. In the third step, an advanced oriented least square matching (OLSM) method is applied to improve the positional accuracy of the refined tie-points. Finally, the input image is warped using an appropriate transformation model. To investigate the impact of the transformation function, the capability of some models, including, polynomials of degrees 2 to 4, piecewise linear (PL), weighted mean (WM) and multiquadric (MQ) are compared. The proposed method has been evaluated with five pairs of high-resolution remote sensing images from IRS-P5, SPOT 5, SPOT 6, IKONOS, Geoeye, Quickbird, and Worldview sensors, and the registration results demonstrate its robustness and capability. The MATLAB code of the proposed method can be downloaded from https://www.researchgate.net/publication/320354469\_HRImReg.},
	number = {7},
	urldate = {2021-12-17},
	journal = {International Journal of Remote Sensing},
	author = {Sedaghat, Amin and Mohammadi, Nazila},
	month = apr,
	year = {2019},
	pages = {2576--2601},
}

@article{oyallon_analysis_2015,
	title = {An analysis of the {SURF} method},
	volume = {5},
	journal = {Image Processing On Line},
	author = {Oyallon, Edouard and Rabin, Julien},
	year = {2015},
	pages = {176--218},
}

@article{saleem_robust_2014,
	title = {A robust {SIFT} descriptor for multispectral images},
	volume = {21},
	number = {4},
	journal = {IEEE signal processing letters},
	author = {Saleem, Sajid and Sablatnig, Robert},
	year = {2014},
	pages = {400--403},
}

@article{matese_assessment_2017,
	title = {Assessment of a canopy height model ({CHM}) in a vineyard using {UAV}-based multispectral imaging},
	volume = {38},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431161.2016.1226002},
	doi = {10.1080/01431161.2016.1226002},
	abstract = {Biomass is one of the most important parameters in order for the farmer to choose the best canopy management within the field and it can be estimated using plant canopy height. In combination with a non-vegetation ground model, plant height can be obtained by quantifying the height of a canopy using crop surface models (CSMs). A modified Mikrokopter Okto unmanned aerial vehicle (UAV) acquired high-resolution multispectral images (4 cm) and a processing chain was developed to construct a 3D digital surface model (DSM) for the creation of precise digital terrain models (DTMs) based on Structure from Motion (SfM) computer vision algorithms. The DTM was then subtracted from the DSM to obtain a canopy height model (CHM) of a vineyard. The results show a good separation of ground pixels from vine rows, but their elevations were not quite in accordance with the actual height of the vines due to a smoothing effect of the reconstructed CHM. A further comparison between CHM and a vigour map obtained from normalized difference vegetation index (NDVI) values showed a good correlation. A preliminary assessment of biomass volume was made using the average canopy height and vine row width for three different homogeneous classes. This is a preliminary study on how a 3D model developed by UAV images can be useful for a simple and prompt biomass evaluation.},
	number = {8-10},
	urldate = {2021-12-17},
	journal = {International Journal of Remote Sensing},
	author = {Matese, Alessandro and Di Gennaro, Salvatore Filippo and Berton, Andrea},
	month = may,
	year = {2017},
	pages = {2150--2160},
}

@inproceedings{qian_pu-gcn_2021,
	title = {Pu-gcn: {Point} cloud upsampling using graph convolutional networks},
	shorttitle = {Pu-gcn},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Qian, Guocheng and Abualshour, Abdulellah and Li, Guohao and Thabet, Ali and Ghanem, Bernard},
	year = {2021},
	pages = {11683--11692},
}

@inproceedings{zhou_dup-net_2019,
	title = {Dup-net: {Denoiser} and upsampler network for 3d adversarial point clouds defense},
	shorttitle = {Dup-net},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Zhou, Hang and Chen, Kejiang and Zhang, Weiming and Fang, Han and Zhou, Wenbo and Yu, Nenghai},
	year = {2019},
	pages = {1961--1970},
}

@article{angin_agrilora_2020,
	title = {{AgriLoRa}: {A} {Digital} {Twin} {Framework} for {Smart} {Agriculture}},
	copyright = {Attribution-NonCommercial-NoDerivatives 4.0 International},
	shorttitle = {{AgriLoRa}},
	url = {https://open.metu.edu.tr/handle/11511/93896},
	language = {tr},
	urldate = {2021-11-19},
	author = {Angın, Pelin and Anisi, Mohammad Hossein and Goksel, Furkan and Gursoy, Ceren and Buyukgulcu, Asaf},
	month = dec,
	year = {2020},
}

@article{zhou_assessment_2021,
	title = {Assessment for crop water stress with infrared thermal imagery in precision agriculture: {A} review and future prospects for deep learning applications},
	volume = {182},
	issn = {0168-1699},
	shorttitle = {Assessment for crop water stress with infrared thermal imagery in precision agriculture},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169921000375},
	doi = {10.1016/j.compag.2021.106019},
	abstract = {With the increasing global water scarcity, efficient assessment methods for crop water stress have become a prerequisite to perform precision irrigation scheduling. The 1accessibility of infrared thermal sensor provides a powerful tool to detect and quantify crop water stress. This paper reviews the current practices of infrared thermal imagery utilized to assess crop water stress. Overall, three technological aspects of infrared thermal sensing applications for crop water stress assessment are reviewed along with the challenges and recommendations: (i) introduction of uncooled thermal camera and platforms, including ground-based platform and unmanned aerial vehicles (UAVs) platforms, for thermal imaging acquisition, (ii) strategies of canopy segmentation in thermal imaging used to obtain average canopy temperature for CWSI calculation, (iii) correlation between three forms of crop water stress index (CWSI) i.e. theoretical CWSI (CWSIt), empirical CWSI (CWSIe), and statistic CWSI (CWSIs) and physiological indicators. The emphasis is on imaging process techniques for canopy segmentation in thermal imaging. As a future perspective, the potential use of deep learning approaches to assess crop water stress has been elaborated highlighting the future trends.},
	language = {en},
	urldate = {2021-11-19},
	journal = {Computers and Electronics in Agriculture},
	author = {Zhou, Zheng and Majeed, Yaqoob and Diverres Naranjo, Geraldine and Gambacorta, Elena M. T.},
	month = mar,
	year = {2021},
	keywords = {CWSI, Canopy segmentation, Canopy temperature, Deep learning},
	pages = {106019},
}

@article{qiu_detection_2021,
	title = {Detection of the {3D} temperature characteristics of maize under water stress using thermal and {RGB}-{D} cameras},
	volume = {191},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169921005688},
	doi = {10.1016/j.compag.2021.106551},
	abstract = {Global warming and water resource shortage greatly influence the crop growth and negatively affect the crop yield. Breeding water–stress resistant crop varieties is one of the effective ways to handle this problem. The surface temperatures of the crop are essential for assessing their water stress resistance. Thermal imaging has been widely used to acquire the crop surface temperatures. However, most studies focus on 2D measurement. A 3D thermal imaging system is designed, and a method is developed by using the thermal and RGB-D data to acquire 3D thermal information of maize under water stress conditions. First, thermal and RGB-D cameras were used to collect the thermal and color images, and depth data of maize at the jointing stage and the thermal and color images were processed to extract the edge images of maize. Second, the KAZE feature was selected to register the edge images. Testing results showed that the KAZE feature has better performance than the SURF and BRISK features in the thermal and color image registration of maize. On the basis of the thermal and color image registration, the depth data of maize were assigned with temperature values. Third, the depth data of maize were further processed with denoising, maize extraction, amplification, smoothing, and temperature correction steps to improve the data qualities. Finally, the crop water stress index and canopy–air temperature difference values of each point were calculated. The results demonstrated that the system and the proposed method can effectively detect the water stress characteristics of maize in 3D, which can be combined with the morphological traits of leaves to synthetically analyze the water stress resistance of the crop.},
	language = {en},
	urldate = {2021-11-19},
	journal = {Computers and Electronics in Agriculture},
	author = {Qiu, Ruicheng and Miao, Yanlong and Zhang, Man and Li, Han},
	month = dec,
	year = {2021},
	keywords = {3D distribution, Image registration, Maize, Temperature detection, Water stress},
	pages = {106551},
}

@article{xu_development_2021,
	title = {Development and {Testing} of a {UAV}-{Based} {Multi}-{Sensor} {System} for {Plant} {Phenotyping} and {Precision} {Agriculture}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/13/17/3517},
	doi = {10.3390/rs13173517},
	abstract = {Unmanned aerial vehicles have been used widely in plant phenotyping and precision agriculture. Several critical challenges remain, however, such as the lack of cross-platform data acquisition software system, sensor calibration protocols, and data processing methods. This paper developed an unmanned aerial system that integrates three cameras (RGB, multispectral, and thermal) and a LiDAR sensor. Data acquisition software supporting data recording and visualization was implemented to run on the Robot Operating System. The design of the multi-sensor unmanned aerial system was open sourced. A data processing pipeline was proposed to preprocess the raw data and to extract phenotypic traits at the plot level, including morphological traits (canopy height, canopy cover, and canopy volume), canopy vegetation index, and canopy temperature. Protocols for both field and laboratory calibrations were developed for the RGB, multispectral, and thermal cameras. The system was validated using ground data collected in a cotton field. Temperatures derived from thermal images had a mean absolute error of 1.02 °C, and canopy NDVI had a mean relative error of 6.6\% compared to ground measurements. The observed error for maximum canopy height was 0.1 m. The results show that the system can be useful for plant breeding and precision crop management.},
	language = {en},
	number = {17},
	urldate = {2021-11-19},
	journal = {Remote Sensing},
	author = {Xu, Rui and Li, Changying and Bernardes, Sergio},
	month = jan,
	year = {2021},
	keywords = {UAV, multispectral imaging, phenotyping, thermal imaging},
	pages = {3517},
}

@article{ghamisi_multisource_2019,
	title = {Multisource and {Multitemporal} {Data} {Fusion} in {Remote} {Sensing}: {A} {Comprehensive} {Review} of the {State} of the {Art}},
	volume = {7},
	issn = {2168-6831},
	shorttitle = {Multisource and {Multitemporal} {Data} {Fusion} in {Remote} {Sensing}},
	doi = {10.1109/MGRS.2018.2890023},
	abstract = {This article brings together the advances of multisource and multitemporal data fusion approaches with respect to the various research communities and provides a thorough and discipline-specific starting point for researchers at different levels (i.e., students, researchers, and senior researchers) willing to conduct novel investigations on this challenging topic by supplying sufficient detail and references. More specifically, this work provides a bird's-eye view of many important contributions specifically dedicated to the topics of pansharpening and resolution enhancement, point cloud data fusion, hyperspectral and lidar data fusion, multitemporal data fusion, and big data and social media. In addition, the main challenges and possible future research in each area are outlined and discussed.},
	number = {1},
	journal = {IEEE Geoscience and Remote Sensing Magazine},
	author = {Ghamisi, Pedram and Rasti, Behnood and Yokoya, Naoto and Wang, Qunming and Hofle, Bernhard and Bruzzone, Lorenzo and Bovolo, Francesca and Chi, Mingmin and Anders, Katharina and Gloaguen, Richard and Atkinson, Peter M. and Benediktsson, Jon Atli},
	month = mar,
	year = {2019},
	keywords = {Atmospheric modeling, Data fusion, Data integration, Data models, Optical variables measurement, Remote sensing, Sensors, Spatial resolution},
	pages = {6--39},
}

@article{chaux_digital_2021,
	title = {A {Digital} {Twin} {Architecture} to {Optimize} {Productivity} within {Controlled} {Environment} {Agriculture}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2076-3417/11/19/8875},
	doi = {10.3390/app11198875},
	abstract = {To ensure food security, agricultural production systems should innovate in the direction of increasing production while reducing utilized resources. Due to the higher level of automation with respect to traditional agricultural systems, Controlled Environment Agriculture (CEA) applications generally achieve better yields and quality crops at the expenses of higher energy consumption. In this context, Digital Twin (DT) may constitute a fundamental tool to reach the optimization of the productivity, intended as the ratio between production and resource consumption. For this reason, a DT Architecture for CEA systems is introduced within this work and applied to a case study for its validation. The proposed architecture is potentially able to optimize productivity since it utilizes simulation software that enables the optimization of: (i) Climate control strategies related to the control of the crop microclimate; (ii) treatments related to crop management. Due to the importance of food security in the worldwide landscape, the authors hope that this work may impulse the investigation of strategies for improving the productivity of CEA systems.},
	language = {en},
	number = {19},
	urldate = {2021-11-19},
	journal = {Applied Sciences},
	author = {Chaux, Jesus David and Sanchez-Londono, David and Barbieri, Giacomo},
	month = jan,
	year = {2021},
	keywords = {architecture, controlled environment agriculture, digital twin, optimization, productivity},
	pages = {8875},
}

@article{zhang_fusion_2018,
	series = {{ISPRS} {Journal} of {Photogrammetry} and {Remote} {Sensing} {Theme} {Issue} “{Point} {Cloud} {Processing}”},
	title = {Fusion of images and point clouds for the semantic segmentation of large-scale {3D} scenes based on deep learning},
	volume = {143},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271618301291},
	doi = {10.1016/j.isprsjprs.2018.04.022},
	abstract = {We address the issue of the semantic segmentation of large-scale 3D scenes by fusing 2D images and 3D point clouds. First, a Deeplab-Vgg16 based Large-Scale and High-Resolution model (DVLSHR) based on deep Visual Geometry Group (VGG16) is successfully created and fine-tuned by training seven deep convolutional neural networks with four benchmark datasets. On the val set in CityScapes, DVLSHR achieves a 74.98\% mean Pixel Accuracy (mPA) and a 64.17\% mean Intersection over Union (mIoU), and can be adapted to segment the captured images (image resolution 2832 ∗ 4256 pixels). Second, the preliminary segmentation results with 2D images are mapped to 3D point clouds according to the coordinate relationships between the images and the point clouds. Third, based on the mapping results, fine features of buildings are further extracted directly from the 3D point clouds. Our experiments show that the proposed fusion method can segment local and global features efficiently and effectively.},
	language = {en},
	urldate = {2021-11-04},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Zhang, Rui and Li, Guangyun and Li, Minglei and Wang, Li},
	month = sep,
	year = {2018},
	keywords = {2D image, 3D point cloud, 3D scene segmentation, High-resolution, Large-scale},
	pages = {85--96},
}

@article{kotaridis_remote_2021,
	title = {Remote sensing image segmentation advances: {A} meta-analysis},
	volume = {173},
	issn = {0924-2716},
	shorttitle = {Remote sensing image segmentation advances},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271621000265},
	doi = {10.1016/j.isprsjprs.2021.01.020},
	abstract = {The advances in remote sensing sensors during the last two decades have led to the production of very high spatial resolution multispectral images. In order to adapt to this rapid development and handle these data, object-based analysis has emerged. A critical part of such an analysis is image segmentation. The selection of optimal segmentation parameters' values generates a qualitative segmentation output and has a direct impact on feature extraction and subsequent overall classification accuracy. Even though several image segmentation methods have been developed and suggested in the literature, each of them has advantages and disadvantages. This article presents the conceptual characteristics of image segmentation methods with a special focus on semantic segmentation. In addition, a meta-analysis was conducted through a comprehensive review of recent image segmentation case studies. It includes statistics and quantitative data regarding the applied segmentation algorithm, the software utilized and the data source among others. Since there is no miraculous segmentation algorithm, the statistical results depict only the recent trend. Finally, a few interesting subjects are addressed, including identification of current problems, image segmentation on non-traditional data and hot topics for future research.},
	language = {en},
	urldate = {2021-11-04},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Kotaridis, Ioannis and Lazaridou, Maria},
	month = mar,
	year = {2021},
	keywords = {Image segmentation, Meta-analysis, Remote sensing, Review, Semantic segmentation},
	pages = {309--322},
}

@misc{agency_spatial_2021,
	title = {Spatial - {Resolutions} - {Sentinel}-2 {MSI} - {User} {Guides} - {Sentinel} {Online} - {Sentinel} {Online}},
	url = {https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/resolutions/spatial},
	urldate = {2021-10-18},
	author = {Agency, The European Space},
	year = {2021},
}

@inproceedings{bennis_contours_2013,
	title = {Contours based approach for thermal image and terrestrial point cloud registration},
	volume = {XL-5-W2},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-5-W2/97/2013/},
	doi = {10.5194/isprsarchives-XL-5-W2-97-2013},
	abstract = {{\textbackslash}textlessp{\textbackslash}textgreater{\textbackslash}textlessstrong class="journal-contentHeaderColor"{\textbackslash}textgreaterAbstract.{\textbackslash}textless/strong{\textbackslash}textgreater Building energetic performances strongly depend on the thermal insulation. However the performance of the insulation materials tends to decrease over time which necessitates the continuous monitoring of the building in order to detect and repair the anomalous zones. In this paper, it is proposed to couple 2D infrared images representing the surface temperature of the building with 3D point clouds acquired with Terrestrial Laser Scanner (TLS) resulting in a semi-automatic approach allowing the texturation of TLS data with infrared image of buildings. A contour-based algorithm is proposed whose main features are : 1) the extraction of high level primitive is not required 2) the use of projective transform allows to handle perspective effects 3) a point matching refinement procedure allows to cope with approximate control point selection. The procedure is applied to test modules aiming at investigating the thermal properties of material.{\textbackslash}textless/p{\textbackslash}textgreater},
	language = {English},
	urldate = {2021-10-18},
	booktitle = {The {International} {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Bennis, A. and Bombardier, V. and Thiriet, P. and Brie, D.},
	month = jul,
	year = {2013},
	pages = {97--101},
}

@misc{noauthor_uav-based_nodate,
	title = {{UAV}-based multispectral remote sensing for precision agriculture: {A} comparison between different cameras - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/abs/pii/S0924271618302533},
	urldate = {2021-10-18},
}

@incollection{ose_2_2016,
	title = {2 - {Multispectral} {Satellite} {Image} {Processing}},
	isbn = {978-1-78548-102-4},
	url = {https://www.sciencedirect.com/science/article/pii/B9781785481024500028},
	abstract = {Having begun in the 1970s with the American Landsat program, the use of optical satellite images for civilian purposes has since experienced significant technological advancements. In the space of 10 years, the images provided by Earth observation satellites have gone from high to very high spatial resolution, from a decametric resolution to a submetric resolution. In addition, the new generation of satellites known as “agile” allow for the reduction of acquisition time above the same area, while providing a synoptic and multispectral view of the territory. Long reserved for scientific use, these data conquer new fields of application and are reaching a much wider audience, competing sometimes with aerial photography.},
	language = {en},
	urldate = {2021-10-18},
	booktitle = {Optical {Remote} {Sensing} of {Land} {Surface}},
	publisher = {Elsevier},
	author = {Ose, Kenji and Corpetti, Thomas and Demagistri, Laurent},
	editor = {Baghdadi, Nicolas and Zribi, Mehrez},
	month = jan,
	year = {2016},
	doi = {10.1016/B978-1-78548-102-4.50002-8},
	keywords = {Additive synthesis, Clustering, Color composite images, Geometric preprocessing, Image files storage, Monochromatic display, Multispectral Satellite Image Processing, Radiometric preprocessing, Regression, Satellite multispectral image},
	pages = {57--124},
}

@article{dua_comprehensive_2020,
	title = {Comprehensive review of hyperspectral image compression algorithms},
	volume = {59},
	issn = {0091-3286, 1560-2303},
	url = {https://www.spiedigitallibrary.org/journals/optical-engineering/volume-59/issue-9/090902/Comprehensive-review-of-hyperspectral-image-compression-algorithms/10.1117/1.OE.59.9.090902.full},
	doi = {10.1117/1.OE.59.9.090902},
	abstract = {Rapid advancement in the development of hyperspectral image analysis techniques has led to specialized hyperspectral missions. It results in the bulk transmission of hyperspectral images from sensors to analysis centers and finally to data centers. Storage of these large size images is a critical issue that is handled by compression techniques. This survey focuses on different hyperspectral image compression algorithms that have been classified into two broad categories based on eight internal and six external parameters. In addition, we identified research challenges and suggested future scope for each technique. The detailed classification used in this paper can categorize other compression algorithms and may help in selecting research objectives.},
	number = {9},
	urldate = {2021-10-16},
	journal = {Optical Engineering},
	author = {Dua, Yaman and Kumar, Vinod and Singh, Ravi Shankar},
	month = sep,
	year = {2020},
	pages = {090902},
}

@misc{noauthor_agisoft_2021,
	title = {Agisoft {Metashape}},
	url = {https://www.agisoft.com/},
	urldate = {2021-10-16},
	year = {2021},
}

@article{legros_expectation-maximization_2020,
	title = {Expectation-{Maximization} {Based} {Approach} to {3D} {Reconstruction} {From} {Single}-{Waveform} {Multispectral} {Lidar} {Data}},
	volume = {6},
	issn = {2333-9403},
	doi = {10.1109/TCI.2020.2997305},
	abstract = {In this article, we present a novel Bayesian approach for estimating spectral and range profiles from single-photon Lidar waveforms associated with single surfaces in the photon-limited regime. In contrast to classical multispectral Lidar signals, we consider a single Lidar waveform per pixel, whereby a single detector is used to acquire information simultaneously at multiple wavelengths. A new observation model based on a mixture of distributions is developed. It relates the unknown parameters of interest to the observed waveforms containing information from multiple wavelengths. Adopting a Bayesian approach, several prior models are investigated and a stochastic Expectation-Maximization algorithm is proposed to estimate the spectral and depth profiles. The reconstruction performance and computational complexity of our approach are assessed, for different prior models, through a series of experiments using synthetic and real data under different observation scenarios. The results obtained demonstrate a significant speed-up (up to 100 times faster for four bands) without significant degradation of the reconstruction performance when compared to existing methods in the photon-starved regime.},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Legros, Quentin and Meignen, Sylvain and McLaughlin, Stephen and Altmann, Yoann},
	year = {2020},
	keywords = {3D imaging, Bayes methods, Bayesian estimation, Computational modeling, Estimation, Expectation-Maximization, Imaging, Laser radar, Multispectral imaging, Photonics, Three-dimensional displays, single-photon Lidar},
	pages = {1033--1043},
}

@article{mathews_visualizing_2013,
	title = {Visualizing and {Quantifying} {Vineyard} {Canopy} {LAI} {Using} an {Unmanned} {Aerial} {Vehicle} ({UAV}) {Collected} {High} {Density} {Structure} from {Motion} {Point} {Cloud}},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/5/5/2164},
	doi = {10.3390/rs5052164},
	abstract = {This study explores the use of structure from motion (SfM), a computer vision technique, to model vine canopy structure at a study vineyard in the Texas Hill Country. Using an unmanned aerial vehicle (UAV) and a digital camera, 201 aerial images (nadir and oblique) were collected and used to create a SfM point cloud. All points were classified as ground or non-ground points. Non-ground points, presumably representing vegetation and other above ground objects, were used to create visualizations of the study vineyard blocks. Further, the relationship between non-ground points in close proximity to 67 sample vines and collected leaf area index (LAI) measurements for those same vines was also explored. Points near sampled vines were extracted from which several metrics were calculated and input into a stepwise regression model to attempt to predict LAI. This analysis resulted in a moderate R2 value of 0.567, accounting for 57 percent of the variation of LAISQRT using six predictor variables. These results provide further justification for SfM datasets to provide three-dimensional datasets necessary for vegetation structure visualization and biophysical modeling over areas of smaller extent. Additionally, SfM datasets can provide an increased temporal resolution compared to traditional three-dimensional datasets like those captured by light detection and ranging (lidar).},
	language = {en},
	number = {5},
	urldate = {2021-10-09},
	journal = {Remote Sensing},
	author = {Mathews, Adam J. and Jensen, Jennifer L. R.},
	month = may,
	year = {2013},
	keywords = {LAI, SfM, UAV, bundle adjustment, point cloud, structure from motion, vegetation, vineyard},
	pages = {2164--2183},
}

@article{zhang_three-dimensional_2020,
	title = {Three-dimensional convolutional neural network model for tree species classification using airborne hyperspectral images},
	volume = {247},
	issn = {0034-4257},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425720303084},
	doi = {10.1016/j.rse.2020.111938},
	abstract = {Airborne hyperspectral remote sensing data with both rich spectral and spatial features can effectively improve the classification accuracy of vegetation species. However, the spectral data of hundreds of bands brings about problems such as dimensional explosion, which poses a huge challenge for hyperspectral remote sensing classification based on classical parameters models. Deep learning methods have been used for remotely sensed images classification in recent years, but the popular HSI datasets including Kennedy Space Center, Indian Pines, Pavia University scene and Salinas scene, have low spatial resolution, significant differences between categories, and regular boundaries. When applied to the classification of forestry tree species, the accuracy often decreases because the spectral response of different plants of the same family and genus are very similar, especially under the fragmented species distribution, complex topography and the occluded canopy. So we collect new data sets, selected Gaofeng State Owned Forest Farm in Guangxi province in south China as the research area and adopted the airborne hyperspectral data obtained by the LiCHy system of the Chinese Academy of Forestry to explore an improved three-dimensional convolutional neural network(3D-CNN) model for tree species classification. The proposed model uses raw data as input without dimension reduction or feature screening, and simultaneously extracts spectral and spatial features. After the 3D convolutional layer, the captured high-level semantic concept is a joint spatial spectral feature representation, so we can turn it into a one-dimensional feature as a new input to learn a more abstract level of expression. The widely used earlystop method is also used to prevent overfitting. The proposed model is a lightweight, generalized, and fast convergence classification model, by which the short-time and large-area of multiple tree species classification with high-precision can be realized. The result shows that the 3D-1D CNN model can shorten the training time of the 3D CNN model by 60\% and achieve a classification accuracy of 93.14\% within 50 ha in 6.37 min, which provides a basis for the classification of tree species, the mapping of forest form and the inventory of forest resources.},
	language = {en},
	urldate = {2021-10-08},
	journal = {Remote Sensing of Environment},
	author = {Zhang, Bin and Zhao, Lin and Zhang, Xiaoli},
	month = sep,
	year = {2020},
	keywords = {3D-1D-CNN, 3D-CNN, Airborne hyperspectral image, Classification, Tree species},
	pages = {111938},
}

@article{mayra_tree_2021,
	title = {Tree species classification from airborne hyperspectral and {LiDAR} data using {3D} convolutional neural networks},
	volume = {256},
	issn = {0034-4257},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425721000407},
	doi = {10.1016/j.rse.2021.112322},
	abstract = {During the last two decades, forest monitoring and inventory systems have moved from field surveys to remote sensing-based methods. These methods tend to focus on economically significant components of forests, thus leaving out many factors vital for forest biodiversity, such as the occurrence of species with low economical but high ecological values. Airborne hyperspectral imagery has shown significant potential for tree species classification, but the most common analysis methods, such as random forest and support vector machines, require manual feature engineering in order to utilize both spatial and spectral features, whereas deep learning methods are able to extract these features from the raw data. Our research focused on the classification of the major tree species Scots pine, Norway spruce and birch, together with an ecologically valuable keystone species, European aspen, which has a sparse and scattered occurrence in boreal forests. We compared the performance of three-dimensional convolutional neural networks (3D-CNNs) with the support vector machine, random forest, gradient boosting machine and artificial neural network in individual tree species classification from hyperspectral data with high spatial and spectral resolution. We collected hyperspectral and LiDAR data along with extensive ground reference data measurements of tree species from the 83 km2 study area located in the southern boreal zone in Finland. A LiDAR-derived canopy height model was used to match ground reference data to aerial imagery. The best performing 3D-CNN, utilizing 4 m image patches, was able to achieve an F1-score of 0.91 for aspen, an overall F1-score of 0.86 and an overall accuracy of 87\%, while the lowest performing 3D-CNN utilizing 10 m image patches achieved an F1-score of 0.83 and an accuracy of 85\%. In comparison, the support-vector machine achieved an F1-score of 0.82 and an accuracy of 82.4\% and the artificial neural network achieved an F1-score of 0.82 and an accuracy of 81.7\%. Compared to the reference models, 3D-CNNs were more efficient in distinguishing coniferous species from each other, with a concurrent high accuracy for aspen classification. Deep neural networks, being black box models, hide the information about how they reach their decision. We used both occlusion and saliency maps to interpret our models. Finally, we used the best performing 3D-CNN to produce a wall-to-wall tree species map for the full study area that can later be used as a reference prediction in, for instance, tree species mapping from multispectral satellite images. The improved tree species classification demonstrated by our study can benefit both sustainable forestry and biodiversity conservation.},
	language = {en},
	urldate = {2021-10-08},
	journal = {Remote Sensing of Environment},
	author = {Mäyrä, Janne and Keski-Saari, Sarita and Kivinen, Sonja and Tanhuanpää, Topi and Hurskainen, Pekka and Kullberg, Peter and Poikolainen, Laura and Viinikka, Arto and Tuominen, Sakari and Kumpula, Timo and Vihervaara, Petteri},
	month = apr,
	year = {2021},
	keywords = {Convolutional neural network, Deep learning, Hyperspectral imaging, Tree species classification},
	pages = {112322},
}

@article{regaieg_assessing_2021,
	title = {Assessing impacts of canopy {3D} structure on chlorophyll fluorescence radiance and radiative budget of deciduous forest stands using {DART}},
	volume = {265},
	issn = {0034-4257},
	url = {https://www.sciencedirect.com/science/article/pii/S003442572100393X},
	doi = {10.1016/j.rse.2021.112673},
	abstract = {Although remote sensing (RS) of solar-induced chlorophyll fluorescence (SIF) is increasingly used as a valuable source of information about vegetation photosynthetic activity, the RS SIF observations are significantly influenced by canopy-specific structural features (i.e., canopy architecture including leaf area index and presence of woody components), atmospheric conditions during their acquisition (e.g., proportion of direct and diffuse irradiance) and observational geometric configurations (e.g., sun and viewing directions). Radiative transfer (RT) models have the potential to provide a better understanding of the canopy structural effects on the SIF emission and RS signals. Here, we used the DART model to assess the daily influence, from morning to evening, of forest 3D architecture on SIF nadir radiance, emission, escape factor and nadir yield of eight 100 m × 100 m forest study plots established in a temperate deciduous forest of the Smithsonian Environmental Research Center (Edgewater, MD, USA). The 3D architecture of each plot was derived from airborne LiDAR. DART simulations of these 3D forest plots and their 1D (i.e., vertical profile of sun-adapted and shade-adapted leaves) and 0D (i.e., homogeneous layer of sun-adapted leaves above an homogeneous layer of shade-adapted leaves) abstractions were compared to assess the relative errors (ε1D−3D and ε0D−3D) associated with horizontal and vertical structural heterogeneity, respectively. Forest 3D structure, especially horizontal heterogeneity, had a great influence on forest nadir SIF radiance, resulting in ε1D−3D up to 55\% at 8:00 and 18:00 (i.e., for oblique sun directions). The key indicators of this impact, in the descending order of importance, were the SIF escape factor (ε1D−3D up to 40\%), the attenuation of incident photosynthetically active radiation (ε1D−3D less than 5\%), and the SIF emission yield (ε1D−3D less than 2\%). The influence of forest architecture on the nadir SIF escape factor and SIF yield (ε1D−3D up to 40\%) varied over time, with differences in forest stand structure, and per spectral domain, being always larger between 640 and 700 nm than between 700 and 850 nm. In addition, woody elements demonstrated a large influence on forest SIF radiance due to their “shading” effect (ε up to 17\%) and their “blocking” effect (ε ≈ 10\%), both of them higher for far-red than for red SIF. These results underline the importance of 3D forest canopy architecture, especially 2D heterogeneity, and inclusion of woody elements in RT modeling used for interpretation of the RS SIF signal, and subsequently for the estimation of gross primary production and detection of vegetation stress.},
	language = {en},
	urldate = {2021-10-08},
	journal = {Remote Sensing of Environment},
	author = {Regaieg, Omar and Yin, Tiangang and Malenovský, Zbyněk and Cook, Bruce D. and Morton, Douglas C. and Gastellu-Etchegorry, Jean-Philippe},
	month = nov,
	year = {2021},
	keywords = {3D forest architecture, Escape factor, Photosynthetic active radiation, Radiative budget, Radiative transfer modeling, SIF, Wood},
	pages = {112673},
}

@article{jayakumari_object-level_2021,
	title = {Object-level classification of vegetable crops in {3D} {LiDAR} point cloud using deep learning convolutional neural networks},
	volume = {22},
	issn = {1573-1618},
	url = {https://doi.org/10.1007/s11119-021-09803-0},
	doi = {10.1007/s11119-021-09803-0},
	abstract = {Crop discrimination at the plant or patch level is vital for modern technology-enabled agriculture. Multispectral and hyperspectral remote sensing data have been widely used for crop classification. Even though spectral data are successful in classifying row-crops and orchards, they are limited in discriminating vegetable and cereal crops at plant or patch level. Terrestrial laser scanning is a potential remote sensing approach that offers distinct structural features useful for classification of crops at plant or patch level. The objective of this research is the improvement and application of an advanced deep learning framework for object-based classification of three vegetable crops: cabbage, tomato, and eggplant using high-resolution LiDAR point cloud. Point clouds from a terrestrial laser scanner (TLS) were acquired over experimental plots of the University of Agricultural Sciences, Bengaluru, India. As part of the methodology, a deep convolution neural network (CNN) model named CropPointNet is devised for the semantic segmentation of crops from a 3D perspective. The CropPointNet is an adaptation of the PointNet deep CNN model developed for the segmentation of indoor objects in a typical computer vision scenario. Apart from adapting to 3D point cloud segmentation of crops, the significant methodological improvements made in the CropPointNet are a random sampling scheme for training point cloud, and optimization of the network architecture to enable structural attribute-based segmentation of point clouds of unstructured objects such as TLS point clouds crops. The performance of the 3D crop classification has been validated and compared against two popular deep learning architectures: PointNet, and the Dynamic Graph-based Convolutional Neural Network (DGCNN). Results indicate consistent plant level object-based classification of crop point cloud with overall accuracies of 81\% or better for all the three crops. The CropPointNet architecture proposed in this research can be generalized for segmentation and classification of other row crops and natural vegetation types.},
	language = {en},
	number = {5},
	urldate = {2021-10-08},
	journal = {Precision Agriculture},
	author = {Jayakumari, Reji and Nidamanuri, Rama Rao and Ramiya, Anandakumar M.},
	month = oct,
	year = {2021},
	pages = {1617--1633},
}

@article{poblete_detection_2020,
	title = {Detection of {Xylella} fastidiosa infection symptoms with airborne multispectral and thermal imagery: {Assessing} bandset reduction performance from hyperspectral analysis},
	volume = {162},
	issn = {0924-2716},
	shorttitle = {Detection of {Xylella} fastidiosa infection symptoms with airborne multispectral and thermal imagery},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271620300496},
	doi = {10.1016/j.isprsjprs.2020.02.010},
	abstract = {Xylella fastidiosa (Xf) is a harmful plant pathogenic bacterium, able to infect over 500 plant species worldwide. Successful eradication and containment strategies for harmful pathogens require large-scale monitoring techniques for the detection of infected hosts, even when they do not display visual symptoms. Although a previous study using airborne hyperspectral and thermal imagery has shown promising results for the early detection of Xf-infected olive (Olea europaea) trees, further work is needed when adopting these techniques for large scale monitoring using multispectral cameras on board airborne platforms and satellites. We used hyperspectral and thermal imagery collected during a two-year airborne campaign in a Xf-infected area in southern Italy to assess the performance of spectrally constrained machine-learning algorithms for this task. The algorithms were used to assess multispectral bandsets, selected from the original hyperspectral imagery, that were compatible with large-scale monitoring from unmanned platforms and manned aircraft. In addition, the contribution of solar–induced chlorophyll fluorescence (SIF) and the temperature-based Crop Water Stress Index (CWSI) retrieved from hyperspectral and thermal imaging, respectively, were evaluated to quantify their relative importance in the algorithms used to detect Xf infection. The detection performance using support vector machine algorithms decreased from ∼80\% (kappa, κ = 0.42) when using the original full hyperspectral dataset including SIF and CWSI to ∼74\% (κ = 0.36) when the optimal set of six spectral bands most sensitive to Xf infection were used in addition to the CWSI thermal indicator. When neither SIF nor CWSI were used, the detection yielded less than 70\% accuracy (decreasing κ to very low performance, 0.29), revealing that tree temperature was more important than chlorophyll fluorescence for the Xf detection. This work demonstrates that large-scale Xf monitoring can be supported using airborne platforms carrying multispectral and thermal cameras with a limited number of spectral bands (e.g., six to 12 bands with 10 nm bandwidths) as long as they are carefully selected by their sensitivity to the Xf symptoms. More precisely, the blue (bands between 400 and 450 nm to derive the NPQI index) and thermal (to derive CWSI from tree temperature) were the most critical spectral regions for their sensitivity to Xf symptoms in olive.},
	language = {en},
	urldate = {2021-10-08},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Poblete, T. and Camino, C. and Beck, P. S. A. and Hornero, A. and Kattenborn, T. and Saponari, M. and Boscia, D. and Navas-Cortes, J. A. and Zarco-Tejada, P. J.},
	month = apr,
	year = {2020},
	keywords = {Airborne, Hyperspectral, Machine learning, Multispectral, Radiative transfer, Thermal},
	pages = {27--40},
}

@article{poblete_discriminating_2021,
	title = {Discriminating {Xylella} fastidiosa from {Verticillium} dahliae infections in olive trees using thermal- and hyperspectral-based plant traits},
	volume = {179},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271621001994},
	doi = {10.1016/j.isprsjprs.2021.07.014},
	abstract = {Globally, olive (Olea europaea L.) productivity is threatened by plant pathogens, particularly the fungus Verticillium dahliae (Vd) and the bacterium Xylella fastidiosa (Xf). Infections by these pathogens restrict water and nutrient flow through xylem, producing a similar set of symptoms that can also be confounded with water stress. Conventional in situ monitoring techniques are time consuming and expensive, necessitating the development of large-scale detection methods. Airborne hyperspectral and thermal imagery have been successfully used to detect both Xf and Vd infection symptoms independently, i.e., when only one of the two diseases is present. Nevertheless, the discrimination of Vd from Xf infections in contexts where both pathogens are present has not been addressed to date. This study proposes a three-stage machine learning algorithm to distinguish Vd infections from Xf infections, using a series of datasets from 27 olive orchards affected by Xf and Vd outbreaks in Italy and Spain between 2011 and 2017. Plant traits were derived from airborne hyperspectral and thermal imagery, including physiological indices from radiative transfer model inversion, Solar-induced Fluorescence emission (SIF@760), the Crop Water Stress Index (CWSI), and a selection of narrow–band hyperspectral indices. Several distinct spectral traits successfully discriminated Xf from Vd infections. The three-stage method generated a false-positive rate of 9\%, an overall accuracy (OA) of 98\%, and a kappa coefficient (κ) of 0.7 when identifying Vd infections using a mixed Vd + Xf dataset. When identifying Xf infections, the false-positive rate was 4\%, the OA was 92\%, and κ was 0.8. These results indicate that hyperspectral and thermal traits can be used to discriminate Xf from Vd infection caused by the two xylem–limited pathogens that trigger similar visual symptoms.},
	language = {en},
	urldate = {2021-10-08},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Poblete, T. and Navas-Cortes, J. A. and Camino, C. and Calderon, R. and Hornero, A. and Gonzalez-Dugo, V. and Landa, B. B. and Zarco-Tejada, P. J.},
	month = sep,
	year = {2021},
	keywords = {Hyperspectral, Machine learning, Plant traits, Thermal},
	pages = {133--144},
}

@article{schiefer_mapping_2020,
	title = {Mapping forest tree species in high resolution {UAV}-based {RGB}-imagery by means of convolutional neural networks},
	volume = {170},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271620302938},
	doi = {10.1016/j.isprsjprs.2020.10.015},
	abstract = {The use of unmanned aerial vehicles (UAVs) in vegetation remote sensing allows a time-flexible and cost-effective acquisition of very high-resolution imagery. Still, current methods for the mapping of forest tree species do not exploit the respective, rich spatial information. Here, we assessed the potential of convolutional neural networks (CNNs) and very high-resolution RGB imagery from UAVs for the mapping of tree species in temperate forests. We used multicopter UAVs to obtain very high-resolution ({\textbackslash}textless2 cm) RGB imagery over 51 ha of temperate forests in the Southern Black Forest region, and the Hainich National Park in Germany. To fully harness the end-to-end learning capabilities of CNNs, we used a semantic segmentation approach (U-net) that concurrently segments and classifies tree species from imagery. With a diverse dataset in terms of study areas, site conditions, illumination properties, and phenology, we accurately mapped nine tree species, three genus-level classes, deadwood, and forest floor (mean F1-score 0.73). A larger tile size during CNN training negatively affected the model accuracies for underrepresented classes. Additional height information from normalized digital surface models slightly increased the model accuracy but increased computational complexity and data requirements. A coarser spatial resolution substantially reduced the model accuracy (mean F1-score of 0.26 at 32 cm resolution). Our results highlight the key role that UAVs can play in the mapping of forest tree species, given that air- and spaceborne remote sensing currently does not provide comparable spatial resolutions. The end-to-end learning capability of CNNs makes extensive preprocessing partly obsolete. The use of large and diverse datasets facilitate a high degree of generalization of the CNN, thus fostering transferability. The synergy of high-resolution UAV imagery and CNN provide a fast and flexible yet accurate means of mapping forest tree species.},
	language = {en},
	urldate = {2021-10-08},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Schiefer, Felix and Kattenborn, Teja and Frick, Annett and Frey, Julian and Schall, Peter and Koch, Barbara and Schmidtlein, Sebastian},
	month = dec,
	year = {2020},
	keywords = {Convolutional neural networks, Deep learning, Forest inventory, Temperate forests, Tree species classification, Unmanned aerial systems},
	pages = {205--215},
}

@article{zhang_multi-source_2010,
	title = {Multi-source remote sensing data fusion: status and trends},
	volume = {1},
	issn = {1947-9832},
	shorttitle = {Multi-source remote sensing data fusion},
	url = {https://doi.org/10.1080/19479830903561035},
	doi = {10.1080/19479830903561035},
	abstract = {With the fast development of remote sensor technologies, e.g. the appearance of Very High Resolution (VHR) optical sensors, SAR, LiDAR, etc., mounted on either airborne or spaceborne platforms, multi-source remote sensing data fusion techniques are emerging due to the demand for new methods and algorithms. The general fusion techniques have been well developed and applied in various fields ranging from satellite earth observation to computer vision, medical image processing, defence security and so on. Despite the fast development, the techniques remain challenging for multi-source data fusion within varying spatial and temporal resolutions. This article reviews current techniques of multi-source remote sensing data fusion and discusses their future trends and challenges through the concept of hierarchical classification, i.e., pixel/data level, feature level and decision level. This article concentrates on discussing optical panchromatic and multi-spectral data fusing methods. So far, the pixel level fusion methods have mainly focused on optical data fusion; high-level fusion includes feature level and decision level fusion of multi-source data, such as synthetic aperture radar, optical images, LiDAR and other types of data. Finally, this article summarises several trends tending to broaden the application of multi-source data fusion.},
	number = {1},
	urldate = {2021-10-08},
	journal = {International Journal of Image and Data Fusion},
	author = {Zhang, Jixian},
	month = mar,
	year = {2010},
	keywords = {LiDAR, SAR, data fusion, multi-source remote sensing data},
	pages = {5--24},
}

@article{maimaitijiang_crop_2020,
	title = {Crop {Monitoring} {Using} {Satellite}/{UAV} {Data} {Fusion} and {Machine} {Learning}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/12/9/1357},
	doi = {10.3390/rs12091357},
	abstract = {Non-destructive crop monitoring over large areas with high efficiency is of great significance in precision agriculture and plant phenotyping, as well as decision making with regards to grain policy and food security. The goal of this research was to assess the potential of combining canopy spectral information with canopy structure features for crop monitoring using satellite/unmanned aerial vehicle (UAV) data fusion and machine learning. Worldview-2/3 satellite data were tasked synchronized with high-resolution RGB image collection using an inexpensive unmanned aerial vehicle (UAV) at a heterogeneous soybean (Glycine max (L.) Merr.) field. Canopy spectral information (i.e., vegetation indices) was extracted from Worldview-2/3 data, and canopy structure information (i.e., canopy height and canopy cover) was derived from UAV RGB imagery. Canopy spectral and structure information and their combination were used to predict soybean leaf area index (LAI), aboveground biomass (AGB), and leaf nitrogen concentration (N) using partial least squares regression (PLSR), random forest regression (RFR), support vector regression (SVR), and extreme learning regression (ELR) with a newly proposed activation function. The results revealed that: (1) UAV imagery-derived high-resolution and detailed canopy structure features, canopy height, and canopy coverage were significant indicators for crop growth monitoring, (2) integration of satellite imagery-based rich canopy spectral information with UAV-derived canopy structural features using machine learning improved soybean AGB, LAI, and leaf N estimation on using satellite or UAV data alone, (3) adding canopy structure information to spectral features reduced background soil effect and asymptotic saturation issue to some extent and led to better model performance, (4) the ELR model with the newly proposed activated function slightly outperformed PLSR, RFR, and SVR in the prediction of AGB and LAI, while RFR provided the best result for N estimation. This study introduced opportunities and limitations of satellite/UAV data fusion using machine learning in the context of crop monitoring.},
	language = {en},
	number = {9},
	urldate = {2021-10-08},
	journal = {Remote Sensing},
	author = {Maimaitijiang, Maitiniyazi and Sagan, Vasit and Sidike, Paheding and Daloye, Ahmad M. and Erkbol, Hasanjan and Fritschi, Felix B.},
	month = jan,
	year = {2020},
	keywords = {activation function, crop monitoring, data fusion, extreme learning machine (ELM), machine learning, unmanned aerial vehicle (UAV)},
	pages = {1357},
}

@article{almeida_monitoring_2021,
	title = {Monitoring restored tropical forest diversity and structure through {UAV}-borne hyperspectral and lidar fusion},
	volume = {264},
	issn = {0034-4257},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425721003023},
	doi = {10.1016/j.rse.2021.112582},
	abstract = {Remote sensors, onboard orbital platforms, aircraft, or unmanned aerial vehicles (UAVs) have emerged as a promising technology to enhance our understanding of changes in ecosystem composition, structure, and function of forests, offering multi-scale monitoring of forest restoration. UAV systems can generate high-resolution images that provide accurate information on forest ecosystems to aid decision-making in restoration projects. However, UAV technological advances have outpaced practical application; thus, we explored combining UAV-borne lidar and hyperspectral data to evaluate the diversity and structure of restoration plantings. We developed novel analytical approaches to assess twelve 13-year-old restoration plots experimentally established with 20, 60 or 120 native tree species in the Brazilian Atlantic Forest. We assessed (1) the congruence and complementarity of lidar and hyperspectral-derived variables, (2) their ability to distinguish tree richness levels and (3) their ability to predict aboveground biomass (AGB). We analyzed three structural attributes derived from lidar data—canopy height, leaf area index (LAI), and understory LAI—and eighteen variables derived from hyperspectral data—15 vegetation indices (VIs), two components of the minimum noise fraction (related to spectral composition) and the spectral angle (related to spectral variability). We found that VIs were positively correlated with LAI for low LAI values, but stabilized for LAI greater than 2 m2/m2. LAI and structural VIs increased with increasing species richness, and hyperspectral variability was significantly related to species richness. While lidar-derived canopy height better predicted AGB than hyperspectral-derived VIs, it was the fusion of UAV-borne hyperspectral and lidar data that allowed effective co-monitoring of both forest structural attributes and tree diversity in restoration plantings. Furthermore, considering lidar and hyperspectral data together more broadly supported the expectations of biodiversity theory, showing that diversity enhanced biomass capture and canopy functional attributes in restoration. The use of UAV-borne remote sensors can play an essential role during the UN Decade of Ecosystem Restoration, which requires detailed forest monitoring on an unprecedented scale.},
	language = {en},
	urldate = {2021-10-08},
	journal = {Remote Sensing of Environment},
	author = {Almeida, Danilo Roberti Alves de and Broadbent, Eben North and Ferreira, Matheus Pinheiro and Meli, Paula and Zambrano, Angelica Maria Almeyda and Gorgens, Eric Bastos and Resende, Angelica Faria and de Almeida, Catherine Torres and do Amaral, Cibele Hummel and Corte, Ana Paula Dalla and Silva, Carlos Alberto and Romanelli, João P. and Prata, Gabriel Atticciati and de Almeida Papa, Daniel and Stark, Scott C. and Valbuena, Ruben and Nelson, Bruce Walker and Guillemot, Joannes and Féret, Jean-Baptiste and Chazdon, Robin and Brancalion, Pedro H. S.},
	month = oct,
	year = {2021},
	keywords = {Drones, Forest landscape restoration, Hyperspectral remote sensing, Leaf area density, Lidar remote sensing, Tropical forests, Vegetation indices},
	pages = {112582},
}

@techreport{noauthor_landsat_2020,
	address = {Reston, VA},
	type = {{USGS} {Numbered} {Series}},
	title = {Landsat 9},
	url = {http://pubs.er.usgs.gov/publication/fs20193008},
	abstract = {Landsat 9 is a partnership between the National Aeronautics and Space Administration (NASA) and the U.S. Geological Survey (USGS) that will continue the Landsat program’s critical role of repeat global observations for monitoring, understanding, and managing Earth’s natural resources. Since 1972, Landsat data have provided a unique resource for those who work in agriculture, geology, forestry, regional planning, education, mapping, and global-change research. Landsat images have also proved invaluable to the International Charter: Space and Major Disasters, supporting emergency response and disaster relief to save lives. With the addition of Landsat 9, the Landsat program’s record of land imaging will be extended to over half a century.},
	number = {2019-3008},
	urldate = {2021-10-07},
	institution = {U.S. Geological Survey},
	year = {2020},
	doi = {10.3133/fs20193008},
	pages = {2},
}

@article{heckel_predicting_2020,
	title = {Predicting {Forest} {Cover} in {Distinct} {Ecosystems}: {The} {Potential} of {Multi}-{Source} {Sentinel}-1 and -2 {Data} {Fusion}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Predicting {Forest} {Cover} in {Distinct} {Ecosystems}},
	url = {https://www.mdpi.com/2072-4292/12/2/302},
	doi = {10.3390/rs12020302},
	abstract = {The fusion of microwave and optical data sets is expected to provide great potential for the derivation of forest cover around the globe. As Sentinel-1 and Sentinel-2 are now both operating in twin mode, they can provide an unprecedented data source to build dense spatial and temporal high-resolution time series across a variety of wavelengths. This study investigates (i) the ability of the individual sensors and (ii) their joint potential to delineate forest cover for study sites in two highly varied landscapes located in Germany (temperate dense mixed forests) and South Africa (open savanna woody vegetation and forest plantations). We used multi-temporal Sentinel-1 and single time steps of Sentinel-2 data in combination to derive accurate forest/non-forest (FNF) information via machine-learning classifiers. The forest classification accuracies were 90.9\% and 93.2\% for South Africa and Thuringia, respectively, estimated while using autocorrelation corrected spatial cross-validation (CV) for the fused data set. Sentinel-1 only classifications provided the lowest overall accuracy of 87.5\%, while Sentinel-2 based classifications led to higher accuracies of 91.9\%. Sentinel-2 short-wave infrared (SWIR) channels, biophysical parameters (Leaf Area Index (LAI), and Fraction of Absorbed Photosynthetically Active Radiation (FAPAR)) and the lower spectrum of the Sentinel-1 synthetic aperture radar (SAR) time series were found to be most distinctive in the detection of forest cover. In contrast to homogenous forests sites, Sentinel-1 time series information improved forest cover predictions in open savanna-like environments with heterogeneous regional features. The presented approach proved to be robust and it displayed the benefit of fusing optical and SAR data at high spatial resolution.},
	language = {en},
	number = {2},
	urldate = {2021-10-08},
	journal = {Remote Sensing},
	author = {Heckel, Kai and Urban, Marcel and Schratz, Patrick and Mahecha, Miguel D. and Schmullius, Christiane},
	month = jan,
	year = {2020},
	keywords = {Germany, Sentinel-1, Sentinel-2, South Africa, data fusion, forest cover, machine-learning, savanna, temperate forest},
	pages = {302},
}

@techreport{noauthor_fact_2018,
	type = {Fact {Sheet}},
	title = {Fact {Sheet}},
	language = {en},
	year = {2018},
}

@inproceedings{de_oca_uas_2021,
	title = {A {UAS} equipped with a thermal imaging system with temperature calibration for {Crop} {Water} {Stress} {Index} computation},
	doi = {10.1109/ICUAS51884.2021.9476863},
	abstract = {This paper presents the development of an Unmanned Aerial System (UAS) for thermal imaging through the equipment of a thermal camera Flir® Boson 640. The proposed UAS can capture and process the thermal imagery to compute an important vegetation index, as is the Crop Water Stress Index (CWSI). Such index is applicable to determine water stress and evaluate the irrigation process among vegetation. For the latter purposes, we present the image processing workflow to obtain the temperature map required for the computation of the CWSI. The image processing tasks applied to the imagery are a) image correction, b) orthomosaic generation, c) temperature calibration and d) vegetation index computation. For every task, we provide a detailed description. We have also designed an interface system based on a Raspberry Pi. The interface systems allows to collect the required imagery through the thermal camera, according to the trigger signal commanded by the autopilot. To assess the performance of the proposed system and methodology, we conduct experiments over a small area of a park. The results show that the CWSI can be computed effectively with our proposed methodology. The computed CWSI highlights water-stressed spots within the vegetation area. The developed code and open-source software used in this work are available on our Github page.},
	booktitle = {2021 {International} {Conference} on {Unmanned} {Aircraft} {Systems} ({ICUAS})},
	author = {de Oca, Andres Montes and Flores, Gerardo},
	month = jun,
	year = {2021},
	keywords = {Calibration, Cameras, Image processing, Indexes, Stress, Task analysis, Thermal imaging, UAV, Vegetation Index, Vegetation mapping},
	pages = {714--720},
}

@article{iwaszczuk_camera_2017,
	title = {Camera pose refinement by matching uncertain {3D} building models with thermal infrared image sequences for high quality texture extraction},
	volume = {132},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271616305391},
	doi = {10.1016/j.isprsjprs.2017.08.006},
	abstract = {Thermal infrared (TIR) images are often used to picture damaged and weak spots in the insulation of the building hull, which is widely used in thermal inspections of buildings. Such inspection in large-scale areas can be carried out by combining TIR imagery and 3D building models. This combination can be achieved via texture mapping. Automation of texture mapping avoids time consuming imaging and manually analyzing each face independently. It also provides a spatial reference for façade structures extracted in the thermal textures. In order to capture all faces, including the roofs, façades, and façades in the inner courtyard, an oblique looking camera mounted on a flying platform is used. Direct geo-referencing is usually not sufficient for precise texture extraction. In addition, 3D building models have also uncertain geometry. In this paper, therefore, methodology for co-registration of uncertain 3D building models with airborne oblique view images is presented. For this purpose, a line-based model-to-image matching is developed, in which the uncertainties of the 3D building model, as well as of the image features are considered. Matched linear features are used for the refinement of the exterior orientation parameters of the camera in order to ensure optimal co-registration. Moreover, this study investigates whether line tracking through the image sequence supports the matching. The accuracy of the extraction and the quality of the textures are assessed. For this purpose, appropriate quality measures are developed. The tests showed good results on co-registration, particularly in cases where tracking between the neighboring frames had been applied.},
	language = {en},
	urldate = {2021-10-04},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Iwaszczuk, Dorota and Stilla, Uwe},
	month = oct,
	year = {2017},
	keywords = {3D, Building, Matching, Sequences, Texturing, Thermal},
	pages = {33--47},
}

@article{yuan_uav-based_2021,
	title = {{UAV}-{Based} {Heating} {Requirement} {Determination} for {Frost} {Management} in {Apple} {Orchard}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/13/2/273},
	doi = {10.3390/rs13020273},
	abstract = {Frost is a natural disaster that can cause catastrophic damages in agriculture, while traditional temperature monitoring in orchards has disadvantages such as being imprecise and laborious, which can lead to inadequate or wasteful frost protection treatments. In this article, we presented a heating requirement assessment methodology for frost protection in an apple orchard utilizing unmanned aerial vehicle (UAV)-based thermal and RGB cameras. A thermal image stitching algorithm using the BRISK feature was developed for creating georeferenced orchard temperature maps, which attained a sub-centimeter map resolution and a stitching speed of 100 thermal images within 30 s. YOLOv4 classifiers for six apple flower bud growth stages in various network sizes were trained based on 5040 RGB images, and the best model achieved a 71.57\% mAP for a test dataset consisted of 360 images. A flower bud mapping algorithm was developed to map classifier detection results into dense growth stage maps utilizing RGB image geoinformation. Heating requirement maps were created using artificial flower bud critical temperatures to simulate orchard heating demands during frost events. The results demonstrated the feasibility of the proposed orchard heating requirement determination methodology, which has the potential to be a critical component of an autonomous, precise frost management system in future studies.},
	language = {en},
	number = {2},
	urldate = {2021-09-16},
	journal = {Remote Sensing},
	author = {Yuan, Wenan and Choi, Daeun},
	month = jan,
	year = {2021},
	keywords = {BRISK, UAV, YOLOv4, apple, flower bud, frost, image stitching, mapping, object detection},
	pages = {273},
}

@article{maes_optimizing_2017,
	title = {Optimizing the {Processing} of {UAV}-{Based} {Thermal} {Imagery}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/9/5/476},
	doi = {10.3390/rs9050476},
	abstract = {The current standard procedure for aligning thermal imagery with structure-from-motion (SfM) software uses GPS logger data for the initial image location. As input data, all thermal images of the flight are rescaled to cover the same dynamic scale range, but they are not corrected for changes in meteorological conditions during the flight. This standard procedure can give poor results, particularly in datasets with very low contrast between and within images or when mapping very complex 3D structures. To overcome this, three alignment procedures were introduced and tested: camera pre-calibration, correction of thermal imagery for small changes in air temperature, and improved estimation of the initial image position by making use of the alignment of RGB (visual) images. These improvements were tested and evaluated in an agricultural (low temperature contrast data) and an afforestation (complex 3D structure) dataset. In both datasets, the standard alignment procedure failed to align the images properly, either by resulting in point clouds with several gaps (images that were not aligned) or with unrealistic 3D structure. Using initial thermal camera positions derived from RGB image alignment significantly improved thermal image alignment in all datasets. Air temperature correction had a small yet positive impact on image alignment in the low-contrast agricultural dataset, but a minor effect in the afforestation area. The effect of camera calibration on the alignment was limited in both datasets. Still, in both datasets, the combination of all three procedures significantly improved the alignment, in terms of number of aligned images and of alignment quality.},
	language = {en},
	number = {5},
	urldate = {2021-09-09},
	journal = {Remote Sensing},
	author = {Maes, Wouter H. and Huete, Alfredo R. and Steppe, Kathy},
	month = may,
	year = {2017},
	keywords = {UAV, drone, structure-from-motion, thermal remote sensing, thermography},
	pages = {476},
}

@phdthesis{javadnejad_small_2017,
	address = {Oregon},
	type = {{PhD} {Thesis}},
	title = {Small {Unmanned} {Aircraft} {Systems} ({UAS}) for {Engineering} {Inspections} and {Geospatial} {Mapping}},
	url = {https://ir.library.oregonstate.edu/concern/parent/6969z572s/file_sets/2j62sb01c},
	urldate = {2021-10-04},
	school = {Oregon State University},
	author = {Javadnejad, Farid},
	month = nov,
	year = {2017},
}

@article{nishar_thermal_2016,
	title = {Thermal infrared imaging of geothermal environments and by an unmanned aerial vehicle ({UAV}): {A} case study of the {Wairakei} – {Tauhara} geothermal field, {Taupo}, {New} {Zealand}},
	volume = {86},
	issn = {0960-1481},
	shorttitle = {Thermal infrared imaging of geothermal environments and by an unmanned aerial vehicle ({UAV})},
	url = {https://www.sciencedirect.com/science/article/pii/S0960148115303219},
	doi = {10.1016/j.renene.2015.09.042},
	abstract = {Recent advances in unmanned aerial vehicles (UAVs) for civilian use make it possible to regularly monitor geothermal environments at spatial and temporal scales that would be difficult to achieve using conventional methods. Previous aerial monitoring of geothermal environments has been expensive and time consuming. This paper demonstrates the use of a small ({\textbackslash}textless2 kg), cost effective quadcopter UAV to safely and accurately map physical and biological characteristics of these unique habitats. Thermal infrared imaging and photogrammetry are used to capture detailed information of geothermal surface features and surrounding vegetation within the Wairakei – Tauhara geothermal field near Taupo, New Zealand. The study highlights advanced techniques in sampling, processing and analysing UAV images and identifies some research challenges and limitations in the use of UAV platforms and sensors. The application of UAVs to describe and monitor geothermal features and other environments is a rapidly developing field in science and natural resource management. This project demonstrates the utility of UAV applications in geothermal science and the potential for their use in many other areas of research.},
	language = {en},
	urldate = {2021-09-13},
	journal = {Renewable Energy},
	author = {Nishar, Abdul and Richards, Steve and Breen, Dan and Robertson, John and Breen, Barbara},
	month = feb,
	year = {2016},
	keywords = {Aerial monitoring, GIS, Geothermal, Remote sensing, Thermal infrared imaging, UAV},
	pages = {1256--1264},
}

@article{quattrochi_thermal_1999,
	title = {Thermal infrared remote sensing for analysis of landscape ecological processes: methods and applications},
	volume = {14},
	issn = {1572-9761},
	shorttitle = {Thermal infrared remote sensing for analysis of landscape ecological processes},
	url = {https://doi.org/10.1023/A:1008168910634},
	doi = {10.1023/A:1008168910634},
	abstract = {Thermal infrared (TIR) remote sensing data can provide important measurements of surface energy fluxes and temperatures, which are integral to understanding landscape processes and responses. One example of this is the successful application of TIR remote sensing data to estimate evapotranspiration and soil moisture, where results from a number of studies suggest that satellite-based measurements from TIR remote sensing data can lead to more accurate regional-scale estimates of daily evapotranspiration. With further refinement in analytical techniques and models, the use of TIR data from airborne and satellite sensors could be very useful for parameterizing surface moisture conditions and developing better simulations of landscape energy exchange over a variety of conditions and space and time scales. Thus, TIR remote sensing data can significantly contribute to the observation, measurement, and analysis of energy balance characteristics (i.e., the fluxes and redistribution of thermal energy within and across the land surface) as an implicit and important aspect of landscape dynamics and landscape functioning.},
	language = {en},
	number = {6},
	urldate = {2021-09-10},
	journal = {Landscape Ecology},
	author = {Quattrochi, Dale A. and Luvall, Jeffrey C.},
	month = dec,
	year = {1999},
	pages = {577--598},
}

@article{wachs_low_2010,
	title = {Low and high-level visual feature-based apple detection from multi-modal images},
	volume = {11},
	issn = {1573-1618},
	url = {https://doi.org/10.1007/s11119-010-9198-x},
	doi = {10.1007/s11119-010-9198-x},
	abstract = {Automated harvesting requires accurate detection and recognition of the fruit within a tree canopy in real-time in uncontrolled environments. However, occlusion, variable illumination, variable appearance and texture make this task a complex challenge. Our research discusses the development of a machine vision system, capable of recognizing occluded green apples within a tree canopy. This involves the detection of “green” apples within scenes of “green leaves”, shadow patterns, branches and other objects found in natural tree canopies. The system uses both thermal infra-red and color image modalities in order to achieve improved performance. Maximization of mutual information is used to find the optimal registration parameters between images from the two modalities. We use two approaches for apple detection based on low and high-level visual features. High-level features are global attributes captured by image processing operations, while low-level features are strong responses to primitive parts-based filters (such as Haar wavelets). These features are then applied separately to color and thermal infra-red images to detect apples from the background. These two approaches are compared and it is shown that the low-level feature-based approach is superior (74\% recognition accuracy) over the high-level visual feature approach (53.16\% recognition accuracy). Finally, a voting scheme is used to improve the detection results, which drops the false alarms with little effect on the recognition rate. The resulting classifiers acting independently can partially recognize the on-tree apples, however, when combined the recognition accuracy is increased.},
	language = {en},
	number = {6},
	urldate = {2021-09-13},
	journal = {Precision Agriculture},
	author = {Wachs, J. P. and Stern, H. I. and Burks, T. and Alchanatis, V.},
	month = dec,
	year = {2010},
	pages = {717--735},
}

@misc{noauthor_inpecbers_2021,
	title = {{INPE}/{CBERS}},
	url = {http://www.cbers.inpe.br/},
	urldate = {2021-09-13},
	year = {2021},
}

@article{lafi_3d_2017,
	title = {{3D} {Thermal} and {Spatial} {Modeling} of a {Subway} {Tunnel}: {A} {Case} {Study}},
	shorttitle = {{3D} {Thermal} and {Spatial} {Modeling} of a {Subway} {Tunnel}},
	url = {https://ascelibrary.org/doi/abs/10.1061/9780784480823.046},
	doi = {10.1061/9780784480823.046},
	abstract = {Infrared thermography (IR) is a modern, non-destructive evaluation technology for monitoring and assessing civil infrastructure conditions. It mainly relies on measuring the infrastructure surface temperature to identify any potential defects. Currently, most of the existing research studies in IR rely on 2D thermal images which are time-consuming and labor-intensive. This paper describes a case study that examines the use of both infrared and visual sensing in recording thermal and spatial conditions of a subway tunnel segment in the city of Montreal, Canada. In the case study, both thermal and visible images of the infrastructure conditioning data were collected separately. Next, the visible images were used to generate a 3D point cloud model by applying the structure from motion approach. In parallel, each set of overlapping thermal images were stitched to form a thermal panoramic image that covers a large surface area with an accurate temperature representation. The stitched thermal images were finally mapped to the 3D point cloud in order to produce both thermal and metric measurements of a subway tunnel segment. The results of the proposed framework demonstrate that 3D thermal modeling using visual and infrared sensing is able to generate geometric and thermal information of indoor infrastructure environments. Furthermore, this approach is affordable in terms of cost and time.},
	language = {en},
	urldate = {2021-09-03},
	author = {Lafi, Ghassan Al and Zhu, Zhenhua and Dawood, Thikra and Zayed, Tarek},
	month = jun,
	year = {2017},
	pages = {386--394},
}

@article{clarkson_thermal_2017,
	title = {Thermal {3D} modelling},
	url = {http://www.iaarc.org/publications/2017_proceedings_of_the_34rd_isarc/thermal_3d_modelling.html},
	language = {en-US},
	urldate = {2021-09-03},
	journal = {ISARC Proceedings},
	author = {Clarkson, Gregory and Luo, Shan and Fuentes, Raul},
	month = jul,
	year = {2017},
	pages = {493--499},
}

@inproceedings{truong_registration_2017,
	title = {Registration of {RGB} and {Thermal} {Point} {Clouds} {Generated} by {Structure} {From} {Motion}},
	doi = {10.1109/ICCVW.2017.57},
	abstract = {Thermal imaging has become a valuable tool in various fields for remote sensing and can provide relevant information to perform object recognition or classification. In this paper, we present an automated method to obtain a 3D model fusing data from a visible and a thermal camera. The RGB and thermal point clouds are generated independently by structure from motion. The registration process includes a normalization of the point cloud scale, a global registration based on calibration data and the output of the structure from motion, and a fine registration employing a variant of the Iterative Closest Point optimization. Experimental results demonstrate the accuracy and robustness of the overall process.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCVW})},
	author = {Truong, Trong Phuc and Yamaguchi, Masahiro and Mori, Shohei and Nozick, Vincent and Saito, Hideo},
	month = oct,
	year = {2017},
	keywords = {Calibration, Cameras, Distortion, Image reconstruction, Thermal sensors, Three-dimensional displays},
	pages = {419--427},
}

@inproceedings{dahaghin_3d_2019,
	title = {{3D} thermal mapping of building roofs based on fusion of thermal and visible point clouds in uav imagery},
	volume = {42},
	doi = {10.5194/isprs-archives-XLII-4-W18-271-2019},
	abstract = {Thermography is a robust method for detecting thermal irregularities on the roof of the buildings as one of the main energy dissipation parts. Recently, UAVs are presented to be useful in gathering 3D thermal data of the building roofs. In this topic, the low spatial resolution of thermal imagery is a challenge which leads to a sparse resolution in point clouds. This paper suggests the fusion of visible and thermal point clouds to generate a high-resolution thermal point cloud of the building roofs. For the purpose, camera calibration is performed to obtain internal orientation parameters, and then thermal point clouds and visible point clouds are generated. In the next step, both two point clouds are geo-referenced by control points. To extract building roofs from the visible point cloud, CSF ground filtering is applied, and the vegetation layer is removed by RGBVI index. Afterward, a predefined threshold is applied to the normal vectors in the z-direction in order to separate facets of roofs from the walls. Finally, the visible point cloud of the building roofs and registered thermal point cloud are combined and generate a fused dense point cloud. Results show mean re-projection error of 0.31 pixels for thermal camera calibration and mean absolute distance of 0.2 m for point clouds registration. The final product is a fused point cloud, which its density improves up to twice of the initial thermal point cloud density and it has the spatial accuracy of visible point cloud along with thermal information of the building roofs. © 2019 M. Dahaghin et al.},
	language = {English},
	author = {Dahaghin, M. and Samadzadegan, F. and Dadras Javan, F.},
	year = {2019},
	keywords = {Building Roof, Fusion, Point Cloud Generation, Thermal Infrared Imaging, Unmanned Aerial Vehicles},
	pages = {271--277},
}

@inproceedings{zhu_direct_2019,
	title = {Direct co-registration of {TIR} images and {MLS} point clouds by corresponding keypoints},
	volume = {4},
	doi = {10.5194/isprs-annals-IV-2-W7-235-2019},
	abstract = {In this work, we discussed how to directly combine thermal infrared image (TIR) and the point cloud without additional assistance from GCPs or 3D models. Specifically, we propose a point-based co-registration process for combining the TIR image and the point cloud for the buildings. The keypoints are extracted from images and point clouds via primitive segmentation and corner detection, then pairs of corresponding points are identified manually. After that, the estimated camera pose can be computed with EPnP algorithm. Finally, the point cloud with thermal information provided by IR images can be generated as a result, which is helpful in the tasks such as energy inspection, leakage detection, and abnormal condition monitoring. This paper provides us more insight about the probability and ideas about the combining TIR image and point cloud. © 2019 ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. All rights reserved.},
	language = {English},
	author = {Zhu, J. and Xu, Y. and Hoegner, L. and Stilla, U.},
	year = {2019},
	keywords = {MLS, TIR images, co-registration, point cloud, point-based, pose estimation, segmentation},
	pages = {235--242},
}

@article{kniaz_thermal_2018,
	title = {Thermal {Texture} {Generation} and 3d {Model} {Reconstruction} {Using} {SFM} and {Gan}},
	volume = {422},
	issn = {2194-9034 The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	url = {https://ui.adsabs.harvard.edu/abs/2018ISPAr.422..519K},
	doi = {10.5194/isprs-archives-XLII-2-519-2018},
	abstract = {Realistic 3D models with textures representing thermal emission of the object are widely used in such fields as dynamic scene analysis, autonomous driving, and video surveillance. Structure from Motion (SfM) methods provide a robust approach for the generation of textured 3D models in the visible range. Still, automatic generation of 3D models from the infrared imagery is challenging due to an absence of the feature points and low sensor resolution. Recent advances in Generative Adversarial Networks (GAN) have proved that they can perform complex image-to-image transformations such as a transformation of day to night and generation of imagery in a different spectral range. In this paper, we propose a novel method for generation of realistic 3D models with thermal textures using the SfM pipeline and GAN. The proposed method uses visible range images as an input. The images are processed in two ways. Firstly, they are used for point matching and dense point cloud generation. Secondly, the images are fed into a GAN that performs the transformation from the visible range to the thermal range. We evaluate the proposed method using real infrared imagery captured with a FLIR ONE PRO camera. We generated a dataset with 2000 pairs of real images captured in thermal and visible range. The dataset is used to train the GAN network and to generate 3D models using SfM. The evaluation of the generated 3D models and infrared textures proved that they are similar to the ground truth model in both thermal emissivity and geometrical shape.},
	urldate = {2021-09-03},
	journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Kniaz, V. V. and Mizginov, V. A.},
	month = may,
	year = {2018},
	pages = {519--524},
}

@inproceedings{maset_photogrammetric_2017,
	title = {{PHOTOGRAMMETRIC} {3D} {BUILDING} {RECONSTRUCTION} {FROM} {THERMAL} {IMAGES}},
	volume = {IV-2-W3},
	url = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2-W3/25/2017/},
	doi = {10.5194/isprs-annals-IV-2-W3-25-2017},
	abstract = {{\textbackslash}textlessp{\textbackslash}textgreater{\textbackslash}textlessstrong class="journal-contentHeaderColor"{\textbackslash}textgreaterAbstract.{\textbackslash}textless/strong{\textbackslash}textgreater This paper addresses the problem of 3D building reconstruction from thermal infrared (TIR) images. We show that a commercial Computer Vision software can be used to automatically orient sequences of TIR images taken from an Unmanned Aerial Vehicle (UAV) and to generate 3D point clouds, without requiring any GNSS/INS data about position and attitude of the images nor camera calibration parameters. Moreover, we propose a procedure based on Iterative Closest Point (ICP) algorithm to create a model that combines high resolution and geometric accuracy of RGB images with the thermal information deriving from TIR images. The process can be carried out entirely by the aforesaid software in a simple and efficient way.{\textbackslash}textless/p{\textbackslash}textgreater},
	language = {English},
	urldate = {2021-09-03},
	booktitle = {{ISPRS} {Annals} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Maset, E. and Fusiello, A. and Crosilla, F. and Toldo, R. and Zorzetto, D.},
	month = aug,
	year = {2017},
	pages = {25--32},
}

@inproceedings{adan_fusion_2017,
	title = {Fusion of thermal imagery and {LiDAR} data for generating {TBIM} models},
	doi = {10.1109/ICSENS.2017.8234261},
	abstract = {This paper presents a system that provides dense thermal 3D point clouds as a preliminary step towards generating Thermal Building Information Models (TBIM). The system is a fusion of three sensors: a 3D laser scanner, an RGB camera and a thermal camera which are integrated under the same framework. The methodology and the calibration system have got novel aspects that improve the accuracy and soundness of earlier proposals. Additionally, the approach is able to obtain the whole 3D thermal map of the visible space in building indoors, issue that has not been totally solved yet. The system has been tested on large indoor scenarios with promising results.},
	booktitle = {2017 {IEEE} {SENSORS}},
	author = {Adán, A. and Prado, T. and Prieto, S. A. and Quintana, B.},
	month = oct,
	year = {2017},
	keywords = {3D Data Processing, 3D Digitalization, 3D Laser Scanner, 3D Thermal Modelling, Buildings, Calibration, Cameras, Solid modeling, Thermal Camera, Thermal sensors, Three-dimensional displays},
	pages = {1--3},
}

@inproceedings{stojcsics_high_2018,
	title = {High {Resolution} {3D} {Thermal} {Imaging} {Using} {FLIR} {DUO} {R} {Sensor}},
	isbn = {978-1-5386-1122-7},
	doi = {10.1109/INES.2018.8523914},
	abstract = {With the spread of photogrammetry processes, photo-based 2D/3D reconstruction became general, in research as well as in the industry. Source images are taken using either a handheld camera or an automated camera fixed to the carrier, a UAV, then they are matched during post-processing. The price of digital microbolometer-based high-resolution (1 megapixel) thermal cameras is currently very high, but these, compared to RGB cameras (16-20 megapixel), are still considered to have very low-resolution, in this way employing photogrammetry in this present case is not feasible. In the article, a novel method developed by us is introduced which by using a low thermal resolution camera (FLIR DUO R), based on which a 3D thermal image can be produced with the help of a camera capable of dual imaging (RGB and Thermal). The work is illustrated using measurements, and post-production was conducted using the MATLAB software. The process is adequate for producing 3D thermal images taken using UAV devices. © 2018 IEEE.},
	language = {English},
	author = {Stojcsics, D. and Lovas, I. and Domozi, Z. and Molnar, A.},
	year = {2018},
	keywords = {3D thermal imaging, FLIR, MATLAB, large scale point cloud, object reconstruction, photogrammetry},
	pages = {000311--000316},
}

@inproceedings{landmann_multimodal_2019,
	title = {Multimodal sensor: {High}-speed {3D} and thermal measurement},
	volume = {11144},
	isbn = {978-1-5106-2981-3},
	shorttitle = {Multimodal sensor},
	doi = {10.1117/12.2531950},
	abstract = {For the measurement of three-dimensional (3D) shapes, active optical measurement systems based on pattern projection are widely used. These sensors work without contact and are non-destructive. Between one camera and the projector or between two cameras, the 3D reconstruction is performed by detection and triangulation of corresponding image points. Recently, we developed a 3D stereo sensor working in the visible range of light (VIS). It consists of two highspeed cameras and a GOBO projection-based high-speed pattern projector. Our system allows us to successfully reconstruct 3D point clouds of fast processes such as athletes in motion or even crash tests. Simultaneously measuring the surface temperature would be of great benefit as fast processes usually exhibit local temperature changes. In order to include thermal data into the evaluation, we have extended our existing high-speed 3D sensor by including an additional high-speed long-wave infrared (LWIR) camera. The thermal camera detects radiation in the spectral range between 7.5 and 12 μm. We map the measured temperatures as texture onto the reconstructed 3D points. In this contribution, we present the design of this novel 5D (three spatial coordinates, temperature, and time) sensor. The simultaneous calibration process of the VIS cameras and the LWIR camera in a common coordinate system is described. First promising measurements of an inflating airbag, a basketball player, and the crushing of a metal tube performed at a frame rate of 1 kHz are shown. © 2019 SPIE.},
	language = {English},
	author = {Landmann, M. and Heist, S. and Dietrich, P. and Lutzke, P. and Gebhart, I. and Kühmstedt, P. and Notni, G.},
	year = {2019},
	keywords = {3D shape measurement, Aperiodic sinusoidal patterns, GOBO projection, High-speed visible (VIS) and infrared (IR) cameras, Multimodal sensor, Temperature mapping},
}

@inproceedings{huang_combining_2018,
	title = {Combining the {3D} model generated from point clouds and thermography to identify the defects presented on the facades of a building},
	volume = {10599},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10599/105990G/Combining-the-3D-model-generated-from-point-clouds-and-thermography/10.1117/12.2297656.short},
	doi = {10.1117/12.2297656},
	abstract = {Defects presented on the facades of a building do have profound impacts on extending the life cycle of the building. How to identify the defects is a crucial issue; destructive and non-destructive methods are usually employed to identify the defects presented on a building. Destructive methods always cause the permanent damages for the examined objects; on the other hand, non-destructive testing (NDT) methods have been widely applied to detect those defects presented on exterior layers of a building. However, NDT methods cannot provide efficient and reliable information for identifying the defects because of the huge examination areas. Infrared thermography is often applied to quantitative energy performance measurements for building envelopes. Defects on the exterior layer of buildings may be caused by several factors: ventilation losses, conduction losses, thermal bridging, defective services, moisture condensation, moisture ingress, and structure defects. Analyzing the collected thermal images can be quite difficult when the spatial variations of surface temperature are small. In this paper the authors employ image segmentation to cluster those pixels with similar surface temperatures such that the processed thermal images can be composed of limited groups. The surface temperature distribution in each segmented group is homogenous. In doing so, the regional boundaries of the segmented regions can be identified and extracted. A terrestrial laser scanner (TLS) is widely used to collect the point clouds of a building, and those point clouds are applied to reconstruct the 3D model of the building. A mapping model is constructed such that the segmented thermal images can be projected onto the 2D image of the specified 3D building. In this paper, the administrative building in Chaoyang University campus is used as an example. The experimental results not only provide the defect information but also offer their corresponding spatial locations in the 3D model.},
	urldate = {2021-09-03},
	booktitle = {Nondestructive {Characterization} and {Monitoring} of {Advanced} {Materials}, {Aerospace}, {Civil} {Infrastructure}, and {Transportation} {XII}},
	publisher = {International Society for Optics and Photonics},
	author = {Huang, Yishuo and Chiang, Chih-Hung and Hsu, Keng-Tsang},
	month = mar,
	year = {2018},
	pages = {105990G},
}

@inproceedings{yang_thermal-stress_2019,
	title = {Thermal-stress analysis of {3D}-{IC} based on artificial neural network},
	isbn = {978-1-72811-409-5},
	doi = {10.1109/ICCCBDA.2019.8725696},
	abstract = {Three-Dimensional Integrated Circuits (3D-IC) is a new processing technology emerging in the field of microelectronics in recent years. Through Silicon Via (TSV) technology is used to connect different wafers vertically. Due to the increasing number of transistors in 3D-IC, its heat dissipation and thermal-stress mechanical problems are increasing. These problems can cause performance degradation of components even fail. When the chip is working, the working conditions of various components are constantly changing. The thermal stress of each point is also changing. It is very difficult to get real-time data, so the dynamic thermal stress management technology will be severely limited. The paper mainly proposes a method for quickly predicting the maximum thermal stress at various points in the work of TSV-based 3D-IC chips. Firstly, the finite element simulation software COMSOL is used to establish the overall chip finite element model of TSV based 3D-IC. TSV temperature and chip thermal stress were calculated to find the maximum thermal stress near each TSV and obtain the temperature distribution within a certain range of TSV. Then, an RBF neural network model is built through sample training based on the derived temperature and thermal stress data. RBF neural network model is used to provide some information support for dynamic thermal stress management technology of working chip. Through the extraction of data characteristic information, the thermal stress around TSV is predicted accurately, which provides an accurate and efficient solution for dynamic thermal stress management when the chip works. © 2019 IEEE.},
	language = {English},
	author = {Yang, X. and Tan, J. and Zhou, B. and Hua, D. and Tang, G. and Qiao, T.},
	year = {2019},
	keywords = {3D-IC, RBF network, TSV, finite element simulation, thermal-stress},
	pages = {105--110},
}

@article{hosoi_estimating_2019,
	title = {Estimating {3D} chlorophyll content distribution of trees using an image fusion method between {2D} camera and {3D} portable scanning lidar},
	volume = {11},
	issn = {2072-4292},
	doi = {10.3390/rs11182134},
	abstract = {An image fusion method has been proposed for plant images taken using a two-dimensional (2D) camera and three-dimensional (3D) portable lidar for obtaining a 3D distribution of physiological and biochemical plant properties. In this method, a 2D multispectral camera with five bands (475-840 nm) and a 3D high-resolution portable scanning lidar were applied to three sets of sample trees. After producing vegetation index (VI) images from multispectral images, 3D point cloud lidar data were projected onto the 2D plane based on perspective projection, keeping the depth information of each of the lidar points. The VI images were 2D registered to the lidar projected image based on the projective transformation and VI 3D point cloud images were reconstructed based on the depth information. Based on the relationship between the VI values and chlorophyll contents taken by a soil and plant analysis development (SPAD)-502 plus chlorophyll meter, 3D distribution images of the chlorophyll contents were produced. Similarly, a thermal 3D image for a sample was also produced. The resultant chlorophyll distribution images offered vertical and horizontal distributions, and those for each orientation for each sample, showing the spatial variability of the distribution and the difference between the samples. © 2019 by the authors.},
	language = {English},
	number = {18},
	journal = {Remote Sensing},
	author = {Hosoi, F. and Umeyama, S. and Kuo, K.},
	year = {2019},
	keywords = {3D imaging, 3D plant property, Chlorophyll distribution, Image fusion, Lidar, Multispectral camera, Plant biochemistry, Plant physiology},
}

@inproceedings{cecchini_3d_2019,
	title = {A 3d platform for energy data visualization of building assets},
	volume = {296},
	doi = {10.1088/1755-1315/296/1/012035},
	abstract = {With an exemplary role, the improvement of energy efficiency in public buildings is in the forefront of the European policies for smart and sustainable growth. However, very often the sector is characterized by large and old constructions that may also be marked by historical and cultural value and whose energy consumption is hard to be reduced, due to specific constraints. In order to operate in this field, the definition of a solid knowledge framework on the built environment appears to be the only viable starting point. Therefore, the analysis of the delicate balance between conservation and transformation should be investigated with a multi-scalar approach able to move from the city to the building elements. For this reason, it is extremely important to provide tools for monitoring and analysing the energy behaviour of the public building stocks to those actors that are involved in their management. The research here presented proposes a workflow to implement a web platform based on a three-dimensional GIS (Geographic Information System) interoperable with BIM (Building Information Modeling) and able to store, handle and display information on building assets and their energy consumption. With the aim of defining a repeatable model, the process starts from easily retrievable data on the built environment and uses standard data models and classification systems. The three-dimensional model is built in a semi-automated way from the combination of the two-dimensional GIS cartography of the municipality and from the point cloud resulting from a LiDAR (Light Detection And Ranging) national survey campaign. The set of thermal properties and energy data can be retrieved from the energy performance certificate of the buildings. In order to test and validate the process, an application on the building stock owned by the University of Pavia (Italy) is presented. Nine complexes distribute inside the historical centre of the city and heterogeneously dated from the X to the XX century are considered. After the definition of the model and its representation inside the web environment, an example of use is displayed with reference to a comparative energy analysis of different buildings. © 2019 IOP Publishing Ltd. All rights reserved.},
	language = {English},
	author = {Cecchini, C. and Magrini, A. and Gobbi, L.},
	year = {2019},
}

@article{alsadik_flight_2020,
	title = {Flight planning for {LiDAR}-based {UAS} mapping applications},
	volume = {9},
	issn = {2220-9964},
	doi = {10.3390/ijgi9060378},
	abstract = {In the last two decades, unmanned aircraft systems (UAS) were successfully used in different environments for diverse applications like territorial mapping, heritage 3D documentation, as built surveys, construction monitoring, solar panel placement and assessment, road inspections, etc. These applications were correlated to the onboard sensors like RGB cameras, multi-spectral cameras, thermal sensors, panoramic cameras, or LiDARs. According to the different onboard sensors, a different mission plan is required to satisfy the characteristics of the sensor and the project aims. For UAS LiDAR-based mapping missions, requirements for the flight planning are different with respect to conventional UAS image-based flight plans because of different reasons related to the LiDAR scanning mechanism, scanning range, output scanning rate, field of view (FOV), rotation speed, etc. Although flight planning for image-based UAS missions is a well-known and solved problem, flight planning for a LiDAR-based UAS mapping is still an open research topic that needs further investigations. The article presents the developments of a LiDAR-based UAS flight planning tool, tested with simulations in real scenarios. The flight planning simulations considered an UAS platform equipped, alternatively, with three low-cost multi-beam LiDARs, namely Quanergy M8, Velodyne VLP-16, and the Ouster OS-1-16. The specific characteristics of the three sensors were used to plan flights and acquired dense point clouds. Comparisons and analyses of the results showed clear relationships between point density, flying speeds, and flying heights. © 2020 by the authors.},
	language = {English},
	number = {6},
	journal = {ISPRS International Journal of Geo-Information},
	author = {Alsadik, B. and Remondino, F.},
	year = {2020},
	keywords = {Flight planning, Multi-beam LiDAR, Point density, UAS LiDAR mapping system},
}

@article{anton_engineering_2021,
	title = {Engineering graphics for thermal assessment: {3D} thermal data visualisation based on infrared thermography, {GIS} and {3D} point cloud processing software},
	volume = {13},
	issn = {2073-8994},
	shorttitle = {Engineering graphics for thermal assessment},
	doi = {10.3390/sym13020335},
	abstract = {Engineering graphics are present in the design stage, but also constitute a way to com-municate, analyse, and synthesise. In the Architecture-Engineering-Construction sector, graphical data become essential in analysing buildings and constructions throughout their lifecycles, such as in the thermal behaviour assessment of building envelopes. Scientific research has addressed the thermal image mapping onto three-dimensional (3D) models for visualisation and analysis. How-ever, the 3D point cloud data creation of buildings’ thermal behaviour directly from rectified infrared thermography (IRT) thermograms is yet to be investigated. Therefore, this paper develops an open-source software graphical method to produce 3D thermal data from IRT images for temperature visualisation and subsequent analysis. This low-cost approach uses both a geographic information system for the thermographic image rectification and the point clouds production, and 3D point cloud processing software. The methodology has been proven useful to obtain, without perspective distortions, 3D thermograms even from non-radiometric raster images. The results also revealed that non-rectangular thermograms enable over 95\% of the 3D thermal data generated from IRT against rectangular shapes (over 85\%). Finally, the 3D thermal data produced allow further thermal behaviour assessment, including calculating the object’s heat loss and thermal transmit-tance for diverse applications such as energy audits, restoration, monitoring, or product quality control. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	language = {English},
	number = {2},
	journal = {Symmetry},
	author = {Antón, D. and Amaro-Mellado, J.-L.},
	year = {2021},
	keywords = {3D point cloud processing software, 3D thermal data, 3D thermograms, Engineering graphics, GIS, Heat loss, Infrared thermography, Point cloud data, Temperature, Visualisation},
	pages = {1--20},
}

@inproceedings{guilbert_fusion_2020,
	title = {Fusion of thermal and visible point clouds: application to the {Vaches} {Noires} landslide, {Normandy}, {France}},
	volume = {43},
	shorttitle = {{FUSION} of {THERMAL} and {VISIBLE} {POINT} {CLOUDS}},
	doi = {10.5194/isprs-archives-XLIII-B2-2020-227-2020},
	abstract = {In this paper, we present a methodology to fusion 3D visible and thermal infrared (TIR) information on a coastal landslide area located in Normandy, France. A reflex and TIR camera on-board an Unmanned Aerial Vehicle are utilized to generate a 3D visible and a thermal model using Photogrammetry. A Python-written algorithm is then used to associate the thermal scalar on the TIR model to the closest point on the visible point cloud, before applying \&alpha;-blending to ease the visualization of both data sets. This methodology leads to the generation of an integrated 3D thermo-visible model, allowing the direct analysis of the surface temperatures, visible data and geometric configuration of the landslide. © 2020 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives.},
	language = {English},
	author = {Guilbert, V. and Antoine, R. and Heinkelé, C. and Maquaire, O. and Costa, S. and Gout, C. and Davidson, R. and Sorin, J.-L. and Beaucamp, B. and Fauchard, C.},
	year = {2020},
	keywords = {Fusion, Landslide, Photogrammetry, Thermal Infrared, UAV, Visible},
	pages = {227--232},
}

@inproceedings{hoegner_fusion_2018,
	address = {Berlin},
	title = {Fusion of {TLS} and {RGB} point clouds with {TIR} images for indoor mobile mapping},
	doi = {10.21611/qirt.2018.019},
	author = {Hoegner, Ludwig and Abmayr, Thomas and Tosic, Dragana and Turzer, S. and Stilla, Uwe},
	month = jan,
	year = {2018},
}

@inproceedings{schneider_fusing_2010,
	title = {Fusing vision and {LIDAR} - {Synchronization}, correction and occlusion reasoning},
	doi = {10.1109/ivs.2010.5548079},
	booktitle = {2010 {IEEE} {Intelligent} {Vehicles} {Symposium}},
	publisher = {IEEE},
	author = {Schneider, Sebastian and Himmelsbach, Michael and Luettel, Thorsten and Wuensche, Hans-Joachim},
	month = jun,
	year = {2010},
}

@article{hu_fast_2012,
	title = {Fast occlusion and shadow detection for high resolution remote sensing image combined with {LiDAR} point cloud},
	volume = {XXXIX-B7},
	doi = {10.5194/isprsarchives-xxxix-b7-399-2012},
	journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Hu, X. and Li, X.},
	month = aug,
	year = {2012},
	note = {Publisher: Copernicus GmbH},
	pages = {399--402},
}

@incollection{beauchemin_modelling_2001,
	title = {Modelling and {Removing} {Radial} and {Tangential} {Distortions} in {Spherical} {Lenses}},
	booktitle = {Multi-{Image} {Analysis}},
	publisher = {Springer Berlin Heidelberg},
	author = {Beauchemin, Seven S. and Bajcsy, Ruzena},
	year = {2001},
	doi = {10.1007/3-540-45134-x_1},
	pages = {1--21},
}

@article{evangelidis_parametric_2008,
	title = {Parametric {Image} {Alignment} {Using} {Enhanced} {Correlation} {Coefficient} {Maximization}},
	volume = {30},
	doi = {10.1109/TPAMI.2008.113},
	number = {10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Evangelidis, G. D. and Psarakis, E. Z.},
	year = {2008},
	pages = {1858--1865},
}

@article{sklansky_finding_1982,
	title = {Finding the convex hull of a simple polygon},
	volume = {1},
	issn = {0167-8655},
	doi = {https://doi.org/10.1016/0167-8655(82)90016-2},
	number = {2},
	journal = {Pattern Recognition Letters},
	author = {Sklansky, Jack},
	year = {1982},
	pages = {79 -- 83},
}

@inproceedings{park_pi-sift_2008,
	title = {pi-{SIFT}: {A} photometric and {Scale} {Invariant} {Feature} {Transform}},
	doi = {10.1109/ICPR.2008.4761181},
	booktitle = {{ICPR} 2008 19th {International} {Conference} on {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Park, J. and Park, K. and Baeg, Seung-Ho and Baeg, Moon-Hong},
	month = dec,
	year = {2008},
	note = {ISSN: 1051-4651},
}

@article{matese_practical_2018,
	title = {Practical applications of a multisensor uav platform based on multispectral, thermal and rgb high resolution images in precision viticulture},
	volume = {8},
	number = {7},
	journal = {Agriculture},
	author = {Matese, Alessandro and Di Gennaro, Salvatore Filippo},
	year = {2018},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {116},
}

@article{tucci_multi-sensor_2019,
	title = {Multi-sensor {UAV} application for thermal analysis on a dry-stone terraced vineyard in rural {Tuscany} landscape},
	volume = {8},
	number = {2},
	journal = {ISPRS International Journal of Geo-Information},
	author = {Tucci, Grazia and Parisi, Erica Isabella and Castelli, Giulio and Errico, Alessandro and Corongiu, Manuela and Sona, Giovanna and Viviani, Enea and Bresci, Elena and Preti, Federico},
	year = {2019},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {87},
}

@inproceedings{hakim_band_2018,
	title = {Band coregistration modeling of {LAPAN}-{A3}/{IPB} multispectral imager based on satellite attitude},
	volume = {149},
	booktitle = {{IOP} {Conf}. {Ser}.: {Earth} and {Environmental} {Science}},
	author = {Hakim, PR and Syafrudin, AH and Utama, S and Jayani, APS},
	year = {2018},
	pages = {1--7},
}

@article{jhan_investigation_2017,
	title = {Investigation of parallax issues for multi-lens multispectral camera band co-registration},
	volume = {42},
	journal = {The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Jhan, JP and Rau, Jiann-Yeou and Haala, N and Cramer, M},
	year = {2017},
	note = {Publisher: Copernicus GmbH},
	pages = {157},
}

@article{jhan_band--band_2016,
	title = {Band-to-band registration and ortho-rectification of multilens/multispectral imagery: {A} case study of {MiniMCA}-12 acquired by a fixed-wing {UAS}},
	volume = {114},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Jhan, Jyun-Ping and Rau, Jiann-Yeou and Huang, Cho-Ying},
	year = {2016},
	note = {Publisher: Elsevier},
	pages = {66--77},
}

@inproceedings{shen_multi-modal_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multi-modal and {Multi}-spectral {Registration} for {Natural} {Images}},
	isbn = {978-3-319-10593-2},
	doi = {10.1007/978-3-319-10593-2_21},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Shen, Xiaoyong and Xu, Li and Zhang, Qi and Jia, Jiaya},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {dense matching, multi-modal, multi-spectral, variational model},
	pages = {309--324},
}

@article{santini_using_2019,
	title = {Using unmanned aerial vehicle-based multispectral, {RGB} and thermal imagery for phenotyping of forest genetic trials: {A} case study in {Pinus} halepensis},
	volume = {174},
	number = {2},
	journal = {Annals of Applied Biology},
	author = {Santini, Filippo and Kefauver, Shawn C and Resco de Dios, Victor and Araus, José L and Voltas, Jordi},
	year = {2019},
	note = {Publisher: Wiley Online Library},
	pages = {262--276},
}

@article{bavirisetti_fusion_2015,
	title = {Fusion of infrared and visible sensor images based on anisotropic diffusion and {Karhunen}-{Loeve} transform},
	volume = {16},
	number = {1},
	journal = {IEEE Sensors Journal},
	author = {Bavirisetti, Durga Prasad and Dhuli, Ravindra},
	year = {2015},
	note = {Publisher: IEEE},
	pages = {203--209},
}

@article{zheng_evaluation_2018,
	title = {Evaluation of {RGB}, {Color}-{Infrared} and {Multispectral} {Images} {Acquired} from {Unmanned} {Aerial} {Systems} for the {Estimation} of {Nitrogen} {Accumulation} in {Rice}},
	volume = {10},
	issn = {2072-4292},
	doi = {10.3390/rs10060824},
	number = {6},
	journal = {Remote Sensing},
	author = {Zheng, Hengbiao and Cheng, Tao and Li, Dong and Zhou, Xiang and Yao, Xia and Tian, Yongchao and Cao, Weixing and Zhu, Yan},
	month = may,
	year = {2018},
	note = {Publisher: MDPI AG},
	pages = {824},
}

@article{garcia_corn_2020,
	title = {Corn {Grain} {Yield} {Estimation} from {Vegetation} {Indices}, {Canopy} {Cover}, {Plant} {Density}, and a {Neural} {Network} {Using} {Multispectral} and {RGB} {Images} {Acquired} with {Unmanned} {Aerial} {Vehicles}},
	volume = {10},
	doi = {10.3390/agriculture10070277},
	journal = {Agriculture},
	author = {Garcia, Héctor and Flores, Hector and Ascencio-Hernández, Roberto and Khalil-Gardezi, Abdul and Tijerina-Chávez, Leonardo and Mancilla, Oscar and Peña, Vázquez Mario},
	month = jul,
	year = {2020},
}

@article{zhang_application_2012,
	title = {The application of small unmanned aerial systems for precision agriculture: a review},
	volume = {13},
	number = {6},
	journal = {Precision agriculture},
	author = {Zhang, Chunhua and Kovacs, John M},
	year = {2012},
	note = {Publisher: Springer},
	pages = {693--712},
}

@article{liu_estimates_2018,
	title = {Estimates of rice lodging using indices derived from {UAV} visible and thermal infrared images},
	volume = {252},
	journal = {Agricultural and Forest Meteorology},
	author = {Liu, Tao and Li, Rui and Zhong, Xiaochun and Jiang, Min and Jin, Xiuliang and Zhou, Ping and Liu, Shengping and Sun, Chengming and Guo, Wenshan},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {144--154},
}

@article{barrero_rgb_2018,
	title = {{RGB} and multispectral {UAV} image fusion for {Gramineae} weed detection in rice fields},
	volume = {19},
	number = {5},
	journal = {Precision Agriculture},
	author = {Barrero, Oscar and Perdomo, Sammy A},
	year = {2018},
	note = {Publisher: Springer},
	pages = {809--822},
}

@article{pablo_j_zarco-tejada_precision_2014,
	title = {Precision agriculture: an opportunity for {EU} farmers: potential support with the {CAP} 2014-2020},
	abstract = {Precision Agriculture (PA) is a whole-farm management approach using information technology, satellite positioning (GNSS) data, remote sensing and proximal data gathering. These technologies have the goal of optimising returns on inputs whilst potentially reducing environmental impacts. The state-of-the-art of PA on arable land, permanent crops and within dairy farming are reviewed, mainly in the European context, together with some economic aspects of the adoption of PA.},
	language = {en},
	author = {Pablo J. Zarco-Tejada, Neil Hubbard and Loudjani, Philippe},
	year = {2014},
	pages = {56},
}

@article{tang_high-accuracy_2020,
	title = {High-accuracy, high-resolution downwash flow field measurements of an unmanned helicopter for precision agriculture},
	volume = {173},
	journal = {Computers and Electronics in Agriculture},
	author = {Tang, Qing and Zhang, Ruirui and Chen, Liping and Xu, Gang and Deng, Wei and Ding, Chenchen and Xu, Min and Yi, Tongchuan and Wen, Yao and Li, Longlong},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {105390},
}

@article{maes_perspectives_2019,
	title = {Perspectives for remote sensing with unmanned aerial vehicles in precision agriculture},
	volume = {24},
	number = {2},
	journal = {Trends in plant science},
	author = {Maes, Wouter H and Steppe, Kathy},
	year = {2019},
	note = {Publisher: Elsevier},
	pages = {152--164},
}

@article{lu_recent_2020,
	title = {Recent {Advances} of {Hyperspectral} {Imaging} {Technology} and {Applications} in {Agriculture}},
	volume = {12},
	number = {16},
	journal = {Remote Sensing},
	author = {Lu, Bing and Dao, Phuong D and Liu, Jiangui and He, Yuhong and Shang, Jiali},
	year = {2020},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {2659},
}

@inproceedings{de_villiers_centi-pixel_2008,
	title = {Centi-pixel accurate real-time inverse distortion correction},
	volume = {7266},
	doi = {10.1117/12.804771},
	booktitle = {Optomechatronic {Technologies} 2008},
	publisher = {SPIE},
	author = {de Villiers, Jason P. and Leuschner, F. Wilhelm and Geldenhuys, Ronelle},
	editor = {Wen, John T. and Hodko, Dalibor and Otani, Yukitoshi and Kofman, Jonathan and Kaynak, Okyay},
	year = {2008},
	note = {Backup Publisher: International Society for Optics and Photonics},
	keywords = {Inverse distortion, distortion characterization, numerical optimization, real-time},
	pages = {320 -- 327},
}

@article{sanchez_robust_2020,
	title = {Robust normal vector estimation in {3D} point clouds through iterative principal component analysis},
	volume = {163},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271620300575},
	doi = {10.1016/j.isprsjprs.2020.02.018},
	abstract = {This paper introduces a robust normal vector estimator for point cloud data. It can handle sharp features as well as smooth areas. Our method is based on the inclusion of a robust estimator into a Principal Component Analysis in the neighborhood of the studied point, so that it can detect and reject outliers automatically during the estimation. A projection process ensures robustness against noise. Two automatic initializations are computed, leading to independent optimizations making the algorithm robust to neighborhood anisotropy around sharp features. An evaluation has been carried out in which the algorithm is compared to state-of-the-art methods. The results show that it is more robust against low and/or non-uniform samplings, high noise levels and outliers. Moreover, our algorithm is fast relative to existing methods handling sharp features. The code is available on the website: https://projet.liris.cnrs.fr/pcr/, and integrated in the platform: https://github.com/MEPP-team/MEPP2.},
	language = {en},
	urldate = {2022-09-01},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Sanchez, Julia and Denis, Florence and Coeurjolly, David and Dupont, Florent and Trassoudaine, Laurent and Checchin, Paul},
	month = may,
	year = {2020},
	keywords = {M-estimator, Normal vector, Point cloud, Sharp features, Weighted PCA},
	pages = {18--35},
}

@article{zhang_data-driven_2014,
	title = {Data-driven synthetic modeling of trees},
	volume = {20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905277882&doi=10.1109%2fTVCG.2014.2316001&partnerID=40&md5=34e3232c586d4200ae937edb6991f251},
	doi = {10.1109/TVCG.2014.2316001},
	abstract = {In this paper, we develop a data-driven technique to model trees from a single laser scan. A multi-layer representation of the tree structure is proposed to guide the modeling process. In this process, a marching cylinder algorithm is first developed to construct visible branches from the laser scan data. Three levels of crown feature points are then extracted from the scan data to synthesize three layers of non-visible branches. Based on the hierarchical particle flow technique, the branch synthesis method has the advantage of producing visually convincing tree models that are consistent with scan data. User intervention is extremely limited. The robustness of this technique has been validated on both conifer and broadleaf trees. © 1995-2012 IEEE.},
	number = {9},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Zhang, X. and Li, H. and Dai, M. and Ma, W. and Quan, L.},
	year = {2014},
	pages = {1214--1226},
}

@inproceedings{forstner_efficient_2017,
	title = {Efficient and {Accurate} {Registration} of {Point} {Clouds} with {Plane} to {Plane} {Correspondences}},
	doi = {10.1109/iccvw.2017.253},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCVW})},
	publisher = {IEEE},
	author = {Forstner, Wolfgang and Khoshelham, Kourosh},
	month = oct,
	year = {2017},
}

@article{cha_comprehensive_2007,
	title = {Comprehensive {Survey} on {Distance}/{Similarity} {Measures} {Between} {Probability} {Density} {Functions}},
	volume = {1},
	journal = {Int. J. Math. Model. Meth. Appl. Sci.},
	author = {Cha, Sung-Hyuk},
	month = jan,
	year = {2007},
}

@article{shewchuk_delaunay_2002,
	title = {Delaunay refinement algorithms for triangular mesh generation},
	volume = {22},
	issn = {0925-7721},
	doi = {https://doi.org/10.1016/S0925-7721(01)00047-5},
	number = {1},
	journal = {Computational Geometry},
	author = {Shewchuk, Jonathan Richard},
	year = {2002},
	pages = {21--74},
}

@article{martinez-carricondo_assessment_2018,
	title = {Assessment of {UAV}-photogrammetric mapping accuracy based on variation of ground control points},
	volume = {72},
	doi = {10.1016/j.jag.2018.05.015},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Martínez-Carricondo, Patricio and Agüera-Vega, Francisco and Carvajal-Ramírez, Fernando and Mesas-Carrascosa, Francisco-Javier and García-Ferrer, Alfonso and Pérez-Porras, Fernando-Juan},
	month = oct,
	year = {2018},
	note = {Publisher: Elsevier BV},
	pages = {1--10},
}

@article{yager_generalized_2009,
	title = {On generalized {Bonferroni} mean operators for multi-criteria aggregation},
	volume = {50},
	issn = {0888-613X},
	doi = {https://doi.org/10.1016/j.ijar.2009.06.004},
	number = {8},
	journal = {International Journal of Approximate Reasoning},
	author = {Yager, Ronald R.},
	year = {2009},
	pages = {1279--1286},
}

@article{bustince_definition_2017,
	title = {On the definition of penalty functions in data aggregation},
	volume = {323},
	doi = {10.1016/j.fss.2016.09.011},
	journal = {Fuzzy Sets and Systems},
	author = {Bustince, Humberto and Beliakov, Gleb and Dimuro, Graçaliz Pereira and Bedregal, Benjamín and Mesiar, Radko},
	month = sep,
	year = {2017},
	note = {Publisher: Elsevier BV},
	pages = {1--18},
}

@article{zhou_aggregating_2010,
	title = {On aggregating uncertain information by type-2 {OWA} operators for soft decision making},
	url = {https://doi.org/10.1002/int.20420},
	doi = {10.1002/int.20420},
	journal = {International Journal of Intelligent Systems},
	author = {Zhou, Shang-Ming and John, Robert I. and Chiclana, Francisco and Garibaldi, Jonathan M.},
	year = {2010},
	note = {Publisher: Wiley},
	pages = {n/a--n/a},
}

@inproceedings{paternain_color_2012,
	title = {Color image reduction by minimizing penalty functions},
	doi = {10.1109/fuzz-ieee.2012.6250794},
	booktitle = {2012 {IEEE} {International} {Conference} on {Fuzzy} {Systems}},
	publisher = {IEEE},
	author = {Paternain, Daniel and Jurio, Aranzazu and Beliakov, Gleb},
	month = jun,
	year = {2012},
	pages = {1--7},
}

@incollection{bustince_penalty_2017,
	title = {Penalty {Function} in {Optimization} {Problems}: {A} {Review} of {Recent} {Developments}},
	booktitle = {Soft {Computing} {Based} {Optimization} and {Decision} {Models}},
	publisher = {Springer International Publishing},
	author = {Bustince, Humberto and Fernandez, Javier and Burillo, Pedro},
	month = aug,
	year = {2017},
	doi = {10.1007/978-3-319-64286-4_17},
	pages = {275--287},
}

@inproceedings{mallon_precise_2004,
	title = {Precise radial un-distortion of images},
	volume = {1},
	doi = {10.1109/ICPR.2004.1333995},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Pattern} {Recognition}, 2004. {ICPR} 2004.},
	author = {Mallon, J. and Whelan, P. F.},
	month = aug,
	year = {2004},
	note = {ISSN: 1051-4651},
	pages = {18--21 Vol.1},
}

@article{sanz-ablanedo_accuracy_2018,
	title = {Accuracy of {Unmanned} {Aerial} {Vehicle} ({UAV}) and {SfM} {Photogrammetry} {Survey} as a {Function} of the {Number} and {Location} of {Ground} {Control} {Points} {Used}},
	volume = {10},
	issn = {2072-4292},
	doi = {10.3390/rs10101606},
	number = {10},
	journal = {Remote Sensing},
	author = {Sanz-Ablanedo, Enoc and Chandler, Jim H. and Rodríguez-Pérez, José Ramón and Ordóñez, Celestino},
	year = {2018},
}

@article{jakob_optimizing_2021,
	title = {Optimizing {LBVH}-{Construction} and {Hierarchy}-{Traversal} to accelerate {kNN} {Queries} on {Point} {Clouds} using the {GPU}},
	volume = {40},
	doi = {https://doi.org/10.1111/cgf.14177},
	number = {1},
	journal = {Computer Graphics Forum},
	author = {Jakob, J. and Guthe, M.},
	year = {2021},
	keywords = {acceleration structures, bounding volume hierarchies, bvh optimization, nearest neighbour, radius queries},
	pages = {124--137},
}

@article{connor_fast_2010,
	title = {Fast construction of k-nearest neighbor graphs for point clouds},
	volume = {16},
	doi = {10.1109/TVCG.2010.9},
	number = {4},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Connor, M. and Kumar, P.},
	year = {2010},
	pages = {599--608},
}

@article{nurunnabi_robust_2014,
	title = {Robust statistical approaches for local planar surface fitting in {3D} laser scanning data},
	volume = {96},
	issn = {0924-2716},
	doi = {https://doi.org/10.1016/j.isprsjprs.2014.07.004},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Nurunnabi, Abdul and Belton, David and West, Geoff},
	year = {2014},
	pages = {106--122},
}

@book{minkina_infrared_2009,
	title = {Infrared {Thermography}: {Errors} and {Uncertainties}},
	isbn = {978-0-470-68224-1},
	publisher = {Wiley},
	author = {Minkina, W. and Dudzik, S.},
	year = {2009},
}

@article{vidas_real-time_2015,
	title = {Real-{Time} {Mobile} {3D} {Temperature} {Mapping}},
	volume = {15},
	doi = {10.1109/JSEN.2014.2360709},
	number = {2},
	journal = {IEEE Sensors Journal},
	author = {Vidas, S. and Moghadam, P. and Sridharan, S.},
	year = {2015},
	pages = {1145--1152},
}

@article{bian_simplified_2019,
	title = {Simplified {Evaluation} of {Cotton} {Water} {Stress} {Using} {High} {Resolution} {Unmanned} {Aerial} {Vehicle} {Thermal} {Imagery}},
	volume = {11},
	issn = {2072-4292},
	doi = {10.3390/rs11030267},
	number = {3},
	journal = {Remote Sensing},
	author = {Bian, Jiang and Zhang, Zhitao and Chen, Junying and Chen, Haiying and Cui, Chenfeng and Li, Xianwen and Chen, Shuobo and Fu, Qiuping},
	year = {2019},
}

@inproceedings{behley_efficient_2015,
	title = {Efficient radius neighbor search in three-dimensional point clouds},
	doi = {10.1109/ICRA.2015.7139702},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Behley, J. and Steinhage, V. and Cremers, A. B.},
	year = {2015},
	pages = {3625--3630},
}

@article{feito_orientation_1995,
	title = {Orientation, simplicity, and inclusion test for planar polygons},
	volume = {19},
	issn = {0097-8493},
	doi = {https://doi.org/10.1016/0097-8493(95)00037-D},
	number = {4},
	journal = {Computers and Graphics},
	author = {Feito, F. and Torres, J. C. and Ureña, A.},
	year = {1995},
	pages = {595--600},
}

@book{vollmer_advanced_2017,
	title = {Advanced {Methods} in {IR} {Imaging}},
	isbn = {978-3-527-69330-6},
	publisher = {John Wiley and Sons, Ltd},
	author = {Vollmer, Michael and Möllmann, Klaus‐Peter},
	year = {2017},
	doi = {https://doi.org/10.1002/9783527693306},
	note = {Publication Title: Infrared Thermal Imaging},
}

@article{rodriguez-gonzalvez_mobile_2017,
	title = {Mobile {LiDAR} {System}: {New} {Possibilities} for the {Documentation} and {Dissemination} of {Large} {Cultural} {Heritage} {Sites}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Mobile {LiDAR} {System}},
	url = {https://www.mdpi.com/2072-4292/9/3/189},
	doi = {10.3390/rs9030189},
	abstract = {Mobile LiDAR System is an emerging technology that combines multiple sensors. Active sensors, together with Inertial and Global Navigation System, are synchronized on a mobile platform to produce an accurate and precise geospatial 3D point cloud. They allow obtaining a large amount of georeferenced 3D information in a fast and efficient way, which can be used in several applications such as the 3D recording and reconstruction of complex urban areas and/or landscapes. In this study the Mobile LiDAR System is applied in the field of Cultural Heritage aiming to evaluate its performance with the purpose to document, divulgate, or to develop an architectural analysis. This study was focused on the Medieval Wall of Avila (Spain) and, specifically, the performed accuracy tests were applied in the “Alcazar” gate (National Monument from 1884). The Mobile LiDAR System is then compared to the most commonly employed active sensors (Terrestrial Laser Scanner) for large Cultural Heritage sites in regard to time, accuracy and resolution of the point cloud. The discrepancies between both technologies are established comparing directly the 3D point clouds generated, highlighting the errors affecting the architectural structures. Consequently, and based on a detailed geometrical analysis, an optimization methodology is proposed, establishing a segmented and classified cluster for the structures. Furthermore, three main clusters are settled, according to the curvature: (i) planar or low curvature; (ii) cylindrical, mild transitions and medium curvature; and (iii) the abrupt transitions of high curvature. The obtained 3D point clouds in each cluster are analyzed and optimized, considering the reference spatial sampling, according to a confidence interval and the feature curvature. The presented results suggest that Mobile LiDAR System is an optimal approach, allowing a high-speed data acquisition and providing an adequate accuracy for large Cultural Heritage sites.},
	language = {en},
	number = {3},
	urldate = {2021-11-09},
	journal = {Remote Sensing},
	author = {Rodríguez-Gonzálvez, Pablo and Jiménez Fernández-Palacios, Belén and Muñoz-Nieto, Ángel Luis and Arias-Sanchez, Pedro and Gonzalez-Aguilera, Diego},
	month = mar,
	year = {2017},
	keywords = {accuracy assessment, cultural heritage, mobile LiDAR system, optimization, point cloud, terrestrial laser scanner},
	pages = {189},
}

@article{cheng_simple_2007,
	title = {Simple reconstruction of tree branches from a single range image},
	volume = {22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-36549073566&doi=10.1007%2fs11390-007-9095-6&partnerID=40&md5=bd0c39dac87150c2796b9608b7e03c12},
	doi = {10.1007/s11390-007-9095-6},
	abstract = {3D modeling of trees in real environments is a challenge in computer graphics and computer vision, since the geometric shape and topological structure of trees are more complex than conventional artificial objects. In this paper, we present a multi-process approach that is mainly performed in 2D space to faithfully construct a 3D model of the trunk and main branches of a real tree from a single range image. The range image is first segmented into patches by jump edge detection based on depth discontinuity. Coarse skeleton points and initial radii are then computed from the contour of each patch. Axis directions are estimated using cylinder fitting in the neighborhood of each coarse skeleton point. With the help of axis directions, skeleton nodes and corresponding radii are computed. Finally, these skeleton nodes are hierarchically connected, and improper radii are modified based on plant knowledge. 3D models generated from single range images of real trees demonstrate the effectiveness of our method. The main contributions of this paper are simple reconstruction by virtue of image storage order of single scan and skeleton computation based on axis directions. © 2007 Science Press, Beijing, China and Springer Science + Business Media, LLC, USA.},
	number = {6},
	journal = {Journal of Computer Science and Technology},
	author = {Cheng, Z.-L. and Zhang, X.-P. and Chen, B.-Q.},
	year = {2007},
	keywords = {\_tablet},
	pages = {846--858},
}

@article{barrios_performance_2022,
	title = {Performance {Assessment} of the {CCSDS}-123 {Standard} for {Panchromatic} {Video} {Compression} on {Space} {Missions}},
	volume = {19},
	issn = {1558-0571},
	doi = {10.1109/LGRS.2021.3099032},
	abstract = {The use of next-generation and high-resolution imaging sensors is gaining interest for space missions, because of their properties for identification and exploration purposes. It is expected that the demand for video sensors in the space industry will increase during the next years, mainly for monitoring and exploration missions. In this context, onboard video compression techniques emerge as mandatory, because memory resources are available on space missions and the downlink bandwidth with ground prevents for the transmission of raw video, especially when near real-time capabilities are required. The complexity of the current commercial video encoders supposes a challenge for their implementation on the hardware used on space missions, because of their high architectural complexity and computational requirements, deriving in an area overhead and an unacceptable power consumption. In this context, we propose the use of the CCSDS-123.0-B-2 standard, originally focused on the near-lossless on-board compression of multispectral and hyperspectral images, for compressing panchromatic video. The proposed solution presents low complexity, because it is specifically designed for working on space missions and also has the capability of compressing both 3-D images and panchromatic video by using a single processing core. Results demonstrate that this solution achieves high compression ratios, maintaining a considerable video quality after reconstruction on ground and achieving a fine-grain control of the losses introduced in the compression chain.},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Barrios, Yubal and Guerra, Raúl and López, Sebastián and Sarmiento, Roberto},
	year = {2022},
	keywords = {Compression algorithms, Consultative Committee for Space Data Systems (CCSDS), Hyperspectral imaging, Image coding, Sensors, Space missions, Standards, Video compression, Video sequences, hyperspectral imaging, on-board processing, panchromatic video compression, space missions},
	pages = {1--5},
}

@article{lintermann_interactive_1999,
	title = {Interactive modeling of plants},
	volume = {19},
	issn = {1558-1756},
	doi = {10.1109/38.736469},
	abstract = {We present a rule-based approach combined with traditional geometric modelling techniques that allows easy generation of many branching objects including flowers, bushes, trees, and even nonbotanical objects. A set of components describing structural and geometrical elements of plants maps to a graph that forms the description of a specific plant and generates the geometry. Users get immediate feedback on what they've created-geometrical parameters, tropisms, and free-form deformations can control the overall shape of a plant. We demonstrate that our method handles the complexity of most real plants.},
	number = {1},
	journal = {IEEE Computer Graphics and Applications},
	author = {Lintermann, B. and Deussen, O.},
	month = jan,
	year = {1999},
	keywords = {Feedback, Fractals, Geometry, Interactive systems, Power generation, Power system modeling, Prototypes, Solid modeling, Spline, Tree graphs},
	pages = {56--65},
}

@article{riviere_multispectral_2012,
	title = {Multispectral polarized {BRDF}: {Design} of a highly resolved reflectometer and development of a data inversion method},
	volume = {42},
	shorttitle = {Multispectral polarized {BRDF}},
	doi = {10.5277/oa120101},
	abstract = {Multispectral and polarized light reflectance measurements are very useful to characterize materials such as paint coatings. This article presents an overview of an automated high-angular resolved, in-plane multispectral polarized reflectometer and its calibration process. A compre-hensive study based on multispectral BRDF and DOLP measurements is conducted on different colour and glossy aspects of paint coatings. An original inverse method from in-plane measurements is used to model the out-of-plane BRDF and to investigate the role of the surface and subsurface scattering phenomena in its components.},
	journal = {Optica Applicata},
	author = {Riviere, Nicolas and Ceolato, Romain and Hespel, Laurent},
	month = jan,
	year = {2012},
}

@article{bienert_comparison_2018,
	title = {Comparison and {Combination} of {Mobile} and {Terrestrial} {Laser} {Scanning} for {Natural} {Forest} {Inventories}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1999-4907/9/7/395},
	doi = {10.3390/f9070395},
	abstract = {Terrestrial laser scanning (TLS) has been successfully used for three-dimensional (3D) data capture in forests for almost two decades. Beyond the plot-based data capturing capabilities of TLS, vehicle-based mobile laser scanning (MLS) systems have the clear advantage of fast and precise corridor-like 3D data capture, thus providing a much larger coverage within shorter acquisition time. This paper compares and discusses advantages and disadvantages of multi-temporal MLS data acquisition compared to established TLS data recording schemes. In this pilot study on integrated TLS and MLS data processing in a forest, it could be shown that existing TLS data evaluation routines can be used for MLS data processing. Methods of automatic laser scanner data processing for forest inventory parameter determination and quantitative structure model (QSM) generation were tested in two sample plots using data from both scanning methods and from different seasons. TLS in a multi-scan configuration delivers very high-density 3D point clouds, which form a valuable basis for generating high-quality QSMs. The pilot study shows that MLS is able to provide high-quality data for an equivalent determination of relevant forest inventory parameters compared to TLS. Parameters such as tree position, diameter at breast height (DBH) or tree height can be determined from MLS data with an accuracy similar to the accuracy of the parameter derived from TLS data. Results for instance in DBH determination by cylinder fitting yielded a standard deviation of 1.1 cm for trees in TLS data and 3.7 cm in MLS data. However, the resolution of MLS scans was found insufficient for successful QSM generation. The registration of MLS data in forests furthermore requires additional effort in considering effects caused by poor GNSS signal.},
	language = {en},
	number = {7},
	urldate = {2021-11-09},
	journal = {Forests},
	author = {Bienert, Anne and Georgi, Louis and Kunz, Matthias and Maas, Hans-Gerd and Von Oheimb, Goddert},
	month = jul,
	year = {2018},
	keywords = {crown projection area, diameter at breast height, forest inventory parameters, quantitative structure models, wood volume},
	pages = {395},
}

@article{jurado_out--core_2022,
	title = {An out-of-core method for {GPU} image mapping on large {3D} scenarios of the real world},
	volume = {134},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000978},
	doi = {10.1016/j.future.2022.03.022},
	abstract = {Image mapping on 3D huge scenarios of the real world is one of the most fundamental and computational expensive processes for the integration of multi-source sensing data. Recent studies focused on the observation and characterization of Earth have been enhanced by the proliferation of Unmanned Aerial Vehicle (UAV) and sensors able to capture massive datasets with a high spatial resolution. Despite the advances in manufacturing new cameras and versatile platforms, only a few methods have been developed to characterize the study area by fusing heterogeneous data such as thermal, multispectral or hyperspectral images with high-resolution 3D models. The main reason for this lack of solutions is the challenge to integrate multi-scale datasets and high computational efforts required for image mapping on dense and complex geometric models. In this paper, we propose an efficient pipeline for multi-source image mapping on huge 3D scenarios. Our GPU-based solution significantly reduces the run time and allows us to generate enriched 3D models on-site. The proposed method is out-of-core and it uses available resources of the GPU’s machine to perform two main tasks: (i) image mapping and (ii) occlusion testing. We deploy highly-optimized GPU-kernels for image mapping and detection of self-hidden geometry in the 3D model, as well as a GPU-based parallelization to manage the 3D model considering several spatial partitions according to the GPU capabilities. Our method has been tested on 3D scenarios with different point cloud densities (66M, 271M, 542M) and two sets of multispectral images collected by two drone flights. We focus on launching the proposed method on three platforms: (i) System on a Chip (SoC), (ii) a user-grade laptop and (iii) a PC. The results demonstrate the method’s capabilities in terms of performance and versatility to be computed by commodity hardware. Thus, taking advantage of GPUs, this method opens the door for embedded and edge computing devices for 3D image mapping on large-scale scenarios in near real-time.},
	language = {en},
	urldate = {2022-09-01},
	journal = {Future Generation Computer Systems},
	author = {Jurado, Juan M. and Padrón, Emilio J. and Jiménez, J. Roberto and Ortega, Lidia},
	month = sep,
	year = {2022},
	keywords = {3D model, GPGPU, GPU, Heterogeneous data fusion, Image mapping, Multi-source data fusion, Parallel computing},
	pages = {66--77},
}

@article{yang_dom_2017,
	title = {The {DOM} {Generation} and {Precise} {Radiometric} {Calibration} of a {UAV}-{Mounted} {Miniature} {Snapshot} {Hyperspectral} {Imager}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/9/7/642},
	doi = {10.3390/rs9070642},
	abstract = {Hyperspectral remote sensing is used in precision agriculture to remotely and quickly acquire crop phenotype information. This paper describes the generation of a digital orthophoto map (DOM) and radiometric calibration for images taken by a miniaturized snapshot hyperspectral camera mounted on a lightweight unmanned aerial vehicle (UAV). The snapshot camera is a relatively new type of hyperspectral sensor that can acquire an image cube with one spectral and two spatial dimensions at one exposure. The images acquired by the hyperspectral snapshot camera need to be mosaicked together to produce a DOM and radiometrically calibrated before analysis. However, the spatial resolution of hyperspectral cubes is too low to mosaic the images together. Furthermore, there are no systematic radiometric calibration methods or procedures for snapshot hyperspectral images acquired from low-altitude carrier platforms. In this study, we obtained hyperspectral imagery using a snapshot hyperspectral sensor mounted on a UAV. We quantitatively evaluated the radiometric response linearity (RRL) and radiometric response variation (RRV) and proposed a method to correct the RRV effect. We then introduced a method to interpolate position and orientation system (POS) information and generate a DOM with low spatial resolution and a digital elevation model (DEM) using a 3D mesh model built from panchromatic images with high spatial resolution. The relative horizontal geometric precision of the DOM was validated by comparison with a DOM generated from a digital RGB camera. A surface crop model (CSM) was produced from the DEM, and crop height for 48 sampling plots was extracted and compared with the corresponding field-measured crop height to verify the relative precision of the DEM. Finally, we applied two absolute radiometric calibration methods to the generated DOM and verified their accuracy via comparison with spectra measured with an ASD Field Spec Pro spectrometer (Analytical Spectral Devices, Boulder, CO, USA). The DOM had high relative horizontal accuracy, and compared with the digital camera-derived DOM, spatial differences were below 0.05 m (RMSE = 0.035). The determination coefficient for a regression between DEM-derived and field-measured crop height was 0.680. The radiometric precision was 5\% for bands between 500 and 945 nm, and the reflectance curve in the infrared spectral region did not decrease as in previous research. The pixel and data sizes for the DOM corresponding to a field area of approximately 85 m × 34 m were small (0.67 m and approximately 13.1 megabytes, respectively), which is convenient for data transmission, preprocessing and analysis. The proposed method for radiometric calibration and DOM generation from hyperspectral cubes can be used to yield hyperspectral imagery products for various applications, particularly precision agriculture.},
	language = {en},
	number = {7},
	urldate = {2022-03-26},
	journal = {Remote Sensing},
	author = {Yang, Guijun and Li, Changchun and Wang, Yanjie and Yuan, Huanhuan and Feng, Haikuan and Xu, Bo and Yang, Xiaodong},
	month = jul,
	year = {2017},
	keywords = {POS interpolation, UAV, hyperspectral DOM, radiometric calibration, radiometric response variation, snapshot hyperspectral imaging},
	pages = {642},
}

@book{pu_hyperspectral_2017,
	address = {Boca Raton},
	title = {Hyperspectral {Remote} {Sensing}: {Fundamentals} and {Practices}},
	isbn = {978-1-315-12060-7},
	shorttitle = {Hyperspectral {Remote} {Sensing}},
	abstract = {Advanced imaging spectral technology and hyperspectral analysis techniques for multiple applications are the key features of the book. This book will present in one volume complete solutions from concepts, fundamentals, and methods of acquisition of hyperspectral data to analyses and applications of the data in a very coherent manner. It will help readers to fully understand basic theories of HRS, how to utilize various field spectrometers and bioinstruments, the importance of radiometric correction and atmospheric correction, the use of analysis, tools and software, and determine what to do with HRS technology and data.},
	publisher = {CRC Press},
	author = {Pu, Ruiliang},
	month = aug,
	year = {2017},
	doi = {10.1201/9781315120607},
}

@article{argudo_single-picture_2016,
	title = {Single-picture reconstruction and rendering of trees for plausible vegetation synthesis},
	volume = {57},
	issn = {0097-8493},
	url = {http://www.sciencedirect.com/science/article/pii/S0097849316300188},
	doi = {10.1016/j.cag.2016.03.005},
	abstract = {State-of-the-art approaches for tree reconstruction either put limiting constraints on the input side (requiring multiple photographs, a scanned point cloud or intensive user input) or provide a representation only suitable for front views of the tree. In this paper we present a complete pipeline for synthesizing and rendering detailed trees from a single photograph with minimal user effort. Since the overall shape and appearance of each tree is recovered from a single photograph of the tree crown, artists can benefit from georeferenced images to populate landscapes with native tree species. A key element of our approach is a compact representation of dense tree crowns through a radial distance map. Our first contribution is an automatic algorithm for generating such representations from a single exemplar image of a tree. We create a rough estimate of the crown shape by solving a thin-plate energy minimization problem, and then add detail through a simplified shape-from-shading approach. The use of seamless texture synthesis results in an image-based representation that can be rendered from arbitrary view directions at different levels of detail. Distant trees benefit from an output-sensitive algorithm inspired on relief mapping. For close-up trees we use a billboard cloud where leaflets are distributed inside the crown shape through a space colonization algorithm. In both cases our representation ensures efficient preservation of the crown shape. Major benefits of our approach include: it recovers the overall shape from a single tree image, involves no tree modeling knowledge and minimal authoring effort, and the associated image-based representation is easy to compress and thus suitable for network streaming.},
	language = {en},
	urldate = {2020-02-05},
	journal = {Computers \& Graphics},
	author = {Argudo, Oscar and Chica, Antonio and Andujar, Carlos},
	month = jun,
	year = {2016},
	keywords = {Tree reconstruction, Tree rendering, Vegetation synthesis, \_tablet, ★},
	pages = {55--67},
}

@incollection{rodkaew_particle_2003,
	address = {Beijing},
	edition = {B.-G. Hu and M. Jaeger},
	title = {Particle {Systems} for {Plant} {Modeling}},
	abstract = {This paper presents a new algorithm for modeling plant structures. The main motivation of this algorithm is based on certain natural phenomena. A property of the plant structure is to transport and exchange energy, water and sustenance between roots, branches and leaves. The plant structures should be suitable for efficient transportation. The algorithm employs Particle Systems. The algorithm is initiated by randomly scattering particles inside a given shape. Each particle contains energy. The transportation rule directs each particle toward a target. When particles are in close proximity, they are combined. The trails of moving particles are used to reconstruct plant structures. In addition, the light effect from an environment is incorporated. The algorithm is effective, it has been tested with various shapes. It is computationally efficient, and has only a few parameters. The resulting images are realistic.},
	language = {en},
	booktitle = {Plant growth modeling and applications. {Proceedings} {PMA03}.},
	publisher = {Tsinghua University Press and Springer},
	author = {Rodkaew, Yodthong and Chongstitvatana, Prabhas and Siripant, Suchada and Lursinsap, Chidchanok},
	year = {2003},
	pages = {210--217},
}

@article{lucieer_hyperuasimaging_2014,
	title = {{HyperUAS}—{Imaging} {Spectroscopy} from a {Multirotor} {Unmanned} {Aircraft} {System}},
	volume = {31},
	issn = {1556-4967},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21508},
	doi = {10.1002/rob.21508},
	abstract = {One of the key advantages of a low-flying unmanned aircraft system (UAS) is its ability to acquire digital images at an ultrahigh spatial resolution of a few centimeters. Remote sensing of quantitative biochemical and biophysical characteristics of small-sized spatially fragmented vegetation canopies requires, however, not only high spatial, but also high spectral (i.e., hyperspectral) resolution. In this paper, we describe the design, development, airborne operations, calibration, processing, and interpretation of image data collected with a new hyperspectral unmanned aircraft system (HyperUAS). HyperUAS is a remotely controlled multirotor prototype carrying onboard a lightweight pushbroom spectroradiometer coupled with a dual frequency GPS and an inertial movement unit. The prototype was built to remotely acquire imaging spectroscopy data of 324 spectral bands (162 bands in a spectrally binned mode) with bandwidths between 4 and 5 nm at an ultrahigh spatial resolution of 2–5 cm. Three field airborne experiments, conducted over agricultural crops and over natural ecosystems of Antarctic mosses, proved operability of the system in standard field conditions, but also in a remote and harsh, low-temperature environment of East Antarctica. Experimental results demonstrate that HyperUAS is capable of delivering georeferenced maps of quantitative biochemical and biophysical variables of vegetation and of actual vegetation health state at an unprecedented spatial resolution of 5 cm.},
	language = {en},
	number = {4},
	urldate = {2022-11-04},
	journal = {Journal of Field Robotics},
	author = {Lucieer, Arko and Malenovský, Zbyněk and Veness, Tony and Wallace, Luke},
	year = {2014},
	pages = {571--590},
}

@article{lopez_modeling_2010,
	title = {Modeling {Complex} {Unfoliaged} {Trees} from a {Sparse} {Set} of {Images}},
	volume = {29},
	copyright = {© 2010 The Author(s) Journal compilation © 2010 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2010.01794.x},
	doi = {10.1111/j.1467-8659.2010.01794.x},
	abstract = {We present a novel image-based technique for modeling complex unfoliaged trees. Existing tree modeling tools either require capturing a large number of views for dense 3D reconstruction or rely on user inputs and botanic rules to synthesize natural-looking tree geometry. In this paper, we focus on faithfully recovering real instead of realistically-looking tree geometry from a sparse set of images. Our solution directly integrates 2D/3D tree topology as shape priors into the modeling process. For each input view, we first estimate a 2D skeleton graph from its matte image and then find a 2D skeleton tree from the graph by imposing tree topology. We develop a simple but effective technique for computing the optimal 3D skeleton tree most consistent with the 2D skeletons. For each edge in the 3D skeleton tree, we further apply volumetric reconstruction to recover its corresponding curved branch. Finally, we use piecewise cylinders to approximate each branch from the volumetric results. We demonstrate our framework on a variety of trees to illustrate the robustness and usefulness of our technique.},
	language = {en},
	number = {7},
	urldate = {2021-06-18},
	journal = {Computer Graphics Forum},
	author = {Lopez, Luis D. and Ding, Yuanyuan and Yu, Jingyi},
	year = {2010},
	keywords = {Applications, Computer, Graphics, Graphics:, I.3.7, Realism—Modeling, Three-Dimensional, \_tablet, and},
	pages = {2075--2082},
}

@article{schutz_rendering_2021,
	title = {Rendering {Point} {Clouds} with {Compute} {Shaders} and {Vertex} {Order} {Optimization}},
	volume = {40},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14345},
	doi = {10.1111/cgf.14345},
	abstract = {In this paper, we present several compute-based point cloud rendering approaches that outperform the hardware pipeline by up to an order of magnitude and achieve significantly better frame times than previous compute-based methods. Beyond basic closest-point rendering, we also introduce a fast, high-quality variant to reduce aliasing. We present and evaluate several variants of our proposed methods with different flavors of optimization, in order to ensure their applicability and achieve optimal performance on a range of platforms and architectures with varying support for novel GPU hardware features. During our experiments, the observed peak performance was reached rendering 796 million points (12.7GB) at rates of 62 to 64 frames per second (50 billion points per second, 802GB/s) on an RTX 3090 without the use of level-of-detail structures. We further introduce an optimized vertex order for point clouds to boost the efficiency of GL\_POINTS by a factor of 5× in cases where hardware rendering is compulsory. We compare different orderings and show that Morton sorted buffers are faster for some viewpoints, while shuffled vertex buffers are faster in others. In contrast, combining both approaches by first sorting according to Morton-code and shuffling the resulting sequence in batches of 128 points leads to a vertex buffer layout with high rendering performance and low sensitivity to viewpoint changes.},
	language = {en},
	number = {4},
	urldate = {2022-09-01},
	journal = {Computer Graphics Forum},
	author = {Schütz, Markus and Kerbl, Bernhard and Wimmer, Michael},
	year = {2021},
	keywords = {CCS Concepts, Computer Science - Graphics, • Computing methodologies → Rasterization},
	pages = {115--126},
}

@inproceedings{cao_point_2010,
	title = {Point cloud skeletons via laplacian based contraction},
	booktitle = {2010 {Shape} {Modeling} {International} {Conference}},
	publisher = {IEEE},
	author = {Cao, Junjie and Tagliasacchi, Andrea and Olson, Matt and Zhang, Hao and Su, Zhinxun},
	year = {2010},
	pages = {187--197},
}

@article{ramirez-paredes_low-altitude_2016,
	title = {Low-altitude {Terrestrial} {Spectroscopy} from a {Pushbroom} {Sensor}},
	volume = {33},
	issn = {1556-4967},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21624},
	doi = {10.1002/rob.21624},
	abstract = {Hyperspectral cameras sample many different spectral bands at each pixel, enabling advanced detection and classification algorithms. However, their limited spatial resolution and the need to measure the camera motion to create hyperspectral images makes them unsuitable for nonsmooth moving platforms such as unmanned aerial vehicles (UAVs). We present a procedure to build hyperspectral images from line sensor data without camera motion information or extraneous sensors. Our approach relies on an accompanying conventional camera to exploit the homographies between images for mosaic construction. We provide experimental results from a low-altitude UAV, achieving high-resolution spectroscopy with our system.},
	language = {en},
	number = {6},
	urldate = {2022-11-03},
	journal = {Journal of Field Robotics},
	author = {Ramirez-Paredes, Juan-Pablo and Lary, David J. and Gans, Nicholas R.},
	year = {2016},
	pages = {837--852},
}

@article{du_adtree_2019,
	title = {{AdTree}: {Accurate}, {Detailed}, and {Automatic} {Modelling} of {Laser}-{Scanned} {Trees}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {{AdTree}},
	url = {https://www.mdpi.com/2072-4292/11/18/2074},
	doi = {10.3390/rs11182074},
	abstract = {Laser scanning is an effective tool for acquiring geometric attributes of trees and vegetation, which lays a solid foundation for 3-dimensional tree modelling. Existing studies on tree modelling from laser scanning data are vast. However, some works cannot guarantee sufficient modelling accuracy, while some other works are mainly rule-based and therefore highly depend on user inputs. In this paper, we propose a novel method to accurately and automatically reconstruct detailed 3D tree models from laser scans. We first extract an initial tree skeleton from the input point cloud by establishing a minimum spanning tree using the Dijkstra shortest-path algorithm. Then, the initial tree skeleton is pruned by iteratively removing redundant components. After that, an optimization-based approach is performed to fit a sequence of cylinders to approximate the geometry of the tree branches. Experiments on various types of trees from different data sources demonstrate the effectiveness and robustness of our method. The overall fitting error (i.e., the distance between the input points and the output model) is less than 10 cm. The reconstructed tree models can be further applied in the precise estimation of tree attributes, urban landscape visualization, etc. The source code of this work is freely available at https://github.com/tudelft3d/adtree.},
	language = {en},
	number = {18},
	urldate = {2020-04-14},
	journal = {Remote Sensing},
	author = {Du, Shenglan and Lindenbergh, Roderik and Ledoux, Hugo and Stoter, Jantien and Nan, Liangliang},
	month = jan,
	year = {2019},
	keywords = {\_tablet, laser scanning, point cloud, precision forestry, tree modelling},
	pages = {2074},
}

@incollection{gobeawan_tree_2021,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Tree {Species} {Modelling} for {Digital} {Twin} {Cities}},
	isbn = {978-3-662-63170-6},
	url = {https://doi.org/10.1007/978-3-662-63170-6_2},
	abstract = {Creating vegetation contents for a digital twin city entails generating dynamic 3D plant models in a large scale to represent the actual vegetation in the city. To enable high-fidelity environmental simulations and analysis applications, we model individual trees at a species level of detail. The 3D models are generated procedurally based on their botanical species profiles within the constraints of measurements and growth spaces derived from laser-scanned point cloud data. Users can conveniently define the known profile of a species by using a species profile template that we formulated based on species growth processes and patterns. Based on the given species profile and solving for the unknowns within the growth space constraints, a species model will be grown through iterations of our formulated growth rules. We show that this methodology produces structurally-representative species models with respect to their actual physical and species characteristics.},
	language = {en},
	urldate = {2021-12-30},
	booktitle = {Transactions on {Computational} {Science} {XXXVIII}},
	publisher = {Springer},
	author = {Gobeawan, Like and Wise, Daniel J. and Wong, Sum Thai and Yee, Alex T. K. and Lim, Chi Wan and Su, Yi},
	editor = {Gavrilova, Marina L. and Tan, C.J. Kenneth},
	year = {2021},
	doi = {10.1007/978-3-662-63170-6_2},
	keywords = {Digital twins, Optimisation, Procedural modelling, Species growth, Tree modelling, Tree species architecture},
	pages = {17--35},
}

@article{li_learning_2021,
	title = {Learning to reconstruct botanical trees from single images},
	volume = {40},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3478513.3480525},
	doi = {10.1145/3478513.3480525},
	abstract = {We introduce a novel method for reconstructing the 3D geometry of botanical trees from single photographs. Faithfully reconstructing a tree from single-view sensor data is a challenging and open problem because many possible 3D trees exist that fit the tree's shape observed from a single view. We address this challenge by defining a reconstruction pipeline based on three neural networks. The networks simultaneously mask out trees in input photographs, identify a tree's species, and obtain its 3D radial bounding volume - our novel 3D representation for botanical trees. Radial bounding volumes (RBV) are used to orchestrate a procedural model primed on learned parameters to grow a tree that matches the main branching structure and the overall shape of the captured tree. While the RBV allows us to faithfully reconstruct the main branching structure, we use the procedural model's morphological constraints to generate realistic branching for the tree crown. This constraints the number of solutions of tree models for a given photograph of a tree. We show that our method reconstructs various tree species even when the trees are captured in front of complex backgrounds. Moreover, although our neural networks have been trained on synthetic data with data augmentation, we show that our pipeline performs well for real tree photographs. We evaluate the reconstructed geometries with several metrics, including leaf area index and maximum radial tree distances.},
	number = {6},
	urldate = {2021-12-26},
	journal = {ACM Transactions on Graphics},
	author = {Li, Bosheng and Kałużny, Jacek and Klein, Jonathan and Michels, Dominik L. and Pałubicki, Wojtek and Benes, Bedrich and Pirk, Sören},
	month = dec,
	year = {2021},
	keywords = {botanical tree models, bounding volumes, shape reconstruction, tree reconstruction},
	pages = {231:1--231:15},
}

@misc{noauthor_hyperspectral_nodate,
	title = {Hyperspectral {Remote} {Sensing} {Scenes} - {Grupo} de {Inteligencia} {Computacional} ({GIC})},
	url = {http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes#Pavia_Centre_and_University},
	urldate = {2022-05-11},
	note = {Publication Title: Hyperspectral Remote Sensing Scenes},
}

@article{liu_treepartnet_2021,
	title = {{TreePartNet}: neural decomposition of point clouds for {3D} tree reconstruction},
	volume = {40},
	issn = {0730-0301},
	shorttitle = {{TreePartNet}},
	url = {https://doi.org/10.1145/3478513.3480486},
	doi = {10.1145/3478513.3480486},
	abstract = {We present TreePartNet, a neural network aimed at reconstructing tree geometry from point clouds obtained by scanning real trees. Our key idea is to learn a natural neural decomposition exploiting the assumption that a tree comprises locally cylindrical shapes. In particular, reconstruction is a two-step process. First, two networks are used to detect priors from the point clouds. One detects semantic branching points, and the other network is trained to learn a cylindrical representation of the branches. In the second step, we apply a neural merging module to reduce the cylindrical representation to a final set of generalized cylinders combined by branches. We demonstrate results of reconstructing realistic tree geometry for a variety of input models and with varying input point quality, e.g., noise, outliers, and incompleteness. We evaluate our approach extensively by using data from both synthetic and real trees and comparing it with alternative methods.},
	number = {6},
	urldate = {2022-04-18},
	journal = {ACM Transactions on Graphics},
	author = {Liu, Yanchao and Guo, Jianwei and Benes, Bedrich and Deussen, Oliver and Zhang, Xiaopeng and Huang, Hui},
	month = dec,
	year = {2021},
	keywords = {3D reconstruction, deep learning, geometric modeling, optimization, procedural generation, procedural modeling},
	pages = {232:1--232:16},
}

@inproceedings{chaudhury_3d_2020,
	address = {Cham},
	title = {{3D} plant phenotyping: {All} you need is labelled point cloud data},
	isbn = {978-3-030-65414-6},
	abstract = {In the realm of modern digital phenotyping technological advancements, the demand of annotated datasets is increasing for either training machine learning algorithms or evaluating 3D phenotyping systems. While a few 2D datasets have been proposed in the community in last few years, very little attention has been paid to the construction of annotated 3D point cloud datasets. There are several challenges associated with the creation of such annotated datasets. Acquiring the data requires instruments having good precision and accuracy levels. Reconstruction of full 3D model from multiple views is a challenging task considering plant architecture complexity and plasticity, as well as occlusion and missing data problems. In addition, manual annotation of the data is a cumbersome task that cannot easily be automated. In this context, the design of synthetic datasets can play an important role. In this paper, we propose an idea of automatic generation of synthetic point cloud data using virtual plant models. Our approach leverages the strength of the classical procedural approach (like L-systems) to generate the virtual models of plants, and then perform point sampling on the surface of the models. By applying stochasticity in the procedural model, we are able to generate large number of diverse plant models and the corresponding point cloud data in a fully automatic manner. The goal of this paper is to present a general strategy to generate annotated 3D point cloud datasets from virtual models. The code (along with some generated point cloud models) are available at: https://gitlab.inria.fr/mosaic/publications/lpy2pc.},
	booktitle = {Computer vision – {ECCV} 2020 workshops},
	publisher = {Springer International Publishing},
	author = {Chaudhury, Ayan and Boudon, Frédéric and Godin, Christophe},
	editor = {Bartoli, Adrien and Fusiello, Andrea},
	year = {2020},
	pages = {244--260},
}

@article{liu_single_2021,
	title = {Single {Image} {Tree} {Reconstruction} via {Adversarial} {Network}},
	volume = {117},
	issn = {1524-0703},
	url = {https://www.sciencedirect.com/science/article/pii/S1524070321000205},
	doi = {10.1016/j.gmod.2021.101115},
	abstract = {Realistic 3D tree reconstruction is still a tedious and time-consuming task in the graphics community. In this paper, we propose a simple and efficient method for reconstructing 3D tree models with high fidelity from a single image. The key to single image-based tree reconstruction is to recover 3D shape information of trees via a deep neural network learned from a set of synthetic tree models. We adopted a conditional generative adversarial network (cGAN) to infer the 3D silhouette and skeleton of a tree respectively from edges extracted from the image and simple 2D strokes drawn by the user. Based on the predicted 3D silhouette and skeleton, a realistic tree model that inherits the tree shape in the input image can be generated using a procedural modeling technique. Experiments on varieties of tree examples demonstrate the efficiency and effectiveness of the proposed method in reconstructing realistic 3D tree models from a single image.},
	language = {en},
	urldate = {2022-04-18},
	journal = {Graphical Models},
	author = {Liu, Zhihao and Wu, Kai and Guo, Jianwei and Wang, Yunhai and Deussen, Oliver and Cheng, Zhanglin},
	month = sep,
	year = {2021},
	keywords = {adversarial network, single image reconstruction, tree reconstruction},
	pages = {101115},
}

@article{mahesh_hyperspectral_2015,
	title = {Hyperspectral imaging to classify and monitor quality of agricultural materials},
	volume = {61},
	issn = {0022-474X},
	url = {https://www.sciencedirect.com/science/article/pii/S0022474X15000089},
	doi = {10.1016/j.jspr.2015.01.006},
	abstract = {Hyperspectral imaging has been acknowledged as an emerging technology for monitoring quality parameters and improving grading of agricultural materials, such as field crops (e.g., cereals, pulses, oil seeds) and horticultural crops (e.g., apples, strawberries). It has become a popular research tool that facilitates thorough non-destructive analyses by simultaneous acquisition of both spectral and spatial information of agricultural samples. The technique is an extension of multispectral imaging, which provides a large data set by applying conventional imaging, radiometry, and spectroscopic principles when acquiring images. Hyperspectral imaging was initially used for remote sensing applications, but now has been developed to facilitate complete and reliable analyses of intrinsic properties and external characteristics of samples. This paper reviews applications of using hyperspectral imaging for routine grain industry operations such as grading, classification, and chemometric analyses of major constituents of agricultural materials.},
	language = {en},
	urldate = {2022-05-06},
	journal = {Journal of Stored Products Research},
	author = {Mahesh, S. and Jayas, D. S. and Paliwal, J. and White, N. D. G.},
	month = mar,
	year = {2015},
	keywords = {Agricultural, Digital imaging processing, Grading, Hyperspectral imaging, Quality},
	pages = {17--26},
}

@article{chen_hyperspectral_2019,
	title = {Hyperspectral lidar point cloud segmentation based on geometric and spectral information},
	volume = {27},
	copyright = {\&\#169; 2019 Optical Society of America},
	issn = {1094-4087},
	url = {https://opg.optica.org/oe/abstract.cfm?uri=oe-27-17-24043},
	doi = {10.1364/OE.27.024043},
	abstract = {Light detection and ranging (lidar) can record a 3D environment as point clouds, which are unstructured and difficult to process efficiently. Point cloud segmentation is an effective technology to solve this problem and plays a significant role in various applications, such as forestry management and 3D building reconstruction. The spectral information from images could improve the segmentation result, but suffers from the varying illumination conditions and the registration problem. New hyperspectral lidar sensor systems can solve these problems, with the capacity to obtain spectral and geometric information simultaneously. The former segmentation on hyperspectral lidar were mainly based on spectral information. The geometric segmentation method widely used by single wavelength lidar was not employed for hyperspectral lidar yet. This study aims to fill this gap by proposing a hyperspectral lidar segmentation method with three stages. First, Connected-Component Labeling (CCL) using the geometric information is employed for base segmentation. Second, the output components of the first stage are split by the spectral difference using Density-Based Spatial Clustering of Applications with Noise (DBSCAN). Third, the components of the second stage are merged based on the spectral similarity using Spectral Angle Match (SAM). Two indoor experimental scenes were setup for validation. We compared the performance of our mothed with that of the 3D and intensity feature based method. The quantitative analysis indicated that, our proposed method improved the point-weighted score by 19.35\% and 18.65\% in two experimental scenes, respectively. These results showed that the geometric segmentation method for single wavelength lidar could be combined with the spectral information, and contribute to the more effective hyperspectral lidar point cloud segmentation.},
	language = {EN},
	number = {17},
	urldate = {2022-05-06},
	journal = {Optics Express},
	author = {Chen, Biwu and Shi, Shuo and Shi, Shuo and Sun, Jia and Gong, Wei and Gong, Wei and Yang, Jian and Du, Lin and Guo, Kuanghui and Wang, Binhui and Chen, Bowen},
	month = aug,
	year = {2019},
	pages = {24043--24059},
}

@article{james_guidelines_2019,
	title = {Guidelines on the use of structure-from-motion photogrammetry in geomorphic research},
	volume = {44},
	issn = {1096-9837},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/esp.4637},
	doi = {10.1002/esp.4637},
	abstract = {As a topographic modelling technique, structure-from-motion (SfM) photogrammetry combines the utility of digital photogrammetry with a flexibility and ease of use derived from multi-view computer vision methods. In conjunction with the rapidly increasing availability of imagery, particularly from unmanned aerial vehicles, SfM photogrammetry represents a powerful tool for geomorphological research. However, to fully realize this potential, its application must be carefully underpinned by photogrammetric considerations, surveys should be reported in sufficient detail to be repeatable (if practical) and results appropriately assessed to understand fully the potential errors involved. To deliver these goals, robust survey and reporting must be supported through (i) using appropriate survey design, (ii) applying suitable statistics to identify systematic error (bias) and to estimate precision within results, and (iii) propagating uncertainty estimates into the final data products. © 2019 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {10},
	urldate = {2022-05-06},
	journal = {Earth Surface Processes and Landforms},
	author = {James, Mike R. and Chandler, Jim H. and Eltner, Anette and Fraser, Clive and Miller, Pauline E. and Mills, Jon P. and Noble, Tom and Robson, Stuart and Lane, Stuart N.},
	year = {2019},
	keywords = {bias and precision, structure-from-motion photogrammetry, survey design, systematic error, topographic survey},
	pages = {2081--2084},
}

@inproceedings{norouzi_hamming_2012,
	title = {Hamming {Distance} {Metric} {Learning}},
	volume = {25},
	url = {https://papers.nips.cc/paper/2012/hash/59b90e1005a220e2ebc542eb9d950b1e-Abstract.html},
	abstract = {Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs. We develop a new loss-augmented inference algorithm that is quadratic in the code length. We show strong retrieval performance on CIFAR-10 and MNIST, with promising classification results using no more than kNN on the binary codes.},
	urldate = {2022-05-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Norouzi, Mohammad and Fleet, David J and Salakhutdinov, Russ R},
	year = {2012},
}

@article{zhao_parallel_2020,
	title = {Parallel {Computing} for {Obtaining} {Regional} {Scale} {Rice} {Growth} {Conditions} {Based} on {WOFOST} and {Satellite} {Images}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3043003},
	abstract = {It is very important to obtain continuous regional crop parameters efficiently in the agricultural field. However, remote sensing data can provide spatial-continuous / temporal-disperse crop information while crop growth model can provide temporal-continuous / spatial-disperse crop information. Therefore, the assimilation between crop growth model and remote sensing data is an efficient way for obtaining continuous vegetation growth information. This study aims to present a parallel method based on graphic processing unit (GPU) to improve the efficiency of the assimilation between RS data and crop growth model to estimate rice growth parameters. Remote sensing data, Landsat and HJ-1 images, were collected and the World Food Studies (WOFOST) crop growth model which has a strong flexibility was employed. To acquire continuous regional crop parameters, particle swarm optimization (PSO) data assimilation method was used to combine remote sensing images and WOFOST and this process is accompanied by a parallel method based on the Compute Unified Device Architecture (CUDA) platform of NVIDIA GPU. With these methods, we obtained daily rice growth parameters of Zhuzhou City, Hunan, China and compared the efficiency and precision of parallel method and non-parallel method. Results showed that the parallel program has a remarkable speedup (reaching 240 times) compared with the non-parallel program with a similar accuracy. This study indicated that the parallel implementation based on GPU was successful in improving the efficiency of the assimilation between RS data and the WOFOST model.},
	journal = {IEEE Access},
	author = {Zhao, Bingyu and Liu, Meiling and Wu, Jianjun and Liu, Xiangnan and Liu, Mengxue and Wu, Ling},
	year = {2020},
	keywords = {Agriculture, Atmospheric modeling, Computational modeling, Data models, Earth, Graphics processing units, Remote sensing, WOFOST model, data assimilation, parallel algorithm, remote sensing},
	pages = {223675--223685},
}

@article{casella_exploiting_2019,
	title = {Exploiting multi-core and {GPU} hardware to speed up the registration of range images by means of {Differential} {Evolution}},
	volume = {133},
	issn = {0743-7315},
	url = {https://www.sciencedirect.com/science/article/pii/S0743731518304738},
	doi = {10.1016/j.jpdc.2018.07.002},
	abstract = {Within this paper a general-purpose distributed evolutionary algorithm is presented, and is applied to the pair-wise registration of range images. Registration is carried out by utilizing the Grid Closest Point (GCP) for the graphical registration operations and the distributed algorithm to search for the best possible transformation of a scene image that, merged with the model image, yields a 3D reconstruction of the original object. The evolutionary algorithm is a distributed Differential Evolution algorithm that exploits an asynchronous migration mechanism and a multi-population recombination information exchange. Such an algorithm is provided with an adaptive updating scheme based on chaotic features for dynamically updating the control parameters. The scope of the paper is to speed up the registration process by using processor specialized to handle graphical operations and multi-core platforms. On the one hand, we investigate the use of either Graphic Processing Units (GPUs) or multi-core architectures to lower the execution time of the GCP procedure. On the other hand, we evaluate the performance of the distributed evolutionary algorithm in terms of solution quality by examining different multi-core architectures. Experimental results on a set of publicly available images show that, to perform the GCP, reductions in the execution times by one order of magnitude are obtained by harnessing the computational power of GPU and multi-core platforms with respect to the execution on a CPU-based framework. Furthermore, a comparison with the state-of-the-art sequential evolutionary algorithm for range image registration reveals that the adaptive distributed Differential Evolution algorithm allows attaining integral 3D models from 3D scan datasets that are better in terms of both quality and robustness.},
	language = {en},
	urldate = {2022-05-06},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Casella, A. and De Falco, I. and Della Cioppa, A. and Scafuri, U. and Tarantino, E.},
	month = nov,
	year = {2019},
	keywords = {Distributed computing, Heuristics, Range image registration},
	pages = {307--318},
}

@article{salah_accelerated_2020,
	title = {Accelerated {CPU}–{GPUs} implementations for quaternion polar harmonic transform of color images},
	volume = {107},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X19302171},
	doi = {10.1016/j.future.2020.01.051},
	abstract = {Image moments are used to capture image features. Moments are successfully used in object descriptions, recognition, and other applications. However, image representation and recognition using quaternion moments are compute-intensive processes. This obstacle makes this type of moment unsuitable for real-time or large-scale applications, despite their high accuracy. In this work, we expose the challenges to parallelizing quaternion moment computations and transform the sequential algorithm to a parallel-friendly counterpart. We propose a set of parallel implementations for parallelizing quaternion moment image representations on different parallel architectures. The proposed implementations target multicore CPU-only, GPU-only, and hybrid CPU–GPUs (with multicore CPU and multi-GPU) environments. The loop mitigation technique is proposed to boost the level of parallelism in massively parallel environments, balance the parallel workload, and reduce both the space complexity and synchronization overhead of the proposed implementations. The loop mitigation technique could be applicable to other applications with similar loop imbalances. Finally, experiments are performed to evaluate the proposed implementations. Applying a moment order of 60 on a color image of 1024×1024 pixels, the proposed implementation achieved, on four P100 GPUs and a CPU with 16 cores, speedup of 257× and 277× over the baseline performance on a single Intel Xeon E5-2609 CPU core for image reconstruction and quaternion moment computation, respectively. In addition, 180× speedup is achieved for color image watermarking application.},
	language = {en},
	urldate = {2022-05-06},
	journal = {Future Generation Computer Systems},
	author = {Salah, Ahmad and Li, Kenli and Hosny, Khalid M. and Darwish, Mohamed M. and Tian, Qi},
	month = jun,
	year = {2020},
	keywords = {CPU–GPU, Multi-GPU, Parallel implementation, Quaternion moments, Speedup},
	pages = {368--382},
}

@inproceedings{rublee_orb_2011,
	title = {{ORB}: {An} efficient alternative to {SIFT} or {SURF}},
	shorttitle = {{ORB}},
	doi = {10.1109/ICCV.2011.6126544},
	abstract = {Feature matching is at the base of many computer vision problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for detection and matching. In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magnitude faster than SIFT, while performing as well in many situations. The efficiency is tested on several real-world applications, including object detection and patch-tracking on a smart phone.},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
	month = nov,
	year = {2011},
	keywords = {Boats},
	pages = {2564--2571},
}

@phdthesis{foo_gonioreflectometer_2001,
	type = {{PhD} {Thesis}},
	title = {A {Gonioreflectometer} {For} {Measuring} {The} {Bidirectional} {Reflectance} {Of} {Material} {For} {Use} {In} {Illumination} {Computation}},
	abstract = {This thesis presents the detailed design and measurement procedures of an automated three-axis gonioreflectometer for measuring the bidirectional reflectance distribution function (BRDF) of an isotropic material for use in computer graphics rendering. The working gonioreflectometer is a modification of a partially completed instrument donated by Kodak to the Light Measurement Laboratory of the Cornell Program of Computer Graphics. The gonioreflectometer consists of a broad band light source that emits parallel rays in the visible wavelength range. The light source revolves around the test sample which itself has two degrees of freedom of rotation. The detector is stationary and views the test sample via a folding mirror. The detector consists of external focusing optics, a diffraction grating spectrograph, and a diode-array sensor. The system has a high dynamic range and can measure the reflection at high grazing angles (up to 86 ffi ). Both absolute calibration by measuring the direct irradiance, and relative calibration by using a reference sample, can be used for converting measurements to BRDF values. BRDFs of three different samples, a white PTFE sample (Spectralon), a latex blue paint, and a matte-finished steel plate (Q-Panel R-46), were measured with the gonioreflectometer. The results are presented along with close comparisons to data published by other facilities. Specular reflectance and directional-hemispherical reflectance measured with the instrumentwere cross-checked to measurements made with an Optronic Spectroradiometer (OL-750). Biographical Sketch Sing Choong Foo was born in Ipoh, Perak, Malaysia. After graduating from Poi Lam High School in Ipoh, Sing Choong came to Cornell University in 1989. Sing Choong completed a B.S. and an M.Eng in applie...},
	author = {Foo, Sing},
	month = feb,
	year = {2001},
}

@article{mangalraj_recent_2022,
	title = {Recent trends and advances in hyperspectral imaging techniques to estimate solar induced fluorescence for plant phenotyping},
	volume = {137},
	issn = {1470-160X},
	url = {https://www.sciencedirect.com/science/article/pii/S1470160X22001923},
	doi = {10.1016/j.ecolind.2022.108721},
	abstract = {Inevitable environmental changes empower the researchers to understand and analyze the plant traits for improving the ecosystem. Solar-induced fluorescence (SIF) is one of the functional traits to analyze the vegetation and assess plant phenotyping. Estimation of SIF through hyperspectral imaging technique gaining its popularity in the recent days than any other estimating techniques due to its contiguous spectrum property, which allows us to obtain more information. Another merit of hyperspectral images is that they can be used to acquire data on different scales. In our review, we have focused on three major areas as follows, a.) Hyperspectral imaging techniques in estimating SIF in different scales varying from Ground Scale to Orbital Scales. b.) Correlation of other functional traits and factors influences the SIF estimation c.) Machine learning techniques used to interpret the SIF traits for Agricultural Monitoring. Moreover, the aforementioned areas are becoming crucial in the recent trend, and we confine our review with the state-of-the-art techniques exclusively from 2010 to 2021. We comprehend the details in the review to provide insights on the breakthrough made in hyperspectral imaging for SIF estimation, allowing the reader to deepen their understanding in the areas of plant phenotyping, which would enable them to explore the field for future research.},
	language = {en},
	urldate = {2022-05-11},
	journal = {Ecological Indicators},
	author = {Mangalraj, P. and Cho, Byoung-Kwan},
	month = apr,
	year = {2022},
	keywords = {Hyperspectral imaging, Machine learning, Plant phenotyping, Solar induced fluorescence},
	pages = {108721},
}

@article{nex_uav_2014,
	title = {{UAV} for {3D} mapping applications: a review},
	volume = {6},
	issn = {1866-928X},
	shorttitle = {{UAV} for {3D} mapping applications},
	url = {https://doi.org/10.1007/s12518-013-0120-x},
	doi = {10.1007/s12518-013-0120-x},
	abstract = {Unmanned aerial vehicle (UAV) platforms are nowadays a valuable source of data for inspection, surveillance, mapping, and 3D modeling issues. As UAVs can be considered as a low-cost alternative to the classical manned aerial photogrammetry, new applications in the short- and close-range domain are introduced. Rotary or fixed-wing UAVs, capable of performing the photogrammetric data acquisition with amateur or SLR digital cameras, can fly in manual, semiautomated, and autonomous modes. Following a typical photogrammetric workflow, 3D results like digital surface or terrain models, contours, textured 3D models, vector information, etc. can be produced, even on large areas. The paper reports the state of the art of UAV for geomatics applications, giving an overview of different UAV platforms, applications, and case studies, showing also the latest developments of UAV image processing. New perspectives are also addressed.},
	language = {en},
	number = {1},
	urldate = {2022-02-18},
	journal = {Applied Geomatics},
	author = {Nex, Francesco and Remondino, Fabio},
	month = mar,
	year = {2014},
	pages = {1--15},
}

@article{paternain_construction_2015,
	series = {Theme: {Aggregation} operators},
	title = {Construction of image reduction operators using averaging aggregation functions},
	volume = {261},
	issn = {0165-0114},
	url = {https://www.sciencedirect.com/science/article/pii/S0165011414001122},
	doi = {10.1016/j.fss.2014.03.008},
	abstract = {In this work we present an image reduction algorithm based on averaging aggregation functions. We axiomatically define the concepts of image reduction operator and local reduction operator. We study the construction of the latter by means of averaging functions and we propose an image reduction algorithm (image reduction operator). We analyze the properties of several averaging functions and their effect on the image reduction algorithm. Finally, we present experimental results where we apply our algorithm in two different applications, analyzing the best operators for each concrete application.},
	language = {en},
	urldate = {2022-03-30},
	journal = {Fuzzy Sets and Systems},
	author = {Paternain, D. and Fernandez, J. and Bustince, H. and Mesiar, R. and Beliakov, G.},
	month = feb,
	year = {2015},
	keywords = {Aggregation functions, Averaging functions, Image reduction, Local reduction operators, Reduction operators},
	pages = {87--111},
}

@article{nita_testing_2021,
	title = {Testing {Forestry} {Digital} {Twinning} {Workflow} {Based} on {Mobile} {LiDAR} {Scanner} and {AI} {Platform}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1999-4907/12/11/1576},
	doi = {10.3390/f12111576},
	abstract = {Climate-smart forestry is a sustainable forest management approach for increasing positive climate impacts on society. As climate-smart forestry is focusing on more sustainable solutions that are resource-efficient and circular, digitalization plays an important role in its implementation. The article aimed to validate an automatic workflow of processing 3D pointclouds to produce digital twins for every tree on large 1-ha sample plots using a GeoSLAM mobile LiDAR scanner and VirtSilv AI platform. Specific objectives were to test the efficiency of segmentation technique developed in the platform for individual trees from an initial cloud of 3D points observed in the field and to quantify the efficiency of digital twinning by comparing the automatically generated results of (DBH, H, and Volume) with traditional measurements. A number of 1399 trees were scanned with LiDAR to create digital twins and, for validation, were measured with traditional tools such as forest tape and vertex. The segmentation algorithm developed in the platform to extract individual 3D trees recorded an accuracy varying between 95 and 98\%. This result was higher in accuracy than reported by other solutions. When compared to traditional measurements the bias for diameter at breast height (DBH) and height was not significant. Digital twinning offers a blockchain solution for digitalization, and AI platforms are able to provide technological advantage in preserving and restoring biodiversity with sustainable forest management.},
	language = {en},
	number = {11},
	urldate = {2021-12-30},
	journal = {Forests},
	author = {Niță, Mihai Daniel},
	month = nov,
	year = {2021},
	keywords = {LiDAR, artificial intelligence, climate smart, digital twinning, digitalization},
	pages = {1576},
}

@article{rosell_obtaining_2009,
	title = {Obtaining the three-dimensional structure of tree orchards from remote {2D} terrestrial {LIDAR} scanning},
	volume = {149},
	issn = {0168-1923},
	url = {https://www.sciencedirect.com/science/article/pii/S0168192309000926},
	doi = {10.1016/j.agrformet.2009.04.008},
	abstract = {In recent years, LIDAR (light detection and ranging) sensors have been widely used to measure environmental parameters such as the structural characteristics of trees, crops and forests. Knowledge of the structural characteristics of plants has a high scientific value due to their influence in many biophysical processes including, photosynthesis, growth, CO2-sequestration and evapotranspiration, playing a key role in the exchange of matter and energy between plants and the atmosphere, and affecting terrestrial, above-ground, carbon storage. In this work, we report the use of a 2D LIDAR scanner in agriculture to obtain three-dimensional (3D) structural characteristics of plants. LIDAR allows fast, non-destructive measurement of the 3D structure of vegetation (geometry, size, height, cross-section, etc.). LIDAR provides a 3D cloud of points, which is easily visualized with Computer Aided Design software. Three-dimensional, high density data are uniquely valuable for the qualitative and quantitative study of the geometric parameters of plants. Results are demonstrated in fruit and citrus orchards and vineyards, leading to the conclusion that the LIDAR system is able to measure the geometric characteristics of plants with sufficient precision for most agriculture applications. The developed system made it possible to obtain 3D digitalized images of crops, from which a large amount of plant information – such as height, width, volume, leaf area index and leaf area density – could be obtained. There was a great degree of concordance between the physical dimensions, shape and global appearance of the 3D digital plant structure and the real plants, revealing the coherence of the 3D tree model obtained from the developed system with respect to the real structure. For some selected trees, the correlation coefficient obtained between manually measured volumes and those obtained from the 3D LIDAR models was as high as 0.976.},
	language = {en},
	number = {9},
	urldate = {2021-12-30},
	journal = {Agricultural and Forest Meteorology},
	author = {Rosell, Joan R. and Llorens, Jordi and Sanz, Ricardo and Arnó, Jaume and Ribes-Dasi, Manel and Masip, Joan and Escolà, Alexandre and Camp, Ferran and Solanelles, Francesc and Gràcia, Felip and Gil, Emilio and Val, Luis and Planas, Santiago and Palacín, Jordi},
	month = sep,
	year = {2009},
	keywords = {3D Plant structure, Geometrical characteristics of plants, Laser measurements, Plant modelling, Terrestrial LIDAR, Tree volume},
	pages = {1505--1515},
}

@article{graciano_quadstack_2021,
	title = {{QuadStack}: {An} {Efficient} {Representation} and {Direct} {Rendering} of {Layered} {Datasets}},
	volume = {27},
	issn = {1941-0506},
	shorttitle = {{QuadStack}},
	doi = {10.1109/TVCG.2020.2981565},
	abstract = {We introduce QuadStack, a novel algorithm for volumetric data compression and direct rendering. Our algorithm exploits the data redundancy often found in layered datasets which are common in science and engineering fields such as geology, biology, mechanical engineering, medicine, etc. QuadStack first compresses the volumetric data into vertical stacks which are then compressed into a quadtree that identifies and represents the layered structures at the internal nodes. The associated data (color, material, density, etc.) and shape of these layer structures are decoupled and encoded independently, leading to high compression rates (4× to 54× of the original voxel model memory footprint in our experiments). We also introduce an algorithm for value retrieving from the QuadStack representation and we show that the access has logarithmic complexity. Because of the fast access, QuadStack is suitable for efficient data representation and direct rendering. We show that our GPU implementation performs comparably in speed with the state-of-the-art algorithms (18-79 MRays/s in our implementation), while maintaining a significantly smaller memory footprint.},
	number = {9},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Graciano, Alejandro and Rueda, Antonio J. and Pospíšil, Adam and Bittner, Jiří and Benes, Bedrich},
	month = sep,
	year = {2021},
	keywords = {Computer graphics, Data models, Data structures, Data visualization, Geology, Graphics processing units, Ray tracing, Rendering (computer graphics), graphics data structures and data types, object hierarchies},
	pages = {3733--3744},
}

@article{brell_3d_2019,
	title = {{3D} hyperspectral point cloud generation: {Fusing} airborne laser scanning and hyperspectral imaging sensors for improved object-based information extraction},
	volume = {149},
	issn = {0924-2716},
	shorttitle = {{3D} hyperspectral point cloud generation},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271619300309},
	doi = {10.1016/j.isprsjprs.2019.01.022},
	abstract = {Remote Sensing technologies allow to map biophysical, biochemical, and earth surface parameters of the land surface. Of especial interest for various applications in environmental and urban sciences is the combination of spectral and 3D elevation information. However, those two data streams are provided separately by different instruments, namely airborne laser scanner (ALS) for elevation and a hyperspectral imager (HSI) for high spectral resolution data. The fusion of ALS and HSI data can thus lead to a single data entity consistently featuring rich structural and spectral information. In this study, we present the application of fusing the first pulse return information from ALS data at a sub-decimeter spatial resolution with the lower-spatial resolution hyperspectral information available from the HSI into a hyperspectral point cloud (HSPC). During the processing, a plausible hyperspectral spectrum is assigned to every first-return ALS point. We show that the complementary implementation of spectral and 3D information at the point-cloud scale improves object-based classification and information extraction schemes. This improvements have great potential for numerous land-cover mapping and environmental applications.},
	language = {en},
	urldate = {2022-03-30},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Brell, Maximilian and Segl, Karl and Guanter, Luis and Bookhagen, Bodo},
	month = mar,
	year = {2019},
	keywords = {Data fusion, Imaging spectroscopy, In-flight, Laser return intensity, Lidar, Multispectral point cloud, Pixel level, Point cloud segmentation, Preprocessing, Semantic labeling, Sensor fusion, Sharpening, Unmixing},
	pages = {200--214},
}

@inproceedings{martin_enhanced_2014,
	title = {Enhanced material identification using polarimetric hyperspectral imaging},
	doi = {10.1109/AIPR.2014.7041920},
	abstract = {Polarimetric and hyperspectral imaging are two of the most frequently used remote sensing modalities. While extensive work has been done in both fields independently, relatively little work has been done using both in conjunction with one another. Combining these two common remote sensing techniques, we hope to estimate index of refraction, without a priori knoweledge of local weather conditions or object surface temperature. In general, this is an underdetermined problem, but modeling the spectral behavior of the index of refraction reduces the number of parameters needed to describe the index of refraction, and thus the reflectively. This allows additional scene parameters needed to describe the radiance signature from a target to be simulataneously solved for. The method uses spectrally resolved S0 and Si radiance measurements, taken using an IFTS with a linear polarizer mounted to the front, to simultaneously solve for a materials index of refraction, surface temperature, and downwelling radiance. Measurements at multiple angles relative to the surface normal can also be taken to provide further constraining information in the fit. Results on both simulated and measured data are presented showing that this technique is largely robust to changes in object temperature.},
	booktitle = {2014 {IEEE} {Applied} {Imagery} {Pattern} {Recognition} {Workshop} ({AIPR})},
	author = {Martin, Jacob A. and Gross, Kevin C.},
	month = oct,
	year = {2014},
	keywords = {Atmospheric measurements, Atmospheric modeling, Indexes, Materials, Rough surfaces, Surface waves, Temperature measurement},
	pages = {1--6},
}

@article{can_semantic_2021,
	title = {Semantic segmentation on {Swiss3DCities}: {A} benchmark study on aerial photogrammetric {3D} pointcloud dataset},
	volume = {150},
	issn = {0167-8655},
	shorttitle = {Semantic segmentation on {Swiss3DCities}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865521001938},
	doi = {10.1016/j.patrec.2021.06.004},
	abstract = {We introduce a new outdoor urban 3D pointcloud dataset, covering a total area of 2.7km2, sampled from three Swiss cities with different characteristics. The dataset is manually annotated for semantic segmentation with per-point labels, and is built using photogrammetry from images acquired by multirotors equipped with high-resolution cameras. In contrast to datasets acquired with ground LiDAR sensors, the resulting point clouds are uniformly dense and complete, and are useful to disparate applications, including autonomous driving, gaming and smart city planning. As a benchmark, we report quantitative results of PointNet++, an established point-based deep 3D semantic segmentation model; on this model, we additionally study the impact of using different cities for model generalization.},
	language = {en},
	urldate = {2022-03-30},
	journal = {Pattern Recognition Letters},
	author = {Can, Gülcan and Mantegazza, Dario and Abbate, Gabriele and Chappuis, Sébastien and Giusti, Alessandro},
	month = oct,
	year = {2021},
	keywords = {3D Data, Deep learning, Model generalization, Photogrammetry, Pointcloud, Semantic segmentation},
	pages = {108--114},
}

@article{wang_lidar_2018,
	title = {{LiDAR} {Point} {Clouds} to 3-{D} {Urban} {Models}\$:\$ {A} {Review}},
	volume = {11},
	issn = {2151-1535},
	shorttitle = {{LiDAR} {Point} {Clouds} to 3-{D} {Urban} {Models}\$},
	doi = {10.1109/JSTARS.2017.2781132},
	abstract = {Three-dimensional (3-D) urban models are an integral part of numerous applications, such as urban planning and performance simulation, mapping and visualization, emergency response training and entertainment, among others. We consolidate various algorithms proposed for reconstructing 3-D models of urban objects from point clouds. Urban models addressed in this review include buildings, vegetation, utilities such as roads or power lines and free-form architectures such as curved buildings or statues, all of which are ubiquitous in a typical urban scenario. While urban modeling, building reconstruction, in particular, clearly demand specific traits in the models, such as regularity, symmetry, and repetition; most of the traditional and state-of-the-art 3-D reconstruction algorithms are designed to address very generic objects of arbitrary shapes and topology. The recent efforts in the urban reconstruction arena, however, strive to accommodate the various pressing needs of urban modeling. Strategically, urban modeling research nowadays focuses on the usage of specialized priors, such as global regularity, Manhattan-geometry or symmetry to aid the reconstruction, or efficient adaptation of existing reconstruction techniques to the urban modeling pipeline. Aimed at an in-depth exploration of further possibilities, we review the existing urban reconstruction algorithms, prevalent in computer graphics, computer vision and photogrammetry disciplines, evaluate their performance in the architectural modeling context, and discuss the adaptability of generic mesh reconstruction techniques to the urban modeling pipeline. In the end, we suggest a few directions of research that may be adopted to close in the technology gaps.},
	number = {2},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Wang, Ruisheng and Peethambaran, Jiju and Chen, Dong},
	month = feb,
	year = {2018},
	keywords = {Adaptation models, Airborne laser scanning (ALS), Atmospheric modeling, Buildings, Image reconstruction, Remote sensing, Three-dimensional displays, Urban planning, airborne light detection and ranging, façade modeling, light detection and ranging (LiDAR), mobile LiDAR, mobile laser scanning (MLS), point clouds, powerline (PL) reconstruction, road modeling, rooftop modeling, surface reconstruction, terrestrial laser scanning (TLS), tree modeling, urban models},
	pages = {606--627},
}

@inproceedings{hu_towards_2021,
	title = {Towards {Semantic} {Segmentation} of {Urban}-{Scale} {3D} {Point} {Clouds}: {A} {Dataset}, {Benchmarks} and {Challenges}},
	shorttitle = {Towards {Semantic} {Segmentation} of {Urban}-{Scale} {3D} {Point} {Clouds}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Hu_Towards_Semantic_Segmentation_of_Urban-Scale_3D_Point_Clouds_A_Dataset_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-03-30},
	author = {Hu, Qingyong and Yang, Bo and Khalid, Sheikh and Xiao, Wen and Trigoni, Niki and Markham, Andrew},
	year = {2021},
	pages = {4977--4987},
}

@article{guarnera_turning_2019,
	title = {Turning a {Digital} {Camera} into an {Absolute} {2D} {Tele}-{Colorimeter}},
	volume = {38},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13393},
	doi = {10.1111/cgf.13393},
	abstract = {We present a simple and effective technique for absolute colorimetric camera characterization, invariant to changes in exposure/aperture and scene irradiance, suitable in a wide range of applications including image-based reflectance measurements, spectral pre-filtering and spectral upsampling for rendering, to improve colour accuracy in high dynamic range imaging. Our method requires a limited number of acquisitions, an off-the-shelf target and a commonly available projector, used as a controllable light source, other than the reflected radiance to be known. The characterized camera can be effectively used as a 2D tele-colorimeter, providing the user with an accurate estimate of the distribution of luminance and chromaticity in a scene, without requiring explicit knowledge of the incident lighting power spectra. We validate the approach by comparing our estimated absolute tristimulus values (XYZ data in ) with the measurements of a professional 2D tele-colorimeter, for a set of scenes with complex geometry, spatially varying reflectance and light sources with very different spectral power distribution.},
	language = {en},
	number = {1},
	urldate = {2022-03-30},
	journal = {Computer Graphics Forum},
	author = {Guarnera, G. C. and Bianco, S. and Schettini, R.},
	year = {2019},
	keywords = {I.3.3 Computer Graphics: Picture/Image Generation—I.4.1 Image Processing and Computer Vision: Digitization and Image Capture—I.4.8 Image Processing and Computer Vision: Scene Analysis, appearance modelling, colour, image and video processing, image-based rendering, modelling, rendering},
	pages = {73--86},
}

@article{tefera_estimating_2022,
	title = {Estimating early season growth and biomass of field pea for selection of divergent ideotypes using proximal sensing},
	volume = {277},
	issn = {0378-4290},
	url = {https://www.sciencedirect.com/science/article/pii/S0378429021003531},
	doi = {10.1016/j.fcr.2021.108407},
	abstract = {The aims of this study were to (i) test ground and aerial-based remote sensing vegetation indices (VIs) for trait-based breeding line selection, (ii) improve our understanding of the association between measured plant traits and readings derived from active and passive sensors and (iii) establish an optimal time for growth assessments in relation to field pea vigour and seed yield. Multispectral sensors were deployed with the handheld Crop Circle (CC) and a sensor mounted on an unmanned aerial vehicle (UAV) to collect data from field trials conducted between 2017 and 2020 at Beulah and Horsham in Victoria and Yenda, Wagga Wagga and Ardlethan in New South Wales in Australia. The result showed that normalised difference vegetation index (NDVI) derived from an aerial-based passive sensor (UAV) was strongly and significantly correlated to NDVI derived from a ground-based active sensor (CC) at both Beulah (R2 = 0.85; n = 1165; p {\textbackslash}textless 0.001) and Horsham (R2 = 0.77; n = 210; p {\textbackslash}textless 0.001). Both methods showed similar NDVI trends in pea genotype rankings. Based on the three seasons of field trial data, NDVI derived from both the CC and UAV sensors were linearly related to biomass production during pre-canopy closure growth. In water limiting environments, seed yield was positively correlated to NDVI measures. Measures calculated from the area under the NDVI curve throughout the growth season, and an additive main effect and multiplicative interaction model (AMMI) identified varieties with high vigour scores (high NDVI). Overall, a high vigour score was correlated to seed yield in lower yielding environments. From these results it appeared that higher vigour helps achieve higher yields in drier environments, however it was correlated with lower yields in better environments.},
	language = {en},
	urldate = {2021-12-30},
	journal = {Field Crops Research},
	author = {Tefera, Abeya Temesgen and Banerjee, Bikram Pratap and Pandey, Babu Ram and James, Laura and Puri, Ramesh Raj and Cooray, Onella and Marsh, Jasmine and Richards, Mark and Kant, Surya and Fitzgerald, Glenn J. and Rosewarne, Garry Mark},
	month = mar,
	year = {2022},
	keywords = {Crop circle, Field pea, NDVI, UAV, Vegetation indices},
	pages = {108407},
}

@inproceedings{marschner_image-based_1999,
	address = {Vienna},
	title = {Image-{Based} {BRDF} {Measurement} {Including} {Human} {Skin}},
	isbn = {978-3-7091-6809-7},
	doi = {10.1007/978-3-7091-6809-7_13},
	abstract = {We present a new image-based process for measuring the bidirectional reflectance of homogeneous surfaces rapidly, completely, and accurately. For simple sample shapes (spheres and cylinders) the method requires only a digital camera and a stable light source. Adding a 3D scanner allows a wide class of curved near-convex objects to be measured. With measurements for a variety of materials from paints to human skin, we demonstrate the new method’y to achieve high resolution and accuracy over a large domain of illumination and reflection directions. We verify our measurements by tests of internal consistency and by comparison against measurements made using a gonioreflectomter.},
	language = {en},
	booktitle = {Rendering {Techniques}’ 99},
	publisher = {Springer},
	author = {Marschner, Stephen R. and Westin, Stephen H. and Lafortune, Eric P. F. and Torrance, Kenneth E. and Greenberg, Donald P.},
	editor = {Lischinski, Dani and Larson, Greg Ward},
	year = {1999},
	pages = {131--144},
}

@inproceedings{francken_high_2008,
	title = {High quality mesostructure acquisition using specularities},
	doi = {10.1109/CVPR.2008.4587782},
	abstract = {We propose a technique for cheap and efficient acquisition of mesostructure normal maps from specularities, which only requires a simple LCD monitor and a digital camera. Coded illumination enables us to capture subtle surface details with only a handful of images. In addition, our method can deal with heterogeneous surfaces, and high albedo materials. We are able to recover highly detailed mesostructures, which was previously only possible with an expensive hardware setup.},
	booktitle = {2008 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Francken, Yannick and Cuypers, Tom and Mertens, Tom and Gielis, Jo and Bekaert, Philippe},
	month = jun,
	year = {2008},
	keywords = {Cameras, Hardware, Layout, Light sources, Lighting, Monitoring, Photometry, Shape, Stereo vision, Surface reconstruction},
	pages = {1--7},
}

@inproceedings{gardner_linear_2003,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '03},
	title = {Linear light source reflectometry},
	isbn = {978-1-58113-709-5},
	url = {https://doi.org/10.1145/1201775.882342},
	doi = {10.1145/1201775.882342},
	abstract = {This paper presents a technique for estimating the spatially-varying reflectance properties of a surface based on its appearance during a single pass of a linear light source. By using a linear light rather than a point light source as the illuminant, we are able to reliably observe and estimate the diffuse color, specular color, and specular roughness of each point of the surface. The reflectometry apparatus we use is simple and inexpensive to build, requiring a single direction of motion for the light source and a fixed camera viewpoint. Our model fitting technique first renders a reflectance table of how diffuse and specular reflectance lobes would appear under moving linear light source illumination. Then, for each pixel we compare its series of intensity values to the tabulated reflectance lobes to determine which reflectance model parameters most closely produce the observed reflectance values. Using two passes of the linear light source at different angles, we can also estimate per-pixel surface normals as well as the reflectance parameters. Additionally our system records a per-pixel height map for the object and estimates its per-pixel translucency. We produce real-time renderings of the captured objects using a custom hardware shading algorithm. We apply the technique to a test object exhibiting a variety of materials as well as to an illuminated manuscript with gold lettering. To demonstrate the technique's accuracy, we compare renderings of the captured models to real photographs of the original objects.},
	urldate = {2022-03-30},
	booktitle = {{ACM} {SIGGRAPH} 2003 {Papers}},
	publisher = {Association for Computing Machinery},
	author = {Gardner, Andrew and Tchou, Chris and Hawkins, Tim and Debevec, Paul},
	month = jul,
	year = {2003},
	pages = {749--758},
}

@inproceedings{mukaigawa_multiplexed_2007,
	address = {Berlin, Heidelberg},
	title = {Multiplexed {Illumination} for {Measuring} {BRDF} {Using} an {Ellipsoidal} {Mirror} and a {Projector}},
	isbn = {978-3-540-76390-1},
	doi = {10.1007/978-3-540-76390-1_25},
	abstract = {Measuring a bidirectional reflectance distribution function (BRDF) requires long time because a target object must be illuminated from all incident angles and the reflected light must be measured from all reflected angles. A high-speed method is presented to measure BRDFs using an ellipsoidal mirror and a projector. The method can change incident angles without a mechanical drive. Moreover, it is shown that the dynamic range of the measured BRDF can be significantly increased by multiplexed illumination based on the Hadamard matrix.},
	language = {en},
	booktitle = {Computer {Vision} – {ACCV} 2007},
	publisher = {Springer},
	author = {Mukaigawa, Yasuhiro and Sumino, Kohei and Yagi, Yasushi},
	editor = {Yagi, Yasushi and Kang, Sing Bing and Kweon, In So and Zha, Hongbin},
	year = {2007},
	pages = {246--257},
}

@inproceedings{malzbender_polynomial_2001,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '01},
	title = {Polynomial texture maps},
	isbn = {978-1-58113-374-5},
	url = {https://doi.org/10.1145/383259.383320},
	doi = {10.1145/383259.383320},
	abstract = {In this paper we present a new form of texture mapping that produces increased photorealism. Coefficients of a biquadratic polynomial are stored per texel, and used to reconstruct the surface color under varying lighting conditions. Like bump mapping, this allows the perception of surface deformations. However, our method is image based, and photographs of a surface under varying lighting conditions can be used to construct these maps. Unlike bump maps, these Polynomial Texture Maps (PTMs) also capture variations due to surface self-shadowing and interreflections, which enhance realism. Surface colors can be efficiently reconstructed from polynomial coefficients and light directions with minimal fixed-point hardware. We have also found PTMs useful for producing a number of other effects such as anisotropic and Fresnel shading models and variable depth of focus. Lastly, we present several reflectance function transformations that act as contrast enhancement operators. We have found these particularly useful in the study of ancient archeological clay and stone writings.},
	urldate = {2022-03-30},
	booktitle = {Proceedings of the 28th annual conference on {Computer} graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Malzbender, Tom and Gelb, Dan and Wolters, Hans},
	month = aug,
	year = {2001},
	keywords = {graphics hardware, illumination, image processing, image-based rendering, reflectance \& shading models, texture mapping},
	pages = {519--528},
}

@article{ghosh_estimating_2009,
	title = {Estimating {Specular} {Roughness} and {Anisotropy} from {Second} {Order} {Spherical} {Gradient} {Illumination}},
	volume = {28},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2009.01493.x},
	doi = {10.1111/j.1467-8659.2009.01493.x},
	abstract = {This paper presents a novel method for estimating specular roughness and tangent vectors, per surface point, from polarized second order spherical gradient illumination patterns. We demonstrate that for isotropic BRDFs, only three second order spherical gradients are sufficient to robustly estimate spatially varying specular roughness. For anisotropic BRDFs, an additional two measurements yield specular roughness and tangent vectors per surface point. We verify our approach with different illumination configurations which project both discrete and continuous fields of gradient illumination. Our technique provides a direct estimate of the per-pixel specular roughness and thus does not require off-line numerical optimization that is typical for the measure-and-fit approach to classical BRDF modeling.},
	language = {en},
	number = {4},
	urldate = {2022-03-30},
	journal = {Computer Graphics Forum},
	author = {Ghosh, Abhijeet and Chen, Tongbo and Peers, Pieter and Wilson, Cyrus A. and Debevec, Paul},
	year = {2009},
	keywords = {Computer, Graphics, I.3.7:, Realism, Three-Dimensional, and},
	pages = {1161--1170},
}

@article{chen_reflectance_2014,
	title = {Reflectance scanning: estimating shading frame and {BRDF} with generalized linear light sources},
	volume = {33},
	issn = {0730-0301, 1557-7368},
	shorttitle = {Reflectance scanning},
	url = {https://dl.acm.org/doi/10.1145/2601097.2601180},
	doi = {10.1145/2601097.2601180},
	language = {en},
	number = {4},
	urldate = {2022-03-30},
	journal = {ACM Transactions on Graphics},
	author = {Chen, Guojun and Dong, Yue and Peers, Pieter and Zhang, Jiawan and Tong, Xin},
	month = jul,
	year = {2014},
	pages = {1--11},
}

@article{walter_estimating_2019,
	title = {Estimating {Biomass} and {Canopy} {Height} {With} {LiDAR} for {Field} {Crop} {Breeding}},
	volume = {10},
	issn = {1664-462X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6775483/},
	doi = {10.3389/fpls.2019.01145},
	abstract = {Above-ground biomass (AGB) is a trait with much potential for exploitation within wheat breeding programs and is linked closely to canopy height (CH). However, collecting phenotypic data for AGB and CH within breeding programs is labor intensive, and in the case of AGB, destructive and prone to assessment error. As a result, measuring these traits is seldom a priority for breeders, especially at the early stages of a selection program. LiDAR has been demonstrated as a sensor capable of collecting three-dimensional data from wheat field trials, and potentially suitable for providing objective, non-destructive, high-throughput estimates of AGB and CH for use by wheat breeders. The current study investigates the deployment of a LiDAR system on a ground-based high-throughput phenotyping platform in eight wheat field trials across southern Australia, for the non-destructive estimate of AGB and CH. LiDAR-derived measurements were compared to manual measurements of AGB and CH collected at each site and assessed for their suitability of application within a breeding program. Correlations between AGB and LiDAR Projected Volume (LPV) were generally strong (up to r = 0.86), as were correlations between CH and LiDAR Canopy Height (LCH) (up to r = 0.94). Heritability (H2) of LPV (H2 = 0.32–0.90) was observed to be greater than, or similar to, the heritability of AGB (H2 = 0.12–0.78) for the majority of measurements. A similar level of heritability was observed for LCH (H2 = 0.41–0.98) and CH (H2 = 0.49–0.98). Further to this, measurements of LPV and LCH were shown to be highly repeatable when collected from either the same or opposite direction of travel. LiDAR scans were collected at a rate of 2,400 plots per hour, with the potential to further increase throughput to 7,400 plots per hour. This research demonstrates the capability of LiDAR sensors to collect high-quality, non-destructive, repeatable measurements of AGB and CH suitable for use within both breeding and research programs.},
	urldate = {2021-12-26},
	journal = {Frontiers in Plant Science},
	author = {Walter, James D. C. and Edwards, James and McDonald, Glenn and Kuchel, Haydn},
	month = sep,
	year = {2019},
	pmid = {31611889},
	pmcid = {PMC6775483},
	pages = {1145},
}

@article{wang_estimation_2021,
	title = {Estimation of tree height and aboveground biomass of coniferous forests in {North} {China} using stereo {ZY}-3, multispectral {Sentinel}-2, and {DEM} data},
	volume = {126},
	issn = {1470-160X},
	url = {https://www.sciencedirect.com/science/article/pii/S1470160X21003101},
	doi = {10.1016/j.ecolind.2021.107645},
	abstract = {The forest tree height and aboveground biomass (AGB) are important indicators for monitoring changes and trends in forest carbon storage and terrestrial carbon fluxes. Accurate large-scale wall-to-wall mapping of the forest tree height and AGB remain challenging due to the limited data availability for extraction tree height and the data signal saturation problem in AGB estimation. In this study, we explored the potential of forest tree height mapping using stereo imageries, and analyzed whether accounting for such information, in addition to optical sensor data, could improve the performance of AGB estimations of coniferous forests in a case study in North China. First, a spatially continuous tree height product was obtained using Ziyuan-3 satellite (ZY-3) stereo images combined with a digital elevation model (DEM) obtained from Advanced Land Observing Satellite (ALOS) data. Second, two AGB estimation models were established by combining the forest tree height with vegetation index, spectral, biophysical (from Sentinel-2 images), and topographic variables. A random forest algorithm was utilized to evaluate the effect of including the tree height variable in the AGB estimation. The results showed that the tree height estimation using the nadir and forward views of the ZY-3 stereo images was more accurate than that based on the nadir and backward views from the same images. The AGB estimation model incorporating the tree height variable with a coefficient of determination value of 0.7789, a root mean square error (RMSE) value of 29.815 Mg/ha and a relative RMSE of 23.42\% was more robust and effective, thereby demonstrating thatthe tree height variable can be used to alleviate the data signal saturation issue successfully. The proposed approach can provide new insight into forest tree height mapping and AGB products obtained from satellite stereo images and freely accessible Sentinel-2 multispectral images.},
	language = {en},
	urldate = {2021-12-26},
	journal = {Ecological Indicators},
	author = {Wang, Yueting and Zhang, Xiaoli and Guo, Zhengqi},
	month = jul,
	year = {2021},
	keywords = {Aboveground biomass, Forest tree height, Random forest, Sentinel-2, ZY-3 stereo imagery},
	pages = {107645},
}

@article{zhang_easy--use_2016,
	title = {An {Easy}-to-{Use} {Airborne} {LiDAR} {Data} {Filtering} {Method} {Based} on {Cloth} {Simulation}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/8/6/501},
	doi = {10.3390/rs8060501},
	abstract = {Separating point clouds into ground and non-ground measurements is an essential step to generate digital terrain models (DTMs) from airborne LiDAR (light detection and ranging) data. However, most filtering algorithms need to carefully set up a number of complicated parameters to achieve high accuracy. In this paper, we present a new filtering method which only needs a few easy-to-set integer and Boolean parameters. Within the proposed approach, a LiDAR point cloud is inverted, and then a rigid cloth is used to cover the inverted surface. By analyzing the interactions between the cloth nodes and the corresponding LiDAR points, the locations of the cloth nodes can be determined to generate an approximation of the ground surface. Finally, the ground points can be extracted from the LiDAR point cloud by comparing the original LiDAR points and the generated surface. Benchmark datasets provided by ISPRS (International Society for Photogrammetry and Remote Sensing) working Group III/3 are used to validate the proposed filtering method, and the experimental results yield an average total error of 4.58\%, which is comparable with most of the state-of-the-art filtering algorithms. The proposed easy-to-use filtering method may help the users without much experience to use LiDAR data and related technology in their own applications more easily.},
	language = {en},
	number = {6},
	urldate = {2021-12-26},
	journal = {Remote Sensing},
	author = {Zhang, Wuming and Qi, Jianbo and Wan, Peng and Wang, Hongtao and Xie, Donghui and Wang, Xiaoyan and Yan, Guangjian},
	month = jun,
	year = {2016},
	keywords = {LiDAR point cloud, cloth simulation, ground filtering algorithm},
	pages = {501},
}

@article{tunwattanapong_acquiring_2013,
	title = {Acquiring reflectance and shape from continuous spherical harmonic illumination},
	volume = {32},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2461912.2461944},
	doi = {10.1145/2461912.2461944},
	abstract = {We present a novel technique for acquiring the geometry and spatially-varying reflectance properties of 3D objects by observing them under continuous spherical harmonic illumination conditions. The technique is general enough to characterize either entirely specular or entirely diffuse materials, or any varying combination across the surface of the object. We employ a novel computational illumination setup consisting of a rotating arc of controllable LEDs which sweep out programmable spheres of incident illumination during 1-second exposures. We illuminate the object with a succession of spherical harmonic illumination conditions, as well as photographed environmental lighting for validation. From the response of the object to the harmonics, we can separate diffuse and specular reflections, estimate world-space diffuse and specular normals, and compute anisotropic roughness parameters for each view of the object. We then use the maps of both diffuse and specular reflectance to form correspondences in a multiview stereo algorithm, which allows even highly specular surfaces to be corresponded across views. The algorithm yields a complete 3D model and a set of merged reflectance maps. We use this technique to digitize the shape and reflectance of a variety of objects difficult to acquire with other techniques and present validation renderings which match well to photographs in similar lighting.},
	number = {4},
	urldate = {2022-03-30},
	journal = {ACM Transactions on Graphics},
	author = {Tunwattanapong, Borom and Fyffe, Graham and Graham, Paul and Busch, Jay and Yu, Xueming and Ghosh, Abhijeet and Debevec, Paul},
	month = jul,
	year = {2013},
	keywords = {specular scanning, spherical harmonics, spherical illumination},
	pages = {109:1--109:12},
}

@inproceedings{ben-ezra_led-only_2008,
	title = {An {LED}-only {BRDF} measurement device},
	doi = {10.1109/CVPR.2008.4587766},
	abstract = {Light emitting diodes (LEDs) can be used as light detectors and as light emitters. In this paper, we present a novel BRDF measurement device consisting exclusively of LEDs. Our design can acquire BRDFs over a full hemisphere, or even a full sphere (for the bidirectional transmittance distribution function BTDF), and can also measure a (partial) multi-spectral BRDF. Because we use no cameras, projectors, or even mirrors, our design does not suffer from occlusion problems. It is fast, significantly simpler, and more compact than existing BRDF measurement designs.},
	booktitle = {2008 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Ben-Ezra, Moshe and Wang, Jiaping and Wilburn, Bennett and Li, Xiaoyang and Ma, Le},
	month = jun,
	year = {2008},
	keywords = {Asia, Cameras, Distribution functions, Dynamic range, Light emitting diodes, Lighting, Mirrors, Optical surface waves, Surface waves, Time measurement},
	pages = {1--8},
}

@article{shao_slam-aided_2020,
	title = {{SLAM}-aided forest plot mapping combining terrestrial and mobile laser scanning},
	volume = {163},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271620300782},
	doi = {10.1016/j.isprsjprs.2020.03.008},
	abstract = {Precise structural information collected from plots is significant in the management of and decision-making regarding forest resources. Currently, laser scanning is widely used in forestry inventories to acquire three-dimensional (3D) structural information. There are three main data-acquisition modes in ground-based forest measurements: single-scan terrestrial laser scanning (TLS), multi-scan TLS and multi-single-scan TLS. Nevertheless, each of these modes causes specific difficulties for forest measurements. Due to occlusion effects, the single-scan TLS mode provides scans for only one side of the tree. The multi-scan TLS mode overcomes occlusion problems, however, at the cost of longer acquisition times, more human labor and more effort in data preprocessing. The multi-single-scan TLS mode decreases the workload and occlusion effects but lacks the complete 3D reconstruction of forests. These problems in TLS methods are largely avoided with mobile laser scanning (MLS); however, the geometrical peculiarity of forests (e.g., similarity between tree shapes, placements, and occlusion) complicates the motion estimation and reduces mapping accuracy. Therefore, this paper proposes a novel method combining single-scan TLS and MLS for forest 3D data acquisition. We use single-scan TLS data as a reference, onto which we register MLS point clouds, so they fill in the omission of the single-scan TLS data. To register MLS point clouds on the reference, we extract virtual feature points that are sampling the centerlines of tree stems and propose a new optimization-based registration framework. In contrast to previous MLS-based studies, the proposed method sufficiently exploits the natural geometric characteristics of trees. We demonstrate the effectiveness, robustness, and accuracy of the proposed method on three datasets, from which we extract structural information. The experimental results show that the omission of tree stem data caused by one scan can be compensated for by the MLS data, and the time of the field measurement is much less than that of the multi-scan TLS mode. In addition, single-scan TLS data provide strong global constraints for MLS-based forest mapping, which allows low mapping errors to be achieved, e.g., less than 2.0 cm mean errors in both the horizontal and vertical directions.},
	language = {en},
	urldate = {2021-12-26},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Shao, Jie and Zhang, Wuming and Mellado, Nicolas and Wang, Nan and Jin, Shuangna and Cai, Shangshu and Luo, Lei and Lejemble, Thibault and Yan, Guangjian},
	month = may,
	year = {2020},
	keywords = {Forest mapping, LiDAR, MLS, SLAM, Single-scan TLS},
	pages = {214--230},
}

@article{kuzelka_mathematically_2021,
	title = {Mathematically optimized trajectory for terrestrial close-range photogrammetric {3D} reconstruction of forest stands},
	volume = {178},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271621001714},
	doi = {10.1016/j.isprsjprs.2021.06.013},
	abstract = {Terrestrial close-range photogrammetry offers a low-cost method of three-dimensional (3D) reconstruction of forest stands that provides automatically processable 3D data that can be used to evaluate inventory parameters of forest stands and individual trees. However, fundamental methodological problems in image acquisition and processing remain. This study enhances the methodology of photogrammetric Structure from Motion reconstruction of forest stands by determining the best photographer's trajectory for image acquisition. The study comprises 1) mathematical optimization of the route in a square grid using integer programming, 2) evaluation of point clouds derived from sequences of real photographs, simulating different trajectories, and 3) verification on real trajectories. In a forest research plot, we established a 1 m square grid of 625 (i.e., 25 × 25) photographic positions, and at each position, we captured 16 photographs in uniformly spaced directions. We adopted real tree positions and diameters, and the coordinates of the photographic positions, including orientation angles of captured images, were recorded. We then formulated an integer programming optimization model to find the most efficient trajectory that provided coverage of all sides of all trees with sufficient counts of images. Subsequently, we used the 10,000 captured images to produce image subsets simulating image sequences acquired during the photographer's movement along 84 different systematic trajectories of seven patterns based on either parallel lines or concentric orbits. 3D point clouds derived from the simulated image sequences were evaluated for their suitability for automatic tree detection and estimation of diameters at breast height. The results of the integer programming model indicated that the optimal trajectory consisted of parallel line segments if the camera is pointed forward – in the travel direction, or concentric orbits if the camera is pointed to a side – perpendicular to the travel direction. With point clouds derived from the images of the simulated trajectories, the best diameter estimates on automatically detected trees were achieved with trajectories consisting of parallel lines in two perpendicular directions where each line was passed in both opposite directions. For efficient image acquisition, resulting in point clouds of reasonable quality with low counts of images, a trajectory consisting of concentric orbits, including the plot perimeter with the camera pointed towards the plot center, proved to be the best. Results of simulated trajectories were verified with the photogrammetric reconstruction of the forest stand based on real trajectories for six patterns. The mathematical optimization was consistent with the results of the experiment, which indicated that mathematical optimization may represent a valid tool for planning trajectories for photogrammetric 3D reconstruction of scenes in general.},
	language = {en},
	urldate = {2021-12-26},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Kuželka, Karel and Surový, Peter},
	month = aug,
	year = {2021},
	keywords = {Diameter at breast height, Forestry, Integer programming, Photogrammetry, Structure from motion, Traveling salesman},
	pages = {259--281},
}

@article{noauthor_low-complexity_2021,
	title = {Low-{Complexity} {Lossless} and {Near}-{Lossless} {Multispectral} and {Hyperspectral} {Image} {Compression}},
	language = {en},
	year = {2021},
	pages = {102},
}

@article{barrios_shyloc_2020,
	title = {{SHyLoC} 2.0: {A} {Versatile} {Hardware} {Solution} for {On}-{Board} {Data} and {Hyperspectral} {Image} {Compression} on {Future} {Space} {Missions}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {{SHyLoC} 2.0},
	doi = {10.1109/ACCESS.2020.2980767},
	abstract = {In this paper, we present the design, implementation and results of a set of IP cores that perform on-board hyperspectral image compression according to the CCSDS 123.0-B-1 lossless standard, specifically designed to be suited for on-board systems and for any kind of hyperspectral sensor. As entropy coder, the sample-adaptive entropy coder defined in the 123.0-B-1 standard or the low-complexity block-adaptive encoder defined by the CCSDS 121.0-B-2 lossless standard could be used. Both IPs, 123.0-B-1 and 121.0-B-2, are part of SHyLoC 2.0, and can be used together for compression of hyperspectral images, being also possible the compression of any kind of data using only the 121-IP. SHyLoC 2.0 improves and extends the capabilities of SHyLoC 1.0, currently available at the ESA IP Cores library, increasing its compression efficiency and throughput, without compromising the resources footprint. Moreover, it incorporates new features, such as the unit-delay predictor option defined by the CCSDS 121.0-B-2 standard, and burst capabilities in the external memory interface of the CCSDS 123-IP, among others. Dedicated architectures have been designed for all the possible input image sample arrangements, in order to maximise throughput and reduce the hardware resources utilization. The design is technology-agnostic, enabling the mapping of the VHDL code in different FPGAs or ASICs. Results are presented for a representative group of well-known space-qualified FPGAs, including the new NanoXplore BRAVE family. A maximum throughput of 150 MSamples/s is obtained for Xilinx Virtex XQR5VFX130 when the SHyLoC 2.0 CCSDS-123 IP is configured in Band-Interleaved by Pixel (BIP) order, using only the 4\% of LUTs and less than the 1\% of internal memory.},
	journal = {IEEE Access},
	author = {Barrios, Yubal and Sánchez, Antonio J. and Santos, Lucana and Sarmiento, Roberto},
	year = {2020},
	keywords = {Field programmable gate arrays, Hardware, Hyperspectral imaging, IP networks, Image coding, Standards, compression algorithms, field programmable gate arrays, hardware implementations, on-board data processing, space missions},
	pages = {54269--54287},
}

@article{guarnera_brdf_2016,
	title = {{BRDF} {Representation} and {Acquisition}},
	volume = {35},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12867},
	doi = {10.1111/cgf.12867},
	abstract = {Photorealistic rendering of real world environments is important in a range of different areas; including Visual Special effects, Interior/Exterior Modelling, Architectural Modelling, Cultural Heritage, Computer Games and Automotive Design. Currently, rendering systems are able to produce photorealistic simulations of the appearance of many real-world materials. In the real world, viewer perception of objects depends on the lighting and object/material/surface characteristics, the way a surface interacts with the light and on how the light is reflected, scattered, absorbed by the surface and the impact these characteristics have on material appearance. In order to re-produce this, it is necessary to understand how materials interact with light. Thus the representation and acquisition of material models has become such an active research area. This survey of the state-of-the-art of BRDF Representation and Acquisition presents an overview of BRDF (Bidirectional Reflectance Distribution Function) models used to represent surface/material reflection characteristics, and describes current acquisition methods for the capture and rendering of photorealistic materials.},
	language = {en},
	number = {2},
	urldate = {2022-03-28},
	journal = {Computer Graphics Forum},
	author = {Guarnera, D. and Guarnera, G.c. and Ghosh, A. and Denk, C. and Glencross, M.},
	year = {2016},
	keywords = {Categories and Subject Descriptors (according to ACM CCS), I.3.3 Computer Graphics: Picture/Image Generation—Line and curve generation, I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism—Colour, I.6.8 Computer Graphics: Types of simulation—Monte Carlo, and texture, shading, shadowing},
	pages = {625--650},
}

@article{ferraz_hyperspectral_2021,
	title = {Hyperspectral {Parallel} {Image} {Compression} on {Edge} {GPUs}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/13/6/1077},
	doi = {10.3390/rs13061077},
	abstract = {Edge applications evolved into a variety of scenarios that include the acquisition and compression of immense amounts of images acquired in space remote environments such as satellites and drones, where characteristics such as power have to be properly balanced with constrained memory and parallel computational resources. The CCSDS-123 is a standard for lossless compression of multispectral and hyperspectral images used in on-board satellites and military drones. This work explores the performance and power of 3 families of low-power heterogeneous Nvidia GPU Jetson architectures, namely the 128-core Nano, the 256-core TX2 and the 512-core Xavier AGX by proposing a parallel solution to the CCSDS-123 compressor on embedded systems, reducing development effort, compared to the production of dedicated circuits, while maintaining low power. This solution parallelizes the predictor on the low-power GPU while the entropy encoders exploit the heterogeneous multiple CPU cores and the GPU concurrently. We report more than 4.4 GSamples/s for the predictor and up to 6.7 Gb/s for the complete system, requiring less than 11 W and providing an efficiency of 611 Mb/s/W.},
	language = {en},
	number = {6},
	urldate = {2022-03-30},
	journal = {Remote Sensing},
	author = {Ferraz, Oscar and Silva, Vitor and Falcao, Gabriel},
	month = jan,
	year = {2021},
	keywords = {CCSDS-123, heterogeneous CPU + GPU architectures, high-throughput, hyperspectral image compression, low-power GPU},
	pages = {1077},
}

@book{ngan_experimental_2005,
	title = {Experimental {Analysis} of {BRDF} {Models}},
	isbn = {978-3-905673-23-4},
	url = {https://diglib.eg.org:443/xmlui/handle/10.2312/EGWR.EGSR05.117-126},
	abstract = {The Bidirectional Reflectance Distribution Function (BRDF) describes the appearance of a material by its interaction with light at a surface point. A variety of analytical models have been proposed to represent BRDFs. However, analysis of these models has been scarce due to the lack of high-resolution measured data. In this work we evaluate several well-known analytical models in terms of their ability to fit measured BRDFs. We use an existing high-resolution data set of a hundred isotropic materials and compute the best approximation for each analytical model. Furthermore, we have built a new setup for efficient acquisition of anisotropic BRDFs, which allows us to acquire anisotropic materials at high resolution. We have measured four samples of anisotropic materials (brushed aluminum, velvet, and two satins). Based on the numerical errors, function plots, and rendered images we provide insights into the performance of the various models. We conclude that for most isotropic materials physically-based analytic reflectance models can represent their appearance quite well. We illustrate the important difference between the two common ways of defining the specular lobe: around the mirror direction and with respect to the half-vector. Our evaluation shows that the latter gives a more accurate shape for the reflection lobe. Our analysis of anisotropic materials indicates current parametric reflectance models cannot represent their appearances faithfully in many cases. We show that using a sampled microfacet distribution computed from measurements improves the fit and qualitatively reproduces the measurements.},
	language = {en},
	urldate = {2022-03-29},
	publisher = {The Eurographics Association},
	author = {Ngan, Addy and Durand, Frédo and Matusik, Wojciech},
	year = {2005},
	doi = {10.2312/EGWR/EGSR05/117-126},
}

@article{da_silva_nunes_techniques_2022,
	title = {Techniques for {BRDF} evaluation},
	volume = {38},
	issn = {0178-2789, 1432-2315},
	url = {https://link.springer.com/10.1007/s00371-020-02035-9},
	doi = {10.1007/s00371-020-02035-9},
	abstract = {Bidirectional reﬂectance distribution functions (BRDFs) describe how light interacts with a point on a surface. To propose a new BRDF formulation or to compare different reﬂectance representations, it is necessary to confront the results obtained with those functions against previous work or measured data. Despite the importance of using reliable techniques to evaluate a BRDF, there is a lack of works in the literature that gathers and compares those. This paper proposes a compilation of techniques used to evaluate BRDF representations along with their formal deﬁnitions. Those techniques were classiﬁed into three different groups—comparison functions, rendered images, and plots—and, to illustrate their use, three classical and widely adopted models and one state-of-the-art BRDF representation were evaluated regarding their capacity to preserve the appearance of measured materials. Based on our research regarding comparison functions, a stable and robust BRDF evaluation technique is proposed. It has been observed during both literature review and experiments that each group of techniques provides complementary information about the evaluated BRDFs, which suggests that at least one model from each category should be adopted during the choice of criteria to evaluate a BRDF.},
	language = {en},
	number = {2},
	urldate = {2022-03-29},
	journal = {The Visual Computer},
	author = {da Silva Nunes, Mislene and Melo Nascimento, Fernando and Florêncio Miranda, Gastão and Trinchão Andrade, Beatriz},
	month = feb,
	year = {2022},
	pages = {573--589},
}

@book{hsia_bidirectional_1976,
	title = {Bidirectional reflectometry. {Part} {I}. {A} high resolution laser bidirectional reflectometer with results on several optical coatings},
	copyright = {The Journal of Research of the National Institute of Standards and Technology is a publication of the U.S. Government. The papers are in the public domain and are not subject to copyright in the United States. However, please pay special attention to the},
	url = {http://archive.org/details/jresv80An2p189},
	abstract = {Journal of Research of the National Bureau of Standards},
	language = {English},
	urldate = {2022-03-28},
	publisher = {National Bureau of Standards},
	author = {Hsia, Jack J. and Richmond, Joseph C.},
	year = {1976},
	keywords = {Barium sulphate},
}

@misc{nicodemus_geometrical_1977,
	title = {Geometrical {Considerations} and {Nomenclature} for {Reflectance}},
	url = {https://digital.library.unt.edu/ark:/67531/metadc70423/},
	abstract = {Report presenting a unified approach to the specification of reflectance, in terms of both incident- and reflected- beam geometry. Nomenclature to facilitate this approach is proposed. Nomenclature for categorizing and specifying reflectance quantities for a variety of different beam configurations (both incident and reflected beams) is described, and all are defined and interrelated in terms of the bidirectional reflectance-distribution function. The conditions under which the formalism can be applied, including situations involving considerable sub-surface scattering, are carefully established. The entire treatment is limited to the domain of classical geometrical-optics radiometry and does not take into account interference and diffraction phenomena, such as are frequently encountered with highly coherent radiant flux.},
	language = {English},
	urldate = {2022-03-28},
	author = {Nicodemus, F. E. and Richmond, J. C. and Hsia, J. J. and Ginsberg, I. W. and Limperis, T.},
	month = oct,
	year = {1977},
	note = {Publication Title: UNT Digital Library
Type: Report},
}

@incollection{otani_brdf_2021,
	address = {Cham},
	title = {{BRDF} {Measurement} of {Real} {Materials} {Using} {Handheld} {Cameras}},
	volume = {13017},
	isbn = {978-3-030-90438-8 978-3-030-90439-5},
	url = {https://link.springer.com/10.1007/978-3-030-90439-5_6},
	abstract = {In this paper, we propose a method for measuring the Bidirectional Reﬂectance Distribution Function (BRDF) of real planar materials using a simple apparatus. Our proposed method uses two handheld cameras, a light source mounted on one of the cameras, and a box with markers attached to each face. A planar material is placed on the box and the user acquires video images of the material while moving the two cameras around the material. The system obtains a sampled BRDF using the light source and viewpoint positions and pixel values at a certain point on the material. Then, a dense BRDF is estimated by interpolating the sampled BRDF using the technique of compressed sensing. The experimental results showed that the proposed method can reproduce the reﬂectance properties of real materials. It was also shown that compressed sensing was more suitable than RBF interpolation for estimating dense BRDFs.},
	language = {en},
	urldate = {2022-03-28},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer International Publishing},
	author = {Otani, Haru and Komuro, Takashi},
	editor = {Bebis, George and Athitsos, Vassilis and Yan, Tong and Lau, Manfred and Li, Frederick and Shi, Conglei and Yuan, Xiaoru and Mousas, Christos and Bruder, Gerd},
	year = {2021},
	doi = {10.1007/978-3-030-90439-5_6},
	pages = {65--77},
}

@inproceedings{prusinkiewicz_use_2001,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '01},
	title = {The use of positional information in the modeling of plants},
	isbn = {978-1-58113-374-5},
	url = {https://doi.org/10.1145/383259.383291},
	doi = {10.1145/383259.383291},
	abstract = {We integrate into plant models three elements of plant representation identified as important by artists: posture (manifested in curved stems and elongated leaves), gradual variation of features, and the progression of the drawing process from overall silhouette to local details. The resulting algorithms increase the visual realism of plant models by offering an intuitive control over plant form and supporting an interactive modeling process. The algorithms are united by the concept of expressing local attributes of plant architecture as functions of their location along the stems.},
	urldate = {2020-04-23},
	booktitle = {Proceedings of the 28th annual conference on {Computer} graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Prusinkiewicz, Przemyslaw and Mündermann, Lars and Karwowski, Radoslaw and Lane, Brendan},
	month = aug,
	year = {2001},
	keywords = {Chomsky grammar, L-system, differential turtle geometry, generalized cylinder, interactive procedural modeling, phyllotaxis, plant, positional information, realistic image synthesis},
	pages = {289--300},
}

@article{pu_principles_2019,
	title = {Principles of {Hyperspectral} {Microscope} {Imaging} {Techniques} and {Their} {Applications} in {Food} {Quality} and {Safety} {Detection}: {A} {Review}},
	volume = {18},
	issn = {1541-4337},
	shorttitle = {Principles of {Hyperspectral} {Microscope} {Imaging} {Techniques} and {Their} {Applications} in {Food} {Quality} and {Safety} {Detection}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1541-4337.12432},
	doi = {10.1111/1541-4337.12432},
	abstract = {Hyperspectral imaging (HSI) techniques play an important role in the food industry for providing rapid, nondestructive, and chemical-free detection method, whereas a microscope can provide detailed information about the microstructure of a food item. As an emerging imaging spectroscopy technique, hyperspectral microscope imaging (HMI) technique combines the advantages of HSI with microscopic imaging and has been gradually applied in the food industry. This review introduces the principles of different kinds of HMI techniques, such as fluorescence HMI, visible/near-infrared HMI, Raman HMI, and infrared HMI. Moreover, detailed applications of HMI techniques are summarized, including evaluation of structures of nutrients, and detection of microorganisms and residues. On the other hand, some challenges and future trends in the applications of these techniques are also discussed. It is concluded that by integrating HSI with microscopy, HMI can not only provide both spectral and spatial information about food substances but also provide their chemical information at the molecular or cellular level. Therefore, HMI techniques have great potentials in nondestructive evaluation of structures of nutrients, and detection of microorganisms and residues for the food industry.},
	language = {en},
	number = {4},
	urldate = {2022-03-28},
	journal = {Comprehensive Reviews in Food Science and Food Safety},
	author = {Pu, Hongbin and Lin, Lian and Sun, Da-Wen},
	year = {2019},
	keywords = {Raman, fluorescence, hyperspectral microscope imaging, infrared, visible/near-infrared},
	pages = {853--866},
}

@article{gao_optical_2015,
	title = {Optical hyperspectral imaging in microscopy and spectroscopy – a review of data acquisition},
	volume = {8},
	issn = {1864-0648},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jbio.201400051},
	doi = {10.1002/jbio.201400051},
	abstract = {Rather than simply acting as a photographic camera capturing two-dimensional (x, y) intensity images or a spectrometer acquiring spectra (λ), a hyperspectral imager measures entire three-dimensional (x, y, λ) datacubes for multivariate analysis, providing structural, molecular, and functional information about biological cells or tissue with unprecedented detail. Such data also gives clinical insights for disease diagnosis and treatment. We summarize the principles underpinning this technology, highlight its practical implementation, and discuss its recent applications at microscopic to macroscopic scales. Datacube acquisition strategies in hyperspectral imaging x, y, spatial coordinates; λ, wavelength.},
	language = {en},
	number = {6},
	urldate = {2022-03-28},
	journal = {Journal of Biophotonics},
	author = {Gao, Liang. and Smith, R. Theodore},
	year = {2015},
	keywords = {biomedical imaging, hyperspectral imaging, multispectral imaging},
	pages = {441--456},
}

@article{jurado_efficient_2022,
	title = {An efficient method for acquisition of spectral {BRDFs} in real-world scenarios},
	volume = {102},
	issn = {0097-8493},
	url = {https://www.sciencedirect.com/science/article/pii/S0097849321001850},
	doi = {10.1016/j.cag.2021.08.021},
	abstract = {Modelling of material appearance from reflectance measurements has become increasingly prevalent due to the development of novel methodologies in Computer Graphics. In the last few years, some advances have been made in measuring the light-material interactions, by employing goniometers/reflectometers under specific laboratory’s constraints. A wide range of applications benefit from data-driven appearance modelling techniques and material databases to create photorealistic scenarios and physically based simulations. However, important limitations arise from the current material scanning process, mostly related to the high diversity of existing materials in the real-world, the tedious process for material scanning and the spectral characterisation behaviour. Consequently, new approaches are required both for the automatic material acquisition process and for the generation of measured material databases. In this study, a novel approach for material appearance acquisition using hyperspectral data is proposed. A dense 3D point cloud filled with spectral data was generated from the images obtained by an unmanned aerial vehicle (UAV) equipped with an RGB camera and a hyperspectral sensor. The observed hyperspectral signatures were used to recognise natural and artificial materials in the 3D point cloud according to spectral similarity. Then, a parametrisation of Bidirectional Reflectance Distribution Function (BRDF) was carried out by sampling the BRDF space for each material. Consequently, each material is characterised by multiple samples with different incoming and outgoing angles. Finally, an analysis of BRDF sample completeness is performed considering four sunlight positions and 16x16 resolution for each material. The results demonstrated the capability of the used technology and the effectiveness of our method to be used in applications such as spectral rendering and real-word material acquisition and classification.},
	language = {en},
	urldate = {2022-03-28},
	journal = {Computers \& Graphics},
	author = {Jurado, Juan M. and Jiménez-Pérez, J. Roberto and Pádua, Luís and Feito, Francisco R. and Sousa, Joaquim J.},
	month = feb,
	year = {2022},
	keywords = {3D models, BRDF, Hyperspectral imaging, Material appearance, Reflectance data measurements, UAV-based sensors},
	pages = {154--163},
}

@article{jaud_direct_2018,
	title = {Direct {Georeferencing} of a {Pushbroom}, {Lightweight} {Hyperspectral} {System} for {Mini}-{UAV} {Applications}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/10/2/204},
	doi = {10.3390/rs10020204},
	abstract = {Hyperspectral imagery has proven its potential in many research applications, especially in the field of environmental sciences. Currently, hyperspectral imaging is generally performed by satellite or aircraft platforms, but mini-UAV (Unmanned Aerial Vehicle) platforms ({\textbackslash}textless20 kg) are now emerging. On such platforms, payload restrictions are critical, so sensors must be selected according to stringent specifications. This article presents the integration of a light pushbroom hyperspectral sensor onboard a multirotor UAV, which we have called Hyper-DRELIO (Hyperspectral DRone for Environmental and LIttoral Observations). This article depicts the system design: the UAV platform, the imaging module, the navigation module, and the interfacing between the different elements. Pushbroom sensors offer a better combination of spatial and spectral resolution than full-frame cameras. Nevertheless, data georectification has to be performed line by line, the quality of direct georeferencing being limited by mechanical stability, good timing accuracy, and the resolution and accuracy of the proprioceptive sensors. A georegistration procedure is proposed for geometrical pre-processing of hyperspectral data. The specifications of Hyper-DRELIO surveys are described through two examples of surveys above coastal or inland waters, with different flight altitudes. This system can collect hyperspectral data in VNIR (Visible and Near InfraRed) domain above small study sites (up to about 4 ha) with both high spatial resolution ({\textbackslash}textless10 cm) and high spectral resolution (1.85 nm) and with georectification accuracy on the order of 1 to 2 m.},
	language = {en},
	number = {2},
	urldate = {2022-03-26},
	journal = {Remote Sensing},
	author = {Jaud, Marion and Le Dantec, Nicolas and Ammann, Jérôme and Grandjean, Philippe and Constantin, Dragos and Akhtman, Yosef and Barbieux, Kevin and Allemand, Pascal and Delacourt, Christophe and Merminod, Bertrand},
	month = feb,
	year = {2018},
	keywords = {Unmanned Aerial Vehicle (UAV), georectification, hyperspectral cube reconstruction, hyperspectral mapping},
	pages = {204},
}

@inproceedings{mech_visual_1996,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '96},
	title = {Visual models of plants interacting with their environment},
	isbn = {978-0-89791-746-9},
	url = {https://doi.org/10.1145/237170.237279},
	doi = {10.1145/237170.237279},
	urldate = {2020-06-22},
	booktitle = {Proceedings of the 23rd annual conference on {Computer} graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Měch, Radomír and Prusinkiewicz, Przemyslaw},
	month = aug,
	year = {1996},
	keywords = {L-system, clonal plant, ecosystem, modeling, plant development, realistic image synthesis, root, scientific visualization, simulation, software design, tree},
	pages = {397--410},
}

@inproceedings{lenz_automatic_2014,
	title = {Automatic in-flight boresight calibration considering topography for hyperspectral pushbroom sensors},
	doi = {10.1109/IGARSS.2014.6947103},
	abstract = {This paper suggests a method for automatic in-flight boresight calibration of pushbroom scanner images, using an on-line system with broadband data downlink and near realtime georeferencing of the pushbroom image data. Georeferencing accuracy may decrease during long image acquisition flights due to instable atmospheric conditions, which may lead to geometric changes in the flight platform. Orthorectification of (hyperspectral) pushbroom scanner data demands the knowledge of the extrinsic orientation parameters for every exposure. The most crucial parameters for the transformation of the pose obtained by the inertial navigation system (INS) into the projection center of the imaging sensor are the boresight angles. Utilizing a performant ray tracing algorithm and a digital elevation model (DEM), these parameters can be estimated even while flying in uneven and uninhabited areas. Tie points for solving an extended collinear equation are extracted automatically by the SURF algorithm.},
	booktitle = {2014 {IEEE} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Lenz, Andreas and Schilling, Hendrik and Perpeet, Dominik and Wuttke, Sebastian and Gross, Wolfgang and Middelmann, Wolfgang},
	month = jul,
	year = {2014},
	keywords = {Calibration, Hyperspectral imaging, On-line boresight calibration, Sensor systems, Strips, Three-dimensional displays, hyperspectral imaging, image registration, pushbroom sensor, remote sensing},
	pages = {2981--2984},
}

@article{habib_boresight_2018,
	title = {Boresight {Calibration} of {GNSS}/{INS}-{Assisted} {Push}-{Broom} {Hyperspectral} {Scanners} on {UAV} {Platforms}},
	volume = {11},
	issn = {2151-1535},
	doi = {10.1109/JSTARS.2018.2813263},
	abstract = {Low-cost unmanned aerial vehicles (UAVs) utilizing push-broom hyperspectral scanners are poised to become a popular alternative to conventional remote sensing platforms such as manned aircraft and satellites. In order to employ this emerging technology in fields such as high-throughput phenotyping and precision agriculture, direct georeferencing of hyperspectral data using onboard integrated global navigation satellite systems (GNSSs) and inertial navigation systems (INSs) is required. Directly deriving the scanner position and orientation requires the spatial and rotational relationship between the coordinate systems of the GNSS/INS and hyperspectral scanner to be measured. The spatial offset (lever arm) between the scanner and GNSS/INS unit can be measured manually. However, the angular relationship (boresight angles) between the scanner and GNSS/INS coordinate systems, which is more critical for accurate generation of georeferenced products, is difficult to establish. This paper presents three calibration approaches to estimate the boresight angles relating hyperspectral push-broom scanner and GNSS/INS coordinate systems. For reliable/practical estimation of the boresight angles, this paper starts with establishing the optimal/minimal flight and control/tie point configuration through a bias impact analysis starting from the point positioning equation. Then, an approximate calibration procedure utilizing tie points in overlapping scenes is presented after making some assumptions about the flight trajectory and topography of covered terrain. Next, two rigorous approaches are introduced - one using ground control points and other using tie features. The approximate/rigorous approaches are based on enforcing the collinearity and coplanarity of the light rays connecting the perspective centers of the imaging scanner, object point, and the respective image points. To evaluate the accuracy of the proposed approaches, estimated boresight angles are used for orthorectification of six hyperspectral UAV dataset acquired over an agricultural field. Qualitative and quantitative evaluations of the results have shown significant improvement in the derived orthophotos to a level equivalent to the ground sampling distance of the used scanner (namely, 3-5 cm when flying at 60 m).},
	number = {5},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Habib, Ayman and Zhou, Tian and Masjedi, Ali and Zhang, Zhou and Evan Flatt, John and Crawford, Melba},
	month = may,
	year = {2018},
	keywords = {Agriculture, Boresight calibration, Calibration, Hyperspectral imaging, Satellites, Sensors, Unmanned aerial vehicles, direct georeferencing, hyperspectral imaging, integrated global navigation satellite system/inertial navigation system (GNSS/INS), push-broom scanner, unmanned aerial vehicles (UAVs)},
	pages = {1734--1749},
}

@article{yue_estimation_2017,
	title = {Estimation of {Winter} {Wheat} {Above}-{Ground} {Biomass} {Using} {Unmanned} {Aerial} {Vehicle}-{Based} {Snapshot} {Hyperspectral} {Sensor} and {Crop} {Height} {Improved} {Models}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/9/7/708},
	doi = {10.3390/rs9070708},
	abstract = {Correct estimation of above-ground biomass (AGB) is necessary for accurate crop growth monitoring and yield prediction. We estimated AGB based on images obtained with a snapshot hyperspectral sensor (UHD 185 firefly, Cubert GmbH, Ulm, Baden-Württemberg, Germany) mounted on an unmanned aerial vehicle (UAV). The UHD 185 images were used to calculate the crop height and hyperspectral reflectance of winter wheat canopies from hyperspectral and panchromatic images. We constructed several single-parameter models for AGB estimation based on spectral parameters, such as specific bands, spectral indices (e.g., Ratio Vegetation Index (RVI), NDVI, Greenness Index (GI) and Wide Dynamic Range VI (WDRVI)) and crop height and several models combined with spectral parameters and crop height. Comparison with experimental results indicated that incorporating crop height into the models improved the accuracy of AGB estimations (the average AGB is 6.45 t/ha). The estimation accuracy of single-parameter models was low (crop height only: R2 = 0.50, RMSE = 1.62 t/ha, MAE = 1.24 t/ha; R670 only: R2 = 0.54, RMSE = 1.55 t/ha, MAE = 1.23 t/ha; NDVI only: R2 = 0.37, RMSE = 1.81 t/ha, MAE = 1.47 t/ha; partial least squares regression R2 = 0.53, RMSE = 1.69, MAE = 1.20), but accuracy increased when crop height and spectral parameters were combined (partial least squares regression modeling: R2 = 0.78, RMSE = 1.08 t/ha, MAE = 0.83 t/ha; verification: R2 = 0.74, RMSE = 1.20 t/ha, MAE = 0.96 t/ha). Our results suggest that crop height determined from the new UAV-based snapshot hyperspectral sensor can improve AGB estimation and is advantageous for mapping applications. This new method can be used to guide agricultural management.},
	language = {en},
	number = {7},
	urldate = {2022-03-26},
	journal = {Remote Sensing},
	author = {Yue, Jibo and Yang, Guijun and Li, Changchun and Li, Zhenhai and Wang, Yanjie and Feng, Haikuan and Xu, Bo},
	month = jul,
	year = {2017},
	keywords = {crop height, hyperspectral image, partial least squares regression, unmanned aerial vehicle platforms, winter wheat biomass},
	pages = {708},
}

@article{bareth_low-weight_2015,
	title = {Low-weight and {UAV}-based {Hyperspectral} {Full}-frame {Cameras} for {Monitoring} {Crops}: {Spectral} {Comparison} with {Portable} {Spectroradiometer} {Measurements}},
	issn = {,},
	shorttitle = {Low-weight and {UAV}-based {Hyperspectral} {Full}-frame {Cameras} for {Monitoring} {Crops}},
	url = {https://www.schweizerbart.de/papers/pfg/detail/2015/84644/Low_weight_and_UAV_based_Hyperspectral_Full_frame_?af=crossref},
	doi = {10.1127/pfg/2015/0256},
	language = {en},
	urldate = {2022-03-26},
	journal = {Photogrammetrie - Fernerkundung - Geoinformation},
	author = {Bareth, Georg and Aasen, Helge and Bendig, Juliane and Gnyp, Martin Leon and Bolten, Andreas and Jung, András and Michels, René and Soukkamäki, Jussi},
	month = feb,
	year = {2015},
	pages = {69--79},
}

@article{reeves_approximate_1985,
	title = {Approximate and probabilistic algorithms for shading and rendering structured particle systems},
	volume = {19},
	issn = {0097-8930},
	url = {https://doi.org/10.1145/325165.325250},
	doi = {10.1145/325165.325250},
	abstract = {Detail enhances the visual richness and realism of computer-generated images. Our stochastic modelling approach, called particle systems, builds complex pictures from sets of simple, volume-filling primitives. For example, structured particle systems have been used to generate trees and a grass-covered forest floor. Particle systems can produce so much irregular, three-dimensional detail that exact shading and visible surface calculations become infeasible. We describe approximate and probabilistic algorithms for shading and the visible surface problem. Because particle systems algorithms generate richly-detailed images, it is hard to detect any deviation from an exact rendering. Recent work in stochastic modelling also enables us to model complex motions with random variation, such as a field of grass blowing in the breeze. We analyze the performance of our current algorithms to understand the costs of our stochastic modelling approach.},
	number = {3},
	urldate = {2020-06-24},
	journal = {ACM SIGGRAPH Computer Graphics},
	author = {Reeves, William T. and Blau, Ricki},
	month = jul,
	year = {1985},
	keywords = {approximation, stochastic modelling},
	pages = {313--322},
}

@inproceedings{elbahnasawy_multi-sensor_2018,
	title = {Multi-{Sensor} {Integration} {Onboard} a {UAV}-{Based} {Mobile} {Mapping} {System} for {Agricultural} {Management}},
	doi = {10.1109/IGARSS.2018.8517370},
	abstract = {Due to the advances in technological and industrial fields, remote sensing has been adopted to a considerable extent in precision agricultural applications. Over the past few years, remote sensing utilized Mobile Mapping Systems (MMS) as the platforms for agricultural data collection. For accurate generation of georeferenced products using such MMSs, there should be a robust calibration approach that can accurately estimate the mounting parameters of the involved sensors, i.e., LiDAR unit, camera, and hyperspectral push-broom scanner. In this paper, we propose novel calibration approaches for various sensors onboard a UAV platform - 1) simultaneous estimation of lever arm and boresight angles relating LiDAR unit and camera to the GNSS/INS unit, and 2) estimation of boresight angles relating hyperspectral push-broom scanner and the GNSS/TNS unit.},
	booktitle = {{IGARSS} 2018 - 2018 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Elbahnasawy, Magdv and Shamseldin, Tamer and Ravi, Radhika and Zhou, Tian and Lin, Yun-Jou and Masjedi, Ali and Flatt, Evan and Crawford, Melba and Habib, Ayman},
	month = jul,
	year = {2018},
	keywords = {Calibration, Cameras, Hyperspectral imaging, Laser radar, Mathematical model, Three-dimensional displays, biomass, canopy structure, geospatial data},
	pages = {3412--3415},
}

@inproceedings{fang_novel_2019,
	title = {A {Novel} {Mosaic} {Method} for {UAV}-{Based} {Hyperspectral} {Images}},
	doi = {10.1109/IGARSS.2019.8900057},
	abstract = {In recent years, the rapid development of light remote sensing platforms and remote sensing load has brought unprecedented opportunities for remote sensing of Unmanned Aerial Vehicle (UAV). In order to meet the high requirements of remote sensing applications such as precision agriculture, forestry monitoring and environmental protection etc., we designed and integrated a UAV-based hyperspectral imaging system. This paper briefly introduced the composition of the system, and discussed the geometric processing of hyperspectral data. A Spline function mosaic method based on Sparse Bundle Adjustment (SSBA) is proposed for rapid flight strip images stitching. Hyperspectral data is collected by the UAV imaging system, and the stitching results indicate that our method can obtain better results than SURF.},
	booktitle = {{IGARSS} 2019 - 2019 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Fang, Junyong and Wang, Xiao and Zhu, Tianyi and Liu, Xue and Zhang, Xiaohong and Zhao, Dong},
	month = jul,
	year = {2019},
	keywords = {Bundle adjustment, Hyperspectral data, Hyperspectral imaging, Mosaic Spline function Sparse Matrix, Splines (mathematics), Strips, UAV, Unmanned aerial vehicles},
	pages = {9220--9223},
}

@inproceedings{ivelja_improving_2020,
	title = {{IMPROVING} {VERTICAL} {ACCURACY} {OF} {UAV} {DIGITAL} {SURFACE} {MODELS} {BY} {INTRODUCING} {TERRESTRIAL} {LASER} {SCANS} {ON} {A} {POINT}-{CLOUD} {LEVEL}},
	volume = {XLIII-B1-2020},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLIII-B1-2020/457/2020/},
	doi = {10.5194/isprs-archives-XLIII-B1-2020-457-2020},
	abstract = {{\textbackslash}textlessp{\textbackslash}textgreater{\textbackslash}textlessstrong class="journal-contentHeaderColor"{\textbackslash}textgreaterAbstract.{\textbackslash}textless/strong{\textbackslash}textgreater Digital Surface Models (DSM) generated by image-based scene reconstruction from Unmanned Aerial Vehicle (UAV) and Terrestrial Laser Scanning (TLS)point clouds are highly distinguished in terms of resolution and accuracy. This leads to a situation where users have to choose the most beneficial product to fulfill their needs. In the current study, these techniques no longer compete but complement each other. Experiments were implemented to verify the improvement of vertical accuracy by introducing different amounts and configurations of Terrestrial Laser scans in the photogrammetric Structure from Motion (SfM) workflow for high-resolution 3D-scene reconstruction. Results show that it is possible to significantly improve (\&sim; 49\% ) the vertical accuracy of DSMs by introducing a TLS point clouds. However, accuracy improvement is highly associated with the number of introduced Ground Control Points (GCP) in the SfM workflow procedure.{\textbackslash}textless/p{\textbackslash}textgreater},
	language = {English},
	urldate = {2022-03-26},
	booktitle = {The {International} {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Ivelja, T. and Bechor, B. and Hasan, O. and Miko, S. and Sivan, D. and Brook, A.},
	month = aug,
	year = {2020},
	pages = {457--463},
}

@inproceedings{ravi_uav-based_2019,
	title = {{UAV}-based multi-sensor multi-platform integration for high throughput phenotyping},
	volume = {11008},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11008/110080E/UAV-based-multi-sensor-multi-platform-integration-for-high-throughput/10.1117/12.2519190.full},
	doi = {10.1117/12.2519190},
	abstract = {In recent days, phenotyping of various crops is gaining widespread popularity due to its ability to recognize variations in the effects of different genotypes of a particular crop in terms of its growth, yield, biomass, and so on. Such an application requires extensive data collection and analysis with a high spatial and temporal resolution, which can be attained using multiple sensors onboard Unmanned Aerial Vehicles (UAVs). In this study, we focus on harnessing information from a variety of sensors, such as RGB cameras, LiDAR units, and push-broom hyperspectral sensors – Short-wave Infrared (SWIR) and Visible Near Infrared (VNIR). The major challenge that needs to be overcome in this regard is to ensure an accurate integration of information captured across several days from the different sensor modalities. Moreover, the payload constraint for UAVs restrain us from mounting all the sensors simultaneously during a single flight mission, thus entailing the need for data capture from different sensors mounted on separate platforms that are flown individually over the agricultural field of interest. The first step towards integration of different data modalities is the generation of georeferenced products from each of the flight missions, which is accomplished with the help of Global Navigation Satellite Systems (GNSS) and Inertial Navigation Systems (INS) mounted on the UAVs that are time-synchronized with the onboard LiDAR units, cameras and/or hyperspectral sensors. Furthermore, an accurate georeferencing is achieved by developing robust calibration approaches dedicated towards accurate estimation of mounting parameters of the involved sensors. Finally, the geometric and spectral characteristics, such as canopy cover and leaf count, derived from the different sensors are used to devise a model to analyze the phenotypic traits of crops. The preliminary results indicate that the proposed calibration techniques can attain an accuracy of upto 3 cm.},
	urldate = {2022-03-26},
	booktitle = {Autonomous {Air} and {Ground} {Sensing} {Systems} for {Agricultural} {Optimization} and {Phenotyping} {IV}},
	publisher = {SPIE},
	author = {Ravi, Radhika and Hasheminasab, Seyyed Meghdad and Zhou, Tian and Masjedi, Ali and Quijano, Karoll and Flatt, John Evan and Crawford, Melba and Habib, Ayman},
	month = may,
	year = {2019},
	pages = {106--120},
}

@misc{noauthor_improving_nodate,
	title = {Improving {Orthorectification} of {UAV}-{Based} {Push}-{Broom} {Scanner} {Imagery} {Using} {Derived} {Orthophotos} {From} {Frame} {Cameras} {\textbackslash}textbar {IEEE} {Journal} of {Selected} {Topics} in {Applied} {Earth} {Observations} and {Remote} {Sensing} {\textbackslash}textbar {onAcademic}},
	url = {https://www.onacademic.com/detail/journal_1000038688314810_8aa4.html},
	urldate = {2022-03-26},
}

@article{suomalainen_lightweight_2014,
	title = {A {Lightweight} {Hyperspectral} {Mapping} {System} and {Photogrammetric} {Processing} {Chain} for {Unmanned} {Aerial} {Vehicles}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/6/11/11013},
	doi = {10.3390/rs61111013},
	abstract = {During the last years commercial hyperspectral imaging sensors have been miniaturized and their performance has been demonstrated on Unmanned Aerial Vehicles (UAV). However currently the commercial hyperspectral systems still require minimum payload capacity of approximately 3 kg, forcing usage of rather large UAVs. In this article we present a lightweight hyperspectral mapping system (HYMSY) for rotor-based UAVs, the novel processing chain for the system, and its potential for agricultural mapping and monitoring applications. The HYMSY consists of a custom-made pushbroom spectrometer (400–950 nm, 9 nm FWHM, 25 lines/s, 328 px/line), a photogrammetric camera, and a miniature GPS-Inertial Navigation System. The weight of HYMSY in ready-to-fly configuration is only 2.0 kg and it has been constructed mostly from off-the-shelf components. The processing chain uses a photogrammetric algorithm to produce a Digital Surface Model (DSM) and provides high accuracy orientation of the system over the DSM. The pushbroom data is georectified by projecting it onto the DSM with the support of photogrammetric orientations and the GPS-INS data. Since an up-to-date DSM is produced internally, no external data are required and the processing chain is capable to georectify pushbroom data fully automatically. The system has been adopted for several experimental flights related to agricultural and habitat monitoring applications. For a typical flight, an area of 2–10 ha was mapped, producing a RGB orthomosaic at 1–5 cm resolution, a DSM at 5–10 cm resolution, and a hyperspectral datacube at 10–50 cm resolution.},
	language = {en},
	number = {11},
	urldate = {2022-03-26},
	journal = {Remote Sensing},
	author = {Suomalainen, Juha and Anders, Niels and Iqbal, Shahzad and Roerink, Gerbert and Franke, Jappe and Wenting, Philip and Hünniger, Dirk and Bartholomeus, Harm and Becker, Rolf and Kooistra, Lammert},
	month = nov,
	year = {2014},
	keywords = {Unmanned Aerial Vehicle (UAV), agriculture, hyperspectral mapping system, photogrammetry, remote sensing},
	pages = {11013--11030},
}

@article{turner_pushbroom_2017,
	title = {{PUSHBROOM} {HYPERSPECTRAL} {IMAGING} {FROM} {AN} {UNMANNED} {AIRCRAFT} {SYSTEM} ({UAS}) – {GEOMETRIC} {PROCESSINGWORKFLOW} {AND} {ACCURACY} {ASSESSMENT}},
	volume = {XLII-2-W6},
	issn = {1682-1750, 2194-9034},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2-W6/379/2017/isprs-archives-XLII-2-W6-379-2017.pdf},
	doi = {10.5194/isprs-archives-XLII-2-W6-379-2017},
	abstract = {DOAJ is a community-curated online directory that indexes and provides access to high quality, open access, peer-reviewed journals.},
	language = {en},
	urldate = {2022-03-26},
	journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Turner, D. and Lucieer, A. and McCabe, M. and Parkes, S. and Clarke, I.},
	month = aug,
	year = {2017},
	pages = {379--384},
}

@article{zarco-tejada_spatio-temporal_2013,
	title = {Spatio-temporal patterns of chlorophyll fluorescence and physiological and structural indices acquired from hyperspectral imagery as compared with carbon fluxes measured with eddy covariance},
	volume = {133},
	issn = {0034-4257},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425713000436},
	doi = {10.1016/j.rse.2013.02.003},
	abstract = {This study provides insight into the assessment of the spatio-temporal trends of chlorophyll fluorescence, narrow-band physiological indices, and structural indices acquired with a hyperspectral imager flown over a flux tower in a canopy characterized by small seasonal structural changes and a heterogeneous architecture. A total of seven flights between summer and autumn were conducted with a hyperspectral camera that captured 30cm resolution imagery and 260 spectral bands in the 400-900nm region. This enabled the identification of pure-vegetation tree crown pixels around an eddy covariance flux tower without shadow components or background effects. The hyperspectral imagery was used to study the temporal patterns of canopy fluorescence and reflectance indices related to physiology and canopy structure. The seasonal trends observed in the airborne indices and fluorescence and their relationship with gross primary production (GPP) demonstrated that vegetation indices mostly related to structure such as the normalized difference vegetation index (NDVI) and the enhanced vegetation index (EVI) yielded non-significant relationships (r2=0.17; p{\textbackslash}textgreater0.05) due to the small structural changes in the canopy through the season. By contrast, physiological indices related to chlorophyll content (TCARI/OSAVI), light use efficiency (PRI570), and canopy chlorophyll fluorescence calculated through the Fraunhofer Line Depth principle (FLD3) showed a similar seasonal trend to that of GPP measured at the same time of the flights (r2 in the range 0.75–0.84; p{\textbackslash}textless0.01). These results are consistent with the physiological trend observed during summer and autumn, which showed that chlorophyll content increased by 17.9\% while the NDVI and the estimated tree crown projected LAI (LAIp) remained almost constant during the experiment (3\% variation). The time-series hyperspectral dataset demonstrated that the seasonal trajectories of the NDVI and EVI were weakly related (p{\textbackslash}textgreater0.05) to the physiological indicators such as TCARI/OSAVI, PRI570 and fluorescence. The spatial variability of the hyperspectral indices investigated through the coefficient of variation (CV) showed that fluorescence around the tower varied up to 17\% at the time of the maximum stress (summer), while LAIp showed little variation during that time (CV=1.8\%). After the summer stress period, the CV for fluorescence and chlorophyll content decreased in autumn down to 9\%. This study demonstrates that small physiological changes occurring in an evergreen canopy were still captured by remote sensing physiological indices and high-resolution airborne fluorescence. These indicators are required for GPP monitoring when the vegetation dynamics are not captured by remote sensing structural indices.},
	language = {en},
	urldate = {2022-03-26},
	journal = {Remote Sensing of Environment},
	author = {Zarco-Tejada, P. J. and Morales, A. and Testi, L. and Villalobos, F. J.},
	month = jun,
	year = {2013},
	keywords = {Chlorophyll, Eddy covariance, GPP, Gross primary production, Hyperspectral, Narrow-band indices, Physiological indices, UAV, Unmanned aerial vehicle},
	pages = {102--115},
}

@article{zarco-tejada_tree_2014,
	title = {Tree height quantification using very high resolution imagery acquired from an unmanned aerial vehicle ({UAV}) and automatic {3D} photo-reconstruction methods},
	volume = {55},
	issn = {1161-0301},
	url = {https://www.sciencedirect.com/science/article/pii/S1161030114000069},
	doi = {10.1016/j.eja.2014.01.004},
	abstract = {This study provides insight into the assessment of canopy biophysical parameter retrieval using passive sensors and specifically into the quantification of tree height in a discontinuous canopy using a low-cost camera on board an unmanned aerial vehicle (UAV). The UAV was a 2-m wingspan fixed-wing platform with 5.8kg take-off weight and 63km/h ground speed. It carried a consumer-grade RGB camera modified for color-infrared detection (CIR) and synchronized with a GPS unit. In this study, the configuration of the electric UAV carrying the camera payload enabled the acquisition of 158ha in one single flight. The camera system made it possible to acquire very high resolution (VHR) imagery (5cmpixel−1) to generate ortho-mosaics and digital surface models (DSMs) through automatic 3D reconstruction methods. The UAV followed pre-designed flight plans over each study site to ensure the acquisition of the imagery with large across- and along-track overlaps (i.e. {\textbackslash}textgreater80\%) using a grid of parallel and perpendicular flight lines. The validation method consisted of taking field measurements of the height of a total of 152 trees in two different study areas using a GPS in real-time kinematic (RTK) mode. The results of the validation assessment conducted to estimate tree height from the VHR DSMs yielded R2=0.83, an overall root mean square error (RMSE) of 35cm, and a relative root mean square error (R-RMSE) of 11.5\% for trees with heights ranging between 1.16 and 4.38m. An assessment conducted on the effects of the spatial resolution of the input images acquired by the UAV on the photo-reconstruction method and DSM generation demonstrated stable relationships for pixel resolutions between 5 and 30cm that rapidly degraded for input images with pixel resolutions lower than 35cm. RMSE and R-RMSE values obtained as a function of input pixel resolution showed errors in tree quantification below 15\% when 30cmpixel−1 resolution imagery was used to generate the DSMs. The study conducted in two orchards with this UAV system and the photo-reconstruction method highlighted that an inexpensive approach based on consumer-grade cameras on board a hand-launched unmanned aerial platform can provide accuracies comparable to those of the expensive and computationally more complex light detection and ranging (LIDAR) systems currently operated for agricultural and environmental applications.},
	language = {en},
	urldate = {2022-03-26},
	journal = {European Journal of Agronomy},
	author = {Zarco-Tejada, P. J. and Diaz-Varela, R. and Angileri, V. and Loudjani, P.},
	month = apr,
	year = {2014},
	keywords = {3D image modeling, DSM, Low-cost camera, Photo reconstruction, Remote sensing, Tree height},
	pages = {89--99},
}

@article{lindenmayer_developmental_1971,
	title = {Developmental systems without cellular interactions, their languages and grammars},
	volume = {30},
	issn = {0022-5193},
	url = {http://www.sciencedirect.com/science/article/pii/0022519371900026},
	doi = {10.1016/0022-5193(71)90002-6},
	abstract = {Formal systems are proposed and constructed to generate cellular arrays corresponding to developmental stages of some simple organisms: lower plants, snail embryos and leaves. Sets of these arrays are construed as developmental languages, and their complexity properties and generating grammars are compared with the classes of languages in the Chomsky hierarchy. Various branching patterns are compared with respect to such complexity classes. Theorems were obtained concerning partial characterizations of the class of developmental systems without cellular interactions, and some of the mathematical properties of this class are discussed.},
	language = {en},
	number = {3},
	urldate = {2020-09-03},
	journal = {Journal of Theoretical Biology},
	author = {Lindenmayer, Aristid},
	month = mar,
	year = {1971},
	pages = {455--484},
}

@article{vazquez-arellano_3-d_2018,
	title = {3-{D} reconstruction of maize plants using a time-of-flight camera},
	volume = {145},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042711062&doi=10.1016%2fj.compag.2018.01.002&partnerID=40&md5=55022fd767e8a8e16c49c546ec07febc},
	doi = {10.1016/j.compag.2018.01.002},
	abstract = {Point cloud rigid registration and stitching for plants with complex architecture is a challenging task, however, it is an important process to take advantage of the full potential of 3-D cameras for plant phenotyping and agricultural automation for characterizing production environments in agriculture. A methodology for three-dimensional (3-D) reconstruction of maize crop rows was proposed in this research, using high resolution 3-D images that were mapped into the colour images using state-of-the art software. The point cloud registration methodology was based on the Iterative Closest Point (ICP) algorithm. The incoming point cloud was previously filtered using the Random Sample Consensus (RANSAC) algorithm, by reducing the number of soil points until a threshold value was reached. This threshold value was calculated based on the approximate number of plant points in a single 3-D image. After registration and stitching of the crop rows, a plant/soil segmentation process was done relying again on the RANSAC algorithm. A quantitative comparison showed that the number of points obtained with a time-of-flight (TOF) camera, compared with the ones from two light detection and ranging (LIDARs) from a previous research, was roughly 23 times larger. Finally, the reconstruction was validated by comparing the seedling positions as ground truth and the point cloud clusters, obtained using the k-means clustering, that represent the plant stem positions. The resulted maize positions from the proposed methodology closely agreed with the ground truth with an average mean and standard deviation of 3.4 cm and ±1.3 cm, respectively. © 2018 Elsevier B.V.},
	journal = {Computers and Electronics in Agriculture},
	author = {Vázquez-Arellano, M. and Reiser, D. and Paraforos, D.S. and Garrido-Izard, M. and Burce, M.E.C. and Griepentrog, H.W.},
	year = {2018},
	pages = {235--247},
}

@article{wen_multi-scale_2019,
	title = {Multi-scale {3D} data acquisition of maize},
	volume = {545},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061103963&doi=10.1007%2f978-3-030-06137-1_11&partnerID=40&md5=5dd8d36681273e7a19abc3da4b1b827d},
	doi = {10.1007/978-3-030-06137-1_11},
	abstract = {In recent years, three-dimensional (3D) data acquisition and model reconstruction of plants have been developed as a hot topic of plant scientific researches. However, the morphological structure of plants is very complex and it is hard to describe the details. The data acquisition approaches are diverse for different parts of plants. This study introduces the data acquisition methods of different scales of maize. The grain, leaf and ear, individual plant and maize colony represents the target models for different scales. 3D data acquisition instruments are used to acquire the morphometric of each target. It is found that the organ scale is the simplest to obtain and process. The smallest grain needs high-resolution scanner to acquire the morphological details, while the plant canopy is the hardest one for point cloud process and modeling. The data and reconstructed models are oriented to digital plant, phenotyping analysis, FSPMs research, and popular science education application. © IFIP International Federation for Information Processing 2019.},
	journal = {IFIP Advances in Information and Communication Technology},
	author = {Wen, W. and Guo, X. and Lu, X. and Wang, Y. and Yu, Z.},
	year = {2019},
	pages = {108--115},
}

@article{cuevas-velasquez_segmentation_2020,
	title = {Segmentation and {3D} reconstruction of rose plants from stereoscopic images},
	volume = {171},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080078889&doi=10.1016%2fj.compag.2020.105296&partnerID=40&md5=f228c7874b3a238a1b4aa211603754ab},
	doi = {10.1016/j.compag.2020.105296},
	abstract = {The method proposed in this paper is part of the vision module of a garden robot capable of navigating towards rose bushes and clip them according to a set of pruning rules. The method is responsible for performing the segmentation of the branches and recovering their morphology in 3D. The obtained reconstruction allows the manipulator of the robot to select the candidate branches to be pruned. This method first obtains a stereo pair of images and calculates the disparity image using block matching and the segmentation of the branches using a Fully Convolutional Neuronal Network modified to return a map with the probability at the pixel level of the presence of a branch. A post-processing step combines the segmentation and the disparity in order to improve the results. Then, the skeleton of the plant and the branching structure are calculated, and finally, the 3D reconstruction is obtained. The proposed approach is evaluated with five different datasets, three of them compiled by the authors and two from the state of the art, including indoor and outdoor scenes with uncontrolled environments. The different steps of the proposed pipeline are evaluated and compared with other state-of-the-art methods, showing that the accuracy of the segmentation improves other methods for this task, even with variable lighting, and also that the skeletonization and the reconstruction processes obtain robust results. © 2020 Elsevier B.V.},
	journal = {Computers and Electronics in Agriculture},
	author = {Cuevas-Velasquez, H. and Gallego, A.-J. and Fisher, R.B.},
	year = {2020},
}

@inproceedings{gelard_model-based_2017,
	series = {{VISIGRAPP} 2017 - {Proceedings} of the 12th {International} {Joint} {Conference} on {Computer} {Vision}, {Imaging} and {Computer} {Graphics} {Theory} and {Applications}},
	title = {Model-based segmentation of {3D} point clouds for phenotyping sunflower plants},
	volume = {4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036623530&partnerID=40&md5=0963129878a636d5ecaccc7a8b1f6dfd},
	abstract = {This article presents a model-based segmentation method applied to 3D data acquired on sunflower plants. Our objective is the quantification of the plant growth using observations made automatically from sensors moved around plants. Here, acquisitions are made on isolated plants: A 3D point cloud is computed using Structure from Motion with RGB images acquired all around a plant. Then the proposed method is applied in order to segment and label the plant leaves, i.e. to split up the point cloud in regions corresponding to plant organs: Stem, petioles, and leaves. Every leaf is then reconstructed with NURBS and its area is computed from the triangular mesh. Our segmentation method is validated comparing these areas with the ones measured manually using a planimeter: It is shown that differences between automatic and manual measurements are less than 10\%. The present results open interesting perspectives in direction of high-throughput sunflower phenotyping. © 2017 by SCITEPRESS - Science and Technology Publications, Lda.},
	author = {Gélard, W. and Devy, M. and Herbulot, A. and Burger, P.},
	year = {2017},
	pages = {459--467},
}

@article{andujar_using_2016,
	title = {Using depth cameras to extract structural parameters to assess the growth state and yield of cauliflower crops},
	volume = {122},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956623797&doi=10.1016%2fj.compag.2016.01.018&partnerID=40&md5=60022865c4a9f8e3ec1720d40b508e9a},
	doi = {10.1016/j.compag.2016.01.018},
	abstract = {The use of robotic systems for horticultural crops is widely known. However, the use of these systems in cruciferous vegetables remains a challenge. The case of cauliflower crops is of special relevance because it is a hand-harvested crop for which the cutting time is visually chosen. This methodology leads to a yield reduction, as some inflorescences are cut before ripening because the leaves hide their real state of maturity. This work proposes the use of depth cameras instead of visual estimation. Using Kinect Fusion algorithms, depth cameras create a 3D point cloud from the depth video stream and consequently generate solid 3D models, which have been compared to the actual structural parameters of cauliflower plants. The results show good consistency among depth image models and ground truth from the actual structural parameters. In addition, the best time for individual fruit cutting could be detected using these models, which enabled the optimization of harvesting and increased yields. The accuracy of the models deviated from the ground truth by less than 2 cm in diameter/height, whereas the fruit volume estimation showed an error below 0.6\% overestimation. Analysis of the structural parameters revealed a significant correlation between estimated and actual values of the volume of plants and fruit weight. These results show the potential of depth cameras to be used as a precise tool in estimating the degree of ripeness during the harvesting of cauliflower and thereby optimizing the crop profitability. © 2016 Elsevier B.V.},
	journal = {Computers and Electronics in Agriculture},
	author = {Andújar, D. and Ribeiro, A. and Fernández-Quintanilla, C. and Dorado, J.},
	year = {2016},
	pages = {67--73},
}

@article{bargoti_pipeline_2015,
	title = {A pipeline for trunk detection in trellis structured apple orchards},
	volume = {32},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955197448&doi=10.1002%2frob.21583&partnerID=40&md5=66b8ef6ae942555ce998aa4b36b8c40c},
	doi = {10.1002/rob.21583},
	abstract = {The ability of robots to meticulously cover large areas while gathering sensor data has widespread applications in precision agriculture. For autonomous operations in orchards, a suitable information management system is required, within which we can gather and process data relating to the state and performance of the crop over time, such as distinct yield count, canopy volume, and crop health. An efficient way to structure an information system is to discretize it to the individual tree, for which tree segmentation/detection is a key component. This paper presents a tree trunk detection pipeline for identifying individual trees in a trellis structured apple orchard, using ground-based lidar and image data. A coarse observation of trunk candidates is initially made using a Hough transformation on point cloud lidar data. These candidates are projected into the camera images, where pixelwise classification is used to update their likelihood of being a tree trunk. Detection is achieved by using a hidden semi-Markov model to leverage from contextual information provided by the repetitive structure of an orchard. By repeating this over individual orchard rows, we are able to build a tree map over the farm, which can be either GPS localized or represented topologically by the row and tree number. The pipeline was evaluated at a commercial apple orchard near Melbourne, Australia. Data were collected at different times of year, covering an area of 1.6 ha containing different apple varieties planted on two types of trellis systems: a vertical I-trellis structure and a Güttingen V-trellis structure. The results show good trunk detection performance for both apple varieties and trellis structures during the preharvest season (87-96\% accuracy) and near perfect trunk detection performance (99\% accuracy) during the flowering season. © 2015 Wiley Periodicals, Inc.},
	number = {8},
	journal = {Journal of Field Robotics},
	author = {Bargoti, S. and Underwood, J.P. and Nieto, J.I. and Sukkarieh, S.},
	year = {2015},
	pages = {1075--1094},
}

@inproceedings{ulam_mathematical_1962,
	title = {On some mathematical problems connected with patterns of growth of figures},
	volume = {14},
	booktitle = {Proceedings of symposia in applied mathematics},
	author = {Ulam, Stanislaw},
	year = {1962},
	keywords = {\#nosource},
	pages = {215--224},
}

@inproceedings{smith_plants_1984,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '84},
	title = {Plants, fractals, and formal languages},
	isbn = {978-0-89791-138-2},
	url = {https://doi.org/10.1145/800031.808571},
	doi = {10.1145/800031.808571},
	abstract = {Although fractal models of natural phenomena have received much attention recently, there are other models of complex natural objects which have been around longer in Computer Imagery but are not widely known. These are procedural models of plants and trees. An interesting class of these models is presented here which handles plant growth, sports an efficient data representation, and has a high “database amplification” factor. It is based on an extension of the well-known formal languages of symbol strings to the lesser-known formal languages of labeled graphs. It is so tempting to describe these plant models as “fractal” that the similarities of this class of models with fractal models are explored in an attempt at rapprochement. The models are not fractal so the common parts of fractal theory and plant theory are abstracted to form a class of objects, the graftals. This class may prove to be of great interest to the future of Computer Imagery. Determinism is shown to provide adequate complexity, whereas randomness is only convenient and often inefficient. Finally, a nonfractal, nongraftal family of trees by Bill Reeves is introduced to emphasize some of the paper's nongrammatical themes.},
	urldate = {2020-10-01},
	booktitle = {Proceedings of the 11th annual conference on {Computer} graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Smith, Alvy Ray},
	month = jan,
	year = {1984},
	keywords = {Computer imagery, Database amplification, Fractal, Graftal, L-system, Parallel graph grammar, Particle system, Plant, Tree},
	pages = {1--10},
}

@article{owens_modeling_2016,
	title = {Modeling dense inflorescences},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979978237&doi=10.1145%2f2897824.2925982&partnerID=40&md5=06ba868d5b3dc4e912a2bd3d6e3c687e},
	doi = {10.1145/2897824.2925982},
	abstract = {Showy inflorescences-clusters of flowers-are a common feature of many plants, greatly contributing to their beauty. The large numbers of individual flowers (florets), arranged in space in a systematic manner, make inflorescences a natural target for procedural modeling. We present a suite of biologically motivated algorithms for modeling and animating the development of inflorescences with closely packed florets. These inflorescences share the following characteristics: (i) in their ensemble, the florets form a relatively smooth, often approximately planar surface; (ii) there are numerous collisions between petals of the same or adjacent florets; and (iii) the developmental stage and type of a floret may depend on its position within the inflorescence, with drastic or gradual differences. To model flat-topped branched inflorescences (corymbs and umbels), we propose a florets-first algorithm, in which the branching structure self-organizes to support florets in predetermined positions. This is an alternative to previous branching-first models, in which floret positions were determined by branch arrangement. To obtain realistic visualizations, we complement the algorithms that generate the inflorescence structure with an interactive method for modeling floret corollas (petal sets). The method supports corollaswith both separate and fused petals. We illustrate our techniques with models from several plant families. © 2016 Copyright held by the owner/author(s).},
	number = {4},
	journal = {ACM Transactions on Graphics},
	author = {Owens, A. and Cieslak, M. and Hart, J. and Classen-Bockhoff, R. and Prusinkiewicz, P.},
	year = {2016},
}

@article{elibol_vision-based_2013,
	title = {Vision-based {3D}-reconstruction of barley plants},
	volume = {7887 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883137583&doi=10.1007%2f978-3-642-38628-2_48&partnerID=40&md5=b6b2a8ce4315553096cbcf42e815528f},
	doi = {10.1007/978-3-642-38628-2_48},
	abstract = {For multi-view 3D reconstruction robust standard procedures have been established and can directly be applied to many scenarios. However, the extraction of point correspondences as a prerequisite for reconstruction is demanding for various applications. Here we present a new analysis pipeline for 3D reconstruction in the field of barley plant monitoring. Barley plants show a significant structural and textural similarity rendering the application of standard procedures to extract correspondences impossible. Our new approach overcomes these problems by combining information from various cues over different stages. Experiments on real data prove the suitability of our approach to generate 3D models of the plants from which phenotypical data can easily be derived. © 2013 Springer-Verlag.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Elibol, A. and Posch, S. and Maurer, A. and Pillen, K. and Möller, B.},
	year = {2013},
	pages = {406--415},
}

@inproceedings{floriello_recovering_2012,
	series = {{ACM} {International} {Conference} {Proceeding} {Series}},
	title = {Recovering and analyzing {3D} models of branched structures using computer vision: {A} review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873355833&doi=10.1145%2f2425836.2425900&partnerID=40&md5=4caf71a6a780adccf5fe65389f38825e},
	doi = {10.1145/2425836.2425900},
	abstract = {With the recent developments of computer vision and automated systems in medical imaging or agriculture, there has been, in the last years, the necessity to adopt methods able to analyze and reconstruct three dimensional network structures, such as blood vessels, plant architecture or root structures, given one or more two dimensional images of the observed scene. This paper reviews recent work on this problem. Each of the examples cited designs a new method, each of which is different from previous methods already proposed. In this paper we review some of the developed methods and, due to the evident heterogenity of the methods, we classify them according to whether they apply a model to two dimensional details in images, or methods which recover a three dimensional representation (point cloud, or mesh), and then fit a network structure model to this representation. © 2012 ACM.},
	author = {Floriello, D. and Botterill, T. and Green, R.},
	year = {2012},
	pages = {325--329},
}

@inproceedings{aguilar_3d_2008,
	series = {International {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences} - {ISPRS} {Archives}},
	title = {{3D} surface modelling of tomato plants using close-range photogrammetry},
	volume = {37},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029329455&partnerID=40&md5=4bf903bf06b668f56811dda65fb8a803},
	abstract = {The production of agricultural cultivates under intensive systems requires of big quantities of pesticides for fitosanitary control. Pesticides application is a major component of tomato production costs under greenhouse in Almería, Spain, and the excess in their applications have a great negative impact on the environment. A pesticide application is ideal if the spraying coverage is presented as evenly distributed over the whole crop canopy and, if the product application is correctly adjusted for minimizing the losses towards the soil or the environment. It is proved that for a certain crop stage, there is an optimal volume of application. This ideal volume is related by the canopy Leaf Area Index (LAI), which is the ratio of total upper leaf surface of a crop divided by the surface area of the land on which the crop grows. Our research group is working for the generation of a predictive and empiric model regarding non destructive estimation of LAI. This model will be based on the volume and density of tomato plants. This work seeks, as its main goal, the obtention of a Three-Dimensional (3D) accurate model of tomato plant canopies using close-range photogrammetry and 3D modelling tools. The 'real' volume of the plants can be measured accurately from the 3D model. Since the tomato volume can be measured by means of a simpler manual methodology, using plant measurements of width and height (e.g., Tree Row Volume or Unit Canopy Row), it is expected in the near future to outline a manual method for measuring tomato bush volume which presents a better fitting to the 'real' volume.},
	author = {Aguilar, M.A. and Pozo, J.L. and Aguilar, F.J. and Sanchez-Hermosilla, J. and Páez, F.C. and Negreiros, J.},
	year = {2008},
	pages = {139--144},
}

@inproceedings{huang_realistic_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Realistic {Modeling} of {Tree} {Ramifications} from an {Optimal} {Manifold} {Control} {Mesh}},
	isbn = {978-3-030-34110-7},
	doi = {10.1007/978-3-030-34110-7_27},
	abstract = {Modeling realistic branches and ramifications of trees is a challenging task because of their complex geometric structures. Many approaches have been proposed to generate plausible tree models from images, sketches, point clouds, and botanical rules. However, most approaches focus on a global impression of trees, such as the topological structure of branches and arrangement of leaves, without taking continuity of branch ramifications into consideration. To model a complete tree quadrilateral mesh (quad-mesh) with smooth ramifications, we propose an optimization method to calculate a suitable control mesh for Catmull–Clark subdivision. Given a tree’s skeleton information, we build a local coordinate system for each joint node, and orient each node appropriately based on the angle between a parent branch and its child branch. Then, we create the corresponding basic ramification units using a cuboid-like quad-mesh, which is mapped back to the world coordinate. To obtain a suitable manifold initial control mesh as a main mesh, the ramifications are classified into main and additional ramifications, and a bottom-up optimization approach is applied to adjust the positions of the main ramification units when they connect their neighbors. Next, the first round of Catmull–Clark subdivision is applied to the main ramifications. The additional ramifications, which were selected to alleviate visual distortion in the preceding step, are added back to the main mesh using a cut-paste operation. Finally, the second round of Catmull–Clark subdivision is used to generate the final quad-mesh of the entire tree. The results demonstrated that our method generated a realistic tree quad-mesh effectively from different tree skeletons.},
	language = {en},
	booktitle = {Image and {Graphics}},
	publisher = {Springer International Publishing},
	author = {Huang, Zhengyu and Zhang, Zhiyi and Geng, Nan and Yang, Long and He, Dongjian and Hu, Shaojun},
	editor = {Zhao, Yao and Barnes, Nick and Chen, Baoquan and Westermann, Rüdiger and Kong, Xiangwei and Lin, Chunyu},
	year = {2019},
	keywords = {Catmull–Clark subdivision, Construction optimization, Manifold tree modeling, Tree quad-mesh},
	pages = {316--332},
}

@inproceedings{bournez_tls_2017,
	series = {International {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences} - {ISPRS} {Archives}},
	title = {From {TLS} point clouds to {3D} models of trees: {A} comparison of existing algorithms for {3D} tree reconstruction},
	volume = {42},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021776776&doi=10.5194%2fisprs-archives-XLII-2-W3-113-2017&partnerID=40&md5=283b40b3e91959c0503eb68f32b8741c},
	doi = {10.5194/isprs-archives-XLII-2-W3-113-2017},
	abstract = {3D models of tree geometry are important for numerous studies, such as for urban planning or agricultural studies. In climatology, tree models can be necessary for simulating the cooling effect of trees by estimating their evapotranspiration. The literature shows that the more accurate the 3D structure of a tree is, the more accurate microclimate models are. This is the reason why, since 2013, we have been developing an algorithm for the reconstruction of trees from terrestrial laser scanner (TLS) data, which we call TreeArchitecture. Meanwhile, new promising algorithms dedicated to tree reconstruction have emerged in the literature. In this paper, we assess the capacity of our algorithm and of two others-PlantScan3D and SimpleTree- to reconstruct the 3D structure of trees. The aim of this reconstruction is to be able to characterize the geometric complexity of trees, with different heights, sizes and shapes of branches. Based on a specific surveying workflow with a TLS, we have acquired dense point clouds of six different urban trees, with specific architectures, before reconstructing them with each algorithm. Finally, qualitative and quantitative assessments of the models are performed using reference tree reconstructions and field measurements. Based on this assessment, the advantages and the limits of every reconstruction algorithm are highlighted. Anyway, very satisfying results can be reached for 3D reconstructions of tree topology as well as of tree volume.},
	author = {Bournez, E. and Landes, T. and Saudreau, M. and Kastendeuch, P. and Najjar, G.},
	year = {2017},
	pages = {113--120},
}

@article{sen_modelling_2005,
	title = {Modelling trees and their interaction with the environment: {A} survey},
	volume = {29},
	issn = {0097-8493},
	shorttitle = {Modelling trees and their interaction with the environment},
	url = {http://www.sciencedirect.com/science/article/pii/S009784930500141X},
	doi = {10.1016/j.cag.2005.08.025},
	abstract = {In this paper, we summarize some of the techniques used for modelling certain elements of nature such as trees. We describe some of the current methods in their order of appearance in the classification spectrum given in this survey. We discuss the geometrical and biological aspects that help us to identify the position of the work in our spectrum. The paper also identifies the interaction capability of the current techniques with the environmental factors and external forces.},
	language = {en},
	number = {5},
	urldate = {2020-04-17},
	journal = {Computers \& Graphics},
	author = {Sen, Soner I. and Day, A. M.},
	month = oct,
	year = {2005},
	keywords = {Earth and atmospheric sciences, Geometric algorithms, Graphics data structures and data types, Languages and systems, Physically based modelling, Virtual reality},
	pages = {805--817},
}

@article{saha_survey_2016,
	series = {Special {Issue} on {Skeletonization} and its {Application}},
	title = {A survey on skeletonization algorithms and their applications},
	volume = {76},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865515001233},
	doi = {10.1016/j.patrec.2015.04.006},
	abstract = {Skeletonization provides an effective and compact representation of objects, which is useful for object description, retrieval, manipulation, matching, registration, tracking, recognition, and compression. It also facilitates efficient assessment of local object properties, e.g., scale, orientation, topology, etc. Several computational approaches are available in literature toward extracting the skeleton of an object, some of which are widely different in terms of their principles. In this paper, we present a comprehensive and concise survey of different skeletonization algorithms and discuss their principles, challenges, and benefits. Topology preservation, parallelization, and multi-scale skeletonization approaches are discussed. Finally, various applications of skeletonization are reviewed and the fundamental challenges of assessing the performance of different skeletonization algorithms are discussed.},
	language = {en},
	urldate = {2020-06-12},
	journal = {Pattern Recognition Letters},
	author = {Saha, Punam K. and Borgefors, Gunilla and Sanniti di Baja, Gabriella},
	month = jun,
	year = {2016},
	keywords = {Applications, Centers of maximal balls, Distance transform, Parallel algorithms, Skeletonization, Topology preservation},
	pages = {3--12},
}

@article{reeves_particle_1983,
	title = {Particle {Systems} - a {Technique} for {Modeling} a {Class} of {Fuzzy} {Objects}},
	volume = {2},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/357318.357320},
	doi = {10.1145/357318.357320},
	number = {2},
	urldate = {2020-12-21},
	journal = {ACM Transactions on Graphics},
	author = {Reeves, W. T.},
	month = apr,
	year = {1983},
	pages = {91--108},
}

@article{xu_pm-pm_2015,
	title = {{PM}-{PM}: {PatchMatch} {With} {Potts} {Model} for {Object} {Segmentation} and {Stereo} {Matching}},
	volume = {24},
	issn = {1941-0042},
	shorttitle = {{PM}-{PM}},
	doi = {10.1109/TIP.2015.2416654},
	abstract = {This paper presents a unified variational formulation for joint object segmentation and stereo matching, which takes both accuracy and efficiency into account. In our approach, depth-map consists of compact objects, each object is represented through three different aspects: the perimeter in image space; the slanted object depth plane; and the planar bias, which is to add an additional level of detail on top of each object plane in order to model depth variations within an object. Compared with traditional high quality solving methods in low level, we use a convex formulation of the multilabel Potts Model with PatchMatch stereo techniques to generate depth-map at each image in object level and show that accurate multiple view reconstruction can be achieved with our formulation by means of induced homography without discretization or staircasing artifacts. Our model is formulated as an energy minimization that is optimized via a fast primal-dual algorithm, which can handle several hundred object depth segments efficiently. Performance evaluations in the Middlebury benchmark data sets show that our method outperforms the traditional integer-valued disparity strategy as well as the original PatchMatch algorithm and its variants in subpixel accurate disparity estimation. The proposed algorithm is also evaluated and shown to produce consistently good results for various real-world data sets (KITTI benchmark data sets and multiview benchmark data sets).},
	number = {7},
	journal = {IEEE Transactions on Image Processing},
	author = {Xu, Shibiao and Zhang, Feihu and He, Xiaofei and Shen, Xukun and Zhang, Xiaopeng},
	month = jul,
	year = {2015},
	keywords = {Accuracy, Computational modeling, Estimation, Image reconstruction, Image segmentation, KITTI benchmark data set, Middlebury benchmark data set, Object segmentation, PM-PM, PatchMatch, PatchMatch stereo technique, Potts model, Stereo vision, convex formulation, convex programming, depth-map generation, energy minimization, high quality solving method, image matching, image reconstruction, image segmentation, induced homography, integer programming, integer-valued disparity strategy, multilabel Potts Model, multiple view reconstruction, multiview benchmark data set, object segmentation, patchmatch, potts model, primal-dual algorithm, stereo image processing, stereo matching, subpixel accurate disparity estimation, unified variational formulation},
	pages = {2182--2196},
}

@inproceedings{li_plant_2020,
	address = {Beijing, China},
	title = {Plant leaf point cloud completion based on deep learning},
	isbn = {978-1-5106-3704-7 978-1-5106-3705-4},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11455/2565353/Plant-leaf-point-cloud-completion-based-on-deep-learning/10.1117/12.2565353.full},
	doi = {10.1117/12.2565353},
	urldate = {2020-12-23},
	booktitle = {Sixth {Symposium} on {Novel} {Optoelectronic} {Detection} {Technology} and {Applications}},
	publisher = {SPIE},
	author = {Li, Xudong and Zhou, Zijuan and Xu, Zhengqi and Jiang, Hongzhi and Zhao, Huijie},
	editor = {Jiang, Huilin and Chu, Junhao},
	month = apr,
	year = {2020},
	pages = {349},
}

@article{teng_image-based_2009,
	title = {Image-based tree modeling from a few images with very narrow viewing range},
	volume = {25},
	issn = {1432-2315},
	url = {https://doi.org/10.1007/s00371-008-0269-1},
	doi = {10.1007/s00371-008-0269-1},
	abstract = {Creating 3D tree models from actual trees is a task receiving increasing attention. Some approaches have been developed to reconstruct a tree based on a number of photographs around the tree, typically spanning a wide viewing range. However, due to the environmental restrictions, sometimes it is quite difficult to capture so many acceptable images from so many different viewpoints. In this paper, we propose a tree modeling system which is capable of reconstructing the 3D model of a tree from a few images with very narrow viewing ranges. Because only a few images are required to generate the model, our system has the distinct advantage of fewer environmental restrictions, resulting in the extended usability and flexibility for real applications.},
	language = {en},
	number = {4},
	urldate = {2021-01-14},
	journal = {The Visual Computer},
	author = {Teng, Chin-Hung and Chen, Yung-Sheng},
	month = apr,
	year = {2009},
	pages = {297--307},
}

@inproceedings{viennot_combinatorial_1989,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '89},
	title = {Combinatorial analysis of ramified patterns and computer imagery of trees},
	isbn = {978-0-89791-312-6},
	url = {https://doi.org/10.1145/74333.74336},
	doi = {10.1145/74333.74336},
	abstract = {Herein is presented a new procedural method for generating images of trees. Many other algorithms have already been proposed in the last few years focusing on particle systems, fractals, graftals and L-systems or realistic botanical models. Usually the final visual aspect of the tree depends on the development process leading to this form. Our approach differs from all the previous ones. We begin by defining a certain "measure" of the form of a tree or a branching pattern. This is done by introducing the new concept of ramification matrix of a tree. Then we give an algorithm for generating a random tree having as ramification matrix a given arbitrary stochastic triangular matrix. The geometry of the tree is defined from the combinatorial parameters implied in the analysis of the forms of trees. We obtain a method with powerful control of the final form, simple enough to produce quick designs of trees without loosing in the variety and rendering of the images. We also introduce a new rapid drawing of the leaves. The underlying combinatorics constitute a refinment of some work introduced in hydrogeology in the morphological study of river networks. The concept of ramification matrix has been used very recently in physics in the study of fractal ramified patterns.},
	urldate = {2020-05-25},
	booktitle = {Proceedings of the 16th annual conference on {Computer} graphics and interactive techniques},
	publisher = {Association for Computing Machinery},
	author = {Viennot, X. and Eyrolles, G. and Janey, N. and Arqués, D.},
	month = jul,
	year = {1989},
	pages = {31--40},
}

@incollection{blum_transformation_1967,
	title = {A transformation for extracting new descriptors of shape},
	language = {en},
	booktitle = {Models for the {Perception} of {Speech} and {Visual} {Form}},
	publisher = {MIT Press},
	author = {Blum, H.},
	year = {1967},
	pages = {362--380},
}

@article{li_automatic_2017,
	title = {An {Automatic} {Tree} {Skeleton} {Extracting} {Method} {Based} on {Point} {Cloud} of {Terrestrial} {Laser} {Scanner}},
	volume = {2017},
	issn = {1687-9384},
	url = {https://doi.org/10.1155/2017/5408503},
	doi = {10.1155/2017/5408503},
	abstract = {Tree skeleton could describe the shape and topological structure of a tree, which are useful to forest researchers. Terrestrial laser scanner (TLS) can scan trees with high accuracy and speed to acquire the point cloud data, which could be used to extract tree skeletons. An adaptive extracting method of tree skeleton based on the point cloud data of TLS was proposed in this paper. The point cloud data were segmented by artificial filtration and {\textbackslash}textlessinline-formula{\textbackslash}textgreater{\textbackslash}textlessmml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M1"{\textbackslash}textgreater{\textbackslash}textlessmml:mrow{\textbackslash}textgreater{\textbackslash}textlessmml:mi{\textbackslash}textgreaterk{\textbackslash}textless/mml:mi{\textbackslash}textgreater{\textbackslash}textless/mml:mrow{\textbackslash}textgreater{\textbackslash}textless/mml:math{\textbackslash}textgreater{\textbackslash}textless/inline-formula{\textbackslash}textgreater-means clustering, and the point cloud data of trunk and branches remained to extract skeleton. Then the skeleton nodes were calculated by using breadth first search (BFS) method, quantifying method, and clustering method. Based on their connectivity, the skeleton nodes were connected to generate the tree skeleton, which would be smoothed by using Laplace smoothing method. In this paper, the point cloud data of a toona tree and peach tree were used to test the proposed method and for comparing the proposed method with the shortest path method to illustrate the robustness and superiority of the method. The experimental results showed that the shape of tree skeleton extracted was consistent with the real tree, which showed the method proposed in the paper is effective and feasible.},
	journal = {International Journal of Optics},
	author = {Li, Ronghao and Bu, Guochao and Wang, Pei},
	editor = {Cerullo, Giulio},
	month = oct,
	year = {2017},
	pages = {5408503},
}

@article{wang_structure-aware_2014,
	title = {A {Structure}-{Aware} {Global} {Optimization} {Method} for {Reconstructing} 3-{D} {Tree} {Models} {From} {Terrestrial} {Laser} {Scanning} {Data}},
	volume = {52},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2013.2291815},
	abstract = {A 3-D tree structure plays an important role in many scientific fields, including forestry and agriculture. For example, terrestrial laser scanning (TLS) can efficiently capture high-precision 3-D spatial arrangements and structure of trees as a point cloud. In the past, several methods to reconstruct 3-D trees from the TLS point cloud were proposed. However, in general, they fail to process incomplete TLS data. To address such incomplete TLS data sets, a new method that is based on a structure-aware global optimization approach (SAGO) is proposed. The SAGO first obtains the approximate tree skeleton from a distance minimum spanning tree (DMst) and then defines the stretching directions of the branches on the tree skeleton. Based on these stretching directions, the SAGO recovers missing data in the incomplete TLS point cloud. The DMst is applied again to obtain the refined tree skeleton from the optimized data, and the tree skeleton is smoothed by employing a Laplacian function. To reconstruct 3-D tree models, the radius of each branch section is estimated, and leaves are added to form the crown geometry. The developed methodology has been extensively evaluated by employing a dozen TLS point clouds of various types of trees. Both qualitative and quantitative performance evaluation results have indicated that the SAGO is capable of effectively reconstructing 3-D tree models from grossly incomplete TLS point clouds with significant amounts of missing data.},
	number = {9},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Wang, Z. and Zhang, L. and Fang, T. and Mathiopoulos, P. T. and Qu, H. and Chen, D. and Wang, Y.},
	month = sep,
	year = {2014},
	keywords = {3-D tree models, 3D spatial arrangements, 3D tree model reconstruction, Computational modeling, Data models, Missing data, Noise, Optimization, SAGO approach, Skeleton, Solid modeling, TLS point cloud, Vegetation, agriculture, branch section, crown geometry, distance minimum spanning tree, forestry, geophysical image processing, image reconstruction, optimisation, optimization, remote sensing by laser beam, structure aware global optimization method, terrestrial laser scanning (TLS), terrestrial laser scanning data, tree skeleton, vegetation mapping},
	pages = {5653--5669},
}

@article{boudon_quantitative_2014,
	title = {Quantitative assessment of automatic reconstructions of branching systems obtained from laser scanning},
	volume = {114},
	issn = {0305-7364},
	url = {https://doi.org/10.1093/aob/mcu062},
	doi = {10.1093/aob/mcu062},
	abstract = {Automatic acquisition of plant architecture is a major challenge for the construction of quantitative models of plant development. Recently, 3-D laser scanners have made it possible to acquire 3-D images representing a sampling of an object's surface. A number of specific methods have been proposed to reconstruct plausible branching structures from this new type of data, but critical questions remain regarding their suitability and accuracy before they can be fully exploited for use in biological applications.In this paper, an evaluation framework to assess the accuracy of tree reconstructions is presented. The use of this framework is illustrated on a selection of laser scans of trees. Scanned data were manipulated by experienced researchers to produce reference tree reconstructions against which comparisons could be made. The evaluation framework is given two tree structures and compares both their elements and their topological organization. Similar elements are identified based on geometric criteria using an optimization algorithm. The organization of these elements is then compared and their similarity quantified. From these analyses, two indices of geometrical and structural similarities are defined, and the automatic reconstructions can thus be compared with the reference structures in order to assess their accuracy.The evaluation framework that was developed was successful at capturing the variation in similarities between two structures as different levels of noise were introduced. The framework was used to compare three different reconstruction methods taken from the literature, and allowed sensitive parameters of each one to be determined. The framework was also generalized for the evaluation of root reconstruction from 2-D images and demonstrated its sensitivity to higher architectural complexity of structure which was not detected with a global evaluation criterion.The evaluation framework presented quantifies geometric and structural similarities between two structures. It can be applied to the characterization and comparison of automatic reconstructions of plant structures from laser scanner data and 2-D images. As such, it can be used as a reference test for comparing and assessing reconstruction procedures.},
	number = {4},
	urldate = {2021-01-11},
	journal = {Annals of Botany},
	author = {Boudon, Frédéric and Preuksakarn, Chakkrit and Ferraro, Pascal and Diener, Julien and Nacry, Philippe and Nikinmaa, Eero and Godin, Christophe},
	month = sep,
	year = {2014},
	pages = {853--862},
}

@techreport{boudon_survey_2006,
	type = {Research {Report}},
	title = {Survey on {Computer} {Representations} of {Trees} for {Realistic} and {Efficient} {Rendering}},
	abstract = {This paper gives an overview of computer graphics representations of trees commonly used for the rendering of complex scene of vegetation. Looking for the right compromise between realism and efficiency has lead researchers to consider various types of geometrical plant models with different types of complexity. To achieve realist plant model, a complex structure of plant with full details is generally considered. In contrast, to promote efficiency, other approaches summarize plant geometry with few primitives allowing rapid rendering. Finally, to find a good compromise, structures with adaptive complexity are defined. Theses different types of representations and the ways to use them are presented, classified and discussed. The proposed classification principles rely on the type of structural details used in the plants representations. Characterization of all these methods is completed with various additional criteria including rendering primitive type, distance validity, interactive possibilities, animation ability and lighting properties.},
	language = {en},
	number = {2301},
	urldate = {2020-09-25},
	institution = {LIRIS UMR CNRS 5205},
	author = {Boudon, Frédéric and Meyer, Alexandre and Godin, Christophe},
	year = {2006},
}

@inproceedings{gorte_structuring_2004,
	title = {Structuring laser-scanned trees using {3D} mathematical morphology},
	volume = {35},
	abstract = {The task addressed in this paper is 3D modelling and reconstruction of (real world) trees on the basis of terrestrial laser scans. To identify the structure of a tree in terms of stem and branches, an algorithm has been designed in 3D voxel space, based on a selection of basic and advanced 2D raster (image) processing algorithms, transferred into the 3D domain. The selection includes filtering, mathematical morphology, skeletonization, connected component labeling and shortest route computation. © 2014 ISPRS. All Rights Reserved.},
	booktitle = {International {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences} - {ISPRS} {Archives}},
	author = {Gorte, B. and Pfeifer, N.},
	year = {2004},
	keywords = {\_tablet},
	pages = {929--933},
}

@article{huang_extraction_2007,
	title = {Extraction of {3D} unfoliaged trees from image sequences via a generative statistical approach},
	volume = {4713 LNCS},
	abstract = {In this paper we propose a generative statistical approach for the three dimensional (3D) extraction of the branching structure of unfoliaged deciduous trees from urban image sequences. The trees are generatively modeled in 3D by means of L-systems. A statistical approach, namely Markov Chain Monte Carlo - MCMC is employed together with cross correlation for extraction. Thereby we overcome the complexity and uncertainty of extracting and matching branches in several images due to weak contrast, background clutter, and particularly the varying order of branches when projected into different images. First results show the potential of the approach. © Springer-Verlag Berlin Heidelberg 2007.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Huang, H. and Mayer, H.},
	year = {2007},
	keywords = {\_tablet},
	pages = {385--394},
}

@inproceedings{preuksakarn_reconstructing_2010,
	address = {Davis, United States},
	title = {Reconstructing {Plant} {Architecture} from {3D} {Laser} scanner data},
	abstract = {Automatic acquisition of plant phenotypes constitutes a major bottleneck in the construction of quantitative models of plant development. This issue needs to be addressed to build accurate models of plant, useful for instance in agronomic and forestry applications. In this work, we present a method for reconstructing plant architecture from laser scanner data. A dedicated evaluation procedure based on a detailed comparison between expert and automatic reconstruction was developed to quantify accurately the quality of the reconstruction method.},
	urldate = {2021-01-20},
	booktitle = {6th {International} {Workshop} on {Functional}-{Structural} {Plant} {Models}},
	author = {Preuksakarn, Chakkrit and Boudon, Frédéric and Ferraro, Pascal and Durand, Jean-Baptiste and Nikinmaa, Ekko and Godin, Christophe},
	month = sep,
	year = {2010},
	keywords = {3D Laser scanner, Architectural tree model, Reconstruction evaluation},
	pages = {12--17},
}

@inproceedings{teng_tree_2005,
	series = {Proceedings of the 9th {IAPR} {Conference} on {Machine} {Vision} {Applications}, {MVA} 2005},
	title = {Tree segmentation from an image},
	abstract = {Tree is a very common object in nature, thus its segmentation provides quite important information for scene 3D reconstruction. In this paper, we present an algorithm for segmenting trees from a complex scene. The trunk and leaf regions of a tree can be individually identified and the trunk structure of the tree can also be extracted. The proposed algorithm is mainly composed of a preliminary image segmentation, a trunk structure extraction, and a leaf regions identification process. We model the extraction of trunk structure as an optimization problem, where an energy function is formulated according to the color, position, and orientation of the segmented regions. We propose an algorithm to minimize this energy function and thus extract the trunk structure of the tree. After obtaining the trunk structure, the leaf regions can then be easily identified by finding those leaf regions located above the trunk regions. This algorithm has been tested on some real images and the results indicated that our algorithm performed well for these images. Copyright © 2005 by MVA Conference Committee.},
	author = {Teng, C.-H. and Chen, Y.-S. and Hsu, W.-H.},
	year = {2005},
	pages = {59--63},
}

@article{pirk_interactive_2017,
	title = {Interactive wood combustion for botanical tree models},
	volume = {36},
	issn = {07300301},
	url = {http://dl.acm.org/citation.cfm?doid=3130800.3130814},
	doi = {10.1145/3130800.3130814},
	abstract = {Fig. 1. Combustion of a tree model: a tree is exposed to fire until the branching structure reaches its ignition temperature (a). The combustion releases energy stored in the tree organs and propagates through the entire tree model until it reaches its peak (b). The combustion causes branches to bend and break (c) while the flames conquer more branches (d) and eventually burn the entire tree model (e). We present a novel method for the combustion of botanical tree models. Tree models are represented as connected particles for the branching structure and a polygonal surface mesh for the combustion. Each particle stores biological and physical attributes that drive the kinetic behavior of a plant and the exothermic reaction of the combustion. Coupled with realistic physics for rods, the particles enable dynamic branch motions. We model material properties, such as moisture and charring behavior, and associate them with individual particles. The combustion is efficiently processed in the surface domain of the tree model on a polygonal mesh. A user can dynamically interact with the model by initiating fires and by inducing stress on branches. The flames realistically propagate through the tree model by consuming the available resources. Our method runs at interactive rates and supports multiple tree instances in parallel. We demonstrate the effectiveness of our approach through numerous examples and evaluate its plausibility against the combustion of real wood samples.},
	number = {6},
	urldate = {2019-07-05},
	journal = {ACM Transactions on Graphics},
	author = {Pirk, Sören and Jarząbek, Michał and Hädrich, Torsten and Michels, Dominik L. and Palubicki, Wojciech},
	month = nov,
	year = {2017},
	keywords = {botanical tree models, interactive modeling, natural phenomena, visual models of trees, wood combustion},
	pages = {1--12},
}

@article{okabe_interactive_2005,
	title = {Interactive {Design} of {Botanical} {Trees} using {Freehand} {Sketches} and {Example}-based {Editing}},
	volume = {24},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2005.00874.x},
	doi = {10.1111/j.1467-8659.2005.00874.x},
	language = {en},
	number = {3},
	urldate = {2021-06-08},
	journal = {Computer Graphics Forum},
	author = {Okabe, Makoto and Owada, Shigeru and Igarash, Takeo},
	year = {2005},
	pages = {487--496},
}

@article{yuan_immersive_2021,
	title = {Immersive sketch-based tree modeling in virtual reality},
	volume = {94},
	journal = {Computers \& Graphics},
	author = {Yuan, Qi and Huai, Yongjian},
	year = {2021},
	pages = {132--143},
}

@article{surovy_accuracy_2016,
	title = {Accuracy of reconstruction of the tree stem surface using terrestrial close-range photogrammetry},
	volume = {8},
	number = {2},
	journal = {Remote Sensing},
	author = {Surovỳ, Peter and Yoshimoto, Atsushi and Panagiotidis, Dimitrios},
	year = {2016},
	pages = {123},
}

@article{kattenborn_automatic_2014,
	title = {Automatic single tree detection in plantations using {UAV}-based photogrammetric point clouds},
	volume = {40},
	number = {3},
	journal = {The International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Kattenborn, Teja and Sperlich, Maximilian and Bataua, Kataebati and Koch, Barbara},
	year = {2014},
	pages = {139},
}

@article{gatziolis_3d_2015,
	title = {{3D} tree dimensionality assessment using photogrammetry and small unmanned aerial vehicles},
	volume = {10},
	number = {9},
	journal = {PloS one},
	author = {Gatziolis, Demetrios and Lienard, Jean F. and Vogs, Andre and Strigul, Nikolay S.},
	year = {2015},
	pages = {e0137765},
}

@incollection{chen_sketch-based_2008,
	title = {Sketch-based tree modeling using markov random field},
	booktitle = {{ACM} {SIGGRAPH} {Asia} 2008 papers},
	author = {Chen, Xuejin and Neubert, Boris and Xu, Ying-Qing and Deussen, Oliver and Kang, Sing Bing},
	year = {2008},
	pages = {1--9},
}

@article{mongus_efficient_2015,
	title = {An efficient approach to {3D} single tree-crown delineation in {LiDAR} data},
	volume = {108},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271615001951},
	doi = {10.1016/j.isprsjprs.2015.08.004},
	abstract = {This paper proposes a new method for 3D delineation of single tree-crowns in LiDAR data by exploiting the complementaries of treetop and tree trunk detections. A unified mathematical framework is provided based on the graph theory, allowing for all the segmentations to be achieved using marker-controlled watersheds. Treetops are defined by detecting concave neighbourhoods within the canopy height model using locally fitted surfaces. These serve as markers for watershed segmentation of the canopy layer where possible oversegmentation is reduced by merging the regions based on their heights, areas, and shapes. Additional tree crowns are delineated from mid- and under-storey layers based on tree trunk detection. A new approach for estimating the verticalities of the points’ distributions is proposed for this purpose. The watershed segmentation is then applied on a density function within the voxel space, while boundaries of delineated trees from the canopy layer are used to prevent the overspreading of regions. The experiments show an approximately 6\% increase in the efficiency of the proposed treetop definition based on locally fitted surfaces in comparison with the traditionally used local maxima of the smoothed canopy height model. In addition, 4\% increase in the efficiency is achieved by the proposed tree trunk detection. Although the tree trunk detection alone is dependent on the data density, supplementing it with the treetop detection the proposed approach is efficient even when dealing with low density point-clouds.},
	language = {en},
	urldate = {2021-06-09},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Mongus, Domen and Žalik, Borut},
	month = oct,
	year = {2015},
	keywords = {LiDAR, LoFS, Mathematical morphology, Single tree-crown delineation, Verticality, Watershed},
	pages = {219--233},
}

@article{jaskierniak_individual_2021,
	title = {Individual tree detection and crown delineation from {Unmanned} {Aircraft} {System} ({UAS}) {LiDAR} in structurally complex mixed species eucalypt forests},
	volume = {171},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271620302963},
	doi = {10.1016/j.isprsjprs.2020.10.016},
	abstract = {Estimates of forest stocking density per hectare (NHa) are important in characterising ecological conditions and assessing changes in forest dynamics after disturbances due to pyrogenic, anthropogenic and biotic factors. We use Unmanned Aircraft Systems (UAS) LiDAR with mean point density of 1485 points m−2 across 39 flight sites to develop a bottom-up approach for individual tree and crown delineation (ITCD). The ITCD algorithm was evaluated across mixed species eucalypt forests (MSEF) using 2790 field measured stem locations across a broad range of dominant eucalypt species with randomly leaning trunks and highly irregular intertwined canopy structure. Two top performing ITCD algorithms in benchmarking studies resulted in poor performance when optimised to our plot data (mean Fscore: 0.61 and 0.62), which emphasises the challenge posed for ITCD in the structurally complex conditions of MSEF. To address this, our novel bottom-up ITCD algorithm uses kernel densities to stratify the vegetation profile and differentiate understorey from the rest of the vegetation. For vegetation above understorey, the ITCD algorithm adopted a novel watershed clustering procedure on point density measures within horizontal slices. A Principal Component Analysis (PCA) procedure was then applied to merge the slice-specific clusters into trunks, branches, and canopy clumps, before a voxel connectivity procedure clustered these biomass segments into overstorey trees. The segmentation process only requires two parameters to be calibrated to site-specific conditions across 39 MSEF sites using a Shuffled Complex Evolution (SCE) optimiser. Across the 39 field sites, the ITCD algorithm had mean Fscore of 0.91, True Positive (TP) trees represented 85\% of measured trees and predicted plot-level stocking (NP) averaged 94\% of actual stocking (NOb). As a representation of plot-level basal area (BA), TP trees represented 87\% of BA, omitted trees represented slightly smaller trees and made up 8\% of BA, and a further 5\% of BA had commission error. Spatial maps of NHa using 0.5 m grid-cells showed that omitted trees were more prevalent in high density forest stands, and that 63\% of grid-cells had a perfect estimate of NHa, whereas a further 31\% of the grid-cells overestimate or underestimate one tree within the search window. The parsimonious modelling framework allows for the two calibrated site-specific parameters to be predicted (R2: 0.87 and 0.66) using structural characteristics of vegetation clusters within sites. Using predictions of these two site-specific parameters across all sites results in mean FScore of 0.86 and mean TP of 0.77, under circumstances where no ground observations were required for calibration. This approach generalises the algorithm across new UAS LiDAR data without undertaking time-consuming ground measurements within tall eucalypt forests with complex vegetation structure.},
	language = {en},
	urldate = {2021-06-09},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Jaskierniak, D. and Lucieer, A. and Kuczera, G. and Turner, D. and Lane, P. N. J. and Benyon, R. G. and Haydon, S.},
	month = jan,
	year = {2021},
	keywords = {Canopy segmentation, Eucalyptus, ITCD, ITD, PCA, RPAS, Stocking density, Tree detection, UAS, UAV, Voxel, Watershed},
	pages = {171--187},
}

@inproceedings{lee_analysing_2018,
	title = {Analysing {Forests} {Using} {Dense} {Point} {Clouds}},
	doi = {10.1109/IVCNZ.2018.8634651},
	abstract = {A novel method is proposed to acquire tree metrics for forestry managers in New Zealand. This method involves using a UAV to capture images of trees, and then using photogrammetry to generate a 3D model which includes both a mesh and a point cloud. The point cloud is then analysed, and data and metrics are extracted. Once the metrics have been extracted, they are saved for analysis and exported to a virtual reality (VR) environment. The VR environment will allow end users to remotely interact with sections of forest replicated with photogrammetry and visualise the metrics created through point cloud analysis.},
	booktitle = {2018 {International} {Conference} on {Image} and {Vision} {Computing} {New} {Zealand} ({IVCNZ})},
	author = {Lee, David and Muir, William and Beeston, Samuel and Bates, Samuel and Schofield, Sam D. and Edwards, Matthew J. and Green, Richard D.},
	month = nov,
	year = {2018},
	keywords = {Forest, Forestry, Measurement, Photogrammetry, Point Clouds, Software, Solid modeling, Three-dimensional displays, Trees, UAV, Vegetation, Virtual reality},
	pages = {1--6},
}

@article{feng_hierarchical_2021,
	title = {A {Hierarchical} {Approach} for {Point} {Cloud} {Classification} {With} {3D} {Contextual} {Features}},
	volume = {14},
	issn = {2151-1535},
	doi = {10.1109/JSTARS.2021.3077568},
	abstract = {Classifying point cloud of urban landscapes plays essential roles in many urban applications. However, automating such a task is challenging due to irregular point distribution and complex urban scenes. Incorporating contextual information is crucial in improving classification accuracy of point clouds. In this article, we propose a hierarchical approach for point cloud classification with 3-D contextual features, which comprises three steps:segment-based classification with primitive features and a random forest classifier; extracting novel 3-D contextual features from the initial labels considering spatial relationships between neighboring segments and semantic dependencies; and refining classification with a combination of primitive features and spatial contextual features, and a hierarchical multilayer perceptron classifier that considers primitive features and spatial contextual features at different levels. The proposed method was tested on two point cloud datasets:the National University of Singapore (NUS) dataset and the Vaihingen benchmark dataset of the International Society of Photogrammetry and Remote Sensing. The evaluation results showed that the proposed method achieved an overall accuracy of 92.51\% and 82.34\% for the NUS dataset and Vaihingen dataset, respectively. The feature importance evaluation showed that 3-D spatial contextual features contributed useful information for discriminating different classes, such as roof, facade, grassland, tree, and ground. Quantitative comparisons further showed that the proposed method is more advantageous, especially in the detection of class roof and facade.},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Feng, Chen-Chieh and Guo, Zhou},
	year = {2021},
	keywords = {Classification, Context modeling, Feature extraction, Laser radar, Probabilistic logic, Roads, Semantics, Three-dimensional displays, contextual feature, hierarchical classifier, point cloud},
	pages = {5036--5048},
}

@article{lisein_photogrammetric_2013,
	title = {A photogrammetric workflow for the creation of a forest canopy height model from small unmanned aerial system imagery},
	volume = {4},
	number = {4},
	journal = {Forests},
	author = {Lisein, Jonathan and Pierrot-Deseilligny, Marc and Bonnet, Stéphanie and Lejeune, Philippe},
	year = {2013},
	pages = {922--944},
}

@inproceedings{santos_3d_2014,
	title = {{3D} plant modeling: localization, mapping and segmentation for plant phenotyping using a single hand-held camera},
	shorttitle = {{3D} plant modeling},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Santos, Thiago Teixeira and Koenigkan, Luciano Vieira and Barbedo, Jayme Garcia Arnal and Rodrigues, Gustavo Costa},
	year = {2014},
	pages = {247--263},
}

@article{wang_aerial_2017,
	title = {Aerial {LIDAR} point cloud voxelization with its {3D} ground filtering application},
	volume = {83},
	number = {2},
	journal = {Photogrammetric engineering \& remote sensing},
	author = {Wang, Liying and Xu, Yan and Li, Yu},
	year = {2017},
	pages = {95--107},
}

@article{lu_3d_2021,
	title = {{3D} plant root system reconstruction based on fusion of deep structure-from-motion and {IMU}},
	journal = {Multimedia Tools and Applications},
	author = {Lu, Yawen and Wang, Yuxing and Chen, Zhanjie and Khan, Awais and Salvaggio, Carl and Lu, Guoyu},
	year = {2021},
	pages = {1--17},
}

@inproceedings{lou_accurate_2014,
	title = {Accurate multi-view stereo {3D} reconstruction for cost-effective plant phenotyping},
	booktitle = {International {Conference} {Image} {Analysis} and {Recognition}},
	publisher = {Springer},
	author = {Lou, Lu and Liu, Yonghuai and Han, Jiwan and Doonan, John H.},
	year = {2014},
	pages = {349--356},
}

@article{hinks_point_2013,
	title = {Point cloud data conversion into solid models via point-based voxelization},
	volume = {139},
	number = {2},
	journal = {Journal of Surveying Engineering},
	author = {Hinks, Tommy and Carr, Hamish and Truong-Hong, Linh and Laefer, Debra F.},
	year = {2013},
	pages = {72--83},
}

@article{kohek_interactive_2018,
	title = {Interactive {Large}-{Scale} {Procedural} {Forest} {Construction} and {Visualization} {Based} on {Particle} {Flow} {Simulation}},
	volume = {37},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13304},
	doi = {10.1111/cgf.13304},
	abstract = {Interactive visualization of large forest scenes is challenging due to the large amount of geometric detail that needs to be generated and stored, particularly in scenarios with a moving observer such as forest walkthroughs or overflights. Here, we present a new method for large-scale procedural forest generation and visualization at interactive rates. We propose a hybrid approach by combining geometry-based and volumetric modelling techniques with gradually transitioning level of detail (LOD). Nearer trees are constructed using an extended particle flow algorithm, in which particle trails outline the tree ramification in an inverse direction, i.e. from the leaves towards the roots. Reduced geometric representation of a tree is obtained by subsampling the trails. For distant trees, a new volumetric rendering technique in pixel-space is introduced, which avoids geometry formation altogether and enables visualization of vast forest areas with millions of unique trees. We demonstrate that a GPU-based implementation of the proposed method provides interactive frame rates in forest overflight scenarios, where new trees are constructed and their LOD adjusted on the fly.},
	language = {en},
	number = {1},
	urldate = {2021-06-15},
	journal = {Computer Graphics Forum},
	author = {Kohek, Štefan and Strnad, Damjan},
	year = {2018},
	keywords = {I.3.5 Computer Graphics: Computational Geometry and Object Modelling, I.6.8 Simulation and Modelling: Types of Simulation—Parallel, Visual, level-of-detail algorithms, novel applications of the GPU, particle systems},
	pages = {389--402},
}

@inproceedings{xiong_speed_2010,
	title = {Speed {Tree}-{Based} {Forest} {Simulation} {System}},
	doi = {10.1109/iCECE.2010.738},
	abstract = {We introduce a forest simulation system called SpeedTree-Based Forest Simulation System (SFSS), which is able to simulate a real-time interactive virtual forest after user setting up tree species and forest distribution. SFSS solves the conflict between system overhead and visual effects in forest simulation: in processing data, in order to provide a structural basis for high-speed data processing, SFSS takes SpeedTree Model Library as the source of trees data and encapsulates data according to SpeedTree tree structures; in reducing amount of data, it uses forest culling algorithm culling the virtual forest and Mixed-Level Modeling approach modeling forest to reduce the need of data amount; in improving visual effect, it uses programmable pipeline to enhance the picture quality. We have done a lot of experiments to test SFSS's performance. The average frame rate is 29.3 frames per second (fps) when the view point moved; it is 98.2 fps when view point maintained stillness. The experimental results show that SFSS meets the need of real-time forest simulation with high practical value.},
	booktitle = {2010 {International} {Conference} on {Electrical} and {Control} {Engineering}},
	author = {Xiong, Qing and Huang, Xin-yuan},
	month = jun,
	year = {2010},
	keywords = {Cameras, Data models, Engines, Forest Simulation, Libraries, Middleware, Modeling, Rendering (computer graphics), Solid modeling, SpeedTree, Virtools},
	pages = {3033--3036},
}

@incollection{guo_realistic_2017,
	title = {Realistic procedural plant modeling guided by {3D} point cloud},
	booktitle = {{ACM} {SIGGRAPH} 2017 {Posters}},
	author = {Guo, Jianwei and Cheng, Zhanglin and Xu, Shibiao and Zhang, Xiaopeng},
	year = {2017},
	pages = {1--2},
}

@article{pirk_plastic_2012,
	title = {Plastic trees: interactive self-adapting botanical tree models},
	volume = {31},
	shorttitle = {Plastic trees},
	number = {4},
	journal = {ACM Transactions on Graphics (TOG)},
	author = {Pirk, Sören and Stava, Ondrej and Kratt, Julian and Said, Michel Abdul Massih and Neubert, Boris and Měch, Radomír and Benes, Bedrich and Deussen, Oliver},
	year = {2012},
	pages = {1--10},
}

@incollection{favorskaya_realistic_2017,
	title = {Realistic tree modelling},
	booktitle = {Handbook on {Advances} in {Remote} {Sensing} and {Geographic} {Information} {Systems}},
	publisher = {Springer},
	author = {Favorskaya, Margarita N. and Jain, Lakhmi C.},
	year = {2017},
	pages = {181--202},
}

@article{xie_tree_2015,
	title = {Tree modeling with real tree-parts examples},
	volume = {22},
	number = {12},
	journal = {IEEE transactions on visualization and computer graphics},
	author = {Xie, Ke and Yan, Feilong and Sharf, Andrei and Deussen, Oliver and Huang, Hui and Chen, Baoquan},
	year = {2015},
	pages = {2608--2618},
}

@article{pirk_capturing_2012,
	title = {Capturing and animating the morphogenesis of polygonal tree models},
	volume = {31},
	number = {6},
	journal = {ACM Transactions on Graphics (TOG)},
	author = {Pirk, Sören and Niese, Till and Deussen, Oliver and Neubert, Boris},
	year = {2012},
	pages = {1--10},
}

@inproceedings{benes_virtual_2002,
	title = {Virtual climbing plants competing for space},
	booktitle = {Proceedings of {Computer} {Animation} 2002 ({CA} 2002)},
	publisher = {IEEE},
	author = {Benes, Bedrich and Millán, Erik Uriel},
	year = {2002},
	pages = {33--42},
}

@inproceedings{hadrich_interactive_2017,
	title = {Interactive modeling and authoring of climbing plants},
	volume = {36},
	booktitle = {Computer {Graphics} {Forum}},
	publisher = {Wiley Online Library},
	author = {Hädrich, Torsten and Benes, Bedrich and Deussen, Oliver and Pirk, Sören},
	year = {2017},
	pages = {49--61},
}

@article{yang_efficient_2021,
	title = {Efficient global color correction for large-scale multiple-view images in three-dimensional reconstruction},
	volume = {173},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S092427162030349X},
	doi = {10.1016/j.isprsjprs.2020.12.011},
	abstract = {Consistent global color correction across multiple-view images in three-dimensional (3D) reconstruction is an important and challenging problem. The present work addresses this issue by proposing a novel global color correction method for multi-view images based on a spline curve remapping function. In contrast to existing methods, we obtain a series of optimal functions by minimizing the variance in the color values of all observations of every sparse point generated by the Structure from Motion (SfM) technique. We also find that adding only simple constraints to the spline is required to prevent the loss of image contrast and gradient information. The robustness of the proposed method is ensured by the adoption of strong geometric constraints between multi-view images. Finally, the applicability of the method to large-scale multiple-view images is facilitated by proposing a parallelizable hierarchical image color correction strategy based on a tree structure. The performance of the proposed method is compared with the performances of existing state-of-the-art methods when applied to several challenging datasets. The results indicate that the notable flexibility of the spline curve, along with the proposed optimization process and hierarchical strategy, not only enable the proposed method to perform well with challenging datasets, but also provide high computational efficiency when working with large-scale image sets.},
	language = {en},
	urldate = {2021-06-18},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Yang, Junxing and Liu, Lulu and Xu, Jiabin and Wang, Yi and Deng, Fei},
	month = mar,
	year = {2021},
	keywords = {Color consistency, Hierarchical optimization, Multi-view images, Quadratic splines},
	pages = {209--220},
}

@article{jiang_efficient_2017,
	title = {Efficient structure from motion for oblique {UAV} images based on maximal spanning tree expansion},
	volume = {132},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271617303040},
	doi = {10.1016/j.isprsjprs.2017.09.004},
	abstract = {The primary contribution of this paper is an efficient Structure from Motion (SfM) solution for oblique unmanned aerial vehicle (UAV) images. First, an algorithm, considering spatial relationship constraints between image footprints, is designed for match pair selection with the assistance of UAV flight control data and oblique camera mounting angles. Second, a topological connection network (TCN), represented by an undirected weighted graph, is constructed from initial match pairs, which encodes the overlap areas and intersection angles into edge weights. Then, an algorithm, termed MST-Expansion, is proposed to extract the match graph from the TCN, where the TCN is first simplified by a maximum spanning tree (MST). By further analysis of the local structure in the MST, expansion operations are performed on the vertices of the MST for match graph enhancement, which is achieved by introducing critical connections in the expansion directions. Finally, guided by the match graph, an efficient SfM is proposed. Under extensive analysis and comparison, its performance is verified by using three oblique UAV datasets captured with different multi-camera systems. Experimental results demonstrate that the efficiency of image matching is improved, with speedup ratios ranging from 19 to 35, and competitive orientation accuracy is achieved from both relative bundle adjustment (BA) without GCPs (Ground Control Points) and absolute BA with GCPs. At the same time, images in the three datasets are successfully oriented. For the orientation of oblique UAV images, the proposed method can be a more efficient solution.},
	language = {en},
	urldate = {2021-06-18},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Jiang, San and Jiang, Wanshou},
	month = oct,
	year = {2017},
	keywords = {3D reconstruction, Bundle adjustment, Maximal spanning tree expansion, Oblique photogrammetry, Structure from motion, Unmanned aerial vehicle},
	pages = {140--161},
}

@article{wang_botanical_2017,
	title = {Botanical materials based on biomechanics},
	volume = {36},
	number = {4},
	journal = {ACM Transactions on Graphics (TOG)},
	author = {Wang, Bohan and Zhao, Yili and Barbič, Jernej},
	year = {2017},
	pages = {1--13},
}

@article{tu_optimising_2020,
	title = {Optimising drone flight planning for measuring horticultural tree crop structure},
	volume = {160},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271619302941},
	doi = {10.1016/j.isprsjprs.2019.12.006},
	abstract = {In recent times, multi-spectral drone imagery has proved to be a useful tool for measuring tree crop canopy structure. In this context, establishing the most appropriate flight planning variable settings is an essential consideration due to their controls on the quality of the imagery and derived maps of tree and crop biophysical properties. During flight planning, variables including flight altitude, image overlap, flying direction, flying speed and solar elevation, require careful consideration in order to produce the most suitable drone imagery. Previous studies have assessed the influence of individual variables on image quality, but the interaction of multiple variables has yet to be examined. This study assesses the influence of several flight variables on measures of data quality in each processing step, i.e. photo alignment, point cloud densification, 3D model building, and ortho-mosaicking. The analysis produced a drone flight planning and image processing workflow that delivers accurate measurements of tree crops, including the tie point quality, densified point cloud density, and the measurement accuracy of height and plant projective cover derived from individual trees within a commercial avocado orchard. Results showed that flying along the hedgerow, at high solar elevation and with low image pitch angles improved the data quality. Optimal flying speed needs to be set to achieve the required forward overlap. The impacts of each image acquisition variable are discussed in detail and protocols for flight planning optimisation for three scenarios with different drone settings are suggested. Establishing protocols that deliver optimal image acquisitions for the collection of drone data over horticultural tree crops, will create greater confidence in the accuracy of subsequent algorithms and resultant maps of biophysical properties.},
	language = {en},
	urldate = {2021-06-18},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Tu, Yu-Hsuan and Phinn, Stuart and Johansen, Kasper and Robson, Andrew and Wu, Dan},
	month = feb,
	year = {2020},
	keywords = {Drone, Flight planning, Horticulture, Tree structure measurement, UAS, UAV},
	pages = {83--96},
}

@misc{noauthor_modeling_nodate,
	title = {Modeling and generating moving trees from video {\textbackslash}textbar {ACM} {Transactions} on {Graphics}},
	url = {https://dl.acm.org/doi/10.1145/2070781.2024161},
	urldate = {2021-06-18},
}

@misc{noauthor_shape_nodate,
	title = {The {Shape} {Space} of {3D} {Botanical} {Tree} {Models} {\textbackslash}textbar {ACM} {Transactions} on {Graphics}},
	url = {https://dl.acm.org/doi/10.1145/3144456},
	urldate = {2021-06-18},
}

@book{lin_novel_2020,
	title = {A novel tree-structured point cloud dataset for skeletonization algorithm evaluation},
	author = {Lin, Yan and Liu, Ji and Zhou, Jianlin},
	year = {2020},
}

@article{argudo_image-based_2020,
	title = {Image-{Based} {Tree} {Variations}},
	volume = {39},
	copyright = {© 2019 The Authors Computer Graphics Forum © 2019 Eurographics - The European Association for Computer Graphics and John Wiley \& Sons Ltd},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13752},
	doi = {10.1111/cgf.13752},
	abstract = {The automatic generation of realistic vegetation closely reproducing the appearance of specific plant species is still a challenging topic in computer graphics. In this paper, we present a new approach to generate new tree models from a small collection of frontal RGBA images of trees. The new models are represented either as single billboards (suitable for still image generation in areas such as architecture rendering) or as billboard clouds (providing parallax effects in interactive applications). Key ingredients of our method include the synthesis of new contours through convex combinations of exemplar countours, the automatic segmentation into crown/trunk classes and the transfer of RGBA colour from the exemplar images to the synthetic target. We also describe a fully automatic approach to convert a single tree image into a billboard cloud by extracting superpixels and distributing them inside a silhouette-defined 3D volume. Our algorithm allows for the automatic generation of an arbitrary number of tree variations from minimal input, and thus provides a fast solution to add vegetation variety in outdoor scenes.},
	language = {en},
	number = {1},
	urldate = {2021-06-18},
	journal = {Computer Graphics Forum},
	author = {Argudo, Oscar and Andújar, Carlos and Chica, Antoni},
	year = {2020},
	keywords = {image-based modelling, image-based rendering, modelling, rendering, • Computing methodologies → Image-based rendering},
	pages = {174--184},
}

@article{wang_shape_2018,
	title = {The shape space of 3d botanical tree models},
	volume = {37},
	number = {1},
	journal = {ACM Transactions on Graphics (TOG)},
	author = {Wang, Guan and Laga, Hamid and Xie, Ning and Jia, Jinyuan and Tabia, Hedi},
	year = {2018},
	pages = {1--18},
}

@inproceedings{wang_statistical_2018,
	title = {Statistical modeling of the {3D} geometry and topology of botanical trees},
	volume = {37},
	booktitle = {Computer {Graphics} {Forum}},
	publisher = {Wiley Online Library},
	author = {Wang, Guan and Laga, Hamid and Jia, Jinyuan and Xie, Ning and Tabia, Hedi},
	year = {2018},
	pages = {185--198},
}

@misc{noauthor_poisson_nodate,
	title = {Poisson matting {\textbackslash}textbar {ACM} {Transactions} on {Graphics}},
	url = {https://dl.acm.org/doi/10.1145/1015706.1015721},
	urldate = {2021-08-05},
}

@article{pirk_windy_2014,
	title = {Windy trees: computing stress response for developmental tree models},
	volume = {33},
	issn = {0730-0301},
	shorttitle = {Windy trees},
	url = {https://doi.org/10.1145/2661229.2661252},
	doi = {10.1145/2661229.2661252},
	abstract = {We present a novel method for combining developmental tree models with turbulent wind fields. The tree geometry is created from internal growth functions of the developmental model and its response to external stress is induced by a physically-plausible wind field that is simulated by Smoothed Particle Hydrodynamics (SPH). Our tree models are dynamically evolving complex systems that (1) react in real-time to high-frequent changes of the wind simulation; and (2) adapt to long-term wind stress. We extend this process by wind-related effects such as branch breaking as well as bud abrasion and drying. In our interactive system the user can adjust the parameters of the growth model, modify wind properties and resulting forces, and define the tree's long-term response to wind. By using graphics hardware, our implementation runs at interactive rates for moderately large scenes composed of up to 20 tree models.},
	number = {6},
	urldate = {2020-10-07},
	journal = {ACM Transactions on Graphics},
	author = {Pirk, Sören and Niese, Till and Hädrich, Torsten and Benes, Bedrich and Deussen, Oliver},
	month = nov,
	year = {2014},
	keywords = {animation, generative tree modeling, interactive procedural modeling, plant growth, simulation, visual models of trees},
	pages = {204:1--204:11},
}

@article{stam_stochastic_1997,
	title = {Stochastic {Dynamics}: {Simulating} the {Effects} of {Turbulence} on {Flexible} {Structures}},
	volume = {16},
	issn = {1467-8659},
	shorttitle = {Stochastic {Dynamics}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8659.16.3conferenceissue.17},
	doi = {10.1111/1467-8659.16.3conferenceissue.17},
	abstract = {This paper addresses the problem of realistically simulating the motion of tree-branches subjected to turbulence. Since the resulting motion is random in nature, we model it as a stochastic process. We synthesize this process directly by filtering a white noise in the Fourier domain. The filter is constructed by performing a modal analysis of the tree. We use a sophisticated numerical technique which is able to compute the first few significant modes of large trees. The main advantage of our technique over previous methods is that we are able to compute complicated motions without the necessity of integrating dynamical equations over time. Consequently, a user can view and manipulate tree-motions in real-time. Our technique can be further extended to other flexible structures such as two-dimensional plates.},
	language = {en},
	number = {s3},
	urldate = {2020-10-26},
	journal = {Computer Graphics Forum},
	author = {Stam, Jos},
	year = {1997},
	keywords = {animal models, animation, growth, polygonal meshes, shape evolution},
	pages = {C159--C164},
}

@article{bai_skeleton_2007,
	title = {Skeleton {Pruning} by {Contour} {Partitioning} with {Discrete} {Curve} {Evolution}},
	volume = {29},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2007.59},
	abstract = {In this paper, we introduce a new skeleton pruning method based on contour partitioning. Any contour partition can be used, but the partitions obtained by discrete curve evolution (DCE) yield excellent results. The theoretical properties and the experiments presented demonstrate that obtained skeletons are in accord with human visual perception and stable, even in the presence of significant noise and shape variations, and have the same topology as the original skeletons. In particular, we have proven that the proposed approach never produces spurious branches, which are common when using the known skeleton pruning methods. Moreover, the proposed pruning method does not displace the skeleton points. Consequently, all skeleton points are centers of maximal disks. Again, many existing methods displace skeleton points in order to produces pruned skeletons},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bai, Xiang and Latecki, Longin Jan and Liu, Wen-yu},
	month = mar,
	year = {2007},
	keywords = {Algorithms, Artificial Intelligence, Automated, Computer Society, Computer-Assisted, Humans, Image Enhancement, Image Interpretation, Image retrieval, Information Storage and Retrieval, Noise generators, Noise shaping, Numerical Analysis, Pattern Recognition, Reproducibility of Results, Sensitivity and Specificity, Shape measurement, Signal Processing, Skeleton, Topology, Visual perception, contour partition, contour partitioning, discrete curve evolution, discrete curve evolution., human visual perception, image representation, image thinning, object recognition, object representation, skeleton pruning},
	pages = {449--462},
}

@inproceedings{reche_volumetric_2004,
	title = {Volumetric reconstruction and interactive rendering of trees from photographs},
	doi = {10.1145/1186562.1015785},
	abstract = {Reconstructing and rendering trees is a challenging problem due to the geometric complexity involved, and the inherent difficulties of capture. In this paper we propose a volumetric approach to capture and render trees with relatively sparse foliage. Photographs of such trees typically have single pixels containing the blended projection of numerous leaves/branches and background. We show how we estimate opacity values on a recursive grid, based on alphamattes extracted from a small number of calibrated photographs of a tree. This data structure is then used to render billboards attached to the centers of the grid cells. Each billboard is assigned a set of view-dependent textures corresponding to each input view. These textures are generated by approximating coverage masks based on opacity and depth from the camera. Rendering is performed using a view-dependent texturing algorithm. The resulting volumetric tree structure has low polygon count, permitting interactive rendering of realistic 3D trees. We illustrate the implementation of our system on several different real trees, and show that we can insert the resulting model in virtual scenes. Copyright © 2004 ACM.},
	urldate = {2020-02-04},
	booktitle = {{ACM} {SIGGRAPH} 2004 {Papers}, {SIGGRAPH} 2004},
	author = {Reche, Alex and Martiny, Ignacio and Drettakis, George},
	year = {2004},
	keywords = {3D reconstruction, Interactive rendering, Visibility estimation, \_tablet},
	pages = {720--727},
}

@inproceedings{isokane_probabilistic_2018,
	title = {Probabilistic {Plant} {Modeling} via {Multi}-view {Image}-to-{Image} {Translation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578405/},
	doi = {10.1109/CVPR.2018.00307},
	abstract = {This paper describes a method for inferring three-dimensional (3D) plant branch structures that are hidden under leaves from multi-view observations. Unlike previous geometric approaches that heavily rely on the visibility of the branches or use parametric branching models, our method makes statistical inferences of branch structures in a probabilistic framework. By inferring the probability of branch existence using a Bayesian extension of image-to-image translation applied to each of multi-view images, our method generates a probabilistic plant 3D model, which represents the 3D branching pattern that cannot be directly observed. Experiments demonstrate the usefulness of the proposed approach in generating convincing branch structures in comparison to prior approaches.},
	urldate = {2019-10-31},
	booktitle = {Proceedings of the {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Isokane, Takahiro and Okura, Fumio and Ide, Ayaka and Matsushita, Yasuyuki and Yagi, Yasushi},
	month = jun,
	year = {2018},
	keywords = {\_tablet},
	pages = {2906--2915},
}

@inproceedings{verroust_extracting_1999,
	title = {Extracting skeletal curves from {3D} scattered data},
	doi = {10.1109/SMA.1999.749340},
	abstract = {We introduce a method for extracting skeletal curves from an unorganized collection of scattered data points lying on a surface. These curves may have a tree like structure to capture branching shapes such as blood vessels. The skeletal curves can be used for different applications ranging from surface reconstruction to object recognition.},
	booktitle = {Proceedings {Shape} {Modeling} {International} '99. {International} {Conference} on {Shape} {Modeling} and {Applications}},
	author = {Verroust, A. and Lazarus, F.},
	month = mar,
	year = {1999},
	keywords = {3D scattered data, Clouds, Data mining, Electrical capacitance tomography, Image reconstruction, Read only memory, Scattering, Shape, Skeleton, Surface reconstruction, Time of arrival estimation, curve fitting, image reconstruction, object recognition, scattered data points, skeletal curves, surface fitting, surface reconstruction},
	pages = {194--201},
}

@article{au_skeleton_2008,
	title = {Skeleton extraction by mesh contraction},
	volume = {27},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/1360612.1360643},
	doi = {10.1145/1360612.1360643},
	abstract = {Extraction of curve-skeletons is a fundamental problem with many applications in computer graphics and visualization. In this paper, we present a simple and robust skeleton extraction method based on mesh contraction. The method works directly on the mesh domain, without pre-sampling the mesh model into a volumetric representation. The method first contracts the mesh geometry into zero-volume skeletal shape by applying implicit Laplacian smoothing with global positional constraints. The contraction does not alter the mesh connectivity and retains the key features of the original mesh. The contracted mesh is then converted into a 1D curve-skeleton through a connectivity surgery process to remove all the collapsed faces while preserving the shape of the contracted mesh and the original topology. The centeredness of the skeleton is refined by exploiting the induced skeleton-mesh mapping. In addition to producing a curve skeleton, the method generates other valuable information about the object's geometry, in particular, the skeleton-vertex correspondence and the local thickness, which are useful for various applications. We demonstrate its effectiveness in mesh segmentation and skinning animation.},
	number = {3},
	urldate = {2020-11-12},
	journal = {ACM Transactions on Graphics},
	author = {Au, Oscar Kin-Chung and Tai, Chiew-Lan and Chu, Hung-Kuo and Cohen-Or, Daniel and Lee, Tong-Yee},
	month = aug,
	year = {2008},
	keywords = {Laplacian, mesh contraction, segmentation, skeleton, skinning, smoothing},
	pages = {1--10},
}

@inproceedings{cohen-steiner_variational_2004,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '04},
	title = {Variational shape approximation},
	isbn = {978-1-4503-7823-9},
	url = {https://doi.org/10.1145/1186562.1015817},
	doi = {10.1145/1186562.1015817},
	abstract = {A method for concise, faithful approximation of complex 3D datasets is key to reducing the computational cost of graphics applications. Despite numerous applications ranging from geometry compression to reverse engineering, efficiently capturing the geometry of a surface remains a tedious task. In this paper, we present both theoretical and practical contributions that result in a novel and versatile framework for geometric approximation of surfaces. We depart from the usual strategy by casting shape approximation as a variational geometric partitioning problem. Using the concept of geometric proxies, we drive the distortion error down through repeated clustering of faces into best-fitting regions. Our approach is entirely discrete and error-driven, and does not require parameterization or local estimations of differential quantities. We also introduce a new metric based on normal deviation, and demonstrate its superior behavior at capturing anisotropy.},
	urldate = {2020-10-21},
	booktitle = {{ACM} {SIGGRAPH} 2004 {Papers}},
	publisher = {Association for Computing Machinery},
	author = {Cohen-Steiner, David and Alliez, Pierre and Desbrun, Mathieu},
	month = aug,
	year = {2004},
	keywords = {Lloyd's clustering algorithm, anisotropic remeshing, geometric approximation, geometric error metrics, surfaces},
	pages = {905--914},
}

@article{sherbrooke_differential_1996,
	title = {Differential and {Topological} {Properties} of {Medial} {Axis} {Transforms}},
	volume = {58},
	issn = {1077-3169},
	url = {http://www.sciencedirect.com/science/article/pii/S1077316996900477},
	doi = {10.1006/gmip.1996.0047},
	abstract = {Themedial axis transformis a representation of an object which has been shown to be useful in design, interrogation, animation, finite element mesh generation, performance analysis, manufacturing simulation, path planning, and tolerance specification. In this paper, the theory of the medial axis transform for 3-D objects is developed. For objects with piecewiseC2boundaries, relationships between the curvature of the boundary and the position of the medial axis are developed. Forn-dimensional submanifolds of Rnwith boundaries which are piecewiseC2and completelyG1, a deformation retract is set up between each object and its medial axis, which demonstrates that if the object is path connected, then so is its medial axis. Finally, it is proven that path connected polyhedral solids without cavities have path connected medial axes.},
	language = {en},
	number = {6},
	urldate = {2020-11-03},
	journal = {Graphical Models and Image Processing},
	author = {Sherbrooke, Evan C and Patrikalakis, Nicholas M and Wolter, Franz-Erich},
	month = nov,
	year = {1996},
	pages = {574--592},
}

@article{neubert_approximate_2007,
	title = {Approximate image-based tree-modeling using particle flows},
	volume = {26},
	issn = {07300301},
	url = {http://portal.acm.org/citation.cfm?doid=1275808.1276487},
	doi = {10.1145/1239451.1239539},
	abstract = {We present a method for producing 3D tree models from input photographs with only limited user intervention. An approximate voxel-based tree volume is estimated using image information. The density values of the voxels are used to produce initial positions for a set of particles. Performing a 3D flow simulation, the particles are traced downwards to the tree basis and are combined to form twigs and branches. If possible, the trunk and the first-order branches are determined in the input photographs and are used as attractors for particle simulation. The geometry of the tree skeleton is produced using botanical rules for branch thicknesses and branching angles. Finally, leaves are added. Different initial seeds for particle simulation lead to a variety, yet similar-looking branching structures for a single set of photographs. © 2007 ACM.},
	number = {99},
	urldate = {2019-07-05},
	journal = {ACM Transactions on Graphics},
	author = {Neubert, Boris and Franken, Thomas and Deussen, Oliver},
	year = {2007},
	keywords = {Voxel, \_tablet, botanics, image-based modeling, plant models},
	pages = {88},
}

@article{livny_automatic_2010,
	title = {Automatic reconstruction of tree skeletal structures from point clouds},
	volume = {29},
	issn = {07300301},
	url = {http://portal.acm.org/citation.cfm?doid=1882261.1866177},
	doi = {10.1145/1882261.1866177},
	number = {6},
	urldate = {2019-07-05},
	journal = {ACM Transactions on Graphics},
	author = {Livny, Yotam and Yan, Feilong and Olson, Matt and Chen, Baoquan and Zhang, Hao and El-Sana, Jihad and Livny, Yotam and Yan, Feilong and Olson, Matt and Chen, Baoquan and Zhang, Hao and El-Sana, Jihad},
	month = dec,
	year = {2010},
	keywords = {\_tablet},
	pages = {1},
}

@article{shlyakhter_reconstructing_2001,
	title = {Reconstructing {3D} tree models from instrumented photographs},
	volume = {21},
	issn = {1558-1756},
	doi = {10.1109/38.920627},
	abstract = {Our computer modeling technique reproduces a tree's 3D volume and skeleton from instrumented photographs. The technique involves first constructing the skeleton (trunk and the major branches) of the tree, and then applying an L-system starting from this skeleton. L-systems are one of the better known procedural models in the graphics community, especially after their popularization by P. Prusinkiewicz and A. Lindenmayer (1990).},
	number = {3},
	journal = {IEEE Computer Graphics and Applications},
	author = {Shlyakhter, I. and Rozenoer, M. and Dorsey, J. and Teller, S.},
	month = may,
	year = {2001},
	keywords = {3D tree model reconstruction, 3D volume, Databases, Graphics, Image reconstruction, Image segmentation, Instruments, L-system, Organisms, Robustness, Shape, Skeleton, Tree graphs, \_tablet, botany, computer modeling technique, graphics community, instrumented photographs, major branches, procedural models, realistic images, solid modelling, trunk},
	pages = {53--61},
}

@article{stava_inverse_2014,
	title = {Inverse {Procedural} {Modelling} of {Trees}},
	volume = {33},
	issn = {0167-7055},
	doi = {10.1111/cgf.12282},
	abstract = {Procedural tree models have been popular in computer graphics for their ability to generate a variety of output trees from a set of input parameters and to simulate plant interaction with the environment for a realistic placement of trees in virtual scenes. However, defining such models and their parameters is a difficult task. We propose an inverse modelling approach for stochastic trees that takes polygonal tree models as input and estimates the parameters of a procedural model so that it produces trees similar to the input. Our framework is based on a novel parametric model for tree generation and uses Monte Carlo Markov Chains to find the optimal set of parameters. We demonstrate our approach on a variety of input models obtained from different sources, such as interactive modelling systems, reconstructed scans of real trees and developmental models.},
	language = {English},
	number = {6},
	journal = {Computer Graphics Forum},
	author = {Stava, O. and Pirk, S. and Kratt, J. and Chen, B. and Mech, R. and Deussen, O. and Benes, B.},
	month = sep,
	year = {2014},
	keywords = {\_tablet, biological modeling, distance, mesh generation, natural phenomena, reconstruction},
	pages = {118--131},
}

@article{palubicki_self-organizing_2009,
	title = {Self-organizing tree models for image synthesis},
	volume = {28},
	issn = {07300301},
	url = {http://portal.acm.org/citation.cfm?doid=1576246.1531364},
	doi = {10.1145/1531326.1531364},
	abstract = {We present a method for generating realistic models of temperate-climate trees and shrubs. This method is based on the biological hypothesis that the form of a developing tree emerges from a self-organizing process dominated by the competition of buds and branches for light or space, and regulated by internal signaling mechanisms. Simulations of this process robustly generate a wide range of realistic trees and bushes. The generated forms can be controlled with a variety of interactive techniques, including procedural brushes, sketching, and editing operations such as pruning and bending of branches. We illustrate the usefulness and versatility of the proposed method with diverse tree models, forest scenes, animations of tree development, and examples of combined interactive-procedural tree modeling.},
	number = {3},
	urldate = {2019-07-05},
	journal = {ACM Transactions on Graphics},
	author = {Palubicki, Wojciech and Horel, Kipp and Longay, Steven and Runions, Adam and Lane, Brendan and Měch, Radomír and Prusinkiewicz, Przemyslaw},
	year = {2009},
	keywords = {\_tablet, apical control, bud fate, emergence, generative tree model, interactive-procedural modeling, tree development},
	pages = {1},
}

@article{tan_image-based_2007,
	title = {Image-based tree modeling},
	volume = {26},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/1276377.1276486},
	doi = {10.1145/1276377.1276486},
	abstract = {In this paper, we propose an approach for generating 3D models of natural-looking trees from images that has the additional benefit of requiring little user intervention. While our approach is primarily image-based, we do not model each leaf directly from images due to the large leaf count, small image footprint, and widespread occlusions. Instead, we populate the tree with leaf replicas from segmented source images to reconstruct the overall tree shape. In addition, we use the shape patterns of visible branches to predict those of obscured branches. We demonstrate our approach on a variety of trees.},
	number = {3},
	urldate = {2020-02-05},
	journal = {ACM Transactions on Graphics (TOG)},
	author = {Tan, Ping and Zeng, Gang and Wang, Jingdong and Kang, Sing Bing and Quan, Long},
	month = jul,
	year = {2007},
	keywords = {\_tablet},
	pages = {87--es},
}

@article{tan_single_2008,
	title = {Single image tree modeling},
	volume = {27},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/1409060.1409061},
	doi = {10.1145/1409060.1409061},
	abstract = {In this paper, we introduce a simple sketching method to generate a realistic 3D tree model from a single image. The user draws at least two strokes in the tree image: the first crown stroke around the tree crown to mark up the leaf region, the second branch stroke from the tree root to mark up the main trunk, and possibly few other branch strokes for refinement. The method automatically generates a 3D tree model including branches and leaves. Branches are synthesized by a growth engine from a small library of elementary subtrees that are pre-defined or built on the fly from the recovered visible branches. The visible branches are automatically traced from the drawn branch strokes according to image statistics on the strokes. Leaves are generated from the region bounded by the first crown stroke to complete the tree. We demonstrate our method on a variety of examples.},
	number = {5},
	urldate = {2020-02-05},
	journal = {ACM Transactions on Graphics (TOG)},
	author = {Tan, Ping and Fang, Tian and Xiao, Jianxiong and Zhao, Peng and Quan, Long},
	month = dec,
	year = {2008},
	keywords = {\_tablet},
	pages = {108:1--108:7},
}

@article{xu_knowledge_2007,
	title = {Knowledge and heuristic-based modeling of laser-scanned trees},
	volume = {26},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/1289603.1289610},
	doi = {10.1145/1289603.1289610},
	abstract = {We present a semi-automatic and efficient method for producing full polygonal models of range scanned trees, which are initially represented as sparse point clouds. First, a skeleton of the trunk and main branches of the tree is produced based on the scanned point clouds. Due to the unavoidable incompleteness of the point clouds produced by range scans of trees, steps are taken to synthesize additional branches to produce plausible support for the tree crown. Appropriate dimensions for each branch section are estimated using allometric theory. Using this information, a mesh is produced around the full skeleton. Finally, leaves are positioned, oriented and connected to nearby branches. Our process requires only minimal user interaction, and the full process including scanning and modeling can be completed within minutes.},
	number = {4},
	urldate = {2020-02-05},
	journal = {ACM Transactions on Graphics (TOG)},
	author = {Xu, Hui and Gossett, Nathan and Chen, Baoquan},
	month = oct,
	year = {2007},
	keywords = {Digitizing and scanning, \_tablet, knowledge-based modeling},
	pages = {19--es},
}

@inproceedings{huang_3d_2015,
	series = {{ICSDM} 2015 - {Proceedings} 2015 2nd {IEEE} {International} {Conference} on {Spatial} {Data} {Mining} and {Geographical} {Knowledge} {Services}},
	title = {A {3D} individual tree modeling technique based on terrestrial {LiDAR} point cloud data},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956655555&doi=10.1109%2fICSDM.2015.7298043&partnerID=40&md5=4f1cf8dfe82a31d2e77f637071c9b668},
	doi = {10.1109/ICSDM.2015.7298043},
	abstract = {Terrestrial Laser Scanner (TLS) has been used to record three-dimensional (3D) point cloud data of trees, and this data is being further processed to extract morphological parameters of standing trees and reconstruct 3D geometric model. We present an efficient, high-accuracy and photorealistic 3D single tree reconstruction technique based on the point cloud of individual standing tree acquired mainly from TLS. Our method can process point cloud data of the whole tree directly without the need to first segment data into leaves and branches. It also allows user to interactively edit and adjust model parameters to better fie the data. © 2015 IEEE.},
	author = {Huang, H. and Tang, L. and Chen, C.},
	year = {2015},
	keywords = {\_tablet},
	pages = {152--156},
}

@article{guo_realistic_2020,
	title = {Realistic {Procedural} {Plant} {Modeling} from {Multiple} {View} {Images}},
	volume = {26},
	issn = {2160-9306},
	doi = {10.1109/TVCG.2018.2869784},
	abstract = {In this paper, we describe a novel procedural modeling technique for generating realistic plant models from multi-view photographs. The realism is enhanced via visual and spatial information acquired from images. In contrast to previous approaches that heavily rely on user interaction to segment plants or recover branches in images, our method automatically estimates an accurate depth map of each image and extracts a 3D dense point cloud by exploiting an efficient stereophotogrammetry approach. Taking this point cloud as a soft constraint, we fit a parametric plant representation to simulate the plant growth progress. In this way, we are able to synthesize parametric plant models from real data provided by photos and 3D point clouds. We demonstrate the robustness of the proposed approach by modeling various plants with complex branching structures and significant self-occlusions. We also demonstrate that the proposed framework can be used to reconstruct ground-covering plants, such as bushes and shrubs which have been given little attention in the literature. The effectiveness of our approach is validated by visually and quantitatively comparing with the state-of-the-art approaches.},
	number = {2},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Guo, Jianwei and Xu, Shibiao and Yan, Dong-Ming and Cheng, Zhanglin and Jaeger, Marc and Zhang, Xiaopeng},
	month = feb,
	year = {2020},
	keywords = {Computational modeling, Geometry, Image reconstruction, Multi-view images, Shape, Solid modeling, Three-dimensional displays, Vegetation, \_tablet, dense depth map, plant reconstruction, procedural modeling},
	pages = {1372--1384},
}

@article{guenard_realistic_2014,
	title = {Realistic plant modeling from images based on analysis-by-synthesis},
	volume = {8177 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901834496&doi=10.1007%2f978-3-642-54382-1_12&partnerID=40&md5=67a663b8901b2cec5123f2620f3703c5},
	doi = {10.1007/978-3-642-54382-1_12},
	abstract = {Plants are essential elements of virtual worlds to get pleasant and realistic 3D environments. Even if mature computer vision techniques allow the reconstruction of challenging 3D objects from images, due to high complexity of plant topology, dedicated methods for generating 3D plant models must be devised. We propose an analysis-by-synthesis method which generates 3D models of a plant from both images and a priori knowledge of the plant species. Our method is based on a skeletonisation algorithm which allows to generate a possible skeleton from a foliage segmentation. Then, we build a 3D generative model, based on a parametric model of branching systems that takes into account botanical knowledge. This method extends previous works by constraining the resulting skeleton to follow a natural branching structure. A first instance of a 3D model is generated. A reprojection of this model is compared with the original image. Then, we show that selecting the model from multiple proposals for the main branching structure of the plant and for the foliage improves the quality of the generated 3D model. Varying parameter values of the generative model, we produce a series of candidate models. A criterion based on comparing 3D virtual plant reprojection with the original image selects the best model. Finally, results on different species of plants illustrate the performance of the proposed method. © Springer-Verlag 2014.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Guénard, J. and Morin, G. and Boudon, F. and Charvillat, V.},
	year = {2014},
	keywords = {\_tablet},
	pages = {213--229},
}

@article{guenard_reconstructing_2013,
	title = {Reconstructing plants in {3D} from a single image using analysis-by-synthesis},
	volume = {8033 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888247848&doi=10.1007%2f978-3-642-41914-0_32&partnerID=40&md5=030aedb7733d56bc05380b56fbf891a3},
	doi = {10.1007/978-3-642-41914-0_32},
	abstract = {Mature computer vision techniques allow the reconstruction of challenging 3D objects from images. However, due to high complexity of plant topology, dedicated methods for generating 3D plant models must be devised. We propose to generate a 3D model of a plant, using an analysis-by-synthesis method mixing information from a single image and a priori knowledge of the plant species. First, our dedicated skeletonisation algorithm generates a possible branching structure from the foliage segmentation. Then, a 3D generative model, based on a parametric model of branching systems that takes into account botanical knowledge is built. The resulting skeleton follows the hierarchical organisation of natural branching structures. An instance of a 3D model can be generated. Moreover, varying parameter values of the generative model (main branching structure of the plant and foliage), we produce a series of candidate models. The reconstruction is improved by selecting the model among these proposals based on a matching criterion with the image. Realistic results obtained on different species of plants illustrate the performance of the proposed method. © 2013 Springer-Verlag.},
	number = {PART 1},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Guénard, J. and Morin, G. and Boudon, F. and Charvillat, V.},
	year = {2013},
	pages = {322--332},
}

@article{benes_guided_2011,
	title = {Guided procedural modeling},
	volume = {30},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863966844&doi=10.1111%2fj.1467-8659.2011.01886.x&partnerID=40&md5=0d8df39a67571d011aa7abb48d522d31},
	doi = {10.1111/j.1467-8659.2011.01886.x},
	abstract = {Procedural methods present one of the most powerful techniques for authoring a vast variety of computer graphics models. However, their massive applicability is hindered by the lack of control and a low predictability of the results. In the classical procedural modeling pipeline, the user usually defines a set of rules, executes the procedural system, and by examining the results attempts to infer what should be changed in the system definition in order to achieve the desired output. We present guided procedural modeling, a new approach that allows a high level of top-down control by breaking the system into smaller building blocks that communicate. In our work we generalize the concept of the environment. The user creates a set of guides. Each guide defines a region in which a specific procedural model operates. These guides are connected by a set of links that serve for message passing between the procedural models attached to each guide. The entire model consists of a set of guides with procedural models, a graph representing their connection, and the method in which the guides interact. The modeling process is performed by modifying each of the described elements. The user can control the high-level description by editing the guides or manipulate the low-level description by changing the procedural rules. Changing the connectivity allows the user to create new complex forms in an easy and intuitive way. We show several examples of procedural structures, including an ornamental pattern, a street layout, a bridge, and a model of trees. We also demonstrate interactive examples for quick and intuitive editing using physics-based mass-spring system. © 2010 The Author(s).},
	number = {2},
	journal = {Computer Graphics Forum},
	author = {Beneš, B. and Št'ava, O. and Měch, R. and Miller, G.},
	year = {2011},
	keywords = {\_tablet},
	pages = {325--334},
}

@inproceedings{zeng_plants_2017,
	series = {Proceedings - 2017 {International} {Conference} on {Virtual} {Reality} and {Visualization}, {ICVRV} 2017},
	title = {Plants modeling based on limited points},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067043586&doi=10.1109%2fICVRV.2017.00032&partnerID=40&md5=e170af7ce431de2445289a26859b8bbf},
	doi = {10.1109/ICVRV.2017.00032},
	abstract = {Plants show different beauty since the leaves and structures are in a variety of colors, shapes, sizes and texture in our vision. Thus, noise and edge bonding in plant modeling reconstruction are highly challenging. In order to separate the adhesive and gain ideal point cloud, we present a method of limited detail multi-density point reconstruction. The core of proposed method is a point-To-surface approach, which constantly refines the density of points so as to generate the fuzzy surface. Comparing with traditional mesh reconstruction, our method can solve the problem of adhesion between the branches and leaves. Results show that the limited detail multi-density point reconstruction is feasible, and as well as good effect and fast speed. © 2017 IEEE.},
	author = {Zeng, L. and Zhang, L. and Yang, Y. and Yang, B. and Zhan, Y.},
	year = {2017},
	keywords = {\_tablet},
	pages = {123--124},
}

@inproceedings{gong_three-dimensional_2018,
	series = {International {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences} - {ISPRS} {Archives}},
	title = {Three-{Dimensional} reconstruction of the virtual plant branching structure based on terrestrial {LiDAR} technologies and {L}-system},
	volume = {42},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046966732&doi=10.5194%2fisprs-archives-XLII-3-403-2018&partnerID=40&md5=c99f09c77feda832007a77a1871bff52},
	doi = {10.5194/isprs-archives-XLII-3-403-2018},
	abstract = {For the purpose of extracting productions of some specific branching plants effectively and realizing its 3D reconstruction, Terrestrial LiDAR data was used as extraction source of production, and a 3D reconstruction method based on Terrestrial LiDAR technologies combined with the L-system was proposed in this article. The topology structure of the plant architectures was extracted using the point cloud data of the target plant with space level segmentation mechanism. Subsequently, L-system productions were obtained and the structural parameters and production rules of branches, which fit the given plant, was generated. A three-dimensional simulation model of target plant was established combined with computer visualization algorithm finally. The results suggest that the method can effectively extract a given branching plant topology and describes its production, realizing the extraction of topology structure by the computer algorithm for given branching plant and also simplifying the extraction of branching plant productions which would be complex and time-consuming by L-system. It improves the degree of automation in the L-system extraction of productions of specific branching plants, providing a new way for the extraction of branching plant production rules. © Authors 2018.},
	author = {Gong, Y. and Yang, Y. and Yang, X.},
	year = {2018},
	pages = {403--410},
}

@article{long_reconstructing_2013,
	title = {Reconstructing 3d tree models using motion capture and particle flow},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888883351&doi=10.1155%2f2013%2f363160&partnerID=40&md5=731697f6d861c38ca213c452af6c1fef},
	doi = {10.1155/2013/363160},
	abstract = {Recovering tree shape from motion capture data is a first step toward efficient and accurate animation of trees in wind using motion capture data. Existing algorithms for generating models of tree branching structures for image synthesis in computer graphics are not adapted to the unique data set provided by motion capture. We present a method for tree shape reconstruction using particle flow on input data obtained from a passive optical motion capture system. Initial branch tip positions are estimated from averaged and smoothed motion capture data. Branch tips, as particles, are also generated within a bounding space defined by a stack of bounding boxes or a convex hull. The particle flow, starting at branch tips within the bounding volume under forces, creates tree branches. The forces are composed of gravity, internal force, and external force. The resulting shapes are realistic and similar to the original tree crown shape. Several tunable parameters provide control over branch shape and arrangement. © 2013 Jie Long and Michael D. Jones.},
	journal = {International Journal of Computer Games Technology},
	author = {Long, J. and Jones, M.D.},
	year = {2013},
}

@inproceedings{bartolozzi_hybrid_2017,
	series = {{ACM} {SIGGRAPH} 2017 {Talks}, {SIGGRAPH} 2017},
	title = {A hybrid approach to procedural tree skeletonization},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033379593&doi=10.1145%2f3084363.3085065&partnerID=40&md5=71a387a2d49fe87566ce01163531ebdf},
	doi = {10.1145/3084363.3085065},
	abstract = {The curve hierarchy skeleton has been a foundational component of Pixar's vegetation pipeline since Cars 2 (2011). This skeleton is leveraged when building flow fields for texture synthesis, generating procedural secondary vegetation detail, and as a basis for simulation rigs. Our current skeletonization pipeline is built around mesh contraction [Shek et al. 2010] [Au et al. 2008] which is sensitive to the underlying topology. This method creates undesirable curve structures when modelers add musculature to the trunk mesh. Technical directors would reapply manual fixes to the skeleton over the course of iterating on the model. Recent work examining constructing skeletons using point clouds and volumetrics inspired us to develop a new hybrid approach. This alternative to the traditional mesh contraction algorithm has shown to be fast, reliable, accurate, and minimizes constraints on our modeling artists. © Copyright 2017 Authors.},
	author = {Bartolozzi, J. and Kuruc, M.},
	year = {2017},
	keywords = {\_tablet},
}

@inproceedings{xiangyu_3d_2014,
	series = {International {Geoscience} and {Remote} {Sensing} {Symposium} ({IGARSS})},
	title = {{3D} reconstruction of a single tree from terrestrial {LiDAR} data},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911432856&doi=10.1109%2fIGARSS.2014.6946544&partnerID=40&md5=ec44313dc2cb20a0b98449396094ce38},
	doi = {10.1109/IGARSS.2014.6946544},
	abstract = {Terrestrial LiDAR systems have received lots of attention on three-dimensional (3D) structure reconstruction for trees, especially on the branches skeleton generation. On this basis, a method is proposed to add leaves structures based on point density by dividing small cube in the canopy to reduce the influence of uneven distribution of point cloud, combining gap fraction model to retrieve leaf area of a tree using terrestrial LiDAR data. It is successfully applied to reconstruct 3D trees using points data simulated by ray tracing algorithm as well as field measured points data. The relative error of leaf area between reconstructed and real structure is less than 0.9\%. Meanwhile, the most relative error of directional gap fraction is also less than 4.1\%. The experimental results prove that the method has gotten a satisfied consistency on visual sense and quantitative evaluation between the 3D structure reconstructed and real structure. © 2014 IEEE.},
	author = {Xiangyu, W. and Donghui, X. and Guangjian, Y. and Wuming, Z. and Yan, W. and Yiming, C.},
	year = {2014},
	keywords = {\_tablet},
	pages = {796--799},
}

@inproceedings{liu_reconstruction_2011,
	series = {Proceedings - 2011 {IEEE} {International} {Conference} on {Computer} {Science} and {Automation} {Engineering}, {CSAE} 2011},
	title = {The reconstruction of three-dimensional tree models from terrestrial {LiDAR}},
	volume = {4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051862406&doi=10.1109%2fCSAE.2011.5952871&partnerID=40&md5=db9c5c5cbbf7debf501afd3d6a757e85},
	doi = {10.1109/CSAE.2011.5952871},
	abstract = {This paper presents a way to reconstruct three-dimensional tree models based on point clouds from terrestrial LiDAR (TLiDAR). To input data of fixed format into AutoCAD and 3DMAX to reconstruct models through LiDAR scanning software, with an advantage of that, we can use general software to reconstruct tree model from point clouds; the model is accurate and relatively insensitive to occlusion-induced artifacts with less interference, and any part of the volume size could be taken. The result of the method confirms the appropriateness of the proposed tree reconstruction model for the generation of structurally of existing plant, be able to meet the requirements of 3D green biomass and biomass calculations. © 2011 IEEE.},
	author = {Liu, H.-W. and Zhang, X.-L. and Wang, S. and Chen, L.},
	year = {2011},
	pages = {371--374},
}

@article{zhu_reconstruction_2008,
	title = {Reconstruction of tree crown shape from scanned data},
	volume = {5093 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-48249157069&doi=10.1007%2f978-3-540-69736-7_79&partnerID=40&md5=e35769f252afcce0d50cc8435e492b01},
	doi = {10.1007/978-3-540-69736-7_79},
	abstract = {Reconstruction of a real tree from scattered scanned points is a new challenge in virtual reality. Although many progresses are made on main branch structures and overall shape of a tree, reconstructions are still not satisfactory in terms of silhouette and details. We do think that 3D reconstruction of the tree crown shapes may help to constrain accurate reconstruction of complete real tree geometry. We propose here a novel approach for tree crown reconstruction based on an improvement of alpha shape modeling, where the data are points unevenly distributed in a volume rather than on a surface only. The result is an extracted silhouette mesh model, a concave closure of the input data. We suggest an appropriate scope of proper alpha values, so that the reconstruction of the silhouette mesh is a valid manifold surface. Experimental results show that our technique works well in extracting the crown shapes of real trees. © 2008 Springer-Verlag Berlin Heidelberg.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Zhu, C. and Zhang, X. and Hu, B. and Jaeger, M.},
	year = {2008},
	keywords = {\_tablet},
	pages = {745--756},
}

@inproceedings{zeng_3d_2006,
	series = {Proceedings - {ISDA} 2006: {Sixth} {International} {Conference} on {Intelligent} {Systems} {Design} and {Applications}},
	title = {{3D} tree models reconstruction from a single image},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547508658&doi=10.1109%2fISDA.2006.253878&partnerID=40&md5=a4a6adeb8d053e39bc3992e11bf9acc2},
	doi = {10.1109/ISDA.2006.253878},
	abstract = {Due to their geometric complexity, reconstructing 3D tree models is a challenging problem. In this paper, we propose an approach to reconstruct 3D tree models using only a single image of trees without leaves, which is segmented from the background. This reconstruction process requires less user interaction. Firstly, the undirected graph topology of the plant is extracted from the given tree image. This is performed after image thinning, pixels classification, fakebranches deleting, line-segments fitting and width estimation. Secondly, the tree topology of the plant is obtained in terms of the plant growth regularity by determining the direction, the parent, and the first child of each edge in the undirected graph. Finally, the 3D tree models are reconstructed by rotating the branch and creating the 3D geometry of each branch based on the tree topology of the plant. © 2006 IEEE.},
	author = {Zeng, J. and Zhang, Y. and Zhan, S.},
	year = {2006},
	keywords = {\_tablet},
	pages = {445--450},
}

@inproceedings{cai_robust_2011,
	series = {2011 {International} {Conference} on {Multimedia} {Technology}, {ICMT} 2011},
	title = {Robust thinning algorithm without artefacts for pattern recognition and {3D} plant modelling},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052958041&doi=10.1109%2fICMT.2011.6001674&partnerID=40&md5=29221b9b72b8816ee3bc95685cc62e6f},
	doi = {10.1109/ICMT.2011.6001674},
	abstract = {We present a novel thinning algorithm to automatically extract skeletons from images without artefacts. It is well known that the major problem of existing thinning algorithms is the generation of artefacts such redundant branches due to noises in images. In this approach, we propose to use oriented Gaussian filters to classify ridges and edges, and to determine principal directions. As oriented filters are low-pass filters in the principal directions, they are robust to noise and insignificant extremities. The thinning process of the proposed algorithm is guided by principal directions, thus it can remove edge points without the interference from noise and insignificant extremities. As a result, the extracting skeletons of elongated shapes is smooth and without redundant branches. Experimental results show that the proposed approach is able to handle noise and insignificant extremities to generate smooth skeletons of objects, and also is able to automatically extract 3D structures of cereal plants. © 2011 IEEE.},
	author = {Cai, J.},
	year = {2011},
	pages = {702--706},
}

@inproceedings{binney_3d_2009,
	series = {Proceedings - {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	title = {{3D} tree reconstruction from laser range data},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350371283&doi=10.1109%2fROBOT.2009.5152684&partnerID=40&md5=653dbd4d2661441f9d6f9225d69c31c6},
	doi = {10.1109/ROBOT.2009.5152684},
	abstract = {We present a method for reconstructing 3D models of tree branch structure from laser range data. Our approach is probabilistic, and uses general knowledge of tree structure to guide an iterative reconstruction process. Our goal is to recover parameters such as branch locations, angles, radii, and lengths, as well as connectivity information between branches. These parameters can then be fed into functional-structural plant models to study the relationships between the structure of a plant, its environment, and its internal biology. In this paper we present an algorithm for finding these parameters, and results on both simulated and real datasets. © 2009 IEEE.},
	author = {Binney, J. and Sukhatme, G.S.},
	year = {2009},
	keywords = {\_tablet},
	pages = {1321--1326},
}

@inproceedings{yan_efficient_2009,
	series = {Proceedings - 2009 11th {IEEE} {International} {Conference} on {Computer}-{Aided} {Design} and {Computer} {Graphics}, {CAD}/{Graphics} 2009},
	title = {Efficient and robust reconstruction of botanical branching structure from laser scanned points},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449348160&doi=10.1109%2fCADCG.2009.5246837&partnerID=40&md5=97d68e5cf6d276f3e8b86706124ef734},
	doi = {10.1109/CADCG.2009.5246837},
	abstract = {This paper presents a reconstruction pipeline for recovering branching structure of trees from laser scanned data points. The process is made up of two main blocks: segmentation and reconstruction. Based on a variational k-means clustering algorithm, cylindrical components and ramified regions of data points are identified and located. An adjacency graph is then built from neighborhood information of components. Simple heuristics allow us to extract a skeleton structure and identify branches from the graph. Finally, a B-spline model is computed to give a compact and accurate reconstruction of the branching system. © 2009 IEEE.},
	author = {Yan, D.-M. and Wintz, J. and Mourrain, B. and Wang, W. and Boudon, F. and Godin, C.},
	year = {2009},
	keywords = {\_tablet},
	pages = {572--575},
}

@article{huang_generative_2009,
	title = {Generative statistical 3d reconstruction of unfoliaged trees from terrestrial images},
	volume = {15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867843636&doi=10.1080%2f19475680903464621&partnerID=40&md5=2b41512107446144043784fada9518d7},
	doi = {10.1080/19475680903464621},
	abstract = {This article presents a generative statistical approach for the automatic three-dimensional (3D) extraction and reconstruction of unfoliaged deciduous trees from terrestrial wide-baseline image sequences. Unfoliaged trees are difficult to reconstruct from images because of partially weak contrast, background clutter, occlusions, and particularly the possibly varying order of branches in images from different viewpoints. This work combines generative modeling by L-systems and a statistical approach for maximum a posteriori estimation for the reconstruction of the 3D branching structure of trees. Background estimation is conducted by means of gray scale morphology to provide a good basis for generative modeling. A Gaussian likelihood function based on intensity differences is used to evaluate the hypotheses. The target tree is classified into three typical branching types after the extraction of the first level of branches and specific production rules of an L-system are used. Generic prior distributions for parameters are refined based on already extracted branches in a Bayesian framework and are integrated into the maximum a posteriori estimation. By these means most of the branching structure besides the tiny twigs can be reconstructed. The results are presented in the form of virtual reality modeling language models and show the potential of the approach. © 2009 Taylor \& Francis Group, LLC.},
	number = {2},
	journal = {Annals of GIS},
	author = {Huang, H. and Mayer, H.},
	year = {2009},
	pages = {97--105},
}

@inproceedings{zhu_cluster-based_2009,
	series = {Plant {Growth} {Modeling}, {Simulation}, {Visualization} and {Applications}, {Proceedings} - {PMA09}},
	title = {Cluster-based construction of tree crown from scanned data},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954253616&doi=10.1109%2fPMA.2009.19&partnerID=40&md5=59fbcc551089744b9aa8e6845c4a5229},
	doi = {10.1109/PMA.2009.19},
	abstract = {It is a challenge task to reconstruct a real tree from scattered scanned points in virtual reality. Although many progresses have been made on main branch structures and overall tree shape, reconstructions are still not faithful in terms of silhouette and details. We push the idea that 3D reconstruction of the tree crown shapes may help to constrain reconstruction of complete real tree geometry. We propose here a new approach to reconstruct tree crown based on clusters of points, where the data are unevenly distributed points in a volume rather than lying on a surface. From this approach several extracted silhouette mesh models can be generated; every mesh model represents a crown section of the reconstructed tree crown. Experimental result shows that our technique works well on crown shape of real trees. © 2010 IEEE.},
	author = {Zhu, C. and Zhang, X. and Jaeger, M. and Wang, Y.},
	year = {2009},
	keywords = {\_tablet},
	pages = {352--359},
}

@inproceedings{quan_image-based_2006,
	series = {{ACM} {Transactions} on {Graphics}},
	title = {Image-based plant modeling},
	volume = {25},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749257493&doi=10.1145%2f1141911.1141929&partnerID=40&md5=5affc6e30c562901c8cddc8ab7a03825},
	doi = {10.1145/1141911.1141929},
	abstract = {In this paper, we propose a semi-automatic technique for modeling plants directly from images. Our image-based approach has the distinct advantage that the resulting model inherits the realistic shape and complexity of a real plant. We designed our modeling system to be interactive, automating the process of shape recovery while relying on the user to provide simple hints on segmentation. Segmentation is performed in both image and 3D spaces, allowing the user to easily visualize its effect immediately. Using the segmented image and 3D data, the geometry of each leaf is then automatically recovered from the multiple views by fitting a deformable leaf model. Our system also allows the user to easily reconstruct branches in a similar manner. We show realistic reconstructions of a variety of plants, and demonstrate examples of plant editing.Copyright © 2006 by the Association for Computing Machinery, Inc.},
	author = {Quan, L. and Tan, P. and Zeng, G. and Yuan, L. and Wang, J. and Kang, S.B.},
	year = {2006},
	keywords = {\_tablet},
	pages = {599--604},
}

@article{beyer_reconstructing_2017,
	title = {Reconstructing minimal length tree branch systems from leaf positions},
	volume = {42},
	issn = {1574-9541},
	url = {http://www.sciencedirect.com/science/article/pii/S1574954116301832},
	doi = {10.1016/j.ecoinf.2017.09.010},
	abstract = {We present a method to infer a straight-lines tree branch system from a given set of leaf positions and average branching angles. Among an extensive set of possible branch systems constructed in the process, we choose the one featuring the shortest total length, following an optimality hypothesis by Leopold (1971). The approach is illustrated using empirical low-order skeletons from European beech. Our method further allows to assess, for a given species or individual tree, to what extent its branching pattern accords to Leopold's hypothesis, which we argue to be the case for beech. While yet facing issues of computational intensity for too many leaves, the method can furthermore be used to complement existing tree structure reconstruction methods that otherwise require a rudimentary skeleton as manual input.},
	language = {en},
	urldate = {2020-04-30},
	journal = {Ecological Informatics},
	author = {Beyer, Robert and Bayer, Dominik and Pretzsch, Hans and Cournède, Paul-Henry},
	month = nov,
	year = {2017},
	keywords = {3D reconstruction, Automatic tree modelling, L., Teleonomy, Terrestrial laser scanning, \_tablet},
	pages = {61--66},
}

@article{su_skeleton_2011,
	series = {Mathematical and {Computer} {Modeling} in agriculture ({CCTA} 2010)},
	title = {Skeleton extraction for tree models},
	volume = {54},
	issn = {0895-7177},
	url = {http://www.sciencedirect.com/science/article/pii/S0895717710005340},
	doi = {10.1016/j.mcm.2010.11.043},
	abstract = {The curve skeleton extraction for a given laser-scanned tree model plays an important role in many virtual agricultural applications, such as modelling, animation and growth simulation. To extract the curve skeletons of various tree models, which can capture the essential topology structures, a simple and robust algorithm based on point cloud contraction using constrained Laplacian smoothing is proposed. The adaptive sampling and post-processing steps designed for complex tree-like models can effectively reduce the computation time and reconstruct the correct curve skeleton from a contracted point cloud. The experimental results show that the curve skeletons extracted by this algorithm are faithful and smooth, and they can be well utilized in many fields.},
	language = {en},
	number = {3},
	urldate = {2020-04-21},
	journal = {Mathematical and Computer Modelling},
	author = {Su, Zhixun and Zhao, Yuandi and Zhao, Chunjiang and Guo, Xinyu and Li, Zhiyang},
	month = aug,
	year = {2011},
	keywords = {Curve skeleton, Laplacian contraction, Tree models, \_tablet},
	pages = {1115--1120},
}

@inproceedings{sakaguchi_botanical_1998,
	address = {Orlando, Florida, USA},
	series = {{SIGGRAPH} '98},
	title = {Botanical tree structure modeling based on real image set},
	isbn = {978-1-58113-046-1},
	url = {https://doi.org/10.1145/280953.282241},
	doi = {10.1145/280953.282241},
	urldate = {2020-04-27},
	booktitle = {{ACM} {SIGGRAPH} 98 {Conference} abstracts and applications},
	publisher = {Association for Computing Machinery},
	author = {Sakaguchi, Tatsumi},
	month = jul,
	year = {1998},
	keywords = {\_tablet},
	pages = {272},
}

@inproceedings{sakaguchi_modeling_1999,
	address = {London, United Kingdom},
	series = {{VRST} '99},
	title = {Modeling and animation of botanical trees for interactive virtual environments},
	isbn = {978-1-58113-141-3},
	url = {https://doi.org/10.1145/323663.323685},
	doi = {10.1145/323663.323685},
	abstract = {This paper proposes a new modeling and animation method of botanical tree for interactive virtual environment. Some studies of botanical tree modeling have been based on the Growth Model, which can construct a very natural tree structure. However, this model makes it difficult to predict the final form of tree from given parameters; that is, if an objective form of a tree is given and it is to be reconstructed into a three-dimensional model, we have to change the parameters to reflect the structure by a trial-and-error technique. Thus, we propose a new top-down approach in which a tree's form is defined by volume data that is made from a captured real image set, and the branch structure is realized by simple branching rules. The tree model is described as a set of connected branch segments, and leaf models that consist of leaves and twigs that are attached to the branch segments. To animate the botanical trees, dynamics simulation is performed on the branch segments in two phases. In the first phase, each segment is assumed to be a rigid stick with a fixed end on one side, and rotational movements from influence of external forces are calculated in each segment independently. And the forces propagated from the tip of a branch to the root are calculated from the restoration force and thickness of the branch. Finally, the rotational movements of segments are executed in order from the base segment, and the fixed end of each segment is moved to the free end of the segment to be connected so as to maintain the relative angles between the segments. The proposed model is applied to many kind of botanical trees, and the model can successfully animate tree movements caused by external forces such as winds and human interaction to the branches.},
	urldate = {2020-04-27},
	booktitle = {Proceedings of the {ACM} symposium on {Virtual} reality software and technology},
	publisher = {Association for Computing Machinery},
	author = {Sakaguchi, Tatsumi and Ohya, Jun},
	month = dec,
	year = {1999},
	keywords = {\_tablet, botanical tree modeling, image based modeling, natural phenomena, physically based animation (dynamics), virtual reality},
	pages = {139--146},
}

@article{fu_tree_2020,
	title = {Tree {Skeletonization} for {Raw} {Point} {Cloud} {Exploiting} {Cylindrical} {Shape} {Prior}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2971549},
	abstract = {Tree skeleton extraction plays a fundamental role in reconstructing both biological and structural models of trees. However, traditional approaches can be ineffective and problematic in guaranteeing the topological correctness and centeredness of the tree skeleton when the tree point clouds contain noise and occlusions. To overcome this limitation, we present a tree skeletonization method to generate topologically correct and well-centered tree skeletons. We extract an initial skeleton from the tree point clouds via an octree and level set method, use cylindrical prior constraint (CPC) optimization and the estimated radii of branches to yield corrected positions of improper joints, and finally obtain updated skeletons with improved smoothness. The good centeredness of our proposed method is intrinsically achieved by (1) exploiting the cylindrical shape prior and calculating the CPC in the local neighborhood and (2) feeding the prior knowledge regarding the radii of tree branches into a topology refinement algorithm to yield near-optimal estimates of the positions of the skeleton points. To evaluate our method, we construct a novel tree point cloud data set with known ground truth and propose three quantitative assessment methods: skeleton points deviation (SPD), bifurcation points coverage (BPC) and endpoints coverage (EPC). The quantitative assessment and visual assessment show that the proposed method clearly outperforms traditional ones in terms of topology correctness and centeredness of the extracted tree skeleton.},
	language = {English},
	journal = {Ieee Access},
	author = {Fu, Lixian and Liu, Ji and Zhou, Jianling and Zhang, Min and Lin, Yan},
	year = {2020},
	keywords = {Cylindrical prior constraint, \_tablet, point cloud, skeleton extraction},
	pages = {27327--27341},
}

@article{gao_force_2019,
	title = {Force field driven skeleton extraction method for point cloud trees},
	volume = {12},
	issn = {1865-0473},
	doi = {10.1007/s12145-018-0365-3},
	abstract = {A key step in processing natural trees from point cloud data is to reconstruct the trees' skeleton, which plays an important role in forest investigation and monitoring. Although the techniques for general objects skeletonizing based on point clouds have made a large stride, there are few efficient and simple studies on the natural trees, which have complex topologies. In this paper, we propose a novel method to reconstruct tree skeletons based on the point cloud data, named as the force field driven tree skeleton extracting method, which consists of the following steps. Specifically, to make the point cloud tree a little bit cleaner, the hierarchical subdivision to the original point cloud space is firstly proposed. Then the split-level of the trees' applied space is considered in each layers, and a simplified representation of the feature points for the tree model is then established under the neighbor relationships. After that, the feature points in the peripheral are connected by the geodesic distance. To get the initial skeleton, the surface geodesic lines are compressed into the tree model by applying a visible repulsive force field. Finally, the final skeleton is acquired by polishing the initial skeleton according to a threshold setting. The experimental results on two kinds of representative naturally growing trees, which are the Cherry and Michelia, indicate that this method can provide a satisfactory performance.},
	language = {English},
	number = {2},
	journal = {Earth Science Informatics},
	author = {Gao, Linming and Zhang, Dong and Li, Nan and Chen, Lei},
	month = jun,
	year = {2019},
	keywords = {Laser point cloud, Split-level, Trees skeleton, Visible repulsive force field, \_tablet, forests, models},
	pages = {161--171},
}

@article{delagrange_pypetree_2014,
	title = {{PypeTree}: {A} {Tool} for {Reconstructing} {Tree} {Perennial} {Tissues} from {Point} {Clouds}},
	volume = {14},
	issn = {1424-8220},
	shorttitle = {{PypeTree}},
	doi = {10.3390/s140304271},
	abstract = {The reconstruction of trees from point clouds that were acquired with terrestrial LiDAR scanning (TLS) may become a significant breakthrough in the study and modelling of tree development. Here, we develop an efficient method and a tool based on extensive modifications to the skeletal extraction method that was first introduced by Verroust and Lazarus in 2000. PypeTree, a user-friendly and open-source visual modelling environment, incorporates a number of improvements into the original skeletal extraction technique, making it better adapted to tackle the challenge of tree perennial tissue reconstruction. Within PypeTree, we also introduce the idea of using semi-supervised adjustment tools to address methodological challenges that are associated with imperfect point cloud datasets and which further improve reconstruction accuracy. The performance of these automatic and semi-supervised approaches was tested with the help of synthetic models and subsequently validated on real trees. Accuracy of automatic reconstruction greatly varied in terms of axis detection because small (length {\textbackslash}textless 3.5 cm) branches were difficult to detect. However, as small branches account for little in terms of total skeleton length, mean reconstruction error for cumulated skeleton length only reached 5.1\% and 1.8\% with automatic or semi-supervised reconstruction, respectively. In some cases, using the supervised tools, a perfect reconstruction of the perennial tissue could be achieved.},
	language = {English},
	number = {3},
	journal = {Sensors},
	author = {Delagrange, Sylvain and Jauvin, Christian and Rochon, Pascal},
	month = mar,
	year = {2014},
	keywords = {L-System, Terrestrial LiDAR Scanning (TLS), \_tablet, architecture, botanical trees, colonisation algorithm, models, skeleton, tree reconstruction, validation procedure},
	pages = {4271--4289},
}

@article{bucksch_campino_2008,
	series = {Theme {Issue}: {Terrestrial} {Laser} {Scanning}},
	title = {{CAMPINO} — {A} skeletonization method for point cloud processing},
	volume = {63},
	issn = {0924-2716},
	url = {http://www.sciencedirect.com/science/article/pii/S0924271607001281},
	doi = {10.1016/j.isprsjprs.2007.10.004},
	abstract = {A new algorithm for deriving skeletons and segmentations from point cloud data in O(n) time is explained in this publication. This skeleton is represented as a graph, which can be embedded into the point cloud. The CAMPINO method, (C)ollapsing (A)nd (M)erging (P)rocedures (IN) (O)ctree-graphs, is based on cycle elimination in a graph as derived from an octree based space division procedure. The algorithm is able to extract the skeleton from point clouds generated from either one or multiple viewpoints. The correspondence between the vertices of the graph and the original points of the point cloud is used to derive an initial segmentation of these points. The principle of the algorithm is demonstrated on a synthetic point cloud consisting of 3 connected tori. Initially this algorithm was developed to obtain skeletons from point clouds representing natural trees, measured with the terrestrial laser scanner IMAGER 5003 of Zoller+Fröhlich. The results show that CAMPINO is able to automatically derive realistic skeletons that fit the original point cloud well and are suited as a basis for e.g. further automatic feature extraction or skeleton-based registration.},
	language = {en},
	number = {1},
	urldate = {2020-10-30},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Bucksch, Alexander and Lindenbergh, Roderik},
	month = jan,
	year = {2008},
	keywords = {CAMPINO, Point cloud, Skeletonization, Terrestrial laser scanning},
	pages = {115--127},
}

@article{gelard_leaves_2017,
	title = {Leaves segmentation in {3D} point cloud},
	volume = {10617 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036666042&doi=10.1007%2f978-3-319-70353-4_56&partnerID=40&md5=c5c70d2161c0e2cb177a24192e7f34bf},
	doi = {10.1007/978-3-319-70353-4_56},
	abstract = {This paper presents a 3D plant segmentation method with an emphasis on segmentation of the leaves. This method is part of a 3D plant phenotyping project with a main objective that deals with the development of the leaf area over time. First, a 3D point cloud of a plant is obtained with Structure from Motion technique and the cloud is then segmented into the main components of a plant: the stem and the leaves. As the main objective is to measure leaf area over time, an emphasis was placed on accurate segmentation and the labelling of the leaves. This article presents an original approach which starts by finding the stem in a 3D point cloud and then the leaves. Moreover, this method relies on the model of a plant as well as the agronomic rules to affect a unique label that do not change over time. This method is evaluated using two morphologically distinct plants, sunflower and sorghum. © Springer International Publishing AG 2017.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Gélard, W. and Herbulot, A. and Devy, M. and Debaeke, P. and McCormick, R.F. and Truong, S.K. and Mullet, J.},
	year = {2017},
	pages = {664--674},
}

@article{he_research_2018,
	title = {Research on geometric features and point cloud properties for tree skeleton extraction},
	volume = {22},
	issn = {1617-4909},
	doi = {10.1007/s00779-018-1153-2},
	abstract = {To solve skeleton extraction problems in the tree point cloud model, branch geometric features and local properties of point cloud are utilized to optimize tree skeleton extraction. First of all, according to the attribute information estimation and normal vector adjustment of point cloud neighbor domain, branch segmentation is made by estimated values and geometric features. Skeleton nodes are extracted in the branch subset in segmentations. Then, a graph is constructed based on skeleton node set and tree skeleton is reconstructed in this weighted directed graph. Finally, according to the tree growth characteristics, cubic Hermite curves are utilized to optimize the skeleton curve. This method is applied in the point cloud model of three-kind trees and it is compared with the skeleton extraction method based on voxel switch and point cloud contraction. The experiment results show that this method displays strong anti-interference and high-precision characteristics at branch bifurcation and crossed ending parts of fine tree branches. Thus, features of tree branches can be described more perfectly, obtaining the skeleton curve closer to the main axis.},
	language = {English},
	number = {5-6},
	journal = {Personal and Ubiquitous Computing},
	author = {He, Guizhen and Yang, Jun and Behnke, Seven},
	month = oct,
	year = {2018},
	keywords = {Geometric feature, Optimal control, Point cloud property, Point processes, Skeleton extraction, \_tablet},
	pages = {903--910},
}

@article{bradley_image-based_2013,
	title = {Image-based reconstruction and synthesis of dense foliage},
	volume = {32},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2461912.2461952},
	doi = {10.1145/2461912.2461952},
	abstract = {Flora is an element in many computer-generated scenes. But trees, bushes and plants have complex geometry and appearance, and are difficult to model manually. One way to address this is to capture models directly from the real world. Existing techniques have focused on extracting macro structure such as the branching structure of trees, or the structure of broad-leaved plants with a relatively small number of surfaces. This paper presents a finer scale technique to demonstrate for the first time the processing of densely leaved foliage - computation of 3D structure, plus extraction of statistics for leaf shape and the configuration of neighboring leaves. Our method starts with a mesh of a single exemplar leaf of the target foliage. Using a small number of images, point cloud data is obtained from multi-view stereo, and the exemplar leaf mesh is fitted non-rigidly to the point cloud over several iterations. In addition, our method learns a statistical model of leaf shape and appearance during the reconstruction phase, and a model of the transformations between neighboring leaves. This information is useful in two ways - to augment and increase leaf density in reconstructions of captured foliage, and to synthesize new foliage that conforms to a user-specified layout and density. The result of our technique is a dense set of captured leaves with realistic appearance, and a method for leaf synthesis. Our approach excels at reconstructing plants and bushes that are primarily defined by dense leaves and is demonstrated with multiple examples.},
	number = {4},
	urldate = {2020-05-25},
	journal = {ACM Transactions on Graphics},
	author = {Bradley, Derek and Nowrouzezahrai, Derek and Beardsley, Paul},
	month = jul,
	year = {2013},
	keywords = {\_tablet, flora reconstruction, image-based plant modeling, leaf synthesis},
	pages = {74:1--74:10},
}

@inproceedings{su_tree_2019,
	title = {Tree {Skeleton} {Extraction} {From} {Laser} {Scanned} {Points}},
	doi = {10.1109/IGARSS.2019.8900614},
	abstract = {The tree skeleton is one of the important parameters for building 3D model of the tree. The accurate extraction of tree skeleton is of great significance for tree visualization. This paper proposed an effective method for extracting tree skeleton of individual trees. The complexity of canopy structure was reduced by slicing. The leaf and wood components were separated by combining classification and segmentation methods. L1-median algorithm was used to extract the tree skeleton points accurately. The results show that the method can accurately extract the tree skeleton from terrestrial LiDAR data. The extracted skeleton had a good conformance with the point cloud of the tree in morphology.},
	booktitle = {{IGARSS} 2019 - 2019 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Su, Zhonghua and Li, Shihua and Liu, Hanhu and He, Ze},
	month = jul,
	year = {2019},
	keywords = {Classification algorithms, Data mining, L1-median, Laser radar, LiDAR, Skeleton, Solid modeling, Three-dimensional displays, Vegetation, \_tablet, data visualisation, feature extraction, geophysical techniques, image segmentation, individual trees, laser scanned points, optical radar, solid modelling, terrestrial LiDAR data, tree skeleton, tree skeleton extraction, tree skeleton points, tree visualization, vegetation mapping},
	pages = {6091--6094},
}

@article{guo_inverse_2020,
	title = {Inverse {Procedural} {Modeling} of {Branching} {Structures} by {Inferring} {L}-{Systems}},
	volume = {39},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3394105},
	doi = {10.1145/3394105},
	abstract = {We introduce an inverse procedural modeling approach that learns L-system representations of pixel images with branching structures. Our fully automatic model generates a compact set of textual rewriting rules that describe the input. We use deep learning to discover atomic structures such as line segments or branchings. Orientation and scaling of these structures are determined and the detected structures are combined into a tree. The initial representation is analyzed, and repeating parts are encoded into a small grammar by using greedy optimization while the user can control the size of the detected rules. The output is an L-system that represents the input image as a simple text and a set of terminal symbols. We apply our approach to a variety of examples, demonstrate its robustness against noise and blur, and we show that it can detect user sketches and complex input structures.},
	number = {5},
	urldate = {2020-11-04},
	journal = {ACM Transactions on Graphics},
	author = {Guo, Jianwei and Jiang, Haiyong and Benes, Bedrich and Deussen, Oliver and Zhang, Xiaopeng and Lischinski, Dani and Huang, Hui},
	month = jun,
	year = {2020},
	keywords = {L-systems, grammar induction, procedural generation},
	pages = {155:1--155:13},
}

@inproceedings{gu_3d_2015,
	series = {Proceedings of {SPIE} - {The} {International} {Society} for {Optical} {Engineering}},
	title = {{3D} reconstruction and visualization of plant leaves},
	volume = {9443},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925436551&doi=10.1117%2f12.2179142&partnerID=40&md5=a82a995186f71594cae1ac081f834676},
	doi = {10.1117/12.2179142},
	abstract = {In this paper, a three-dimensional reconstruction method, which is based on point clouds and texture images, is used to realize the visualization of leaves of greenhouse crops. We take Epipremnum aureum as the object for study and focus on applying the triangular meshing method to organize and categorize scattered point cloud input data of leaves, and then construct a triangulated surface with interconnection topology to simulate the real surface of the object. At last we texture-map the leaf surface with real images to present a life-like 3D model which can be used to simulate the growth of greenhouse plants. © 2015 SPIE.},
	author = {Gu, X. and Xu, L. and Li, D. and Zhang, P.},
	year = {2015},
}

@inproceedings{fournier_stochastic_1986,
	series = {Proceedings - {Graphics} {Interface}},
	title = {{STOCHASTIC} {MODELLING} {OF} {TREES}.},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022874202&partnerID=40&md5=58f3ed17874d98df25c9471a0d5b021a},
	abstract = {The authors present here a fast method for the modelling of trees which brings together two interesting techniques. The trees are modelled as convex polyhedra for the description of the gross, shape, and three-dimensional texture mapping is used for the detailed features. The 'essential' volume of the tree is represented as the convex intersection of half spaces. The advantage of this representation is that it allows an adaptive level of detail in the display. We use a special algorithm for the display of the convex intersection which computes it directly in the frame buffer. The algorithm also allows the computation of intersecting polyhedra. To transform the convex polyhedra into a more realistic representation of trees, we use three-dimensional texture mapping to 'modulate' the shape and the colour of the basic polyhedra. We then obtain an irregular non convex object, which is consistent in shape and general appearance regardless of the point of view and the size on screen. Three dimensional fractional Brownian motion is one of the procedural texture used.},
	author = {Fournier, Alain and Grindal, David A.},
	year = {1986},
	pages = {164--172},
}

@inproceedings{de_reffye_plant_1988,
	series = {Proceedings of the 15th {Annual} {Conference} on {Computer} {Graphics} and {Interactive} {Techniques}, {SIGGRAPH} 1988},
	title = {Plant models faithful to botanical structure and development},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946723317&partnerID=40&md5=c724b2a9db172d1962e567f8f1972879},
	abstract = {Some very impressive results have been obtained in the past few years in plants and trees image synthesis. Some algorithms are largely based on the irregularity and fuzziness of the objects, and use fractals, graftals or particle systems. Others focus on the branching pattern of the trees with emphasis on morphology. Our concern here is the faithfulness of the models to the botanical nature of trees and plants. We present a model which integrates botanical knowledge of the architecture of the trees: how they grow, how they occupy space, where and how leaves, flowers or fruits are located, etc. The very first interest of the model we propose is its great richness: the same procedural methods can produce "plants" as different as weeping willows, fir trees, cedar trees, frangipani trees, poplars, pine trees, wild cherry trees, herbs, etc. Another very important benefit one can deriw from the model is the integration of time which enables viewing the aging of a tree (possibility to get different pictures of the same tree at different ages, accurate simulation of the death of leaves and branches for example). The ease to integrate physical parameters such as wind, the incidence of factors such as insects attacks, use of fertilizers, plantation density, and so on makes it a useful tool for agronomy or botany. © 1988 ACM.},
	author = {De Reffye, P. and Edelin, C. and Françon, J. and Jaeger, M. and Puech, C.},
	year = {1988},
	pages = {151--158},
}

@article{waite_modelling_1988,
	title = {Modelling natural branching structures},
	volume = {7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024030476&doi=10.1111%2fj.1467-8659.1988.tb00596.x&partnerID=40&md5=ba0dcbad205471858116f438e95bb384},
	doi = {10.1111/j.1467-8659.1988.tb00596.x},
	abstract = {This paper describes the work done in creating a computer program capable of modelling rooted trees and in particular the branching structures of botanical and bronchial trees. Simplicity of design and flexibility of use was achieved by generalising the program to handle any rooted branching structure with the particular form being dictated by various controlling parameters which can be modified via a direct manipulation interface. The program provides a graphical display of the resultant tree and leaf distribution. The open skeletal design allows the user to tailor the program to a particular model and to easily access data derived by the model. Copyright © 1988, Wiley Blackwell. All rights reserved},
	number = {2},
	journal = {Computer Graphics Forum},
	author = {Waite, K.W.},
	year = {1988},
	pages = {105--115},
}

@article{stava_inverse_2010,
	title = {Inverse {Procedural} {Modeling} by {Automatic} {Generation} of {L}-systems},
	volume = {29},
	copyright = {© 2010 The Author(s) Journal compilation © 2010 The Eurographics Association and Blackwell Publishing Ltd.},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2009.01636.x},
	doi = {https://doi.org/10.1111/j.1467-8659.2009.01636.x},
	abstract = {We present an important step towards the solution of the problem of inverse procedural modeling by generating parametric context-free L-systems that represent an input 2D model. The L-system rules efficiently code the regular structures and the parameters represent the properties of the structure transformations. The algorithm takes as input a 2D vector image that is composed of atomic elements, such as curves and poly-lines. Similar elements are recognized and assigned terminal symbols of an L-system alphabet. The terminal symbols' position and orientation are pair-wise compared and the transformations are stored as points in multiple 4D transformation spaces. By careful analysis of the clusters in the transformation spaces, we detect sequences of elements and code them as L-system rules. The coded elements are then removed from the clusters, the clusters are updated, and then the analysis attempts to code groups of elements in (hierarchies) the same way. The analysis ends with a single group of elements that is coded as an L-system axiom. We recognize and code branching sequences of linearly translated, scaled, and rotated elements and their hierarchies. The L-system not only represents the input image, but it can also be used for various editing operations. By changing the L-system parameters, the image can be randomized, symmetrized, and groups of elements and regular structures can be edited. By changing the terminal and non-terminal symbols, elements or groups of elements can be replaced.},
	language = {en},
	number = {2},
	urldate = {2020-11-13},
	journal = {Computer Graphics Forum},
	author = {Št'ava, O. and Beneš, B. and Měch, R. and Aliaga, D. G. and Krištof, P.},
	year = {2010},
	keywords = {Computer Graphics I.3.5: Computational Geometry and Object Modeling— Geometric algorithms, Mathematical Logic and Formal Languages F.4.2: Grammars and Other Rewriting Systems—Parallel rewriting systems - L-systems, and systems, languages},
	pages = {665--674},
}

@article{congdon_gentree_2003,
	title = {{GenTree}: {An} interactive genetic algorithms system for designing {3D} polygonal tree models},
	volume = {2724},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35248876656&partnerID=40&md5=ca8180a2e07fc71fb1bbe8fe14dfa73a},
	abstract = {The creation of individual 3D models to include within a virtual world can be a time-consuming process. The standard approach to streamline this is to use procedural modeling tools, where the user adjusts a set of parameters that defines the tree. We have designed Gen-Tree, an interactive system that uses a genetic algorithms (GA) approach to evolve procedural 3D tree models. GenTree is a hybrid system, combining the standard parameter adjustment and an interactive GA. The parameters may be changed by the user directly or via the GA process, which combines parameters from pairs of parent trees into those that describe novel trees. The GA component enables the system to be used by someone who is ignorant of the parameters that define the trees. Furthermore, combining the standard interactive design process with GA design decreases the time and patience required to design realistic 3D polygonal trees by either method alone. © Springer-Verlag Berlin Heidelberg 2003.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Congdon, C.B. and Mazza, R.H.},
	year = {2003},
	pages = {2034--2045},
}

@article{livny_texture-lobes_2011,
	title = {Texture-lobes for tree modelling},
	volume = {30},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/2010324.1964948},
	doi = {10.1145/2010324.1964948},
	abstract = {We present a lobe-based tree representation for modeling trees. The new representation is based on the observation that the tree's foliage details can be abstracted into canonical geometry structures, termed lobe-textures. We introduce techniques to (i) approximate the geometry of given tree data and encode it into a lobe-based representation, (ii) decode the representation and synthesize a fully detailed tree model that visually resembles the input. The encoded tree serves as a light intermediate representation, which facilitates efficient storage and transmission of massive amounts of trees, e.g., from a server to clients for interactive applications in urban environments. The method is evaluated by both reconstructing laser scanned trees (given as point sets) as well as re-representing existing tree models (given as polygons).},
	number = {4},
	urldate = {2020-11-04},
	journal = {ACM Transactions on Graphics},
	author = {Livny, Yotam and Pirk, Soeren and Cheng, Zhanglin and Yan, Feilong and Deussen, Oliver and Cohen-Or, Daniel and Chen, Baoquan},
	month = jul,
	year = {2011},
	keywords = {natural phenomena, plants synthesis and reconstruction, point-based modeling, rule-based tree modeling},
	pages = {53:1--53:10},
}

@inproceedings{onishi_interactive_2003,
	series = {Proceedings of the {ACM} {Symposium} on {Virtual} {Reality} {Software} and {Technology}, {VRST}},
	title = {Interactive modeling of trees by using growth simulation},
	volume = {Part F129026},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010415901&doi=10.1145%2f1008653.1008667&partnerID=40&md5=4c4ae99d7e6d4ad15ce70c443313568e},
	doi = {10.1145/1008653.1008667},
	abstract = {We propose a real-time interactive system that enables users to generate, manipulate and edit the shape model of a tree based on growth simulation by directly indicating its global and spatial information. For this purpose, three-dimensional (3D) spatial information is introduced to the well-known Lsystem as an attribute of the growth simulation. Moreover, we propose an efficient data structure of L-strings in order to speed up the process. Copyright © 2003 ACM.},
	author = {Onishi, K. and Hasuike, S. and Kitamura, Y. and Kishino, F.},
	year = {2003},
	pages = {66--72},
}

@inproceedings{galbraith_blobtree_2004,
	series = {Proceedings of {Computer} {Graphics} {International} {Conference}, {CGI}},
	title = {Blobtree trees},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-5444269850&doi=10.1109%2fCGI.2004.1309195&partnerID=40&md5=9e9f11d64baf9a5f0ccedd6ca1c0f512},
	doi = {10.1109/CGI.2004.1309195},
	abstract = {In recent years several methods for modeling botanical trees have been proposed. The geometry and topology of tree skeletons can be well described by L-systems; however, there are several approaches to modeling smooth surfaces to represent branches, and not all of the observed phenomena can be represented by current methods. Many tree types exhibit non-smooth features such as branch bark ridges and collars. In this research a hierarchical implicit modeling system is used to produce models of branching structures that capture smooth branching, branch collars and branch bark ridges. The BlobTree provides several techniques to control the combination of primitives, allowing both smooth and non-smooth effects to be intuitively combined in a single blend volume. Irregular effects are implemented using Precise Contact Modeling, Constructive Solid Geometry and space warping. We show that smooth blends can be obtained, without noticeable bulging, using summation of distance based implicit surfaces. L-systems are used to create the branching structure allowing botanically based simulations to be used as input.},
	author = {Galbraith, C. and MacMurchy, P. and Wyvill, B.},
	year = {2004},
	pages = {78--85},
}

@article{padua_vineyard_2022,
	title = {Vineyard classification using {OBIA} on {UAV}-based {RGB} and multispectral data: {A} case study in different wine regions},
	volume = {196},
	issn = {01681699},
	shorttitle = {Vineyard classification using {OBIA} on {UAV}-based {RGB} and multispectral data},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169922002228},
	doi = {10.1016/j.compag.2022.106905},
	language = {en},
	urldate = {2022-05-03},
	journal = {Computers and Electronics in Agriculture},
	author = {Pádua, Luís and Matese, Alessando and Di Gennaro, Salvatore Filippo and Morais, Raul and Peres, Emanuel and Sousa, Joaquim J.},
	month = may,
	year = {2022},
	keywords = {Aerial imagery, Artificial neural network, Object-based image analysis, Precision viticulture, Random forest, Support vector machine},
	pages = {106905},
}

@article{akhoundi_khezrabad_new_2022,
	title = {A new approach for geometric correction of {UAV}-based pushbroom images through the processing of simultaneously acquired frame images},
	volume = {199},
	issn = {0263-2241},
	url = {https://www.sciencedirect.com/science/article/pii/S0263224122006613},
	doi = {10.1016/j.measurement.2022.111431},
	abstract = {Video-assisted navigation is a strategy that hyperspectral camera manufacturers usually use to deal with the geometric distortions imposed on raw pushbroom images, due to perturbations. In this strategy, a simultaneous video is captured during a pushbroom raw image acquisition. This article proposes an automatic geometric correction method applicable to the sensors utilizing this strategy that does not need ground control data. This method starts with an inter-calibration procedure between the two sensors, which allows using frame-based images in the correction procedure. Then, the method uses sequential transformations among consecutive video frames to produce geometrically corrected products. The experimental results demonstrate that the proposed method's sequential nature makes it so flexible that it causes more reduction in the scenes' random geometric distortions than the common methods. Hence, the accuracy of 2D and 3D transformations relating the image and the ground spaces is increased by 66.9\% compared to the raw pushbroom images.},
	language = {en},
	urldate = {2022-10-17},
	journal = {Measurement},
	author = {Akhoundi Khezrabad, Mojtaba and Valadan Zoej, Mohammad Javad and Safdarinezhad, Alireza},
	month = aug,
	year = {2022},
	keywords = {Camera calibration, Geometric correction, Hyperspectral images, Pushbroom images, Video-assisted navigation},
	pages = {111431},
}

@article{singh_bibliometric_2022,
	title = {A {Bibliometric} {Review} of the {Use} of {Unmanned} {Aerial} {Vehicles} in {Precision} {Agriculture} and {Precision} {Viticulture} for {Sensing} {Applications}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/7/1604},
	doi = {10.3390/rs14071604},
	abstract = {This review focuses on the use of unmanned aerial vehicles (UAVs) in precision agriculture, and specifically, in precision viticulture (PV), and is intended to present a bibliometric analysis of their developments in the field. To this aim, a bibliometric analysis of research papers published in the last 15 years is presented based on the Scopus database. The analysis shows that the researchers from the United States, China, Italy and Spain lead the precision agriculture through UAV applications. In terms of employing UAVs in PV, researchers from Italy are fast extending their work followed by Spain and finally the United States. Additionally, the paper provides a comprehensive study on popular journals for academicians to submit their work, accessible funding organizations, popular nations, institutions, and authors conducting research on utilizing UAVs for precision agriculture. Finally, this study emphasizes the necessity of using UAVs in PV as well as future possibilities.},
	language = {en},
	number = {7},
	urldate = {2022-08-15},
	journal = {Remote Sensing},
	author = {Singh, Abhaya Pal and Yerudkar, Amol and Mariani, Valerio and Iannelli, Luigi and Glielmo, Luigi},
	month = jan,
	year = {2022},
	keywords = {drones, precision agriculture, precision viticulture, remote sensing, unmanned aerial vehicles (UAVs)},
	pages = {1604},
}

@book{nguyen_gpu_2007,
	edition = {First},
	title = {{GPU} {Gems} 3},
	isbn = {978-0-321-54542-8},
	abstract = {“The GPU Gems series features a collection of the most essential algorithms required by Next-Generation 3D Engines.” -Martin Mittring, Lead Graphics Programmer, Crytek This third volume of the best-selling GPU Gems series provides a snapshot of today's latest Graphics Processing Unit (GPU) programming techniques. The programmability of modern GPUs allows developers to not only distinguish themselves from one another but also to use this awesome processing power for non-graphics applications, such as physics simulation, financial analysis, and even virus detection-particularly with the CUDA architecture. Graphics remains the leading application for GPUs, and readers will find that the latest algorithms create ultra-realistic characters, better lighting, and post-rendering compositing effects.Major topics include Geometry Light and Shadows Rendering Image Effects Physics Simulation GPU Computing Contributors are from the following corporations and universities:3Dfacto Adobe Systems Apple Budapest University of Technology and Economics CGGVeritas The Chinese University of Hong Kong Cornell University Crytek Czech Technical University in Prague Dartmouth College Digital Illusions Creative Entertainment Eindhoven University of Technology Electronic Arts Havok Helsinki University of Technology Imperial College London Infinity Ward Juniper Networks LaBRIï INRIA, University of Bordeaux mental images Microsoft Research Move Interactive NCsoft Corporation NVIDIA Corporation Perpetual Entertainment Playlogic Game Factory Polytime Rainbow Studios SEGA Corporation UFRGS (Brazil) Ulm University University of California, Davis University of Central Florida University of Copenhagen University of Girona University of Illinois at Urbana-Champaign University of North Carolina Chapel Hill University of Tokyo University of WaterlooSection Editors include NVIDIA engineers: Cyril Zeller, Evan Hart, Ignacio Castaño, Kevin Bjorke, Kevin Myers, and Nolan Goodnight.The accompanying DVD includes complementary examples and sample programs.},
	publisher = {Addison-Wesley Professional},
	author = {Nguyen, Hubert},
	year = {2007},
}

@misc{pix4d_how_nodate,
	title = {How are the {Internal} and {External} {Camera} {Parameters} defined?},
	url = {https://support.pix4d.com/hc/en-us/articles/202559089-How-are-the-Internal-and-External-Camera-Parameters-defined-},
	abstract = {1. Camera external parameters 2. From 3D to 2D: Camera internal parameters 2.1 Perspective lens 2.1.1 Camera without distortion model2.1.2 Camera with distortion model 2.2 Fisheye lens 3. Camera Ri...},
	language = {en-US},
	urldate = {2022-11-07},
	author = {{Pix4D}},
	note = {Publication Title: Support},
}

@inproceedings{kerbl_effective_2017,
	address = {New York, NY, USA},
	series = {{HPG} '17},
	title = {Effective static bin patterns for sort-middle rendering},
	isbn = {978-1-4503-5101-0},
	url = {https://doi.org/10.1145/3105762.3105777},
	doi = {10.1145/3105762.3105777},
	abstract = {To effectively utilize an ever increasing number of processors during parallel rendering, hardware and software designers rely on sophisticated load balancing strategies. While dynamic load balancing is a powerful solution, it requires complex work distribution and synchronization mechanisms. Graphics hardware manufacturers have opted to employ static load balancing strategies instead. Specifically, triangle data is distributed to processors based on its overlap with screenspace tiles arranged in a fixed pattern. While the current strategy of using simple patterns for a small number of fast rasterizers achieves formidable performance, it is questionable how this approach will scale as the number of processors increases further. To address this issue, we analyze real-world rendering workloads, derive requirements for effective patterns, and present ten different pattern design strategies based on these requirements. In addition to a theoretical evaluation of these design strategies, we compare the performance of select patterns in a parallel sort-middle software rendering pipeline on an extensive set of triangle data captured from eight recent video games. As a result, we are able to identify a set of patterns that scale well and exhibit significantly improved performance over naïve approaches.},
	urldate = {2022-10-30},
	booktitle = {Proceedings of {High} {Performance} {Graphics}},
	publisher = {Association for Computing Machinery},
	author = {Kerbl, Bernhard and Kenzel, Michael and Schmalstieg, Dieter and Steinberger, Markus},
	month = jul,
	year = {2017},
	keywords = {GPU, parallel rendering, pattern, sort-middle, static load balancing},
	pages = {1--10},
}

@article{piao_new_2019,
	title = {A {New} {Deep} {Learning} {Based} {Multi}-{Spectral} {Image} {Fusion} {Method}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/21/6/570},
	doi = {10.3390/e21060570},
	abstract = {In this paper, we present a new effective infrared (IR) and visible (VIS) image fusion method by using a deep neural network. In our method, a Siamese convolutional neural network (CNN) is applied to automatically generate a weight map which represents the saliency of each pixel for a pair of source images. A CNN plays a role in automatic encoding an image into a feature domain for classification. By applying the proposed method, the key problems in image fusion, which are the activity level measurement and fusion rule design, can be figured out in one shot. The fusion is carried out through the multi-scale image decomposition based on wavelet transform, and the reconstruction result is more perceptual to a human visual system. In addition, the visual qualitative effectiveness of the proposed fusion method is evaluated by comparing pedestrian detection results with other methods, by using the YOLOv3 object detector using a public benchmark dataset. The experimental results show that our proposed method showed competitive results in terms of both quantitative assessment and visual quality.},
	language = {en},
	number = {6},
	urldate = {2022-09-05},
	journal = {Entropy},
	author = {Piao, Jingchun and Chen, Yunfan and Shin, Hyunchul},
	month = jun,
	year = {2019},
	keywords = {Siamese network, convolutional neural network, image fusion, infrared, visible},
	pages = {570},
}

@article{pan_visibility-based_2021,
	title = {A visibility-based surface reconstruction method on the {GPU}},
	volume = {84},
	issn = {0167-8396},
	url = {https://www.sciencedirect.com/science/article/pii/S0167839621000029},
	doi = {10.1016/j.cagd.2021.101956},
	abstract = {In this paper, we present a visibility-based surface reconstruction method on the GPU. The visibility-based method has proven to be robust to noise and outliers while preserving the geometric features of the point cloud. However, this method is computationally intensive because it involves Delaunay triangulation, ray intersection testing, and graph cuts. Therefore, we accelerate it by exploiting the computing power of the GPU. Due to the irregularities of the Delaunay tetrahedron, it is difficult to index the geometries on the GPU. To solve this problem, we design a three-level index structure that maps the geometric data to the GPU threads with very low memory overhead. Then we implement an efficient and lightweight graph cuts algorithm, which is dedicated to the Delaunay data structure and uses the length of the segments to achieve fast convergence. Besides, we propose several optimization strategies to further improve the performance of the algorithm without compromising the quality of the reconstructed meshes. The experiments show that the speedup of our GPU-based method over the 32-threaded CPU version is between 25 and 50 depending on model geometry.},
	language = {en},
	urldate = {2022-09-01},
	journal = {Computer Aided Geometric Design},
	author = {Pan, Hailong and Guan, Tao and Luo, Keyang and Luo, Yawei and Yu, Junqing},
	month = jan,
	year = {2021},
	keywords = {CUDA, GPU, Graph cuts, Surface reconstruction},
	pages = {101956},
}

@article{poux_automatic_2022,
	title = {Automatic region-growing system for the segmentation of large point clouds},
	volume = {138},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580522001236},
	doi = {10.1016/j.autcon.2022.104250},
	abstract = {This article describes a complete unsupervised system for the segmentation of massive 3D point clouds. Our system bridges the missing components that permit to go from 99\% automation to 100\% automation for the construction industry. It scales up to billions of 3D points and targets a generic low-level grouping of planar regions usable by a wide range of applications. Furthermore, we introduce a hierarchical multi-level segment definition to cope with potential variations in high-level object definitions. The approach first leverages planar predominance in scenes through a normal-based region growing. Then, for usability and simplicity, we designed an automatic heuristic to determine without user supervision three RANSAC-inspired parameters. These are the distance threshold for the region growing, the threshold for the minimum number of points needed to form a valid planar region, and the decision criterion for adding points to a region. Our experiments are conducted on 3D scans of complex buildings to test the robustness of the “one-click” method in varying scenarios. Labelled and instantiated point clouds from different sensors and platforms (depth sensor, terrestrial laser scanner, hand-held laser scanner, mobile mapping system), in different environments (indoor, outdoor, buildings) and with different objects of interests (AEC-related, BIM-related, navigation-related) are provided as a new extensive test-bench. The current implementation processes ten million points per minutes on a single thread CPU configuration. Moreover, the resulting segments are tested for the high-level task of semantic segmentation over 14 classes, to achieve an F1-score of 90+ averaged over all datasets while reducing the training phase to a fraction of state of the art point-based deep learning methods. We provide this baseline along with six new open-access datasets with 300+ million hand-labelled and instantiated 3D points at: https://www.graphics.rwth-aachen.de/project/45/.},
	language = {en},
	urldate = {2022-09-01},
	journal = {Automation in Construction},
	author = {Poux, F. and Mattes, C. and Selman, Z. and Kobbelt, L.},
	month = jun,
	year = {2022},
	keywords = {3D point cloud, RANSAC, Region-growing, Segmentation, Unsupervised clustering},
	pages = {104250},
}

@inproceedings{wiemann_surface_2018,
	title = {Surface {Reconstruction} from {Arbitrarily} {Large} {Point} {Clouds}},
	doi = {10.1109/IRC.2018.00059},
	abstract = {Generating 3D robotic maps from point cloud data is an active field of research. To handle high resolution data from terrestrial laser scanning to generate maps for mobile robots is still challenging, especially for city scale environments. In this short paper, we present the results of an approach for surface reconstruction from arbitrarily large point clouds. To achieve this, we serialize the large input data into suitable chunks, that are serialized to a shared hard drive. After computation, the partial results are fused into a globally consistent reconstruction.},
	booktitle = {2018 {Second} {IEEE} {International} {Conference} on {Robotic} {Computing} ({IRC})},
	author = {Wiemann, Thomas and Mitschke, Isaak and Mock, Alexander and Hertzberg, Joachim},
	month = jan,
	year = {2018},
	keywords = {Graphics processing units, Image reconstruction, Map Generation, Point Cloud Processing, Random access memory, Robots, Surface Reconstruction, Surface reconstruction, Three-dimensional displays, Urban areas},
	pages = {278--281},
}

@article{mohamad_screening_2021,
	title = {A screening approach for the correction of distortion in {UAV} data for coral community mapping},
	volume = {0},
	issn = {1010-6049},
	url = {https://doi.org/10.1080/10106049.2021.1958066},
	doi = {10.1080/10106049.2021.1958066},
	abstract = {Unmanned Aerial Vehicle (UAV) may provide us super resolution data, however, they are often captured with artefacts and distortion. To investigate impact of distortion on the coral reef information, UAV surveys were deployed using multispectral sensor over two reefs in Bidong Island (Malaysia). Band-specific analysis of distortion revealed five different types of distorted images from the acquisition. This study optimized screening distorted images by comparing the seven distortion correction approaches and validated coral classification maps based on machine learning algorithms [support vector machine (SVM), random forest (RF) and artificial neural network (ANN)]. Results indicate that the screening the green band (b2) alone or the blue band (b1) combined with b2 of UAV data and SVM capable of generating the best coral classification maps, with an overall accuracy of 7–17\% improved compared to that of orthomosaic without distortion correction. The proposed distortion correction method can be applied to similar coral environments.HighlightsFive different types of artefacts and distortions found in UAV data.A optimized screening approach suggested to minimized image distortion.Distortion corrected data and SVM algorithm performed the best in coral habitat mapping.An overall accuracy of 7–17\% improved compared to that of distortion uncorrected maps.A new photogrammetric contribution to the existing automatic orthorectification techniques.},
	number = {0},
	urldate = {2022-09-01},
	journal = {Geocarto International},
	author = {Mohamad, Mohd Nasir and Reba, Mohd Nadzri Md and Hossain, Mohammad Shawkat},
	month = jul,
	year = {2021},
	keywords = {Image artefact, MicaSense RedEdge, South China Sea, drone, processing},
	pages = {1--33},
}

@article{huang_fast_2020,
	title = {Fast {Reconstruction} of {3D} {Point} {Cloud} {Model} {Using} {Visual} {SLAM} on {Embedded} {UAV} {Development} {Platform}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/20/3308},
	doi = {10.3390/rs12203308},
	abstract = {In recent years, the rapid development of unmanned aerial vehicle (UAV) technologies has made data acquisition increasingly convenient, and three-dimensional (3D) reconstruction has emerged as a popular subject of research in this context. These 3D models have many advantages, such as the ability to represent realistic scenes and a large amount of information. However, traditional 3D reconstruction methods are expensive, and require long and complex processing. As a result, they cannot rapidly respond when used in time-sensitive applications, e.g., those for such natural disasters as earthquakes, debris flow, etc. Computer vision-based simultaneous localization and mapping (SLAM) along with hardware development based on embedded systems, can provide a solution to this problem. Based on an analysis of the principle and implementation of the visual SLAM algorithm, this study proposes a fast method to quickly reconstruct a dense 3D point cloud model on a UAV platform combined with an embedded graphics processing unit (GPU). The main contributions are as follows: (1) to resolve the contradiction between the resource limitations and the computational complexity of visual SLAM on UAV platforms, the technologies needed to compute resource allocation, communication between nodes, and data transmission and visualization in an embedded environment were investigated to achieve real-time data acquisition and processing. Visual monitoring to this end is also designed and implemented. (2) To solve the problem of time-consuming algorithmic processing, a corresponding parallel algorithm was designed and implemented based on the parallel programming framework of the compute unified device architecture (CUDA). (3) The visual odometer and methods of 3D “map” reconstruction were designed using under a monocular vision sensor to implement the prototype of the fast 3D reconstruction system. Based on preliminary results of the 3D modeling, the following was noted: (1) the proposed method was feasible. By combining UAV, SLAM, and parallel computing, a simple and efficient 3D reconstruction model of an unknown area was obtained for specific applications. (2) The parallel SLAM algorithm used in this method improved the efficiency of the SLAM algorithm. On the one hand, the SLAM algorithm required 1/6 of the time taken by the structure-from-motion algorithm. On the other hand, the speedup obtained using the parallel SLAM algorithm based on the embedded GPU on our test platform was 7.55 × that of the serial algorithm. (3) The depth map results show that the effective pixel with an error less than 15cm is close to 60\%.},
	language = {en},
	number = {20},
	urldate = {2022-09-01},
	journal = {Remote Sensing},
	author = {Huang, Fang and Yang, Hao and Tan, Xicheng and Peng, Shuying and Tao, Jian and Peng, Siyuan},
	month = jan,
	year = {2020},
	keywords = {CUDA, GPU, ROS, computer vision, embedded system developing, fast 3D reconstruction, parallel computing, unmanned aerial vehicle},
	pages = {3308},
}

@article{alvarez-vanhard_uav_2021,
	title = {{UAV} \& satellite synergies for optical remote sensing applications: {A} literature review},
	volume = {3},
	issn = {2666-0172},
	shorttitle = {{UAV} \& satellite synergies for optical remote sensing applications},
	url = {https://www.sciencedirect.com/science/article/pii/S2666017221000067},
	doi = {10.1016/j.srs.2021.100019},
	abstract = {Unmanned aerial vehicles (UAVs) and satellite constellations are both essential Earth Observation (EO) systems for monitoring land surface dynamics. The former is frequently used for its acquisition flexibility and its ability to supply imagery with very high spatial resolution (VHSR); the latter is interesting for supplying time-series data over large areas. However, each of these data sources is generally used separately even though they are complementary and have strong and promising potential synergies. Data fusion is a well-known technique to exploit this multi-source synergy, but in practice, UAV and satellite synergies are more specific, less well known and need to be formalized. In this article, we review remote sensing studies that addressed both data sources. Current approaches were categorized to distinguish four strategies: “data comparison”, “multiscale explanation”, “model calibration” and “data fusion”. Analysis of the literature revealed emerging trends, the supply of these distinct strategies for several applications and allowed to identify key contributions of UAV data. Finally, the high potential of this synergy seems currently under-exploited; therefore a discussion is proposed about the related implications for data interoperability, machine learning and data sharing to reinforce synergies between UAVs and satellites.},
	language = {en},
	urldate = {2022-09-01},
	journal = {Science of Remote Sensing},
	author = {Alvarez-Vanhard, Emilien and Corpetti, Thomas and Houet, Thomas},
	month = jun,
	year = {2021},
	keywords = {Calibration, Drone, Fusion, Multiscale, Sensor synergy, Spaceborne},
	pages = {100019},
}

@article{park_comparison_2019,
	title = {Comparison between point cloud and mesh models using images from an unmanned aerial vehicle},
	volume = {138},
	issn = {0263-2241},
	url = {https://www.sciencedirect.com/science/article/pii/S0263224119301411},
	doi = {10.1016/j.measurement.2019.02.023},
	abstract = {Structure from motion (SfM) is a well-known algorithm used for the generating of three-dimensional (3D) spatial information using images. The objective of this study is to compare the measurements of objects ascertained from point cloud and mesh models derived from the SfM algorithm. In particular, we analyze a single tree to determine the correlation between the number of acquired images from the UAVs and the object measurement for each model. The results indicate that the number of images does not have a critical impact on surveys and the point cloud is approximately 2\% more accurate than mesh models for individual tree measurement. Our results will be useful in terms of selecting the data acquisition method as well as the data itself for measuring objects based on SfM 3D data.},
	language = {en},
	urldate = {2022-09-01},
	journal = {Measurement},
	author = {Park, Haekyung and Lee, Dongkun},
	month = may,
	year = {2019},
	keywords = {3D spatial data, Mesh, Point cloud, SfM, Tree measurement, Unmanned aerial vehicles},
	pages = {461--466},
}

@article{dlesk_photogrammetric_2022,
	title = {Photogrammetric {Co}-{Processing} of {Thermal} {Infrared} {Images} and {RGB} {Images}},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/4/1655},
	doi = {10.3390/s22041655},
	abstract = {In some applications of thermography, spatial orientation of the thermal infrared information can be desirable. By the photogrammetric processing of thermal infrared (TIR) images, it is possible to create 2D and 3D results augmented by thermal infrared information. On the augmented 2D and 3D results, it is possible to locate thermal occurrences in the coordinate system and to determine their scale, length, area or volume. However, photogrammetric processing of TIR images is difficult due to negative factors which are caused by the natural character of TIR images. Among the negative factors are the lower resolution of TIR images compared to RGB images and lack of visible features on the TIR images. To eliminate these negative factors, two methods of photogrammetric co-processing of TIR and RGB images were designed. Both methods require a fixed system of TIR and RGB cameras and for each TIR image a corresponding RGB image must be captured. One of the methods was termed sharpening and the result of this method is mainly an augmented orthophoto, and an augmented texture of the 3D model. The second method was termed reprojection and the result of this method is a point cloud augmented by thermal infrared information. The details of the designed methods, as well as the experiments related to the methods, are presented in this article.},
	language = {en},
	number = {4},
	urldate = {2022-09-01},
	journal = {Sensors},
	author = {Dlesk, Adam and Vach, Karel and Pavelka, Karel},
	month = jan,
	year = {2022},
	keywords = {augmented orthophoto, augmented point cloud, close-range photogrammetry, photogrammetry, thermal infrared image, thermography},
	pages = {1655},
}

@article{ramon_thermal_2022,
	title = {Thermal point clouds of buildings: {A} review},
	volume = {274},
	issn = {0378-7788},
	shorttitle = {Thermal point clouds of buildings},
	url = {https://www.sciencedirect.com/science/article/pii/S0378778822005965},
	doi = {10.1016/j.enbuild.2022.112425},
	abstract = {New methodologies that explore and identify energy problems of buildings can be defined by using three-dimensional thermal models over time. The acquisition of thermal point clouds is the primary link in a chain that ends with a semantic Building Information Model (BIM) that contains energetic parameters. This literature review attempts to outline the current state of the art as regards the most important stages in this path by discussing the topics of thermal point cloud acquisition systems, 3D thermal semantic models, exportation to thermal models and the current applications of thermal point clouds in Architecture, Engineering and Construction. Special mention is made of the limitations and gaps in each stage. As a summary, it can be stated that only simple thermal BIM models of single constructive elements have been defined to date, and that a long path must be followed in order to automatically obtain a thermal BIM model of a multi-story building.},
	language = {en},
	urldate = {2022-09-01},
	journal = {Energy and Buildings},
	author = {Ramón, Amanda and Adán, Antonio and Javier Castilla, Francisco},
	month = nov,
	year = {2022},
	keywords = {3D Data processing, Building energy model, Building information modelling, Infrared thermography, Thermal point clouds},
	pages = {112425},
}

@article{hu_hyperspectral_2022,
	title = {Hyperspectral {Anomaly} {Detection} {Using} {Deep} {Learning}: {A} {Review}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {Hyperspectral {Anomaly} {Detection} {Using} {Deep} {Learning}},
	url = {https://www.mdpi.com/2072-4292/14/9/1973},
	doi = {10.3390/rs14091973},
	abstract = {Hyperspectral image-anomaly detection (HSI-AD) has become one of the research hotspots in the field of remote sensing. Because HSI’s features of integrating image and spectrum provide a considerable data basis for abnormal object detection, HSI-AD has a huge application potential in HSI analysis. It is difficult to effectively extract a large number of nonlinear features contained in HSI data using traditional machine learning methods, and deep learning has incomparable advantages in the extraction of nonlinear features. Therefore, deep learning has been widely used in HSI-AD and has shown excellent performance. This review systematically summarizes the related reference of HSI-AD based on deep learning and classifies the corresponding methods into performance comparisons. Specifically, we first introduce the characteristics of HSI-AD and the challenges faced by traditional methods and introduce the advantages of deep learning in dealing with these problems. Then, we systematically review and classify the corresponding methods of HSI-AD. Finally, the performance of the HSI-AD method based on deep learning is compared on several mainstream data sets, and the existing challenges are summarized. The main purpose of this article is to give a more comprehensive overview of the HSI-AD method to provide a reference for future research work.},
	language = {en},
	number = {9},
	urldate = {2022-09-01},
	journal = {Remote Sensing},
	author = {Hu, Xing and Xie, Chun and Fan, Zhe and Duan, Qianqian and Zhang, Dawei and Jiang, Linhua and Wei, Xian and Hong, Danfeng and Li, Guoqiang and Zeng, Xinhua and Chen, Wenming and Wu, Dongfang and Chanussot, Jocelyn},
	month = jan,
	year = {2022},
	keywords = {deep learning, hyperspectral image-anomaly detection, remote sensing},
	pages = {1973},
}

@article{jia_survey_2021,
	title = {A survey: {Deep} learning for hyperspectral image classification with few labeled samples},
	volume = {448},
	issn = {0925-2312},
	shorttitle = {A survey},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221004033},
	doi = {10.1016/j.neucom.2021.03.035},
	abstract = {With the rapid development of deep learning technology and improvement in computing capability, deep learning has been widely used in the field of hyperspectral image (HSI) classification. In general, deep learning models often contain many trainable parameters and require a massive number of labeled samples to achieve optimal performance. However, in regard to HSI classification, a large number of labeled samples is generally difficult to acquire due to the difficulty and time-consuming nature of manual labeling. Therefore, many research works focus on building a deep learning model for HSI classification with few labeled samples. In this article, we concentrate on this topic and provide a systematic review of the relevant literature. Specifically, the contributions of this paper are twofold. First, the research progress of related methods is categorized according to the learning paradigm, including transfer learning, active learning and few-shot learning. Second, a number of experiments with various state-of-the-art approaches has been carried out, and the results are summarized to reveal the potential research directions. More importantly, it is notable that although there is a vast gap between deep learning models (that usually need sufficient labeled samples) and the HSI scenario with few labeled samples, the issues of small-sample sets can be well characterized by fusion of deep learning methods and related techniques, such as transfer learning and a lightweight model. For reproducibility, the source codes of the methods assessed in the paper can be found at https://github.com/ShuGuoJ/HSI-Classification.git.},
	language = {en},
	urldate = {2022-09-01},
	journal = {Neurocomputing},
	author = {Jia, Sen and Jiang, Shuguo and Lin, Zhijie and Li, Nanying and Xu, Meng and Yu, Shiqi},
	month = aug,
	year = {2021},
	keywords = {Deep learning, Few-shot learning, Hyperspectral image classification, Transfer learning},
	pages = {179--204},
}

@inproceedings{hoegner_evaluation_2016,
	title = {Evaluation of methods for coregistration and fusion of {RPAS}-based {3D} point clouds and thermal infrared images},
	volume = {XLI-B3},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLI-B3/241/2016/},
	doi = {10.5194/isprs-archives-XLI-B3-241-2016},
	abstract = {{\textbackslash}textlessp{\textbackslash}textgreater{\textbackslash}textlessstrong class="journal-contentHeaderColor"{\textbackslash}textgreaterAbstract.{\textbackslash}textless/strong{\textbackslash}textgreater This paper discusses the automatic coregistration and fusion of 3d point clouds generated from aerial image sequences and corresponding thermal infrared (TIR) images. Both RGB and TIR images have been taken from a RPAS platform with a predefined flight path where every RGB image has a corresponding TIR image taken from the same position and with the same orientation with respect to the accuracy of the RPAS system and the inertial measurement unit. To remove remaining differences in the exterior orientation, different strategies for coregistering RGB and TIR images are discussed: (i) coregistration based on 2D line segments for every single TIR image and the corresponding RGB image. This method implies a mainly planar scene to avoid mismatches; (ii) coregistration of both the dense 3D point clouds from RGB images and from TIR images by coregistering 2D image projections of both point clouds; (iii) coregistration based on 2D line segments in every single TIR image and 3D line segments extracted from intersections of planes fitted in the segmented dense 3D point cloud; (iv) coregistration of both the dense 3D point clouds from RGB images and from TIR images using both ICP and an adapted version based on corresponding segmented planes; (v) coregistration of both image sets based on point features. The quality is measured by comparing the differences of the back projection of homologous points in both corrected RGB and TIR images.{\textbackslash}textless/p{\textbackslash}textgreater},
	language = {English},
	urldate = {2022-09-01},
	booktitle = {The {International} {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Hoegner, L. and Tuttas, S. and Xu, Y. and Eder, K. and Stilla, U.},
	month = jun,
	year = {2016},
	pages = {241--246},
}

@article{lopez_generation_2022,
	title = {Generation of hyperspectral point clouds: {Mapping}, compression and rendering},
	volume = {106},
	issn = {0097-8493},
	shorttitle = {Generation of hyperspectral point clouds},
	url = {https://www.sciencedirect.com/science/article/pii/S0097849322001145},
	doi = {10.1016/j.cag.2022.06.011},
	abstract = {Hyperspectral data are being increasingly used for the characterization and understanding of real-world scenarios. In this field, UAV-based sensors bring the opportunity to collect multiple samples from different viewpoints. Thus, light-material interactions of real objects may be observed in outdoor scenarios with a significant spatial resolution (5 cm/pixel). Nevertheless, the generation of hyperspectral 3D data still poses challenges in post-processing due to the high geometric deformation of images. Most of the current solutions use both LiDAR (Light Detection and Ranging) and hyperspectral sensors, which are integrated into the same acquisition system. However, these present several limitations due to errors derived from inertial measurements for data fusion and the spatial resolution according to the LiDAR capabilities. In this work, a method is proposed for the generation of hyperspectral point clouds. Input data are formed by push-broom hyperspectral images and 3D point clouds. On the one hand, the point clouds may be obtained by applying a typical photogrammetric workflow or LiDAR technology. Then, hyperspectral images are geometrically corrected and aligned with the RGB orthomosaic. Accordingly, hyperspectral data are ready to be mapped on the 3D point cloud. This procedure is implemented on the GPU by testing which points are visible for each pixel of the hyperspectral imagery. This work also provides a novel solution to generate, compress and render 3D hyperspectral point clouds, enabling the study of geometry and the hyperspectral response of natural and artificial materials in the real world.},
	language = {en},
	urldate = {2022-09-01},
	journal = {Computers \& Graphics},
	author = {López, Alfonso and Jurado, Juan M. and Jiménez-Pérez, J. Roberto and Feito, Francisco R.},
	month = aug,
	year = {2022},
	keywords = {Compression, Hyperspectral, Massively parallel algorithms, Rendering},
	pages = {267--276},
}

@article{schutz_software_2022,
	title = {Software {Rasterization} of 2 {Billion} {Points} in {Real} {Time}},
	volume = {5},
	url = {https://doi.org/10.1145/3543863},
	doi = {10.1145/3543863},
	abstract = {The accelerated collection of detailed real-world 3D data in the form of ever-larger point clouds is sparking a demand for novel visualization techniques that are capable of rendering billions of point primitives in real-time. We propose a software rasterization pipeline for point clouds that is capable of rendering up to two billion points in real-time (60 FPS) on commodity hardware. Improvements over the state of the art are achieved by batching points, enabling a number of batch-level optimizations before rasterizing them within the same rendering pass. These optimizations include frustum culling, level-of-detail (LOD) rendering, and choosing the appropriate coordinate precision for a given batch of points directly within a compute workgroup. Adaptive coordinate precision, in conjunction with visibility buffers, reduces the required data for the majority of points to just four bytes, making our approach several times faster than the bandwidth-limited state of the art. Furthermore, support for LOD rendering makes our software rasterization approach suitable for rendering arbitrarily large point clouds, and to meet the elevated performance demands of virtual reality applications.},
	number = {3},
	urldate = {2022-09-01},
	journal = {Proceedings of the ACM on Computer Graphics and Interactive Techniques},
	author = {Schütz, Markus and Kerbl, Bernhard and Wimmer, Michael},
	month = jul,
	year = {2022},
	keywords = {point cloud rendering, rasterization, real-time rendering, virtual reality},
	pages = {24:1--24:17},
}

@book{hexagon_imagine_1994,
	title = {Imagine {OrthoMax} {User}’s {Guide} {Version} 8,2},
	publisher = {Vision International},
	author = {Hexagon, AB.},
	year = {1994},
}

@article{ducke_multiview_2011,
	title = {Multiview {3D} reconstruction of the archaeological site at {Weymouth} from image series},
	volume = {35},
	issn = {0097-8493},
	url = {https://bit.ly/3CvdHiA},
	doi = {https://doi.org/cvc45q},
	abstract = {Multiview (n-view or multiple view) 3D reconstruction is the computationally complex process by which a full 3D model is derived from a series of overlapping images. It is based on research in the field of computer vision, which in turn relies on older methods from photogrammetry. This report presents a multiview reconstruction tool chain composed from various freely available, open source components and a practical application example in the form of a 3D model of an archaeological site.},
	number = {2},
	journal = {Computers \& Graphics},
	author = {Ducke, Benjamin and Score, David and Reeves, Joseph},
	year = {2011},
	keywords = {3D reconstruction, Archaeology, Computer vision, Open source, Photogrammetry, Weymouth},
	pages = {375--382},
}

@article{puerta_tricas_iglesias_2000,
	title = {Iglesias prerrománicas hispánicas ({Siglos} {VIII} al {XI})},
	journal = {Ensayo de tipología arquitectónica, Mainake, XXI-XXII, Diputación provincial, 1999-2000},
	author = {Puerta Tricas, R.},
	year = {2000},
	pages = {139--198},
}

@article{garrido_dense_2013,
	title = {Dense {Regional} {Active} {Networks} and {High} {Accuracy} {Positioning} {Services}. {A} {Case} {Study} {Based} on the {Andalusian} {Positioning} {Network} ({Southern} {Spain})},
	volume = {6},
	issn = {2151-1535},
	doi = {http://doi.org/f5jb4c},
	number = {6},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Garrido, M. S. and Giménez, E. and Lacy, M. C. de and Gil, A. J.},
	year = {2013},
	pages = {2421--2433},
}

@book{gomez-moreno_iglesias_1919,
	address = {Madrid},
	title = {Iglesias {Mozárabes}. {Arte} español de los siglos {IX} al {XI}},
	publisher = {Centro de Estudios Históricos},
	author = {Gomez-Moreno, M.},
	year = {1919},
}

@techreport{de_mergelina_bobastro_1927,
	title = {Bobastro. {Memoria} de las excavaciones realizadas en las {Mesas} de {Villaverde}. {El} {Chorro} ({Málaga})},
	number = {87 (num 7, 1925-26)},
	institution = {Memorias de la Junta Superior de excavaciones},
	author = {de Mergelina, C.},
	year = {1927},
}

@misc{cano_medina_mapa_nodate,
	title = {Mapa geológico y memoria explicativa de la hoja 1038 ({Ardales}). {Escala} 1:50.000},
	publisher = {I.G.M.E. Ministerio de Industria},
	author = {Cano Medina, F},
}

@article{de_mergelina_arquitectura_1925,
	title = {De arquitectura mozárabe. {La} iglesia rupestre de {Bobastro}},
	number = {2},
	journal = {Archivo Español de Arte y Arqueología},
	author = {de Mergelina, C.},
	year = {1925},
	pages = {147--158},
}

@article{frodella_combining_2020,
	title = {Combining {Infrared} {Thermography} and {UAV} {Digital} {Photogrammetry} for the {Protection} and {Conservation} of {Rupestrian} {Cultural} {Heritage} {Sites} in {Georgia}: {A} {Methodological} {Application}},
	volume = {12},
	issn = {2072-4292},
	url = {https://bit.ly/3CwJLTz},
	doi = {10.3390/rs12050892},
	number = {5},
	journal = {Remote Sensing},
	author = {Frodella, William and Elashvili, Mikheil and Spizzichino, Daniele and Gigli, Giovanni and Adikashvili, Luka and Vacheishvili, Nikoloz and Kirkitadze, Giorgi and Nadaraia, Akaki and Margottini, Claudio and Casagli, Nicola},
	month = mar,
	year = {2020},
	note = {Publisher: MDPI AG},
	pages = {892},
}

@inproceedings{sammartano_dem_2016,
	title = {{DEM} {Generation} based on {UAV} {Photogrammetry} {Data} in {Critical} {Areas}},
	doi = {http://doi.org/10/dwk5},
	author = {Sammartano, Giulia and Spano, Antonia},
	month = jan,
	year = {2016},
	pages = {92--98},
}

@article{peppa_photogrammetric_2019,
	title = {Photogrammetric assessment and comparison of {DJI} {Phantom} 4 {Pro} and {Phantom} 4 {RTK} small unmanned aircraft systems},
	volume = {XLII-2/W13},
	url = {https://bit.ly/3iBhXCs},
	doi = {http://doi.org/dwnd},
	journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Peppa, M. V. and Hall, J. and Goodyear, J. and Mills, J. P.},
	year = {2019},
	pages = {503--509},
}

@techreport{bango_torviso_espacio_1992,
	title = {El espacio para enterramientos privilegiados en la arquitectura medieval española},
	institution = {Anuario del Departamento de Historia y Teoría del Arte},
	author = {Bango Torviso, I},
	year = {1992},
}

@article{waagen_new_2019,
	title = {New technology and archaeological practice. {Improving} the primary archaeological recording process in excavation by means of {UAS} photogrammetry},
	volume = {101},
	doi = {http://doi.org/gfpfw9},
	journal = {Journal of Archaeological Science},
	author = {Waagen, J},
	year = {2019},
	pages = {11--20},
}

@book{cobb_reconsidering_2012,
	title = {Reconsidering {Archaeological} {Fieldwork}},
	publisher = {Springer-Verlag New York},
	author = {Cobb, Hannah and Harris, Oliver J. T. and Jones, Cara and Richardson, Philip},
	year = {2012},
	doi = {http://doi.org/10/dw5h},
}

@misc{martinez_enamorado_hallazgo_2009,
	title = {El hallazgo de la iglesia mozárabe de {Bobastro} ({Ardales}, {Málaga})},
	url = {https://bit.ly/3D1FWqZ},
	author = {Martínez Enamorado, Virgilio},
	month = may,
	year = {2009},
	note = {Published: Moros i Cristians. Petrer 2009. Festes de San Bonifaci, Màrtir},
}

@article{wechsler_perceptions_2003,
	title = {Perceptions of digital elevation model uncertainty by {DEM} users},
	volume = {15},
	journal = {URISA Journal},
	author = {Wechsler, Suzanne},
	month = jan,
	year = {2003},
	pages = {57--64},
}

@article{polidori_digital_2020,
	title = {Digital {Elevation} {Model} {Quality} {Assessment} {Methods}: {A} {Critical} {Review}},
	volume = {12},
	doi = {10.3390/rs12213522},
	journal = {Remote Sensing},
	author = {Polidori, Laurent and El Hage, Mhamad},
	month = oct,
	year = {2020},
	pages = {3522},
}

@article{caballero_zoreda_aproximacion_2005,
	title = {Una aproximación a las técnicas constructivas de la {Alta} {Edad} {Media} en la {Península} {Ibérica}. {Entre} visigodos y omeyas,},
	doi = {http://doi.org/d236},
	number = {4},
	journal = {Arqueología de la arquitectura},
	author = {Caballero Zoreda, L. and Utrero Agudo, Mª A.},
	year = {2005},
	pages = {169--192},
}

@article{daza_pardo_edilicia_2007,
	title = {La edilicia rupestre en el norte de {Guadalajara}. {Hábitat} y eremitismo en la transición de la {Antigüedad} a la {Edad} {Media}, {Codex} aquilarensis},
	number = {23},
	journal = {Cuadernos de investigación del Monasterio de Santa María la Real},
	author = {Daza Pardo, E},
	year = {2007},
	pages = {6--27},
}

@article{dominguez_perela_contradicciones_1992,
	title = {Las contradicciones de la cultura mozárabe y el núcleo bizantino del noroeste.},
	volume = {65},
	doi = {http://doi.org/d237},
	number = {165-166},
	journal = {Archivo Español de Arqueología},
	author = {Domínguez Perela, E},
	year = {1992},
	pages = {223--262},
}

@article{puerta_tricas_iglesia_1979,
	title = {La iglesia rupestre de las {Mesas} de {Villaverde} ({Ardales}, {Málaga}),},
	journal = {Mainake, I,},
	author = {Puerta Tricas, R},
	year = {1979},
	pages = {179--204},
}

@article{martinez_enamorado_bobastro_1997,
	title = {Bobastro ({Ardales}, {Málaga}): la ciudad de {Ibn} {Hafsun}},
	volume = {7},
	number = {7},
	journal = {Archéologie Islamique},
	author = {Martínez Enamorado, V},
	year = {1997},
	pages = {27--44},
}

@article{vaze_impact_2007,
	title = {Impact of {DEM} {Resolution} on {Topographic} {Indices} and {Hydrological} {Modelling} {Results}},
	author = {Vaze, J and Teng, J.},
	month = jan,
	year = {2007},
}

@article{takagi_optimum_2002,
	title = {Optimum {Spatial} {Resolution} of {Digital} {Elevation} {Model} for {Topographical} {Analysis}},
	author = {Takagi, Masataka and Asano, Hiroshi and Kikuchi, Yuki},
	month = jan,
	year = {2002},
}

@article{patrucco_3d_2022,
	title = {{3D} {Data} {Fusion} for {Historical} {Analyses} of {Heritage} {Buildings} {Using} {Thermal} {Images}: {The} {Palacio} de {Colomina} as a {Case} {Study}},
	volume = {14},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/22/5699},
	doi = {10.3390/rs14225699},
	number = {22},
	journal = {Remote Sensing},
	author = {Patrucco, Giacomo and Gómez, Antonio and Adineh, Ali and Rahrig, Max and Lerma, José Luis},
	year = {2022},
}

@article{salgado_carmona_assessing_2020,
	title = {Assessing the potential of multispectral and thermal {UAV} imagery from archaeological sites. {A} case study from the {Iron} {Age} hillfort of {Villasviejas} del {Tamuja} ({Cáceres}, {Spain})},
	volume = {31},
	issn = {2352-409X},
	url = {https://bit.ly/3S2vROF},
	doi = {10.1016/j.jasrep.2020.102312},
	language = {en},
	urldate = {2021-10-24},
	journal = {Journal of Archaeological Science: Reports},
	author = {Salgado Carmona, José Angel and Quiros, Elia and Mayoral, Victorino and Charro, Cristina},
	month = jun,
	year = {2020},
	keywords = {Archaeology, Iberian Peninsula, Iron Age, Multispectral imaging, Remote sensing, Thermal Infrared, UAV},
	pages = {102312},
}

@article{mcleester_detecting_2018,
	title = {Detecting prehistoric landscape features using thermal, multispectral, and historical imagery analysis at {Midewin} {National} {Tallgrass} {Prairie}, {Illinois}},
	volume = {21},
	issn = {2352-409X},
	url = {https://www.sciencedirect.com/science/article/pii/S2352409X18303080},
	doi = {10.1016/j.jasrep.2018.08.016},
	language = {en},
	urldate = {2021-10-24},
	journal = {Journal of Archaeological Science: Reports},
	author = {McLeester, Madeleine and Casana, Jesse and Schurr, Mark R. and Hill, Austin Chad and Wheeler, Joseph H.},
	month = oct,
	year = {2018},
	keywords = {Aerial photography, American Midwest, Drone, Late prehistoric, Remote sensing, Thermography, UAV},
	pages = {450--459},
}

@article{brooke_thermal_2018,
	title = {Thermal {Imaging} for the {Archaeological} {Investigation} of {Historic} {Buildings}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://bit.ly/3ME5ayD},
	doi = {10.3390/rs10091401},
	language = {en},
	number = {9},
	urldate = {2021-10-24},
	journal = {Remote Sensing},
	author = {Brooke, Christopher},
	month = sep,
	year = {2018},
	keywords = {archaeology, architectural heritage, ground-based remote sensing, image processing, non-destructive, thermal imaging, thermodynamics, thermography},
	pages = {1401},
}

@article{casana_archaeological_2017,
	title = {Archaeological {Aerial} {Thermography} in {Theory} and {Practice}},
	volume = {5},
	issn = {2326-3768},
	url = {https://bit.ly/2JfuPiu},
	doi = {10.1017/aap.2017.23},
	language = {en},
	number = {4},
	urldate = {2021-10-24},
	journal = {Advances in Archaeological Practice},
	author = {Casana, Jesse and Wiewel, Adam and Cool, Autumn and Hill, Austin Chad and Fisher, Kevin D. and Laugier, Elise J.},
	month = nov,
	year = {2017},
	pages = {310--327},
}

@article{collaro_research_2022,
	title = {Research, {Application}, and {Innovation} of {LiDAR} {Technology} in the {Spatial} {Archeology}},
	journal = {The sixth edition of the Encyclopedia of Information Science and Technology},
	author = {Collaro, Carolina},
	year = {2022},
	note = {Publisher: IGI},
}

@techreport{icao_unmanned_2011,
	address = {999 University Street, Montréal, Quebec, Canada H3C 5H7 For},
	type = {Cir},
	title = {Unmanned {Aircraft} {Systems} ({UAS})},
	number = {328 AN/190},
	institution = {International Civil Aviation Organitation},
	author = {{ICAO}},
	year = {2011},
}

@article{campana_drones_2017,
	title = {Drones in {Archaeology}. {State}-of-the-art and {Future} {Perspectives}},
	volume = {24},
	url = {https://bit.ly/36nDxEY},
	doi = {10.1002/arp.1569},
	number = {4},
	journal = {Archaeological Prospection},
	author = {Campana, S.},
	year = {2017},
	note = {\_eprint: https://bit.ly/3bXrH5t},
	keywords = {3D modelling, 3D survey, UAV archaeology, aerial reconnaissance, woodland archaeology},
	pages = {275--296},
}

@inproceedings{themistocleous_unmanned_2015,
	title = {Unmanned aerial systems and spectroscopy for remote sensing applications in archaeology},
	publisher = {International Society for Photogrammetry and Remote Sensing},
	author = {Themistocleous, Kyriacos and Agapiou, Athos and Cuca, Branka and Hadjimitsis, Diofantos G},
	year = {2015},
}

@misc{wu_visualsfm_2011,
	title = {{VisualSFM}: {A} visual structure from motion system},
	url = {https://bit.ly/3CwjZ1s},
	author = {Wu, Changchang and {others}},
	year = {2011},
}

@inproceedings{pecci_archaeology_2016,
	series = {{EGU} {General} {Assembly} {Conference} {Abstracts}},
	title = {Archaeology, historical site risk assessment and monitoring by {UAV}: approaches and case studies},
	booktitle = {{EGU} {General} {Assembly} {Conference} {Abstracts}},
	author = {Pecci, Antonio and Masini, Nicola},
	month = apr,
	year = {2016},
	pages = {EPSC2016--17424},
}

@article{dubbini_digital_2016,
	title = {Digital elevation models from unmanned aerial vehicle surveys for archaeological interpretation of terrain anomalies: case study of the {Roman} castrum of {Burnum} ({Croatia})},
	volume = {8},
	issn = {2352-409X},
	doi = {10.1016/j.jasrep.2016.05.054},
	journal = {Journal of Archaeological Science: Reports},
	author = {Dubbini, Marco and Curzio, Lucia Irene and Campedelli, Alessandro},
	year = {2016},
	pages = {121 -- 134},
}

@article{lo_brutto_uav_2014,
	title = {{UAV} {Plataforms} for {Cultural} {Heritage} {Survey}: {First} {Results}.},
	volume = {2},
	number = {5},
	journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Lo Brutto, Mauro and Garraffa, A and Meli, P},
	year = {2014},
}

@article{aguera-vega_assessment_2017,
	title = {Assessment of photogrammetric mapping accuracy based on variation ground control points number using unmanned aerial vehicle},
	volume = {98},
	issn = {0263-2241},
	url = {https://bit.ly/2ZFN0EG},
	doi = {http://doi.org/gft3d6},
	journal = {Measurement},
	author = {Agüera-Vega, Francisco and Carvajal-Ramírez, Fernando and Martínez-Carricondo, Patricio},
	year = {2017},
	keywords = {Accuracy evaluation, Georeferencing, Orthophoto, Photogrammetry, UAV},
	pages = {221 -- 227},
}

@article{krsak_use_2016,
	title = {Use of low-cost {UAV} photogrammetry to analyze the accuracy of a digital elevation model in a case study},
	volume = {91},
	issn = {0263-2241},
	url = {https://bit.ly/2Z0rOKs},
	doi = {https://doi.org/10/gc5vsz},
	journal = {Measurement},
	author = {Kršák, B and Blišt'an, P and Pauliková, A and Puškárová, P and Kovanič, L' and Palková, J and Zelizňaková, V},
	year = {2016},
	keywords = {Low-cost UAV, Photogrammetry, Point cloud},
	pages = {276 -- 287},
}

@article{clapuyt_reproducibility_2016,
	title = {Reproducibility of {UAV}-based earth topography reconstructions based on {Structure}-from-{Motion} algorithms},
	volume = {260},
	issn = {0169-555X},
	url = {https://bit.ly/3CUpd8Y},
	doi = {https://doi.org/10/f8kbgd},
	journal = {Geomorphology},
	author = {Clapuyt, F. and Vanacker, V. and Van Oost, K.},
	year = {2016},
	keywords = {3D point cloud, Digital surface model, Precision, Structure-from-Motion, Topography, Unmanned aerial vehicle},
	pages = {4 -- 15},
}

@incollection{limp_measuring_2016,
	title = {Measuring the {Face} of the {Past} and {Facing} the {Measurement}},
	booktitle = {Digital {Methods} and {Remote} {Sensing} in {Archaeology}. {Quantitative} {Methods} in the {Humanities} and {Social} {Sciences}},
	publisher = {Springer},
	author = {Limp, W.F.},
	editor = {Forte M., Campana S. (eds)},
	year = {2016},
	pages = {349--369},
}

@book{agisoft_agisoft_2020,
	title = {Agisoft {Metashape} {User} {Manual}. {Professional} {Edition}, {Version} 1.7},
	publisher = {Agisoft LLC},
	author = {Agisoft, LLC},
	year = {2020},
}

@book{olivares_castillos_1992,
	title = {Castillos de la provincia de {Jaén}},
	publisher = {Instituto de Estudios Giennenses},
	author = {Olivares, Francisco},
	year = {1992},
}

@article{enriquez_uas-based_2020,
	title = {The {UAS}-{Based} {3D} {Image} {Characterization} of {Mozarabic} {Church} {Ruins} in {Bobastro} ({Malaga}), {Spain}},
	volume = {12},
	issn = {2072-4292},
	url = {https://bit.ly/3CSctPZ},
	doi = {10.3390/rs12152377},
	number = {15},
	journal = {Remote Sensing},
	author = {Enríquez, Carlos and Jurado, Juan Manuel and Bailey, Alexandro and Callén, Danilo and Collado, María José and Espina, Gabriel and Marroquín, Pablo and Oliva, Erick and Osla, Edgar and Ramos, María Isabel and Sarceño, Scarlett and Feito, Francisco Ramón},
	year = {2020},
}

@article{jones_photogrammetry_2020,
	title = {Photogrammetry is for everyone: {Structure}-from-motion software user experiences in archaeology},
	volume = {30},
	doi = {10.1016/j.jasrep.2020.102261},
	journal = {Journal of Archaeological Science: Reports},
	author = {Jones, Christine and Church, Elizabeth},
	month = apr,
	year = {2020},
	pages = {102261},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@inproceedings{soudarissanane_optimizing_2012,
	title = {Optimizing {Terrestrial} {Laser} {Scanning} measurement set-up},
	volume = {XXXVIII-5-W12},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XXXVIII-5-W12/127/2011/},
	doi = {10.5194/isprsarchives-XXXVIII-5-W12-127-2011},
	abstract = {{\textbackslash}textlessp{\textbackslash}textgreater{\textbackslash}textlessstrong class="journal-contentHeaderColor"{\textbackslash}textgreaterAbstract.{\textbackslash}textless/strong{\textbackslash}textgreater One of the main applications of the terrestrial laser scanner is the visualization, modeling and monitoring of man-made structures like buildings. Especially surveying applications require on one hand a quickly obtainable, high resolution point cloud but also need observations with a known and well described quality. To obtain a 3D point cloud, the scene is scanned from different positions around the considered object. The scanning geometry plays an important role in the quality of the resulting point cloud. The ideal set-up for scanning a surface of an object is to position the laser scanner in such a way that the laser beam is near perpendicular to the surface. Due to scanning conditions, such an ideal set-up is in practice not possible. The different incidence angles and ranges of the laser beam on the surface result in 3D points of varying quality. The stand-point of the scanner that gives the best accuracy is generally not known. Using an optimal stand-point of the laser scanner on a scene will improve the quality of individual point measurements and results in a more uniform registered point cloud. The design of an optimum measurement setup is deﬁned such that the optimum stand-points are identiﬁed to fulﬁll predeﬁned quality requirements and to ensure a complete spatial coverage. The additional incidence angle and range constraints on the visibility from a view point ensure that individual scans are not affected by bad scanning geometry effects. A complex and large room that would normally require ﬁve view point to be fully covered, would require nineteen view points to obtain full coverage under the range and incidence angle constraints.{\textbackslash}textless/p{\textbackslash}textgreater},
	language = {English},
	urldate = {2023-01-29},
	booktitle = {The {International} {Archives} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Soudarissanane, S. and Lindenbergh, R.},
	month = sep,
	year = {2012},
	pages = {127--132},
}

@article{meyer_geometric_2023,
	title = {Geometric {BIM} verification of indoor construction sites by photogrammetric point clouds and evidence theory},
	volume = {195},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S092427162200329X},
	doi = {10.1016/j.isprsjprs.2022.12.014},
	abstract = {Photogrammetric point clouds offer immense potential for various applications, especially for the AEC industry and ”as-built” BIM. However, despite many advantages such as time and cost efficiency, image based point clouds of indoor environments mostly suffer from inhomogeneous and strongly fluctuating point-wise uncertainties. This lack of area-filling geometric reliability represents a strong barrier for innovations and further development of image based applications for as-built BIM, regarding both software and hardware. Therefore, this paper presents a method for the geometric verification of indoor BIMs by images and uncertainty management in order to unleash the potential of photogrammetry in context of professional building documentation heading towards ”digital twinning”. Individual 3D point accuracies, object’s surface characteristics and BIM related uncertainties according to the Level of Accuracy (LOA) specification are assessed and taken into account. The final decision of whether or not a photogrammetric point cloud confirms a given model within its associated level of accuracy results from a combined reasoning pipeline based on Dempster–Shafer evidence theory. The novel Pho-to-BIM verification method is demonstrated on three real indoor construction sites, each 3D mapped with different image sensors. Based on the experiments it is shown how to set up belief functions for evidence based reasoning individually, depending on the measurement and site characteristics.},
	language = {en},
	urldate = {2023-01-04},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Meyer, Theresa and Brunn, Ansgar and Stilla, Uwe},
	month = jan,
	year = {2023},
	keywords = {BIM, Geometric verification, Indoor, Level of accuracy, Photogrammetric 3D point clouds, Uncertainty},
	pages = {432--445},
}

@article{valero_laser_2022,
	title = {Laser scanning for {BIM}},
	volume = {27},
	url = {http://www.itcon.org/paper/2022/23},
	doi = {10.36680/j.itcon.2022.023},
	abstract = {Obtaining useful data from reality capture devices, such as Terrestrial Laser Scanners (TLS), for the extraction of semantic information and its subsequent use to support Building Information Modelling (BIM) use cases (e.g. Scan-to-BIM or Scan-vs-BIM -based use cases) is a complex task that requires planning and execution expertise. Point clouds of quality need to be produced following a conscientious planning and execution of scanning. And once the point clouds are acquired, methodical pre-processing operations are vital to ensure the point clouds finally are of high quality. This paper summarises some guidelines to surveyors for a successful data acquisition campaign, especially when these data will be employed for automatic processes involving point clouds and BIM, such as Scan-to-BIM or Scan-vs-BIM. The guidelines are also useful to the recipients of the point clouds involved in those processes, such as BIM modellers or Quality Control (QC) managers.},
	language = {en},
	number = {23},
	urldate = {2022-12-15},
	journal = {Journal of Information Technology in Construction (ITcon)},
	author = {Valero, Enrique and Bosché, Frédéric and Bueno, Martin},
	month = apr,
	year = {2022},
	pages = {486--495},
}

@article{rougeron_optimal_2022,
	title = {Optimal positioning of terrestrial {LiDAR} scanner stations in complex {3D} environments with a multiobjective optimization method based on {GPU} simulations},
	volume = {193},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271622002325},
	doi = {10.1016/j.isprsjprs.2022.08.023},
	abstract = {Currently, the scanning of complex industrial sites is commonly performed using terrestrial LiDAR scanners. As the quality of the resulting point cloud depends mainly on the number and positions of LiDAR stations, this scanning process can be preliminarily optimized by means of a 3D model. A previous study proposed multiobjective optimization based on the linear scalarization of three functions to maximize coverage and overlapping of point cloud stations while minimizing their number. Because these objectives conflict, this study proposes the use of MO-CMA-ES, a global multiobjective optimization algorithm, to provide a full Pareto front and allow the user to make an informed decision. Our method is the first to rely on realistic LiDAR simulations that operate in fully 3D complex environments and provide point clouds with optionally noisy coordinates. For performance considerations, ray-traced simulations and objective evaluations were performed using a GPU. Furthermore, clash detection in the proximity of station positions was also considered. After validating our method’s behavior and demonstrating its superiority over the conventional approach in a simple case, we conducted a study on an industrial-grade case based on a 2.7-million-triangle model, further demonstrating our method’s effectiveness by producing a minimal 15-station solution with optimal coverage and overlapping.},
	language = {en},
	urldate = {2022-11-30},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Rougeron, Gilles and Garrec, Jérémie Le and Andriot, Claude},
	month = nov,
	year = {2022},
	keywords = {3D point clouds, GPU computing, Multiobjective problem, Simulation modeling, Survey design, Terrestrial liDAR},
	pages = {60--76},
}

@article{ahn_interactive_2016,
	title = {Interactive scan planning for heritage recording},
	volume = {75},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-015-2473-0},
	doi = {10.1007/s11042-015-2473-0},
	abstract = {Terrestrial laser scanning has received attention as an efficient technology in the cultural heritage domain for recording the geometry of historic monuments quickly and precisely. It is important to find appropriate scanner configurations to make the scanning process efficient and to build reliable records. These configurations should satisfy required constraints such as full coverage, sufficient overlap, scan range limit, and laser incidence angle. This is called the view planning problem. We sought to develop a scan planning scheme for recording large monuments in the cultural heritage domain. Typical approaches to deal with the view planning problem, however, do not consider the specific requirements in this domain. In this paper, we propose an interactive scan planning approach that supports analytic computation as well as heuristic decision. It includes three supporting guides. A next scan grid supports semi-automated optimization in interactive planning, and scan geometry helps the user to intuitively decide the next best position in a feasible region. A knowledge guide, which is reasoned out by similar properties, provides the user with experts’ heuristic solutions to aid practical planning. These guides support efficient scan planning in a complementary manner. We introduce the use of region of interest to obtain more accurate data for focused features. ScanPlanner is implemented on this basis. The result of tests showed that the proposed approach allows users to make efficient and reliable scan plans for heritage recording.},
	language = {en},
	number = {7},
	urldate = {2022-09-07},
	journal = {Multimedia Tools and Applications},
	author = {Ahn, Jaehong and Wohn, KwangYun},
	month = apr,
	year = {2016},
	keywords = {3D scan, Heritage recording, Scan planning, Scan position, Terrestrial laser scanning},
	pages = {3655--3675},
}

@article{biswas_planning_2015,
	title = {Planning for {Scanning} {Using} {Building} {Information} {Models}: {A} {Novel} {Approach} with {Occlusion} {Handling}},
	shorttitle = {Planning for {Scanning} {Using} {Building} {Information} {Models}},
	url = {http://www.iaarc.org/publications/2015_proceedings_of_the_32st_isarc_oulu_finland/planning_for_scanning_using_building_information_models-a_novel_approach_with_occlusion_handling.html},
	language = {en-US},
	urldate = {2022-09-07},
	journal = {ISARC Proceedings},
	author = {Biswas, Humayun and BoschÃ©, FrÃ©dÃ©ric and Sun, Ming},
	month = jun,
	year = {2015},
	pages = {1--8},
}

@article{heidari_mozaffar_optimal_2016,
	title = {Optimal {Placement} of a {Terrestrial} {Laser} {Scanner} with an {Emphasis} on {Reducing} {Occlusions}},
	volume = {31},
	issn = {1477-9730},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/phor.12162},
	doi = {10.1111/phor.12162},
	abstract = {In this paper, a new automated algorithm is proposed that finds the optimum locations of a terrestrial laser scanner (TLS), ensuring completeness of data and minimising the number of scanning locations. The process starts with an initial scan and placing a 3D grid of candidate stations over the entire scan area. A global visibility analysis is then performed to identify the next best view (NBV) location. The TLS is placed on this selected point and a new scan is recorded. Having updated the initial scan with the resulting point cloud, the model is checked for completeness and density. The process is repeated until full coverage of the scan area is achieved by determining the best global arrangement with the minimum number of stations. Experiments show that the algorithm is able to automatically determine the station positions and provide a coverage of 99·5\% for simulated data and 91\% for real data.},
	language = {en},
	number = {156},
	urldate = {2022-09-07},
	journal = {The Photogrammetric Record},
	author = {Heidari Mozaffar, Morteza and Varshosaz, Masood},
	year = {2016},
	keywords = {classic optimisation, next best view planning, occlusions, placement, terrestrial laser scanner, visibility},
	pages = {374--393},
}

@inproceedings{kim_optimal_2014,
	title = {Optimal locations of terrestrial laser scanner for indoor mapping using genetic algorithm},
	doi = {10.1109/ICCAIS.2014.7020546},
	abstract = {A terrestrial laser scanner emerges as a main mapping technology for the indoor 3D model by providing fast and accurate 3D data. However, the field process to acquire the indoor 3D data using a terrestrial laser scanner heavily relies on the expert's field experiences. In this paper, the new method consisting of determination of optimal terrestrial laser scanner locations for indoor mapping using a genetic algorithm (GA) is proposed to acquire indoor 3D data more efficiently. The proposed method using GA utilizes building outlines extracted from 2D CAD drawings and it makes determination of the optimal locations and the number of scanner settings simple and fast. Furthermore, if the result of the proposed method is practically adopted with field experiences, it is expected to create a synergy effect in the process of acquiring indoor 3D data.},
	booktitle = {The 2014 {International} {Conference} on {Control}, {Automation} and {Information} {Sciences} ({ICCAIS} 2014)},
	author = {Kim, Mi-Kyeong and Li, Bin and Park, Je-Sung and Lee, Su-Jin and Sohn, Hong-Gyoo},
	month = dec,
	year = {2014},
	keywords = {Biological cells, Educational institutions, Electronic mail, Genetic algorithms, Laser modes, Three-dimensional displays},
	pages = {140--143},
}

@inproceedings{jia_comparison_2017,
	title = {A comparison of simulated annealing, genetic algorithm and particle swarm optimization in optimal first-order design of indoor {TLS} networks},
	volume = {IV-2-W4},
	url = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2-W4/75/2017/},
	doi = {10.5194/isprs-annals-IV-2-W4-75-2017},
	abstract = {{\textbackslash}textlessp{\textbackslash}textgreater{\textbackslash}textlessstrong class="journal-contentHeaderColor"{\textbackslash}textgreaterAbstract.{\textbackslash}textless/strong{\textbackslash}textgreater The optimal network design problem has been well addressed in geodesy and photogrammetry but has not received the same attention for terrestrial laser scanner (TLS) networks. The goal of this research is to develop a complete design system that can automatically provide an optimal plan for high-accuracy, large-volume scanning networks. The aim in this paper is to use three heuristic optimization methods, simulated annealing (SA), genetic algorithm (GA) and particle swarm optimization (PSO), to solve the first-order design (FOD) problem for a small-volume indoor network and make a comparison of their performances. The room is simplified as discretized wall segments and possible viewpoints. Each possible viewpoint is evaluated with a score table representing the wall segments visible from each viewpoint based on scanning geometry constraints. The goal is to find a minimum number of viewpoints that can obtain complete coverage of all wall segments with a minimal sum of incidence angles. The different methods have been implemented and compared in terms of the quality of the solutions, runtime and repeatability. The experiment environment was simulated from a room located on University of Calgary campus where multiple scans are required due to occlusions from interior walls. The results obtained in this research show that PSO and GA provide similar solutions while SA doesn’t guarantee an optimal solution within limited iterations. Overall, GA is considered as the best choice for this problem based on its capability of providing an optimal solution and fewer parameters to tune.{\textbackslash}textless/p{\textbackslash}textgreater},
	language = {English},
	urldate = {2022-09-07},
	booktitle = {{ISPRS} {Annals} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	publisher = {Copernicus GmbH},
	author = {Jia, F. and Lichti, D.},
	month = sep,
	year = {2017},
	pages = {75--82},
}

@article{blaer_view_2009,
	title = {View planning and automated data acquisition for three-dimensional modeling of complex sites},
	volume = {26},
	issn = {1556-4967},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20318},
	doi = {10.1002/rob.20318},
	abstract = {Constructing highly detailed three-dimensional (3-D) models of large complex sites using range scanners can be a time-consuming manual process. One of the main drawbacks is determining where to place the scanner to obtain complete coverage of a site. We have developed a system for automatic view planning called VuePlan. When combined with our mobile robot, AVENUE, we have a system that is capable of modeling large-scale environments with minimal human intervention throughout both the planning and acquisition phases. The system proceeds in two distinct stages. In the initial phase, the system is given a two-dimensional site footprint with which it plans a minimal set of sufficient and properly constrained covering views. We then use a 3-D laser scanner to take scans at each of these views. When this planning system is combined with our mobile robot it automatically computes and executes a tour of these viewing locations and acquires them with the robot's onboard laser scanner. These initial scans serve as an approximate 3-D model of the site. The planning software then enters a second phase in which it updates this model by using a voxel-based occupancy procedure to plan the next best view (NBV). This NBV is acquired, and further NBVs are sequentially computed and acquired until an accurate and complete 3-D model is obtained. A simulator tool that we developed has allowed us to test our entire view planning algorithm on simulated sites. We have also successfully used our two-phase system to construct precise 3-D models of real-world sites located in New York City: Uris Hall on the campus of Columbia University and Fort Jay on Governors Island. © 2009 Wiley Periodicals, Inc.},
	language = {en},
	number = {11-12},
	urldate = {2022-09-07},
	journal = {Journal of Field Robotics},
	author = {Blaer, Paul S. and Allen, Peter K.},
	year = {2009},
	pages = {865--891},
}

@article{chen_proactive_2018,
	title = {Proactive {2D} model-based scan planning for existing buildings},
	volume = {93},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580517310385},
	doi = {10.1016/j.autcon.2018.05.010},
	abstract = {Creating a building information model (BIM) is known to be valuable during the life-cycle of a building. In most cases, a BIM of an existing building either does not exist or is out of date. For existing buildings, an as-is BIM is needed to leverage the technology towards building life-cycle objectives. To create an as-is BIM, field surveying is a necessary task in collecting current building related information. Terrestrial laser scanners have been widely accepted as field surveying instruments due to their high level of accuracy. However, laser scanning is a time-consuming and labor-intensive process. Site revisiting and reworking of the scanning process is generally unavoidable because of inappropriate data collection processes. In this context, creating a scan plan before going to a job-site can improve the data collection process. In this study, the authors have proposed a 2D proactive scan-planning framework that includes three modules: an information-gathering module, a preparation module, and a searching module. In addition, three search algorithms — a greedy best-first search algorithm, a greedy search algorithm with a backtracking process, and a simulated annealing algorithm — were compared based on 64 actual building site drawings to identify strength and limitations. The experimental results demonstrate that the greedy search algorithm with a backtracking process could be used to compute an initial scan plan and the simulated annealing algorithm could be used to further refine the initial scan plan. This paper will also introduce the results of a case study that deployed the proposed scan-planning framework. In the case study, the resulting 3D-point cloud that was generated based on the proposed framework was compared with the 3D point cloud created with data collected through a planned scanning process performed by a scan technician.},
	language = {en},
	urldate = {2022-09-07},
	journal = {Automation in Construction},
	author = {Chen, Meida and Koc, Eyuphan and Shi, Zhuoya and Soibelman, Lucio},
	month = sep,
	year = {2018},
	keywords = {Laser scanning, Scan planning, View planning, Visibility checking},
	pages = {165--177},
}

@inproceedings{latimer_sensor_2004,
	title = {Sensor space planning with applications to construction environments},
	volume = {5},
	doi = {10.1109/ROBOT.2004.1302419},
	abstract = {Outlined is a new approach to sensor space planning and its application to the construction industry. The software planning tool described here generates sensor placements automatically for use in assessing deviations in construction environments. The first step is to separate construction information goals into clusters, simplifying the planning space in order to reduce computational complexity. For each cluster, the planner generates the space of potential sensor placements for a set of information goals and selects a minimal set of subspaces to take advantage of views that can achieve multiple goals simultaneously. Sensing locations are chosen that maximize the probability of achieving each goal and a path is generated to minimize the transit cost between the various sensing locations within each cluster. Finally, paths are generated that minimize the transit cost between clusters. This method is demonstrated on a desktop computer and shown to support LIDAR information goal sensor planning within a construction site.},
	booktitle = {{IEEE} {International} {Conference} on {Robotics} and {Automation}, 2004. {Proceedings}. {ICRA} '04. 2004},
	author = {Latimer, E. and Latimer, D. and Saxena, R. and Lyons, C. and Michaux-Smith, L. and Thayer, S.},
	month = apr,
	year = {2004},
	keywords = {Application software, Cost function, Inspection, Laser radar, Orbital robotics, Robot sensing systems, Robotics and automation, Sensor phenomena and characterization, Sensor systems, Service robots},
	pages = {4454--4460 Vol.5},
}

@article{zhang_rapid_2016,
	title = {Rapid data quality oriented laser scan planning for dynamic construction environments},
	volume = {30},
	issn = {1474-0346},
	url = {https://www.sciencedirect.com/science/article/pii/S1474034616300465},
	doi = {10.1016/j.aei.2016.03.004},
	abstract = {In construction environments, laser-scanning technologies can perform rapid spatial data collection to monitor construction progress, control construction quality, and support decisions about how to streamline field activities. However, even experienced surveyors cannot guarantee comprehensive laser scanning data collection in the field due to its constantly changing environment, wherein a large number of objects are subject to different data-quality requirements. The current practice of manually planned laser scanning often produces data of insufficient coverage, accuracy, and details. While redundant data collection can improve data quality, this process can also be inefficient and time-consuming. There are many studies on automatic sensor planning methods for guided laser-scanning data collection in the literature. However, fewer studies exist on how to handle exponentially large search space of laser scan plans that consider data quality requirements, such as accuracy and levels of details (LOD). This paper presents a rapid laser scan planning method that overcomes the computational complexity of planning laser scans based on diverse data quality requirements in the field. The goal is to minimize data collection time, while ensuring that the data quality requirements of all objects are satisfied. An analytical sensor model of laser scanning is constructed to create a “divide-and-conquer” strategy for rapid laser scan planning of dynamic environments wherein a graph is generated having specific data quality requirements (e.g., levels of accuracy and detail of certain objects) in terms of nodes and spatial relationships between these requirements as edges (e.g., distance, line-of-sight). A graph-coloring algorithm then decomposes the graph into sub-graphs and identifies “local” optimal laser scan plans of these sub-graphs. A solution aggregation algorithm then combines the local optimal plans to generate a plan for the entire site. Runtime analysis shows that the computation time of the proposed method does not increase exponentially with site size. Validation results of multiple case studies show that the proposed laser scan planning method can produce laser-scanning data with higher quality than data collected by experienced professionals, and without increasing the data collection time.},
	language = {en},
	number = {2},
	urldate = {2022-09-07},
	journal = {Advanced Engineering Informatics},
	author = {Zhang, Cheng and Kalasapudi, Vamsi Sai and Tang, Pingbo},
	month = apr,
	year = {2016},
	keywords = {Data quality, Geometric data collection, Inspection automation, Laser scanning, Level of detail (LOD), Sensor planning},
	pages = {218--232},
}

@article{aryan_planning_2021,
	title = {Planning for terrestrial laser scanning in construction: {A} review},
	volume = {125},
	issn = {0926-5805},
	shorttitle = {Planning for terrestrial laser scanning in construction},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580521000029},
	doi = {10.1016/j.autcon.2021.103551},
	abstract = {Terrestrial Laser Scanning (TLS) is an efficient and reliable method for collecting point clouds which have a range of applications in the Architecture, Engineering and Construction (AEC) domain. To ensure that the acquired point clouds are suitable to any given application, data collection must guarantee that all scanning targets are acquired with the specified data quality, and within time limits. Efficiency of data collection is important to reduce jobsite activity disruptions. Effective and efficient laser scanning data collection can be achieved through a prior planning optimisation process, which can be called Planning for Scanning (P4S). In the construction domain, the P4S problem has attracted increasing interest from the research community and a number of approaches have been proposed. This manuscript presents a systematic review of prior P4S works in the AEC domain and presents a categorisation of point cloud data quality criteria. The review starts with the identification and grouping in three categories of the point cloud data quality criteria that are commonly considered as constraints to the P4S problem. The three categories of data quality criteria include 1) completeness, 2) accuracy and spatial resolution, and 3) ‘registrability’. The prior P4S works are then reviewed in a structured way by contrasting them in the way they formulate the P4S optimisation problem: the type of inputs they assume (model and possible scanning locations), the constraints they consider, and the algorithm they utilise to solve the optimisation problem. This work makes two contributions: (1) it identifies gaps in knowledge that require further research such as the need to establish a fully automated scan plan which provides the optimum coverage in construction domain specifically for indoor construction; and (2) it provides a framework — principally a set of criteria — for others to compare new P4S methods against the existing state of the art in the field. This will not only be valuable for young researchers who want to start research in solving the P4S problem, but also for the ones already working in the domain to rethink the problem from different perspectives.},
	language = {en},
	urldate = {2022-09-07},
	journal = {Automation in Construction},
	author = {Aryan, Afrooz and Bosché, Frédéric and Tang, Pingbo},
	month = may,
	year = {2021},
	keywords = {Building information Modelling (BIM), Computer-aided design (CAD), Data quality, Laser scanning, Level of accuracy (LOA), Level of completeness (LOC), Level of detail (LOD), Network design, Optimisation, Planning for scanning, Point cloud},
	pages = {103551},
}

@article{giorgini_sensor-based_2019,
	title = {Sensor-{Based} {Optimization} of {Terrestrial} {Laser} {Scanning} {Measurement} {Setup} on {GPU}},
	volume = {16},
	issn = {1558-0571},
	doi = {10.1109/LGRS.2019.2899681},
	abstract = {A novel formulation of the set cover problem is presented to find the optimal placement of the scan stations in a terrestrial laser scanning survey. The problem is formulated in 2-D by including sensor-based constraints such as coverage and overlap. The coverage constraint ensures a minimum density of horizontal scan lines on the ground. The overlap constraint enables automatic scan alignment and registration. The optimization problem takes into account both environment occlusions and a maximum allowed incidence angle of the laser beams. The adopted laser model includes fixed parameters such as laser height, angular resolution, field of view, and minimum and maximum sensor range. The sensor placement problem is solved using a numerical approach implemented on graphics processing unit (GPU). Thanks to the GPU acceleration, experiments have been performed in large-scale environments with internal structures.},
	number = {9},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Giorgini, Mikhail and Marini, Stefano and Monica, Riccardo and Aleotti, Jacopo},
	month = sep,
	year = {2019},
	keywords = {Graphics processing unit (GPU) computing, Graphics processing units, Laser beams, Laser modes, Laser transitions, Measurement by laser beam, Optimization, set cover, terrestrial laser},
	pages = {1452--1456},
}

@article{potthast_probabilistic_2014,
	series = {Visual {Understanding} and {Applications} with {RGB}-{D} {Cameras}},
	title = {A probabilistic framework for next best view estimation in a cluttered environment},
	volume = {25},
	issn = {1047-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S1047320313001387},
	doi = {10.1016/j.jvcir.2013.07.006},
	abstract = {In this article, we present an information gain-based variant of the next best view problem for occluded environment. Our proposed method utilizes a belief model of the unobserved space to estimate the expected information gain of each possible viewpoint. More precise, this belief model allows a more precise estimation of the visibility of occluded space and with that a more accurate prediction of the potential information gain of new viewing positions. We present experimental evaluation on a robotic platform for active data acquisition, however due to the generality of our approach it also applies to a wide variety of 3D reconstruction problems. With the evaluation done in simulation and on a real robotic platform, exploring and acquiring data from different environments we demonstrate the generality and usefulness of our approach for next best view estimation and autonomous data acquisition.},
	language = {en},
	number = {1},
	urldate = {2022-09-07},
	journal = {Journal of Visual Communication and Image Representation},
	author = {Potthast, Christian and Sukhatme, Gaurav S.},
	month = jan,
	year = {2014},
	keywords = {3-D perception, Cluttered environments, Missing points, Next best view estimation, Robot exploration, Sensor placement, Sensor planning, View planning},
	pages = {148--164},
}

@article{wakisaka_optimal_2019,
	title = {Optimal {Laser} {Scan} {Planning} for {As}-{Built} {Modeling} of {Plant} {Renovations} {Using} {Mathematical} {Programming}},
	url = {http://www.iaarc.org/publications/2019_proceedings_of_the_36th_isarc/optimal_laser_scan_planning_for_as_built_modeling_of_plant_renovations_using_mathematical_programming.html},
	language = {en-US},
	urldate = {2022-09-07},
	journal = {ISARC Proceedings},
	author = {Wakisaka, Eisuke and Kanai, Satoshi and Date, Hiroaki},
	month = may,
	year = {2019},
	pages = {91--98},
}

@inproceedings{vannucci_genetic_2020,
	address = {Singapore},
	series = {Smart {Innovation}, {Systems} and {Technologies}},
	title = {Genetic {Operators} {Impact} on {Genetic} {Algorithms} {Based} {Variable} {Selection}},
	isbn = {9789811559259},
	doi = {10.1007/978-981-15-5925-9_18},
	abstract = {This paper faces the problem of variables selection through the use of a genetic algorithm based metaheuristic approach. The method is based on the evolution of a population of variables subsets, which is led by the genetic operators determining their selection and improvement through the algorithm generations. The impact of different genetic operators expressly designed for this purpose is assessed through a test campaign. The results show that the use of specific operators can lead to remarkable improvements in terms of selection quality.},
	language = {en},
	booktitle = {Intelligent {Decision} {Technologies}},
	publisher = {Springer},
	author = {Vannucci, Marco and Colla, Valentina and Cateni, Silvia},
	editor = {Czarnowski, Ireneusz and Howlett, Robert J. and Jain, Lakhmi C.},
	year = {2020},
	keywords = {Genetic algorithms, Genetic operators, Varables selection},
	pages = {211--221},
}

@article{servranckx_tabu_2019,
	title = {A tabu search procedure for the resource-constrained project scheduling problem with alternative subgraphs},
	volume = {273},
	issn = {0377-2217},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221718307513},
	doi = {10.1016/j.ejor.2018.09.005},
	abstract = {This paper investigates the resource-constrained project scheduling problem with alternative subgraphs (RCPSP-AS). In this scheduling problem, there exist alternative ways to execute subsets of activities that belong to work packages. One alternative execution mode must be selected for each work package and, subsequently, the selected activities in the project structure should be scheduled. Therefore, the RCPSP-AS consists of two subproblems: a selection and a scheduling subproblem. A key feature of this research is the categorisation of different types of alternative subgraphs in a comprehensive classification matrix based on the dependencies that exist between the alternatives in the project structure. As the existing problem-specific datasets do not support this framework, we propose a new dataset of problem instances using a well-known project network generator. Furthermore, we develop a tabu search that uses information from the proposed classification matrix to guide the search process towards high-quality solutions. We verify the overall performance of the metaheuristic and different improvement strategies using the developed dataset. Moreover, we show the impact of different problem parameters on the solution quality and we analyse the impact of distinct resource characteristics of alternatives on the selection process.},
	language = {en},
	number = {3},
	urldate = {2022-01-11},
	journal = {European Journal of Operational Research},
	author = {Servranckx, Tom and Vanhoucke, Mario},
	month = mar,
	year = {2019},
	keywords = {Alternative subgraphs, Project scheduling, Resource-constrained scheduling, Tabu search},
	pages = {841--860},
}

@article{qiu_tabu_2018,
	title = {A {Tabu} {Search} algorithm for the vehicle routing problem with discrete split deliveries and pickups},
	volume = {100},
	issn = {0305-0548},
	url = {https://www.sciencedirect.com/science/article/pii/S0305054818302053},
	doi = {10.1016/j.cor.2018.07.021},
	abstract = {The Vehicle Routing Problem with Discrete Split Deliveries and Pickups is a variant of the Vehicle Routing Problem with Split Deliveries and Pickups, in which customers’ demands are discrete in terms of batches (or orders). It exists in the practice of logistics distribution and consists of designing a least cost set of routes to serve a given set of customers while respecting constraints on the vehicles’ capacities. In this paper, its features are analyzed. A mathematical model and Tabu Search algorithm with specially designed batch combination and item creation operation are proposed. The batch combination operation is designed to avoid unnecessary travel costs, while the item creation operation effectively speeds up the search and enhances the algorithmic search ability. Computational results are provided and compared with other methods in the literature, which indicate that in most cases the proposed algorithm can find better solutions than those in the literature.},
	language = {en},
	urldate = {2022-01-11},
	journal = {Computers \& Operations Research},
	author = {Qiu, Meng and Fu, Zhuo and Eglese, Richard and Tang, Qiong},
	month = dec,
	year = {2018},
	keywords = {Discrete split, Pickup and delivery, Routing, Tabu search},
	pages = {102--116},
}

@inproceedings{wieckowski_finding_2020,
	address = {Singapore},
	series = {Smart {Innovation}, {Systems} and {Technologies}},
	title = {Finding an {Approximate} {Global} {Optimum} of {Characteristic} {Objects} {Preferences} by {Using} {Simulated} {Annealing}},
	isbn = {9789811559259},
	doi = {10.1007/978-981-15-5925-9_31},
	abstract = {Random processes are increasingly becoming a topic of consideration in many areas where decision-making is an important factor. The random factor affects the difficulty of determining input parameters. The selection of these parameters can be a key element in achieving the correct results. Stochastic optimization methods can be used to solve this problem. In this article, the simulated annealing method was used to obtain an optimal solution, which then, in combination with the COMET method, provided satisfactory results by determining the relationship between the preferences of the initial alternatives and newly identified alternatives. The purpose of this study was to systematize the knowledge of effective selection of input parameters for stochastic methods. The obtained solution indicates how to select a grid to an unknown problem and how to select a step in the simulated annealing method to achieve more precise results.},
	language = {en},
	booktitle = {Intelligent {Decision} {Technologies}},
	publisher = {Springer},
	author = {Więckowski, Jakub and Kizielewicz, Bartłomiej and Kołodziejczyk, Joanna},
	editor = {Czarnowski, Ireneusz and Howlett, Robert J. and Jain, Lakhmi C.},
	year = {2020},
	keywords = {COMET, Multi-criteria decision-making, Simulated annealing},
	pages = {365--375},
}

@article{ribas_iterated_2019,
	title = {An iterated greedy algorithm for solving the total tardiness parallel blocking flow shop scheduling problem},
	volume = {121},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418308078},
	doi = {10.1016/j.eswa.2018.12.039},
	abstract = {This paper proposes an iterated greedy algorithm for scheduling jobs in F parallel flow shops (lines), each consisting of a series of m machines without storage capacity between machines. This constraint can provoke the blockage of machines if a job has finished its operation and the next machine is not available. The criterion considered is the minimization of the sum of tardiness of all the jobs to schedule, i.e., minimization of the total tardiness of jobs. Notice that the proposed method is also valid for solving the Distributed Permutation Blocking Flow Shop Scheduling Problem (DBFSP), which allows modelling the scheduling process in companies with more than one factory when each factory has an identical flow shop configuration. Firstly, several constructive procedures have been implemented and tested to provide an efficient solution in terms of quality and CPU time. This initial solution is later improved upon with an iterated greedy algorithm that includes a variable neighbourhood search for interchanging or reassigning jobs from the critical line to other lines. Next, two strategies have been tested for selecting the critical line; the one with a higher total tardiness of jobs and the one with a job that has the highest tardiness. The experimental design chooses the best combination of initial solution and critical line selection. Finally, we compare the performance of the proposed algorithm against other benchmark algorithms proposed for the DPFSP, which have been adapted to the problem being considered here since, to the best of our knowledge, this is the first attempt to solve either the Parallel Blocking Flow Shop problem or the Distributed Blocking Flow Shop problem with the goal of minimizing total tardiness. This comparison has allowed us to confirm the good performance of the proposed method.},
	language = {en},
	urldate = {2022-01-11},
	journal = {Expert Systems with Applications},
	author = {Ribas, Imma and Companys, Ramon and Tort-Martorell, Xavier},
	month = may,
	year = {2019},
	keywords = {Blocking, Distribution flow shop, Parallel flow shop, Scheduling, Total tardiness},
	pages = {347--361},
}

@article{arnold_knowledge-guided_2019,
	title = {Knowledge-guided local search for the vehicle routing problem},
	volume = {105},
	issn = {0305-0548},
	url = {https://www.sciencedirect.com/science/article/pii/S0305054819300024},
	doi = {10.1016/j.cor.2019.01.002},
	abstract = {Local search has been established as a successful cornerstone to tackle the Vehicle Routing Problem, and is included in many state-of-the-art heuristics. In this paper we aim to demonstrate that a well-implemented local search on its own suffices to create a heuristic that computes high-quality solutions in a short time. To this end we combine three powerful local search techniques, and implement them in an efficient way that minimizes computational effort. We conduct a series of experiments to determine how local search can be effectively combined with perturbation and pruning, and make use of problem-specific knowledge, to guide the search to promising solutions more effectively. The heuristic created in this way not only performs well on many benchmark sets, it is also straightforward in its design and does not contain any components of which the contribution is unclear.},
	language = {en},
	urldate = {2022-01-11},
	journal = {Computers \& Operations Research},
	author = {Arnold, Florian and Sörensen, Kenneth},
	month = may,
	year = {2019},
	keywords = {Heuristics, Local search, Metaheuristics, Vehicle routing problem},
	pages = {32--46},
}

@incollection{michiels_introduction_2007,
	address = {Berlin, Heidelberg},
	series = {Monographs in {Theoretical} {Computer} {Science}, {An} {EATCS} {Series}},
	title = {Introduction},
	isbn = {978-3-540-35854-1},
	url = {https://doi.org/10.1007/978-3-540-35854-1_1},
	language = {en},
	urldate = {2022-01-11},
	booktitle = {Theoretical {Aspects} of {Local} {Search}},
	publisher = {Springer},
	editor = {Michiels, Wil and Korst, Jan and Aarts, Emile},
	year = {2007},
	doi = {10.1007/978-3-540-35854-1_1},
	keywords = {Combinatorial Optimization Problem, Local Optimum, Local Search, Local Search Algorithm, Problem Instance},
	pages = {1--10},
}

@article{yu_simulated_2017,
	title = {A simulated annealing heuristic for the hybrid vehicle routing problem},
	volume = {53},
	issn = {1568-4946},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494616306524},
	doi = {10.1016/j.asoc.2016.12.027},
	abstract = {This study proposes the Hybrid Vehicle Routing Problem (HVRP), which is an extension of the Green Vehicle Routing Problem (G-VRP). We focus on vehicles that use a hybrid power source, known as the Plug-in Hybrid Electric Vehicle (PHEV) and generate a mathematical model to minimize the total cost of travel by driving PHEV. Moreover, the model considers the utilization of electric and fuel power depending on the availability of either electric charging or fuel stations. We develop simulated annealing with a restart strategy (SA\_RS) to solve this problem, and it consists of two versions. The first version determines the acceptance probability of a worse solution using the Boltzmann function, denoted as SA\_RSBF. The second version employs the Cauchy function to determine the acceptance probability of a worse solution, denoted as SA\_RSCF. The proposed SA algorithm is first verified with benchmark data of the capacitated vehicle routing problem (CVRP), with the result showing that it performs well and confirms its efficiency in solving CVRP. Further analysis show that SA\_RSCF is preferable compared to SA\_RSBF and that SA with a restart strategy performs better than without a restart strategy. We next utilize the SA\_RSCF method to solve HVRP. The numerical experiment presents that vehicle type and the number of electric charging stations have an impact on the total travel cost.},
	language = {en},
	urldate = {2022-01-11},
	journal = {Applied Soft Computing},
	author = {Yu, Vincent F. and Redi, A. A. N. Perwira and Hidayat, Yosi Agustina and Wibowo, Oktaviyanto Jimat},
	month = apr,
	year = {2017},
	keywords = {Cauchy function, Hybrid electric vehicle, Hybrid vehicle routing problem, Restart strategy, Simulated annealing},
	pages = {119--132},
}

@misc{burkardt_halton_2010,
	title = {The {Halton} {Quasi} {Monte} {Carlo} ({QMC}) {Sequence}},
	url = {https://people.math.sc.edu/Burkardt/f_src/halton_advanced/halton_advanced.html},
	urldate = {2022-01-09},
	author = {Burkardt, John B},
	month = dec,
	year = {2010},
}

@article{marques_optimal_2019,
	title = {Optimal {Sample} {Weights} for {Hemispherical} {Integral} {Quadratures}},
	volume = {38},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13392},
	doi = {10.1111/cgf.13392},
	abstract = {This paper proposes optimal quadrature rules over the hemisphere for the shading integral. We leverage recent work regarding the theory of quadrature rules over the sphere in order to derive a new theoretical framework for the general case of hemispherical quadrature error analysis. We then apply our framework to the case of the shading integral. We show that our quadrature error theory can be used to derive optimal sample weights (OSW) which account for both the features of the sampling pattern and the bidirectional reflectance distribution function (BRDF). Our method significantly outperforms familiar Quasi Monte Carlo (QMC) and stochastic Monte Carlo techniques. Our results show that the OSW are very effective in compensating for possible irregularities in the sample distribution. This allows, for example, to significantly exceed the regular convergence rate of stochastic Monte Carlo while keeping the exact same sample sets. Another important benefit of our method is that OSW can be applied whatever the sampling points distribution: the sample distribution need not follow a probability density function, which makes our technique much more flexible than QMC or stochastic Monte Carlo solutions. In particular, our theoretical framework allows to easily combine point sets derived from different sampling strategies (e.g. targeted to diffuse and glossy BRDF). In this context, our rendering results show that our approach overcomes MIS (Multiple Importance Sampling) techniques.},
	language = {en},
	number = {1},
	urldate = {2022-01-09},
	journal = {Computer Graphics Forum},
	author = {Marques, Ricardo and Bouville, Christian and Bouatouch, Kadi},
	year = {2019},
	keywords = {Computing methodologies—Rendering, Monte Carlo techniques, Ray tracing, global illumination},
	pages = {59--72},
}

@article{kocis_computational_1997,
	title = {Computational investigations of low-discrepancy sequences},
	volume = {23},
	issn = {0098-3500},
	url = {https://doi.org/10.1145/264029.264064},
	doi = {10.1145/264029.264064},
	abstract = {The Halton, Sobol, and Faure sequences and the Braaten-Weller construction of the generalized Halton sequence are studied in order to assess their applicability for the quasi Monte Carlo integration with large number of variates. A modification of the Halton sequence (the Halton sequence leaped) and a new construction of the generalized Halton sequence are suggested for unrestricted number of dimensions and are shown to improve considerably on the original Halton sequence. Problems associated with estimation of the error in quasi Monte Carlo integration and with the selection of test functions are identified. Then an estimate of the maximum error of the quasi Monte Carlo integration of nine test functions is computed for up to 400 dimensions and is used to evaluate the known generators mentioned above and the two new generators. An empirical formula for the error of the quasi Monte Carlo integration is suggested.},
	number = {2},
	urldate = {2022-01-09},
	journal = {ACM Transactions on Mathematical Software},
	author = {Kocis, Ladislav and Whiten, William J.},
	month = jun,
	year = {1997},
	keywords = {Faure sequence, Halton sequence, Monte Carlo and quasi Monte Carlo integration, Sobol sequence, discrepancy, error of numerical integration, generalized Halton sequence, low-discrepancy sequences},
	pages = {266--294},
}

@inproceedings{pedemonte_bitwise_2011,
	address = {New York, NY, USA},
	series = {{GECCO} '11},
	title = {Bitwise operations for {GPU} implementation of genetic algorithms},
	isbn = {978-1-4503-0690-4},
	url = {https://doi.org/10.1145/2001858.2002031},
	doi = {10.1145/2001858.2002031},
	abstract = {Research on the implementation of evolutionary algorithms in graphics processing units (GPUs) has grown in recent years since it significantly reduces the execution time of the algorithm. A relevant aspect, which has received little attention in the literature, is the impact of the memory space occupied by the population in the performance of the algorithm, due to limited capacity of several memory spaces in the GPUs. In this paper we analyze the differences in performance of a binary Genetic Algorithm implemented on a GPU using a boolean data type or packing multiple bits into a non boolean data type. Our study considers the influence on the performance of single point and double point crossover for solving the classical One-Max problem. The results obtained show that packing bits for storing binary strings can reduce the execution time up to 50\%.},
	urldate = {2021-11-14},
	booktitle = {Proceedings of the 13th annual conference companion on {Genetic} and evolutionary computation},
	publisher = {Association for Computing Machinery},
	author = {Pedemonte, Martín and Alba, Enrique and Luna, Francisco},
	month = jul,
	year = {2011},
	keywords = {binary-coded genetic algorithm, cuda, evolutionary computation, gpgpu, gpu, parallelization},
	pages = {439--446},
}

@article{zhi-bin_novel_2021,
	title = {Novel parallel hybrid genetic algorithms on the {GPU} for the generalized assignment problem},
	issn = {1573-0484},
	url = {https://doi.org/10.1007/s11227-021-03882-6},
	doi = {10.1007/s11227-021-03882-6},
	abstract = {The emergence of GPU-CPU heterogeneous architecture has led to a significant paradigm shift in parallel programming. How to effectively implement Parallel Genetic Algorithm (GA) in these environments has become one of the current hot issues. GA’s calculation and operators are closely related to specific problems, thereby significantly affecting the acceleration method of GA algorithms. The Generalized Assignment Problem (GAP) is a classic NP-hard combinatorial optimization problem. The more widely used genetic algorithms to solve the GAP in the CPU are difficult to be parallelized in a GPU environment due to severe data dependencies. To address this problem, two algorithms suitable for the implementation on the GPU are proposed, namely RPE algorithm and NNE algorithm, which obtain significant performance speedup by alleviating data dependencies and mutually exclusive synchronization overheads. At the same time, considering the new GPU architecture features and programming models, three different granular implementations of parallel genetic algorithms to solve the GAP are proposed, namely \$\${\textbackslash}textbackslashtext\{ GPGA\}\_\{thread\}\$\$, \$\${\textbackslash}textbackslashtext\{ GPGA\}\_\{warpsp\}\$\$and \$\${\textbackslash}textbackslashtext\{ GPGA\}\_\{cgroup\}\$\$, by utilizing the warp-specialization technology and the cooperative group mechanism. GPGA series algorithms obtain better solution quality and very significant performance improvements compared with Serial GA, GTS (the GPU-CPU hybrid implementation of Scatter Search with Tabu lists) and Lagrange Relaxation algorithm on a CPU by solving 16 typical large-scale GAP instances.},
	language = {en},
	urldate = {2021-11-14},
	journal = {The Journal of Supercomputing},
	author = {Zhi-Bin, Huang and Guang-Tao, Fu and Dan-Yang, Dong and Chen, Xiao and Zhe-Lun, Ding and Zhi-Tao, Dai},
	month = may,
	year = {2021},
}

@inproceedings{abdelatti_optimizing_2021,
	address = {New York, NY, USA},
	series = {{GECCO} '21},
	title = {Optimizing a {GPU}-accelerated genetic algorithm for the vehicle routing problem},
	isbn = {978-1-4503-8351-6},
	url = {https://doi.org/10.1145/3449726.3459458},
	doi = {10.1145/3449726.3459458},
	abstract = {The capacitated vehicle routing problem (CVRP) is an NP-hard optimization problem with many applications. Genetic algorithms (GAs) are often used to solve CVRPs but require many parameters and operators to tune. Incorrect settings can result in poor solutions. In this work, a design of experiments (DOE) approach is used to determine the best settings for GA parameters. The GA runs entirely on an NVIDIA RTX 3090 GPU. The GPU execution for a 200-node benchmark shows a speed by a factor of 1700 compared to that on an octa-core i7 CPU with 64 GB RAM. The tuned GA achieved a solution for a 400-node benchmark that is 72\% better than that of an arbitrarily tuned GA after only 263 generations. New best-known values for several benchmarks are also obtained. A comparison between the performance of the algorithm with different hardware and tuning sets is also reported.},
	urldate = {2021-11-14},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {Association for Computing Machinery},
	author = {Abdelatti, Marwan and Hendawi, Abdeltawab and Sodhi, Manbir},
	month = jul,
	year = {2021},
	keywords = {GPU, design of experiment, factorial design, genetic algorithms, local search, parallel computation, vehicle routing problem},
	pages = {117--118},
}

@article{mohamadi_efficient_2021,
	title = {Efficient algorithms for decision making and coverage deployment of connected multi-low-altitude platforms},
	volume = {184},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421009374},
	doi = {10.1016/j.eswa.2021.115529},
	abstract = {Unmanned aerial vehicles (UAVs) have gained significantly in popularity in recent years, and they are coupled with the emerging Internet-of-Things, 5G, and mobile edge computing. Their application domains will continue to expand. UAVs can deliver various IoT-driven applications (e.g., video surveillance UAVs-based applications) anytime and anywhere. Indeed, the rapid deployment of such applications in different environments (e.g., urban or wildlife surveillance/sensing environments) is one of the critical factors to support the emergence of new services and applications (e.g., 5G/B5G UAVs base station, civil and commercial IoT applications). However, several research challenges need to be addressed before the latter can be deployed in the real world. In this paper, we propose a novel approach that enables efficient coverage using a set of UAVs. We define two multi-objective sub-problems. The first sub-problem allows selecting a minimum number of appropriate UAVs for deployment as Low-Altitude-Platforms (LAP) over an area of interest from an extensive collection of options. We model this sub-problem as multi-criteria decision-making (MCDM) problem considering various UAV features and user constraints and solve it using a novel multi-criteria selection algorithm named SAGA. The second sub-problem is a coverage optimization problem that enables controlling UAVs’ hover locations and use gimbal-mounted rotating cameras or multi-lens cameras to increase the coverage accordingly. We propose a strategy aiming at optimizing hovering locations of UAVs and then rotating their cameras. The process comprises two techniques to solve the latter sub-problem: (1) an improved preference-guided genetic algorithm named NSPGGA; (2) a hybrid heuristic DCXGA built upon three algorithmic techniques: divide-and-conquer, greedy search, and exhaustive search. We carried out comparative analyses with seven MCDM methods and ten multi-objective evolutionary/swarm intelligence algorithms. The proposed algorithms outperformed the benchmarking techniques and showed remarkable results, e.g., the selection algorithm SAGA exhibits a high success rate, accuracy, and consistency. The preference-guided genetic algorithm NSPGGA achieves better efficiency, and it is four times faster than the famed NSGA-II. Finally, the hybrid heuristic DCXGA allows having more significant imaging coverages with few camera rotations. The aforementioned vital results are validated through diverse and intensive simulation scenarios.},
	language = {en},
	urldate = {2021-11-14},
	journal = {Expert Systems with Applications},
	author = {Mohamadi, Houssem Eddine and Kara, Nadjia and Lagha, Mohand},
	month = dec,
	year = {2021},
	keywords = {Coverage optimization problem, Genetic algorithm, Low-altitude platform, Multi-criteria decision-making, Unmanned aerial vehicle},
	pages = {115529},
}

@article{cheng_accelerating_2019,
	title = {Accelerating genetic algorithms with {GPU} computing: {A} selective overview},
	volume = {128},
	issn = {0360-8352},
	shorttitle = {Accelerating genetic algorithms with {GPU} computing},
	url = {https://www.sciencedirect.com/science/article/pii/S036083521830665X},
	doi = {10.1016/j.cie.2018.12.067},
	abstract = {The emergence of GPU-CPU heterogeneous architectures has led to a fundamental paradigm shift in parallel programming. Accelerating Genetic Algorithms (GAs) on these architectures has received significant attention from both practitioners and researchers ever since GPUs emerged. In the past decade we have witnessed many progresses on migrating parallel GAs from CPU to GPU (Graphical Processing Unit) architecture, which makes this research field truly enter into the world of High Performance Computing (HPC), and demonstrates a great potential to many research disciplines and industrial worlds that can benefit from the power of GPU accelerated stochastic global search to explore large and complex search spaces for better solutions. Designing a parallel algorithm on GPU is quite different from designing one on CPU. On CPU architecture, we typically consider how to distribute data across tens of CPU threads, while on GPU architecture, we have more than hundreds of thousands of GPU threads running simultaneously. Therefore, we should rethink the design approaches and implementation strategies of parallel algorithms to fully utilize the computing power of GPUs to accelerate the computation of GAs. The intention of this paper is to give an overview on selective works of parallel GAs designed for GPU architecture. In this survey paper, we first reexamine the concept of granularity of parallelism for GAs on GPU architecture, discuss how the aspect of data layout affect the kernel design to maximize memory bandwidth, and explain how to organize threads in grid and blocks to expose sufficient parallelism to GPU. The comprehensive overview on selective works since 2010 then follows. The focus is mainly on the perspective of GPU architecture: how to accelerate GAs with GPU computing. Performance issues are not touched in this review, because most of these works are conducted on very early GPU cards, which are out of date already. We finally discuss some future research suggestions in the last section, especially about how to build up an efficient implementation of parallel GAs for hyper-scale computing. Many industrial and academic disciplines will be benefited from the GPU accelerated parallel GAs, one of the promising area is to evolve better deep neural networks.},
	language = {en},
	urldate = {2021-11-14},
	journal = {Computers \& Industrial Engineering},
	author = {Cheng, John Runwei and Gen, Mitsuo},
	month = feb,
	year = {2019},
	keywords = {GPU computing, Parallel genetic algorithms, Parallelism},
	pages = {514--525},
}

@article{roostapour_pareto_2022,
	title = {Pareto optimization for subset selection with dynamic cost constraints},
	volume = {302},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S000437022100148X},
	doi = {10.1016/j.artint.2021.103597},
	abstract = {We consider the subset selection problem for function f with constraint bound B that changes over time. Within the area of submodular optimization, various greedy approaches are commonly used. For dynamic environments we observe that the adaptive variants of these greedy approaches are not able to maintain their approximation quality. Investigating the recently introduced POMC Pareto optimization approach, we show that this algorithm efficiently computes a ϕ=(αf/2)(1−1eαf)-approximation, where αf is the submodularity ratio of f, for each possible constraint bound b≤B. Furthermore, we show that POMC is able to adapt its set of solutions quickly in the case that B increases. Our experimental investigations for the influence maximization in social networks show the advantage of POMC over generalized greedy algorithms. We also consider EAMC, a new evolutionary algorithm with polynomial expected time guarantee to maintain ϕ approximation ratio, and NSGA-II with two different population sizes as advanced multi-objective optimization algorithm, to demonstrate their challenges in optimizing the maximum coverage problem. Our empirical analysis shows that, within the same number of evaluations, POMC is able to perform as good as NSGA-II under linear constraint, while EAMC performs significantly worse than all considered algorithms in most cases.},
	language = {en},
	urldate = {2021-11-14},
	journal = {Artificial Intelligence},
	author = {Roostapour, Vahid and Neumann, Aneta and Neumann, Frank and Friedrich, Tobias},
	month = jan,
	year = {2022},
	keywords = {Multi-objective optimization, Runtime analysis, Submodular function, Subset selection},
	pages = {103597},
}

@article{wang_solving_2018,
	title = {Solving the {Energy} {Efficient} {Coverage} {Problem} in {Wireless} {Sensor} {Networks}: {A} {Distributed} {Genetic} {Algorithm} {Approach} with {Hierarchical} {Fitness} {Evaluation}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Solving the {Energy} {Efficient} {Coverage} {Problem} in {Wireless} {Sensor} {Networks}},
	url = {https://www.mdpi.com/1996-1073/11/12/3526},
	doi = {10.3390/en11123526},
	abstract = {This paper proposed a distributed genetic algorithm (DGA) to solve the energy efficient coverage (EEC) problem in the wireless sensor networks (WSN). Due to the fact that the EEC problem is Non-deterministic Polynomial-Complete (NPC) and time-consuming, it is wise to use a nature-inspired meta-heuristic DGA approach to tackle this problem. The novelties and advantages in designing our approach and in modeling the EEC problems are as the following two aspects. Firstly, in the algorithm design, we realized DGA in the multi-processor distributed environment, where a set of processors run distributed to evaluate the fitness values in parallel to reduce the computational cost. Secondly, when we evaluate a chromosome, different from the traditional model of EEC problem in WSN that only calculates the number of disjoint sets, we proposed a hierarchical fitness evaluation and constructed a two-level fitness function to count the number of disjoint sets and the coverage performance of all the disjoint sets. Therefore, not only do we have the innovations in algorithm, but also have the contributions on the model of EEC problem in WSN. The experimental results show that our proposed DGA performs better than other state-of-the-art approaches in maximizing the number of disjoin sets.},
	language = {en},
	number = {12},
	urldate = {2021-11-14},
	journal = {Energies},
	author = {Wang, Zi-Jia and Zhan, Zhi-Hui and Zhang, Jun},
	month = dec,
	year = {2018},
	keywords = {distributed genetic algorithm, energy efficient coverage, wireless sensor networks},
	pages = {3526},
}

@article{muhammad_novel_2018,
	title = {A {Novel} {Random} {Scheduling} {Algorithm} based on {Subregions} {Coverage} for {SET} {K}-{Cover} {Problem} in {Wireless} {Sensor} {Networks}},
	volume = {12},
	issn = {1976-7277},
	url = {https://www.koreascience.or.kr/article/JAKO201821464986082.page},
	doi = {10.3837/tiis.2018.06.012},
	abstract = {This paper proposes a novel Random Scheduling Algorithm based on Subregion Coverage (RSASC), to solve the SET K-cover problem (an NP-complete problem). SET K-cover problem distributes the set of sensors into the maximum number of mutually exclusive subsets (MESSs) in such a way that each of them can be scheduled for lifetime extension of WSN. Sensor coverage divides the target region into different subregions. RSASC first sorts the subregions in the ascending order concerning their sensor coverage. Then, it forms the subregion groups according to their similar sensor coverage. Lastly, RSASC ensures the K-coverage of each subregion from every group by randomly scheduling the sensors. We consider the target-coverage and area-coverage applications of WSN to analyze the usefulness of our proposed RSASC algorithm. The distinct quality of RSASC is that it utilizes less number of deployed sensors (33\% less) to form the optimum number of MESSs with the higher computational speed (saves more than 93\% of the time) as compared to the existing three algorithms.},
	language = {eng},
	number = {6},
	urldate = {2021-11-14},
	journal = {KSII Transactions on Internet and Information Systems (TIIS)},
	author = {Muhammad, Zahid and Roy, Abhishek and Ahn, Chang Wook and Sachan, Ruchi and Saxena, Navrati},
	year = {2018},
	pages = {2658--2679},
}

@article{roberge_parallel_2021,
	title = {Parallel {Algorithm} on {GPU} for {Wireless} {Sensor} {Data} {Acquisition} {Using} a {Team} of {Unmanned} {Aerial} {Vehicles}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/21/20/6851},
	doi = {10.3390/s21206851},
	abstract = {This paper proposes a framework for the wireless sensor data acquisition using a team of Unmanned Aerial Vehicles (UAVs). Scattered over a terrain, the sensors detect information about their surroundings and can transmit this information wirelessly over a short range. With no access to a terrestrial or satellite communication network to relay the information to, UAVs are used to visit the sensors and collect the data. The proposed framework uses an iterative k-means algorithm to group the sensors into clusters and to identify Download Points (DPs) where the UAVs hover to download the data. A Single-Source–Shortest-Path algorithm (SSSP) is used to compute optimal paths between every pair of DPs with a constraint to reduce the number of turns. A genetic algorithm supplemented with a 2-opt local search heuristic is used to solve the multi-travelling salesperson problem and to find optimized tours for each UAVs. Finally, a collision avoidance strategy is implemented to guarantee collision-free trajectories. Concerned with the overall runtime of the framework, the SSSP algorithm is implemented in parallel on a graphics processing unit. The proposed framework is tested in simulation using three UAVs and realistic 3D maps with up to 100 sensors and runs in just 20.7 s, a 33.3× speed-up compared to a sequential execution on CPU. The results show that the proposed method is efficient at calculating optimized trajectories for the UAVs for data acquisition from wireless sensors. The results also show the significant advantage of the parallel implementation on GPU.},
	language = {en},
	number = {20},
	urldate = {2021-11-14},
	journal = {Sensors},
	author = {Roberge, Vincent and Tarbouchi, Mohammed},
	month = jan,
	year = {2021},
	keywords = {Data acquisition, Genetic algorithm, Graphics processing units, Parallel computing, Path planning, Unmanned aerial vehicle, Wireless sensors, data acquisition, genetic algorithm, graphics processing units, parallel computing, path planning, unmanned aerial vehicle, wireless sensors},
	pages = {6851},
}

@article{li_probability_2021,
	title = {Probability learning based tabu search for the budgeted maximum coverage problem},
	volume = {183},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421007399},
	doi = {10.1016/j.eswa.2021.115310},
	abstract = {The Budgeted Maximum Coverage Problem (BMCP) is a general model with a number of real-world applications. Given n elements with nonnegative profits, m subsets of elements with nonnegative weights and a total budget, the BMCP aims to select some subsets such that the total weight of the selected subsets does not exceed the budget, while the total profit of the associated elements is maximized. BMCP is NP-hard and thus computationally challenging. We investigate for the first time an effective practical algorithm for solving this problem, which combines reinforcement learning and local search. The algorithm iterates through two distinct phases, namely a tabu search phase and a probability learning based perturbation phase. To assess the effectiveness of the proposed algorithm, we show computational results on a set of 30 benchmark instances introduced in this paper and present comparative studies with respect to the approximation algorithm, the genetic algorithm and the CPLEX solver.},
	language = {en},
	urldate = {2021-11-13},
	journal = {Expert Systems with Applications},
	author = {Li, Liwen and Wei, Zequn and Hao, Jin-Kao and He, Kun},
	month = nov,
	year = {2021},
	keywords = {Budgeted maximum coverage problem, Combinatorial optimization, Learning-based optimization, Tabu search},
	pages = {115310},
}

@article{pehlivanoglu_enhanced_2021,
	title = {An enhanced genetic algorithm for path planning of autonomous {UAV} in target coverage problems},
	volume = {112},
	issn = {1568-4946},
	doi = {10.1016/j.asoc.2021.107796},
	abstract = {Unmanned aerial vehicles (UAV) have become an important and integral part of military and civilian operations in recent years. In many UAV missions, the main purpose is to visit some predetermined checkpoints in operational space. If the number of checkpoints and constraints increases, finding a feasible solution may take up too much time. In this paper; the path planning problem of autonomous UAV in target coverage problems is solved by using artificial intelligent methods including genetic algorithm (GA), ant colony optimizer (ACO), Voronoi diagram, and clustering methods. The main contribution of this article is to propose initial population enhancement methods in GA, and thus accelerate convergence process. The first common enhancement to basic GA structure is to generate a sub-optimal path by implementing ACO. A sub-optimal path can be used to generate initial individuals. However, sub-optimal paths may have the problem that is collision with terrain. To avoid a UAV from any crash three approaches are integrated into an initial population phase of genetic algorithm. The first approach includes Voronoi vertices as additional waypoints to keep clear of trouble. The second approach consists of cluster centers which forms Voronoi vertices as supplemental waypoints. The final proposal comprises again cluster centers but based on a set of collision points. The proposed methods are tested in different three dimensional (3D) environments and the results are compared. Performance results show that collision with terrain surface is a local phenomenon and solving this issue by using the cluster center of collision points provides the best result including at least 70\% or much more decrease in the required number of objective function evaluations. © 2021 Elsevier B.V.},
	language = {English},
	journal = {Applied Soft Computing},
	author = {Pehlivanoglu, Y.V. and Pehlivanoglu, P.},
	year = {2021},
	keywords = {Autonomous UAV, Genetic algorithm, Path planning, Target coverage},
}

@inproceedings{cui_intelligent_2010,
	title = {Intelligent path planning in {3D} scene},
	volume = {3},
	doi = {10.1109/ICCASM.2010.5620400},
	abstract = {This paper describes an improved version of D* algorithm and discusses how to apply this algorithm to intelligent path planning for virtual humans in complex and dynamic three-dimensional scene. Based on dividing the search space with navigation meshes, the improved D* algorithm can quickly calculate the optimal path from an starting position to the target position, and then guides virtual humans in walking in a dynamic 3D scene even though part of environmental information is unknown. At the same time, taking topographic differences into account, terrain costs are introduced into the heuristic function of D* algorithm, in order to get closer to the real effect of path finding. Feasibility and effectiveness of this algorithm are demonstrated by experiments.},
	booktitle = {2010 {International} {Conference} on {Computer} {Application} and {System} {Modeling} ({ICCASM} 2010)},
	author = {Cui, Yao and Qin, Guofeng},
	month = oct,
	year = {2010},
	keywords = {3D scene, D* algorithm, Pipelines, navigation mesh, path planning, real-time},
	pages = {V3--579--V3--583},
}

@inproceedings{abdallah_hybrid_2006,
	title = {Hybrid {Position}-{Based} {3D} {Routing} {Algorithms} with {Partial} {Flooding}},
	doi = {10.1109/CCECE.2006.277328},
	abstract = {Position-based routing algorithms use the location information to reduce routing overhead in mobile ad-hoc networks. In this paper we propose two position-based routing algorithms which combine progress-based routing with restricted directional flooding-based routing algorithms for routing in 3 dimensional environments (3D). The first algorithm 3D\_ABLAR(m) chooses m neighbors according to a space-partition heuristic and forwards the message to all these nodes. The second algorithm (C, G)-3D\_ABLAR(m)-(C, G) uses progress-based routing until a local minimum is reached. The algorithm then switches to the first algorithm for one step and then progress-based routing is resumed. We evaluate our algorithm and compare it with current routing algorithms. The simulation results show a significant improvement in delivery rate (99\% compared to 63\%) and reduction in traffic (up to 50\%)},
	booktitle = {2006 {Canadian} {Conference} on {Electrical} and {Computer} {Engineering}},
	author = {Abdallah, A.E. and Fevens, T. and Opatrny, J.},
	month = may,
	year = {2006},
	keywords = {Ad hoc networks, Computer science, Floods, Mobile computing, Partitioning algorithms, Position-based routing, Routing, Software algorithms, Software engineering, Switches, Traffic control, ad-hoc networks, directional flooding},
	pages = {227--230},
}

@inproceedings{xu_ant_2014,
	title = {Ant system based {3D} fixed-outline floor planning},
	doi = {10.1109/ICSICT.2014.7021260},
	abstract = {The three dimension integrated circuit (3D IC) can alleviate the interconnect issue in the nanoscale era, and is promising for heterogeneous integration. In this paper, we propose a two-phase method, combining ant system algorithm (AS) and simulated annealing to handle the 3D IC floorplanning with fixed-outline constraints. We propose a floorplan construction method for AS, where blocks are packed one by one, and the partitioned sequence pair is used to represent 3D IC floorplans. During the packing procedure, we have to make two decisions: (1) selecting a block to pack and (2) finding a proper position for the selected block in the constructed partial floorplan. The AS is used to explore the appropriate orders for packing blocks. When packing a block, the best position in the partial constructed floorplan will be selected. In the AS, we use the area of a block and the connection degree between blocks to represent the heuristic information, and also modify the pheromone update rule to adapt to the 3D IC floorplanning problem. The experimental results show the effectiveness of the proposed method.},
	booktitle = {2014 12th {IEEE} {International} {Conference} on {Solid}-{State} and {Integrated} {Circuit} {Technology} ({ICSICT})},
	author = {Xu, Qi and Chen, Song and Li, Bin},
	month = oct,
	year = {2014},
	keywords = {Abstracts, Through-silicon vias, Wires},
	pages = {1--3},
}

@misc{noauthor_performance_nodate,
	title = {Performance of {Location} and {Positioning} {Systems}: a {3D}-{Ultrasonic} {System} {Case}},
	shorttitle = {Performance of {Location} and {Positioning} {Systems}},
	url = {https://astesj.com/v03/i02/p13/},
	language = {en-US},
	urldate = {2021-11-11},
	note = {Publication Title: Journal},
}

@article{islambouli_optimized_2019,
	title = {Optimized {3D} {Deployment} of {UAV}-{Mounted} {Cloudlets} to {Support} {Latency}-{Sensitive} {Services} in {IoT} {Networks}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2956150},
	abstract = {The paradigm of Internet of Things (IoT) is transforming physical environments into smart and interactive platforms to offer a wide range of innovative services supported by the evolution towards 5G networks. A major class of emerging services relies on highly intensive computations to make real-time decisions with ultra-low latency. Edge computing has been established as an effective approach to reduce the latency overhead of cloud computing and effectively augment the computational capabilities of IoT devices. In this work, we leverage the mobility and agility of Unmanned Aerial Vehicles (UAVs) as mobile edge servers or cloudlets to offer computation offloading opportunities to IoT devices. In particular, we consider the joint problem of optimizing the number and positions of deployed UAV cloudlets in 3D space and task offloading decisions with cooperation among UAVs, in order to provision IoT services with strict latency requirements. We formulate the problem as a mixed integer program, and propose an efficient meta-heuristic solution based on the ions motion optimization algorithm. The performance of the meta-heuristic solution is evaluated and compared to the optimal solution as a function of various system parameters and for different application use cases. It is shown to achieve near-optimal performance with low complexity and, thus, can efficiently scale up to large IoT network scenarios. © 2013 IEEE.},
	language = {English},
	journal = {IEEE Access},
	author = {Islambouli, R. and Sharafeddine, S.},
	year = {2019},
	keywords = {3D deployment, Edge computing, IoT network, unmanned aerial vehicles},
	pages = {172860--172870},
}

@inproceedings{veronese_accurate_2018,
	title = {An {Accurate} and {Computational} {Efficient} {System} for {Detecting} and {Classifying} {Ego} and {Sides} {Lanes} {Using} {LiDAR}},
	doi = {10.1109/IVS.2018.8500434},
	abstract = {this work, we are proposing a computationally efficient LiDAR based lane detection system that detects both ego and side lanes using 3D LiDARs. Our solution relies on the construction of local gird map around the ego vehicle using the infrared reflectance of combination of LiDARs. To fuse the information of the LiDARs into a map, the vehicle ego-motion is taken into account. The system is built using image processing by binarizing the map to extract the lane markers. The evaluation of computational performance of the final solution is realized on a single ARM core of the NVIDIA Drive PX2 without the need for the GPUs, and achieved a frame rate of 40 Hz. In the absence of a publicly available annotated dataset for LiDAR based lane detection, we evaluate the proposed solution against our proprietary camera based lane detection system. We observed a good correlation between the two in terms of Jaccard and Dice Coefficients.},
	booktitle = {2018 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Veronese, Lucas de Paula and Ismail, Asad and Narayan, Vikram and Schulze, Mathias},
	month = jun,
	year = {2018},
	keywords = {Automobiles, Cameras, Laser radar, Meters, Roads, Three-dimensional displays, Thresholding (Imaging)},
	pages = {1476--1483},
}

@article{pereira_self_2016,
	title = {Self calibration of multiple {LIDARs} and cameras on autonomous vehicles},
	volume = {83},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889016300173},
	doi = {10.1016/j.robot.2016.05.010},
	abstract = {Autonomous navigation is an important field of research and, given the complexity of real world environments, most of the systems rely on a complex perception system combining multiple sensors on board, which reinforces the concern of sensor calibration. Most calibration methods rely on manual or semi-automatic interactive procedures, but reliable fully automatic methods are still missing. However, if some simple objects could be detected and identified automatically by all the sensors from several points of view, then automatic calibration would be possible on the fly. The idea proposed in this paper is to use a ball in motion in front of a set of uncalibrated sensors allowing them to detect its center along the successive positions. This set of centers generates a point cloud per sensor, which, by using segmentation and fitting techniques, allows the calculation of the rigid body transformation among all pairs of sensors. This paper proposes and describes such a method with results demonstrating its validity.},
	language = {en},
	urldate = {2021-11-10},
	journal = {Robotics and Autonomous Systems},
	author = {Pereira, Marcelo and Silva, David and Santos, Vitor and Dias, Paulo},
	month = sep,
	year = {2016},
	keywords = {3D data fitting, Extrinsic calibration, Point cloud},
	pages = {326--337},
}

@inproceedings{na_fusion_2014,
	title = {Fusion of multiple {2D} {LiDAR} and {RADAR} for object detection and tracking in all directions},
	doi = {10.1109/ICCVE.2014.7297512},
	abstract = {For autonomous vehicle and ADAS(Advanced Driver Assistance System), it is essential to detect and to track objects within a certain area in realtime. This paper presents the 2D tracker that fuses with four 2D LiDARs and one RADAR. It independently detects objects according to the type of sensors and represents these in the same space. It continuously associates measurements of the same object and tracks these with KF(Kalman Filter) for prediction and JPDAF(Joint Probabilistic Data Association Filter) for update. The result of our multi-sensor fusion tracker is demonstrated with a visualization tool.},
	booktitle = {2014 {International} {Conference} on {Connected} {Vehicles} and {Expo} ({ICCVE})},
	author = {Na, Kiin and Byun, Jaemin and Roh, Myongchan and Seo, Beomsu},
	month = nov,
	year = {2014},
	keywords = {Laser radar, Radar tracking, Sensor fusion, Tracking, Vehicles},
	pages = {1058--1059},
}

@article{zhu_tum-mls-2016_2020,
	title = {{TUM}-{MLS}-2016: {An} {Annotated} {Mobile} {LiDAR} {Dataset} of the {TUM} {City} {Campus} for {Semantic} {Point} {Cloud} {Interpretation} in {Urban} {Areas}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {{TUM}-{MLS}-2016},
	url = {https://www.mdpi.com/2072-4292/12/11/1875},
	doi = {10.3390/rs12111875},
	abstract = {In the past decade, a vast amount of strategies, methods, and algorithms have been developed to explore the semantic interpretation of 3D point clouds for extracting desirable information. To assess the performance of the developed algorithms or methods, public standard benchmark datasets should invariably be introduced and used, which serve as an indicator and ruler in the evaluation and comparison. In this work, we introduce and present large-scale Mobile LiDAR point clouds acquired at the city campus of the Technical University of Munich, which have been manually annotated and can be used for the evaluation of related algorithms and methods for semantic point cloud interpretation. We created three datasets from a measurement campaign conducted in April 2016, including a benchmark dataset for semantic labeling, test data for instance segmentation, and test data for annotated single 360 \&deg; laser scans. These datasets cover an urban area of approximately 1 km long roadways and include more than 40 million annotated points with eight classes of objects labeled. Moreover, experiments were carried out with results from several baseline methods compared and analyzed, revealing the quality of this dataset and its effectiveness when using it for performance evaluation.},
	language = {en},
	number = {11},
	urldate = {2021-11-10},
	journal = {Remote Sensing},
	author = {Zhu, Jingwei and Gehrung, Joachim and Huang, Rong and Borgmann, Björn and Sun, Zhenghao and Hoegner, Ludwig and Hebel, Marcus and Xu, Yusheng and Stilla, Uwe},
	month = jan,
	year = {2020},
	keywords = {MLS point clouds, instance segmentation, semantic labeling},
	pages = {1875},
}

@article{li_nrli-uav_2019,
	title = {{NRLI}-{UAV}: {Non}-rigid registration of sequential raw laser scans and images for low-cost {UAV} {LiDAR} point cloud quality improvement},
	volume = {158},
	issn = {0924-2716},
	shorttitle = {{NRLI}-{UAV}},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271619302473},
	doi = {10.1016/j.isprsjprs.2019.10.009},
	abstract = {Accurate registration of light detection and ranging (LiDAR) point clouds and images is a prerequisite for integrating the spectral and geometrical information collected by low-cost unmanned aerial vehicle (UAV) systems. Most registration approaches take the directly georeferenced LiDAR point cloud as a rigid body, based on the assumption that the high-precision positioning and orientation system (POS) in the LiDAR system provides sufficient precision, and that the POS errors are negligible. However, due to the large errors of the low-precision POSs commonly used in the low-cost UAV LiDAR systems (ULSs), dramatic deformation may exist in the directly georeferenced ULS point cloud, resulting in non-rigid transformation between the images and the deformed ULS point cloud. As a result, registration may fail when using a rigid transformation between the images and the directly georeferenced LiDAR point clouds. To address this problem, we proposed NRLI-UAV, which is a non-rigid registration method for registration of sequential raw laser scans and images collected by low-cost UAV systems. NRLI-UAV is a two-step registration method that exploits trajectory correction and discrepancy minimization between the depths derived from structure from motion (SfM) and the raw laser scans to achieve LiDAR point cloud quality improvement. Firstly, the coarse registration procedure utilizes global navigation satellite system (GNSS) and inertial measurement unit (IMU)-aided SfM to obtain accurate image orientation and corrects the errors of the low-precision POS. Secondly, the fine registration procedure transforms the original 2D-3D registration to 3D-3D registration. This is performed by setting the oriented images as the reference, and iteratively minimizing the discrepancy between the depth maps derived from SfM and the raw laser scans, resulting in accurate registration between the images and the LiDAR point clouds. In addition, an improved LiDAR point cloud is generated in the mapping frame. Experiments were conducted with data collected by a low-cost UAV system in three challenging scenes to evaluate NRLI-UAV. The final registration errors of the images and the LiDAR point cloud are less than one pixel in image space and less than 0.13 m in object space. The LiDAR point cloud quality was also evaluated by plane fitting, and the results show that the LiDAR point cloud quality is improved by 8.8 times from 0.45 m (root-mean-square error [RMSE] of plane fitting) to 0.05 m (RMSE of plane fitting) using NRLI-UAV, demonstrating a high level of automation, robustness, and accuracy.},
	language = {en},
	urldate = {2021-11-10},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Li, Jianping and Yang, Bisheng and Chen, Chi and Habib, Ayman},
	month = dec,
	year = {2019},
	keywords = {Image sequence, Light detection and ranging (LiDAR), Low-cost, Non-rigid registration, Unmanned aerial vehicle (UAV)},
	pages = {123--145},
}

@article{miao_airborne_2021,
	title = {Airborne {LiDAR} {Assisted} {Obstacle} {Recognition} and {Intrusion} {Detection} towards {Unmanned} {Aerial} {Vehicle}: {Architecture}, {Modeling} and {Evaluation}},
	volume = {22},
	issn = {1524-9050},
	shorttitle = {Airborne {LiDAR} {Assisted} {Obstacle} {Recognition} and {Intrusion} {Detection} towards {Unmanned} {Aerial} {Vehicle}},
	doi = {10.1109/TITS.2020.3023189},
	abstract = {With the rapid development of wireless communication and flight control technologies, the unmanned aerial vehicles (UAVs) have been widely used in multiple application scenarios. A typical scenario is massive crowd management of the multi-millions annual Hajj Pilgrimage to Mecca where UAVs are widely utilized to conduct crowd monitoring by carrying sensory devices. The safe flight of a UAV is crucial for ensuring the successful execution of missions. With the aim to overcome the disadvantage caused by the ground station intrusion detection, the combination of UAV and airborne LiDAR has been widely studied in the field of UAV obstacle recognition. This article studies the UAV network architecture under a common scenario and proposes an obstacle recognition and intrusion detection algorithm for UAV based on an airborne LiDAR (ALORID). First, the preprocessing of the data obtained by a LiDAR, i.e., the coordinate conversion of LiDAR data in combination with UAV motion parameters, is completed. Then, the LiDAR data graph at the current moment is generated by the image noisy point filtering algorithm. After that, the improved density-based spatial clustering of applications with noise (DBSCAN) algorithm is used for image clustering of intrusions to obtain the LiDAR time-domain cumulative graph in a certain detection time. Finally, the motion recognition and location detection of each cluster are completed. The experiment results verify the effectiveness of the proposed algorithm in identifying the moving state of the intrusions. © 2000-2011 IEEE.},
	language = {English},
	number = {7},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Miao, Y. and Tang, Y. and Alzahrani, B.A. and Barnawi, A. and Alafif, T. and Hu, L.},
	year = {2021},
	keywords = {Intrusion detection, light detection and ranging, machine learning, obstacle recognition, unmanned aerial vehicle},
	pages = {4531--4540},
}

@article{khan_design_2022,
	title = {Design and experimental validation of a robust model predictive control for the optimal trajectory tracking of a small-scale autonomous bulldozer},
	volume = {147},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889021001883},
	doi = {10.1016/j.robot.2021.103903},
	abstract = {Trajectory tracking of an unmanned ground vehicle (UGV) is essential due to its extensive construction, agriculture, and military applications. In this paper, we propose an efficient, robust model predictive control (RMPC) for the trajectory tracking of a small-scale autonomous bulldozer in the presence of perturbations by unknown but bounded disturbances. The proposed RMPC is designed by considering a linearised tracking error-based model combined with a feed-forward and optimal control action to achieve the proposed trajectory. The presence of a corrective feedback controller as a time-varying finite-time linear quadratic regulator (LQR) suppresses the uncertainties acting on the real system by regulating around the nominal system. Pose estimation, required for control feedback, is based on sensor data fusion performed by an extended Kalman filter (EKF) map-based localiser, which processes inertial measurement unit (IMU) and light detection and ranging (LiDAR) measurements. Experiments are performed using a real robot (Husky A200) to validate the proposed control scheme’s performance. The experimental results show that the proposed controller can safely track target trajectories with low processing time, small tracking errors, and smooth control actions. Finally, the proposed control scheme is compared with related techniques and outperforms them in tracking accuracy.},
	language = {en},
	urldate = {2021-11-10},
	journal = {Robotics and Autonomous Systems},
	author = {Khan, Subhan and Guivant, Jose and Li, Xuesong},
	month = jan,
	year = {2022},
	keywords = {Model predictive control, Optimal control, Small-scale autonomous bulldozer, Trajectory tracking, Unmanned ground vehicle},
	pages = {103903},
}

@article{ravankar_autonomous_2021,
	title = {Autonomous and {Safe} {Navigation} of {Mobile} {Robots} in {Vineyard} with {Smooth} {Collision} {Avoidance}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2077-0472/11/10/954},
	doi = {10.3390/agriculture11100954},
	abstract = {In recent years, autonomous robots have extensively been used to automate several vineyard tasks. Autonomous navigation is an indispensable component of such field robots. Autonomous and safe navigation has been well studied in indoor environments and many algorithms have been proposed. However, unlike structured indoor environments, vineyards pose special challenges for robot navigation. Particularly, safe robot navigation is crucial to avoid damaging the grapes. In this regard, we propose an algorithm that enables autonomous and safe robot navigation in vineyards. The proposed algorithm relies on data from a Lidar sensor and does not require a GPS. In addition, the proposed algorithm can avoid dynamic obstacles in the vineyard while smoothing the robot’s trajectories. The curvature of the trajectories can be controlled, keeping a safe distance from both the crop and the dynamic obstacles. We have tested the algorithm in both a simulation and with robots in an actual vineyard. The results show that the robot can safely navigate the lanes of the vineyard and smoothly avoid dynamic obstacles such as moving people without abruptly stopping or executing sharp turns. The algorithm performs in real-time and can easily be integrated into robots deployed in vineyards.},
	language = {en},
	number = {10},
	urldate = {2021-11-10},
	journal = {Agriculture},
	author = {Ravankar, Abhijeet and Ravankar, Ankit A. and Rawankar, Arpit and Hoshino, Yohei},
	month = oct,
	year = {2021},
	keywords = {autonomous robots, collision avoidance, feature extraction, navigation, vineyard robots},
	pages = {954},
}

@article{gong_mapping_2021,
	title = {Mapping and {Semantic} {Modeling} of {Underground} {Parking} {Lots} {Using} a {Backpack} {LiDAR} {System}},
	volume = {22},
	issn = {1524-9050},
	doi = {10.1109/TITS.2019.2955734},
	abstract = {Presented in this paper is a novel method for the mapping and semantic modeling of an underground parking lot using 3D point clouds collected by a low-cost Backpack Laser Scanning (BLS) or LiDAR system. Our method consists of two parts: a Simultaneous Localization and Mapping (SLAM) algorithm based on Sparse Point Clouds (SPC) and a semantic modeling algorithm based on a modified PointNet model. The main contributions of this paper are as follows: (1) a probability frontend framework for the alignment of point clouds using the local point cloud surface variance as the weight of registration, which modifies registration failure caused by the lack of features in sparse point clouds, (2) a robust submap-based strategy for loop closure detection and back-end optimization under sparse point clouds, and (3) a modified PointNet model for classifying the point clouds of underground parking lots into four categories: ceiling, floor, wall, others. Experimental results show that our SPC-SLAM algorithm achieves centimeter-level accuracy (0.09\% trajectory error rate) after closed loop processing in a Global Navigation Satellite System (GNSS)-denied underground parking lot, and precision of 84.8\% in semantic segmentation. © 2000-2011 IEEE.},
	language = {English},
	number = {2},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Gong, Z. and Li, J. and Luo, Z. and Wen, C. and Wang, C. and Zelek, J.},
	year = {2021},
	keywords = {Backpack LiDAR, SLAM, point cloud, semantic modeling, underground parking lots},
	pages = {734--746},
}

@article{karam_simple_2021,
	title = {Simple loop closing for continuous {6DOF} {LIDAR}\&{IMU} graph {SLAM} with planar features for indoor environments},
	volume = {181},
	issn = {0924-2716},
	doi = {10.1016/j.isprsjprs.2021.09.020},
	abstract = {Simultaneous localization and mapping (SLAM) is the essential technique in mapping environments that are denied to the global navigation satellite systems (GNSSs), such as indoor spaces. In this article, we present a loop-closing continuous-time LIDAR-IMU SLAM for indoor environments. The design of the proposed SLAM is based on arbitrarily-oriented planar features that allow for point to plane matching for local but also global optimization. Moreover, to update the SLAM graph during the optimization, we propose a simple yet elegant loop closure method in the form of merging the planes together. Representing the SLAM map by planes is advantageous due to the abundant existence of planar structures in indoor built environments. The proposed method was validated on a specific configuration of three 2D LIDAR scanners mounted on a wearable platform (backpack). Scanned point clouds were compared against ones obtained from a commercial mobile mapping system (Viametris iMS3D) and a terrestrial laser scanner (RIEGL VZ-400). The experimental results demonstrate that our SLAM system is capable of mapping multi-storey buildings, staircases, cluttered areas and areas with curved walls. Furthermore, our SLAM system offers comparable performance to that of the commercial system as shown by the low deviation between the point clouds generated by both systems. The majority of the cloud-to-cloud absolute distances – about 92\% – are less than 3 cm. © 2021 The Author(s)},
	language = {English},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Karam, S. and Lehtola, V. and Vosselman, G.},
	year = {2021},
	keywords = {6DOF, Graph SLAM, IMU, Indoor, LIDAR, Laser scanner, Loop closure, Mobile mapping, Planar features, Planar structures hypothesis, Point clouds, Splines},
	pages = {413--426},
}

@book{renslow_manual_2012,
	title = {Manual of {Airborne} {Topographic} {Lidar}},
	isbn = {978-1-57083-097-6},
	language = {en},
	publisher = {American Society for Photogrammetry Remote Sensing},
	author = {Renslow, Michael S.},
	year = {2012},
}

@article{gollob_comparison_2020,
	title = {Comparison of {3D} {Point} {Clouds} {Obtained} by {Terrestrial} {Laser} {Scanning} and {Personal} {Laser} {Scanning} on {Forest} {Inventory} {Sample} {Plots}},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2306-5729/5/4/103},
	doi = {10.3390/data5040103},
	abstract = {In forest inventory, trees are usually measured using handheld instruments; among the most relevant are calipers, inclinometers, ultrasonic devices, and laser range finders. Traditional forest inventory has been redesigned since modern laser scanner technology became available. Laser scanners generate massive data in the form of 3D point clouds. We have developed a novel methodology to provide estimates of the tree positions, stem diameters, and tree heights from these 3D point clouds. This dataset was made publicly accessible to test new software routines for the automatic measurement of forest trees using laser scanner data. Benchmark studies with performance tests of different algorithms are welcome. The dataset contains co-registered raw 3D point-cloud data collected on 20 forest inventory sample plots in Austria. The data were collected by two different laser scanning systems: (1) A mobile personal laser scanner (PLS) (ZEB Horizon, GeoSLAM Ltd., Nottingham, UK) and (2) a static terrestrial laser scanner (TLS) (Focus3D X330, Faro Technologies Inc., Lake Mary, FL, USA). The data also contain digital terrain models (DTMs), field measurements as reference data (ground-truth), and the output of recent software routines for the automatic tree detection and the automatic stem diameter measurement.},
	language = {en},
	number = {4},
	urldate = {2021-11-09},
	journal = {Data},
	author = {Gollob, Christoph and Ritter, Tim and Nothdurft, Arne},
	month = dec,
	year = {2020},
	keywords = {benchmarking, forest inventory, manual reference data, personal laser scanning data, terrestrial laser scanning data},
	pages = {103},
}

@article{lopez_alfonso_gpu-accelerated_nodate,
	title = {A {GPU}-accelerated framework for simulating {LiDAR} scanning},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {{López, Alfonso} and {Ogayar, Carlos J.} and {Jurado. Juan M.} and {Feito, Francisco R.}},
}

@article{kuutti_survey_2021,
	title = {A {Survey} of {Deep} {Learning} {Applications} to {Autonomous} {Vehicle} {Control}},
	volume = {22},
	issn = {1558-0016},
	doi = {10.1109/TITS.2019.2962338},
	abstract = {Designing a controller for autonomous vehicles capable of providing adequate performance in all driving scenarios is challenging due to the highly complex environment and inability to test the system in the wide variety of scenarios which it may encounter after deployment. However, deep learning methods have shown great promise in not only providing excellent performance for complex and non-linear control problems, but also in generalising previously learned rules to new scenarios. For these reasons, the use of deep learning for vehicle control is becoming increasingly popular. Although important advancements have been achieved in this field, these works have not been fully summarised. This paper surveys a wide range of research works reported in the literature which aim to control a vehicle through deep learning methods. Although there exists overlap between control and perception, the focus of this paper is on vehicle control, rather than the wider perception problem which includes tasks such as semantic segmentation and object detection. The paper identifies the strengths and limitations of available deep learning methods through comparative analysis and discusses the research challenges in terms of computation, architecture selection, goal specification, generalisation, verification and validation, as well as safety. Overall, this survey brings timely and topical information to a rapidly evolving field relevant to intelligent transportation systems.},
	number = {2},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Kuutti, Sampo and Bowden, Richard and Jin, Yaochu and Barber, Phil and Fallah, Saber},
	month = feb,
	year = {2021},
	keywords = {Autonomous vehicles, Deep learning, Machine learning, Neural networks, Reinforcement learning, Sensors, Task analysis, Training, advanced driver assistance, autonomous vehicles, computer vision, intelligent control, neural networks},
	pages = {712--733},
}

@article{bolourian_lidar-equipped_2020,
	title = {{LiDAR}-equipped {UAV} path planning considering potential locations of defects for bridge inspection},
	volume = {117},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580519309355},
	doi = {10.1016/j.autcon.2020.103250},
	abstract = {Conventional visual bridge inspection methods are time consuming and may put the inspector in dangerous situations. Unmanned Aerial Vehicles (UAVs) have been recently introduced as an effective tool for bridge inspection. This paper proposes a 3D path planning method for using a UAV equipped with a Light Detection and Ranging (LiDAR) scanner for bridge inspection. The method integrates a Genetic Algorithm (GA) and A* algorithm to solve the Traveling Salesman Problem (TSP) considering the potential locations of bridge surface defects such as cracks. The objective is minimizing time of flight while achieving maximum visibility. The method provides the potential locations of surface defects to efficiently achieve perpendicular and overlapping views for sampling the viewpoints. Calculating the visibility with respect to the level of criticality leads to giving the priority to covering the areas with higher risk levels. The results reveal that considering overlapping views based on the level of criticality of the zones and perpendicular view for all viewpoints result in accurate and time-efficient data collection.},
	language = {en},
	urldate = {2021-11-09},
	journal = {Automation in Construction},
	author = {Bolourian, Neshat and Hammad, Amin},
	month = sep,
	year = {2020},
	pages = {103250},
}

@article{shariq_revolutionising_2020,
	title = {Revolutionising building inspection techniques to meet large-scale energy demands: {A} review of the state-of-the-art},
	volume = {130},
	issn = {1364-0321},
	shorttitle = {Revolutionising building inspection techniques to meet large-scale energy demands},
	url = {https://www.sciencedirect.com/science/article/pii/S1364032120302707},
	doi = {10.1016/j.rser.2020.109979},
	abstract = {The building sector is responsible for 40\% of the overall energy consumption in the EU. Building defects, such as heat losses, moisture, and air leakages, inevitably causes inefficient space heating or cooling, which accounts considerably towards this high energy consumption and associated greenhouse gas emissions. In order to meet the EU's 2050 carbon reduction targets, building inspection techniques need to be revolutionised. Current methods rely on terrestrial or hand-held infrared thermography (IRT) to detect building defects. However, for a large-scale inspection, these methods are generally labour-intensive, time-consuming, costly and often inefficient. The aim of this paper is to highlight the possibility of integrating various state-of-the-art technologies and computational methods with IRT including drones, photogrammetry and AI. This paper presents a comprehensive review of relevant scientific papers and recent developments in such technologies that can retrofit the existing manually intensive methods. Among the findings of this research, feasibility of monocular thermographic photogrammetry integrated on a drone (quadcopter) promises a time-efficient, cost-effective and near-autonomous solution to large-scale building inspections.},
	language = {en},
	urldate = {2021-11-09},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Shariq, M. Hasan and Hughes, Ben Richard},
	month = sep,
	year = {2020},
	keywords = {Building energy audit, Building inspection, Drone 3D mapping, Infrared thermography, Photogrammetry, Real-time thermal modelling},
	pages = {109979},
}

@article{prins_crop_2021,
	title = {Crop type mapping using {LiDAR}, {Sentinel}-2 and aerial imagery with machine learning algorithms},
	volume = {24},
	issn = {1009-5020},
	url = {https://doi.org/10.1080/10095020.2020.1782776},
	doi = {10.1080/10095020.2020.1782776},
	abstract = {LiDAR data are becoming increasingly available, which has opened up many new applications. One such application is crop type mapping. Accurate crop type maps are critical for monitoring water use, estimating harvests and in precision agriculture. The traditional approach to obtaining maps of cultivated fields is by manually digitizing the fields from satellite or aerial imagery and then assigning crop type labels to each field – often informed by data collected during ground and aerial surveys. However, manual digitizing and labeling is time-consuming, expensive and subject to human error. Automated remote sensing methods is a cost-effective alternative, with machine learning gaining popularity for classifying crop types. This study evaluated the use of LiDAR data, Sentinel-2 imagery, aerial imagery and machine learning for differentiating five crop types in an intensively cultivated area. Different combinations of the three datasets were evaluated along with ten machine learning. The classification results were interpreted by comparing overall accuracies, kappa, standard deviation and f-score. It was found that LiDAR data successfully differentiated between different crop types, with XGBoost providing the highest overall accuracy of 87.8\%. Furthermore, the crop type maps produced using the LiDAR data were in general agreement with those obtained by using Sentinel-2 data, with LiDAR obtaining a mean overall accuracy of 84.3\% and Sentinel-2 a mean overall accuracy of 83.6\%. However, the combination of all three datasets proved to be the most effective at differentiating between the crop types, with RF providing the highest overall accuracy of 94.4\%. These findings provide a foundation for selecting the appropriate combination of remotely sensed data sources and machine learning algorithms for operational crop type mapping.},
	number = {2},
	urldate = {2021-11-09},
	journal = {Geo-spatial Information Science},
	author = {Prins, Adriaan Jacobus and Van Niekerk, Adriaan},
	month = apr,
	year = {2021},
	keywords = {LiDAR, crop type classification, machine learning, multispectral imagery, per-pixel classification, sentinel-2},
	pages = {215--227},
}

@article{el-naggar_use_2021,
	title = {The use of terrestrial {LiDAR} to monitor crop growth and account for within-field variability of crop coefficients and water use},
	volume = {190},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169921004336},
	doi = {10.1016/j.compag.2021.106416},
	abstract = {Monitoring the spatio-temporal distribution of crop height and biomass is important for crop management in terms of applying irrigation, fertilizers and pesticides. This paper reports the performance of a terrestrial laser scanner (TLS) for measuring crop height and biomass of a bean crop (Phaseolus vulgaris., cv. ‘Contender’); and crop height for barley (Hordeum vulgare., cv. ‘Carfields CKS1′), and pea (Pisum sativum., cv. ‘Massey’) crops, grown under field conditions. In addition, a method is presented that improves spatio-temporal estimates of crop coefficient (Kc) and crop evapotranspiration (ETc) using TLS data combined with NDVI data (“TLVI method”), compared with standard FAO56 model estimates. Evidence is provided to show that the TLVI method gives accurate estimates of crop growth patterns due to spatial variations in soil, microclimate and plant health, providing a valuable tool for precision crop management. The TLS-derived canopy height method showed a significantly high correlation and low biases and errors with the manually measured canopy height (R2 = o.95, RMSE = 5.85 for barley; R2 = 0.93, RMSE = 3.01 for pea; R2 = 0.91, RMSE = 1.82 for bean). The TLS method also showed potential to estimate bean biomass (R2 = 0.70, RMSE = 37.56). In-field sensor measurements of canopy height and NDVI were used to estimate site specific crop factors required for accurate estimation of daily evapotranspiration losses. These estimates showed general agreement with those predicted by standard dual FAO56 crop coefficients, but in addition were able to detect the spatial variability in crop growth and water requirements which were largely controlled by soil differences. Overall, the results showed that the TLS method has the potential to measure crop height surface and that the TLVI method provides estimates of ETc with a high spatio-temporal resolution for different crops. This approach can be considered a very promising tool for site-specific management.},
	language = {en},
	urldate = {2021-11-09},
	journal = {Computers and Electronics in Agriculture},
	author = {El-Naggar, A. G. and Jolly, B. and Hedley, C. B. and Horne, D. and Roudier, P. and Clothier, B. E.},
	month = nov,
	year = {2021},
	keywords = {Biomass, Crop evapotranspiration, Crop height, Precision agriculture, Spatio-temporal data},
	pages = {106416},
}

@article{mitasova_geospatial_2010,
	title = {Geospatial analysis of vulnerable beach-foredune systems from decadal time series of lidar data},
	volume = {14},
	issn = {1874-7841},
	url = {https://doi.org/10.1007/s11852-010-0088-1},
	doi = {10.1007/s11852-010-0088-1},
	abstract = {Time series of lidar data, acquired over the past decade along the North American East Coast, provide opportunities to gain new insights into 3D evolution of barrier islands and their beach and dune systems. GIS-based per grid cell statistics and map algebra was applied to time series of Digital Surface Models representing two sections of North Carolina barrier islands to quantify elevation change trends, map dynamic and stable locations, identify new and lost buildings, measure relative volume evolution in the beach and foredune systems and analyze shoreline dynamics. Results show a relatively small stable core in both study areas, with beaches and the ocean side of the dunes exhibiting systematic high rates of elevation loss while areas landward from the dunes increase slightly in elevation. Significant number of new homes have been built at locations with very small core surface elevation, and homes built within the shoreline dynamics band have already been lost. The raster-based methodology used in this study can be applied to perform similar analyses in other coastal areas where time series of lidar data are available.},
	language = {en},
	number = {3},
	urldate = {2021-11-09},
	journal = {Journal of Coastal Conservation},
	author = {Mitasova, Helena and Hardin, Eric and Overton, Margery F. and Kurum, Mustafa Onur},
	month = sep,
	year = {2010},
	pages = {161--172},
}

@article{guisado-pintado_3d_2019,
	title = {{3D} mapping efficacy of a drone and terrestrial laser scanner over a temperate beach-dune zone},
	volume = {328},
	issn = {0169-555X},
	url = {https://www.sciencedirect.com/science/article/pii/S0169555X1830504X},
	doi = {10.1016/j.geomorph.2018.12.013},
	abstract = {Understanding dynamic earth surface processes requires various spatial and temporal information to help produce patterns of landform change. Recent developments in sensor technology such as Structure from Motion (SfM), camera-mounted airborne Unmanned Aerial Vehicles (UAVs) and Terrestrial Laser Scanning (TLS) have provided a means of acquiring high-resolution spatial data on land surface topography. Through repeat surveys, these techniques enable much better understanding of what is termed ‘geomorphometry’, where we can examine a geomorphic surface for change over space and time. In coastal environments, change can involve significant alteration and generation of landforms over relatively short periods and, therefore, we require a means of measuring surface morphology quickly and over large areas. Here, we examine a section of a beach-dune system in NW Ireland using SfM-UAV and TLS plus baseline dGPS data points to assess the value of these techniques and to understand their effectiveness (and limitations). Issues such as accuracy, resolution and differences of Digital Elevation Models (DEMs) are assessed for their efficiency, associated challenges and relative performance over variations in terrain types and analytical approaches. We also examine the implications for differences in areal and volume calculations of the coastal landforms using both approaches. We find that sensor performance is highly dependent on the terrain being measured, with undulations, slope, vegetation cover, acquisition resolution (point density) and interpolation issues all having pronounced impacts on effectiveness and data quality. In general, the TLS performed better over flatter, low-angled topography containing sparse/non-vegetated areas than in areas with complex landforms where survey shadows appear to compromise final DEMs. The SfM-UAV shows good performance over different terrains with the exception of relatively flat, featureless areas such as sandy beaches and densely-vegetated surfaces where differences between techniques are {\textbackslash}textgreater1 m. Data acquisition however is much (×30) faster using a SfM-UAV with more extensive survey areas covered than using a TLS.},
	language = {en},
	urldate = {2021-11-09},
	journal = {Geomorphology},
	author = {Guisado-Pintado, Emilia and Jackson, Derek W. T. and Rogers, David},
	month = mar,
	year = {2019},
	keywords = {Beach morphologies, DEMs, Structure from Motion, TLS, Temperate regions, UAV},
	pages = {157--172},
}

@article{starek_viewshed_2020,
	title = {Viewshed simulation and optimization for digital terrain modelling with terrestrial laser scanning},
	volume = {41},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431161.2020.1752952},
	doi = {10.1080/01431161.2020.1752952},
	abstract = {The main objective of this study is the development of a simulation and optimization method for wide-area terrain mapping with terrestrial laser scanning (TLS). The problem can be stated as follows: given a prior digital surface model (DSM) of a region of interest (e.g. from airborne lidar or structure-from-motion photogrammetry), determine the minimum number of scan locations required to seamlessly scan the terrain in the region for a given scanner range and angular field-of-view (FOV). An optimization method for measurement setup is developed using multiple viewshed analysis and simulated annealing (SA) constrained by the system performance characteristics and survey specifications. The method is evaluated at a sediment and erosion control facility with hilly terrain by comparing random scan locations versus optimized three to six scan locations. Statistical results illustrate that average visibility for random sampling increases gradually with scan locations. However, random sampling clearly underperforms in terms of scan visibility relative to five or six optimized scan locations with an average visibility of 100\%. Similar patterns in optimized scan locations demonstrate that certain terrain morphometry at the study site is an essential factor for TLS survey design. Finally, an optimized solution is compared to a brute-force manual solution for determining four scan locations for conducting surveys at the study site. Results show the effectiveness of the optimization method for selecting combinations of scan locations that enable more efficient TLS survey coverage over a wider terrain area compared to manual selection. Furthermore, results demonstrate the adaptability of the method to take into consideration different scan parameters and survey conditions, such as pre-determined scan locations that may be required (e.g. a survey control monument).},
	number = {16},
	urldate = {2021-11-09},
	journal = {International Journal of Remote Sensing},
	author = {Starek, Michael J. and Chu, Tianxing and Mitasova, Helena and Harmon, Russell S.},
	month = aug,
	year = {2020},
	pages = {6409--6426},
}

@article{sehgal_lidar-monocular_2019,
	title = {Lidar-{Monocular} {Visual} {Odometry} with {Genetic} {Algorithm} for {Parameter} {Optimization}},
	url = {http://arxiv.org/abs/1903.02046},
	abstract = {Lidar-Monocular Visual Odometry (LIMO), a odometry estimation algorithm, combines camera and LIght Detection And Ranging sensor (LIDAR) for visual localization by tracking camera features as well as features from LIDAR measurements, and it estimates the motion using Bundle Adjustment based on robust key frames. For rejecting the outliers, LIMO uses semantic labelling and weights of the vegetation landmarks. A drawback of LIMO as well as many other odometry estimation algorithms is that it has many parameters that need to be manually adjusted according to the dynamic changes in the environment in order to decrease the translational errors. In this paper, we present and argue the use of Genetic Algorithm to optimize parameters with reference to LIMO and maximize LIMO’s localization and motion estimation performance. We evaluate our approach on the well known KITTI odometry dataset and show that the genetic algorithm helps LIMO to reduce translation error in different datasets.},
	language = {en},
	urldate = {2021-11-08},
	journal = {arXiv:1903.02046 [cs]},
	author = {Sehgal, Adarsh and Singandhupe, Ashutosh and La, Hung Manh and Tavakkoli, Alireza and Louis, Sushil J.},
	month = mar,
	year = {2019},
	keywords = {Computer Science - Robotics},
}

@inproceedings{tulldahl_lidar-based_2019,
	title = {Lidar-based positioning in forest environments},
	volume = {11160},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11160/1116005/Lidar-based-positioning-in-forest-environments/10.1117/12.2533299.full},
	doi = {10.1117/12.2533299},
	abstract = {Small high-resolution lidar systems can be used in a broad range of applications such as object detection, foliage penetration, and positioning. In this study, a scanning lidar was used together with two visual cameras and a low-cost inertial measurement unit to obtain precise positioning in forest environments. Position accuracy better than 0.05 \% of the traversed path was obtained with the system. The visual cameras and the inertial measurement unit were used to estimate an approximate trajectory and the lidar data were applied to refine the positioning using high level and low level features extracted from the lidar data. Low level features were characterized by planes and sections of the tree stems, and high level features by whole trees. The system was able to operate without support from satellite navigation data or other positioning support. The results can be applied for navigation in forest environments e.g. for small unmanned aerial vehicles or ground vehicles.},
	urldate = {2021-11-08},
	booktitle = {Electro-{Optical} {Remote} {Sensing} {XIII}},
	publisher = {SPIE},
	author = {Tulldahl, Michael and Rydell, Joakim and Holmgren, Johan and Nordlöf, Jonas and Willén, Erik},
	month = oct,
	year = {2019},
	pages = {32--47},
}

@phdthesis{vaquero_gomez_lidar-based_2020,
	type = {Ph.{D}. {Thesis}},
	title = {Lidar-based scene understanding for autonomous driving using deep learning},
	copyright = {Open Access},
	url = {http://www.tdx.cat/handle/10803/671062},
	abstract = {With over 1.35 million fatalities related to traffic accidents worldwide, autonomous driving was foreseen at the beginning of this century as a feasible solution to improve security in our roads. Nevertheless, it is meant to disrupt our transportation paradigm, allowing to reduce congestion, pollution, and costs, while increasing the accessibility, efficiency, and reliability of the transportation for both people and goods. Although some advances have gradually been transferred into commercial vehicles in the way of Advanced Driving Assistance Systems (ADAS) such as adaptive cruise control, blind spot detection or automatic parking, however, the technology is far from mature. A full understanding of the scene is actually needed so that allowing the vehicles to be aware of the surroundings, knowing the existing elements of the scene, as well as their motion, intentions and interactions. In this PhD dissertation, we explore new approaches for understanding driving scenes from 3D LiDAR point clouds by using Deep Learning methods. To this end, in Part I we analyze the scene from a static perspective using independent frames to detect the neighboring vehicles. Next, in Part II we develop new ways for understanding the dynamics of the scene. Finally, in Part III we apply all the developed methods to accomplish higher level challenges such as segmenting moving obstacles while obtaining their rigid motion vector over the ground. More specifically, in Chapter 2 we develop a 3D vehicle detection pipeline based on a multi-branch deep-learning architecture and propose a Front (FR-V) and a Bird’s Eye view (BE-V) as 2D representations of the 3D point cloud to serve as input for training our models. Later on, in Chapter 3 we apply and further test this method on two real uses-cases, for pre-filtering moving obstacles while creating maps to better localize ourselves on subsequent days, as well as for vehicle tracking. From the dynamic perspective, in Chapter 4 we learn from the 3D point cloud a novel dynamic feature that resembles optical flow from RGB images. For that, we develop a new approach to leverage RGB optical flow as pseudo ground truth for training purposes but allowing the use of only 3D LiDAR data at inference time. Additionally, in Chapter 5 we explore the benefits of combining classification and regression learning problems to face the optical flow estimation task in a joint coarse-and-fine manner. Lastly, in Chapter 6 we gather the previous methods and demonstrate that with these independent tasks we can guide the learning of higher challenging problems such as segmentation and motion estimation of moving vehicles from our own moving perspective.},
	language = {eng},
	urldate = {2021-11-08},
	author = {Vaquero Gómez, Víctor},
	month = feb,
	year = {2020},
	keywords = {004, 68, Deep learning, Optical flow, Point clouds, Scene understanding, Vehicle detection, Àrees temàtiques de la UPC::Enginyeria de la telecomunicació},
}

@article{kim_placement_2020,
	title = {Placement {Optimization} of {Multiple} {Lidar} {Sensors} for {Autonomous} {Vehicles}},
	volume = {21},
	issn = {1558-0016},
	doi = {10.1109/TITS.2019.2915087},
	abstract = {The problem with lidar placement is finding a lidar position that reduces the dead zone and improves the point cloud resolution. A lidar placement method is thus proposed for the multiple lidar system of autonomous vehicles to resolve this problem. With the introduction of the lidar occupancy grid, the problem is formulated as an optimization problem. A genetic algorithm is subsequently applied to solve the optimization problem. The experimental results using commercial lidars are subsequently presented to show the usefulness of the proposed method.},
	number = {5},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Kim, Tae-Hyeong and Park, Tae-Hyoung},
	month = may,
	year = {2020},
	keywords = {Autonomous vehicle, Autonomous vehicles, Biological cells, Laser beams, Laser radar, Measurement by laser beam, Optimization, Sensors, genetic algorithm, lidar occupancy grid, lidar placement, optimization problem},
	pages = {2139--2145},
}

@inproceedings{wang_hybrid_2010,
	title = {A hybrid genetic algorithm for {3D} bin packing problems},
	doi = {10.1109/BICTA.2010.5645211},
	abstract = {3D bin packing problem has attracted the wide concern from OR community due to their generalization in many realistic applications. In this paper, a hybrid genetic algorithm is proposed for 3DBPP. Within the framework of the proposed algorithm, a special diploid representation scheme of individual is designed and the heuristic packing methods, which are derived from a deepest bottom left with fill packing method, are employed to accomplish the translation from the individual to a solution. In order to further improve the performance of algorithm, several special GA operations are also designed in this paper. An experimental study over a set of 3DBPP test problems shows that the proposed algorithm is efficient and adaptable to address 3DBPP.},
	booktitle = {2010 {IEEE} {Fifth} {International} {Conference} on {Bio}-{Inspired} {Computing}: {Theories} and {Applications} ({BIC}-{TA})},
	author = {Wang, Hongfeng and Chen, Yanjie},
	month = sep,
	year = {2010},
	pages = {703--707},
}

@article{chen_indoor_2018,
	title = {Indoor high precision three-dimensional positioning system based on visible light communication using modified genetic algorithm},
	volume = {413},
	issn = {0030-4018},
	url = {https://www.sciencedirect.com/science/article/pii/S0030401817311665},
	doi = {10.1016/j.optcom.2017.12.045},
	abstract = {To improve the precision of indoor positioning and actualize three-dimensional positioning, a reversed indoor positioning system based on visible light communication (VLC) using genetic algorithm (GA) is proposed. In order to solve the problem of interference between signal sources, CDMA modulation is used. Each light-emitting diode (LED) in the system broadcasts a unique identity (ID) code using CDMA modulation. Receiver receives mixed signal from every LED reference point, by the orthogonality of spreading code in CDMA modulation, ID information and intensity attenuation information from every LED can be obtained. According to positioning principle of received signal strength (RSS), the coordinate of the receiver can be determined. Due to system noise and imperfection of device utilized in the system, distance between receiver and transmitters will deviate from the real value resulting in positioning error. By introducing error correction factors to global parallel search of genetic algorithm, coordinates of the receiver in three-dimensional space can be determined precisely. Both simulation results and experimental results show that in practical application scenarios, the proposed positioning system can realize high precision positioning service.},
	language = {en},
	urldate = {2021-09-27},
	journal = {Optics Communications},
	author = {Chen, Hao and Guan, Weipeng and Li, Simin and Wu, Yuxiang},
	month = apr,
	year = {2018},
	keywords = {Code division multiple access (CDMA), Genetic algorithm (GA), Indoor positioning system (IPS), Received signal strength (RSS), Visible light communication (VLC)},
	pages = {103--120},
}

@article{aissaoui_designing_2018,
	title = {Designing a camera placement assistance system for human motion capture based on a guided genetic algorithm},
	volume = {22},
	issn = {1359-4338, 1434-9957},
	url = {http://link.springer.com/10.1007/s10055-017-0310-7},
	doi = {10.1007/s10055-017-0310-7},
	abstract = {In multi-camera motion capture systems, determining the optimal camera conﬁguration (camera positions and orientations) is still an unresolved problem. At present, conﬁgurations are primarily guided by a human operator’s intuition, which requires expertise and experience, especially with complex, cluttered scenes. In this paper, we propose a solution to automate camera placement for motion capture applications in order to assist a human operator. Our solution is based on the use of a guided genetic algorithm to optimize camera network placement with an appropriate number of cameras. In order to improve the performance of the genetic algorithm (GA), two techniques are described. The ﬁrst is a distribution and estimation technique, which reduces the search space and generates camera positions for the initial GA population. The second technique is an error metric, which is integrated at GA evaluation level as an optimization function to evaluate the quality of the camera placement in a camera network. Simulation experiments show that our approach is more efﬁcient than other approaches in terms of computation time and quality of the ﬁnal camera network.},
	language = {en},
	number = {1},
	urldate = {2021-09-27},
	journal = {Virtual Reality},
	author = {Aissaoui, Azeddine and Ouafi, Abdelkrim and Pudlo, Philippe and Gillet, Christophe and Baarir, Zine-Eddine and Taleb-Ahmed, Abdelmalik},
	month = mar,
	year = {2018},
	pages = {13--23},
}

@article{ray_genetic_2002,
	title = {A genetic algorithm-based approach to calculate the optimal configuration of ultrasonic sensors in a {3D} position estimation system},
	volume = {41},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889002002920},
	doi = {10.1016/S0921-8890(02)00292-0},
	abstract = {This paper provides a genetic algorithm-based approach to calculate the optimal placement of receivers in a novel 3D position estimation system that uses a single transmitter and multiple receivers. The novelty in the system is the use of the difference in the times of arrival (TOAs) of an ultrasonic wave from the transmitter to the different receivers fixed in 3D space. This is a different approach to traditional systems that use the actual times of flight (TOFs) from the transmitter to the different receivers and triangulate the position of the transmitter. The new approach makes the system more accurate, makes the transmitter independent of the receivers and does not require the need of calculating the time delay term that is inherent in traditional systems due to delays caused by the electronic circuitry. This paper presents a thorough analysis of receiver configurations in the 2D and 3D systems that lead to singularities, i.e. locations of receivers that lead to formulations that cannot be solved due to a shortage of information. It provides guidelines of where not to place receivers so as to get a robust system, and further, presents a detailed analysis of locations that are optimal, i.e. locations that lead to the most accurate estimation of the transmitter positions. The results presented in this paper are not only applicable to ultrasonic systems but all systems that use wave theory, e.g. infrared, laser, etc. This work finds applications in virtual reality cells, robotics, guidance of indoor autonomous vehicles and vibration analysis.},
	language = {en},
	number = {4},
	urldate = {2021-09-27},
	journal = {Robotics and Autonomous Systems},
	author = {Ray, Probir Kumar and Mahajan, Ajay},
	month = dec,
	year = {2002},
	keywords = {3D positioning, Genetic algorithms, Optimal configuration, Ultrasonics},
	pages = {165--177},
}

@article{kattenborn_review_2021,
	title = {Review on {Convolutional} {Neural} {Networks} ({CNN}) in vegetation remote sensing},
	volume = {173},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271620303488},
	doi = {10.1016/j.isprsjprs.2020.12.010},
	abstract = {Identifying and characterizing vascular plants in time and space is required in various disciplines, e.g. in forestry, conservation and agriculture. Remote sensing emerged as a key technology revealing both spatial and temporal vegetation patterns. Harnessing the ever growing streams of remote sensing data for the increasing demands on vegetation assessments and monitoring requires efficient, accurate and flexible methods for data analysis. In this respect, the use of deep learning methods is trend-setting, enabling high predictive accuracy, while learning the relevant data features independently in an end-to-end fashion. Very recently, a series of studies have demonstrated that the deep learning method of Convolutional Neural Networks (CNN) is very effective to represent spatial patterns enabling to extract a wide array of vegetation properties from remote sensing imagery. This review introduces the principles of CNN and distils why they are particularly suitable for vegetation remote sensing. The main part synthesizes current trends and developments, including considerations about spectral resolution, spatial grain, different sensors types, modes of reference data generation, sources of existing reference data, as well as CNN approaches and architectures. The literature review showed that CNN can be applied to various problems, including the detection of individual plants or the pixel-wise segmentation of vegetation classes, while numerous studies have evinced that CNN outperform shallow machine learning methods. Several studies suggest that the ability of CNN to exploit spatial patterns particularly facilitates the value of very high spatial resolution data. The modularity in the common deep learning frameworks allows a high flexibility for the adaptation of architectures, whereby especially multi-modal or multi-temporal applications can benefit. An increasing availability of techniques for visualizing features learned by CNNs will not only contribute to interpret but to learn from such models and improve our understanding of remotely sensed signals of vegetation. Although CNN has not been around for long, it seems obvious that they will usher in a new era of vegetation remote sensing.},
	language = {en},
	urldate = {2023-01-23},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Kattenborn, Teja and Leitloff, Jens and Schiefer, Felix and Hinz, Stefan},
	month = mar,
	year = {2021},
	keywords = {Convolutional Neural Networks (CNN), Deep learning, Earth observation, Plants, Remote sensing, Vegetation},
	pages = {24--49},
}

@book{bandalos_measurement_2018,
	title = {Measurement {Theory} and {Applications} for the {Social} {Sciences}},
	isbn = {978-1-4625-3213-1},
	abstract = {Which types of validity evidence should be considered when determining whether a scale is appropriate for a given measurement situation? What about reliability evidence? Using clear explanations illustrated by examples from across the social and behavioral sciences, this engaging text prepares students to make effective decisions about the selection, administration, scoring, interpretation, and development of measurement instruments. Coverage includes the essential measurement topics of scale development, item writing and analysis, and reliability and validity, as well as more advanced topics such as exploratory and confirmatory factor analysis, item response theory, diagnostic classification models, test bias and fairness, standard setting, and equating. End-of-chapter exercises (with answers) emphasize both computations and conceptual understanding to encourage readers to think critically about the material. ÿ},
	language = {en},
	publisher = {Guilford Publications},
	author = {Bandalos, Deborah L.},
	month = jan,
	year = {2018},
	keywords = {Business \& Economics / Statistics, Education / Testing \& Measurement, Medical / Nursing / Research \& Theory, Psychology / Assessment, Social Science / Statistics, Testing \& Measurement},
}

@article{carneiro_grapevine_2022,
	series = {International {Conference} on {ENTERprise} {Information} {Systems} / {ProjMAN} - {International} {Conference} on {Project} {MANagement} / {HCist} - {International} {Conference} on {Health} and {Social} {Care} {Information} {Systems} and {Technologies} 2021},
	title = {Grapevine {Segmentation} in {RGB} {Images} using {Deep} {Learning}},
	volume = {196},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050921022171},
	doi = {10.1016/j.procs.2021.11.078},
	abstract = {Wine is the most important product from the Douro Region, in Portugal. Ampelographs are disappearing, and farmers need new solutions to identify grapevine varieties to ensure high-quality standards. The development of methodology capable of automatically identify grapevine are in need. In the scenario, deep learning based methods are emerging as the state-of-art in grapevines classification tasks. In previous work, we verify the deep learning models would benefit from focus classification patches in leaves images areas. Deep learning segmentation methods can be used to find grapevine leaves areas. This paper presents a methodology to segment grapevines images automatically based on the U-net model. A private dataset was used, composed of 733 grapevines images frames extracted from 236 videos collected in a natural environment. The trained model obtained a Dice of 95.6\% and an Intersection over Union of 91.6\%, results that fully satisfy the need of localise grapevine leaves.},
	language = {en},
	urldate = {2023-01-04},
	journal = {Procedia Computer Science},
	author = {Carneiro, Gabriel A. and Magalhães, Rafaela and Neto, Alexandre and Sousa, Joaquim J. and Cunha, António},
	month = jan,
	year = {2022},
	keywords = {deep learning, grapevines, segmentation},
	pages = {101--106},
}

@inproceedings{bertolino_sensarea_2012,
	title = {Sensarea: {An} authoring tool to create accurate clickable videos},
	shorttitle = {Sensarea},
	doi = {10.1109/CBMI.2012.6269804},
	abstract = {We present a user-friendly software application that can be used in a post-production environment and that allows to automatically or semi-automatically perform spatiotemporal segmentation of video objects. The central element in the application is a local pyramid-based segmentation algorithm. Several tools are provided to interactively guide or correct the automatic process when necessary. The masks of the extracted objects can be exported in a Flash or XML vectorized format and can be synchronized to the original video for many applications such as clickable videos.},
	booktitle = {2012 10th {International} {Workshop} on {Content}-{Based} {Multimedia} {Indexing} ({CBMI})},
	author = {Bertolino, Pascal},
	month = jun,
	year = {2012},
	keywords = {Context, Manuals, Motion segmentation, Object segmentation, Software, Tracking, Videos},
	pages = {1--4},
}

@article{azeez_joint_2022,
	title = {A {Joint} {Bayesian} {Optimization} for the {Classification} of {Fine} {Spatial} {Resolution} {Remotely} {Sensed} {Imagery} {Using} {Object}-{Based} {Convolutional} {Neural} {Networks}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-445X},
	url = {https://www.mdpi.com/2073-445X/11/11/1905},
	doi = {10.3390/land11111905},
	abstract = {In recent years, deep learning-based image classification has become widespread, especially in remote sensing applications, due to its automatic and strong feature extraction capability. However, as deep learning methods operate on rectangular-shaped image patches, they cannot accurately extract objects’ boundaries, especially in complex urban settings. As a result, combining deep learning and object-based image analysis (OBIA) has become a new avenue in remote sensing studies. This paper presents a novel approach for combining convolutional neural networks (CNN) with OBIA based on joint optimization of segmentation parameters and deep feature extraction. A Bayesian technique was used to find the best parameters for the multiresolution segmentation (MRS) algorithm while the CNN model learns the image features at different layers, achieving joint optimization. The proposed classification model achieved the best accuracy, with 0.96 OA, 0.95 Kappa, and 0.96 mIoU in the training area and 0.97 OA, 0.96 Kappa, and 0.97 mIoU in the test area, outperforming several benchmark methods including Patch CNN, Center OCNN, Random OCNN, and Decision Fusion. The analysis of CNN variants within the proposed classification workflow showed that the HybridSN model achieved the best results compared to 2D and 3D CNNs. The 3D CNN layers and combining 3D and 2D CNN layers (HybridSN) yielded slightly better accuracies than the 2D CNN layers regarding geometric fidelity, object boundary extraction, and separation of adjacent objects. The Bayesian optimization could find comparable optimal MRS parameters for the training and test areas, with excellent quality measured by AFI (0.046, −0.037) and QR (0.945, 0.932). In the proposed model, higher accuracies could be obtained with larger patch sizes (e.g., 9 × 9 compared to 3 × 3). Moreover, the proposed model is computationally efficient, with the longest training being fewer than 25 s considering all the subprocesses and a single training epoch. As a result, the proposed model can be used for urban and environmental applications that rely on VHR satellite images and require information about land use.},
	language = {en},
	number = {11},
	urldate = {2022-12-30},
	journal = {Land},
	author = {Azeez, Omer Saud and Shafri, Helmi Z. M. and Alias, Aidi Hizami and Haron, Nuzul Azam},
	month = nov,
	year = {2022},
	keywords = {Bayesian optimization, Worldview-3, decision-level fusion, deep learning, object-based convolution neural networks},
	pages = {1905},
}

@article{liu_plant_2022,
	title = {Plant {Species} {Classification} {Based} on {Hyperspectral} {Imaging} via a {Lightweight} {Convolutional} {Neural} {Network} {Model}},
	volume = {13},
	issn = {1664-462X},
	url = {https://www.frontiersin.org/articles/10.3389/fpls.2022.855660},
	abstract = {In recent years, many image-based approaches have been proposed to classify plant species. Most methods utilized red green blue (RGB) imaging materials and designed custom features to classify the plant images using machine learning algorithms. Those works primarily focused on analyzing single-leaf images instead of live-crown images. Without considering the additional features of the leaves’ color and spatial pattern, they failed to handle cases that contained leaves similar in appearance due to the limited spectral information of RGB imaging. To tackle this dilemma, this study proposes a novel framework that combines hyperspectral imaging (HSI) and deep learning techniques for plant image classification. We built a plant image dataset containing 1,500 images of 30 different plant species taken by a 470–900 nm hyperspectral camera and designed a lightweight conventional neural network (CNN) model (LtCNN) to perform image classification. Several state-of-art CNN classifiers are chosen for comparison. The impact of using different band combinations as the network input is also investigated. Results show that using simulated RGB images achieves a kappa coefficient of nearly 0.90 while using the combination of 3-band RGB and 3-band near-infrared images can improve to 0.95. It is also found that the proposed LtCNN can obtain a satisfactory performance of plant classification (kappa = 0.95) using critical spectral features of the green edge (591 nm), red-edge (682 nm), and near-infrared (762 nm) bands. This study also demonstrates the excellent adaptability of the LtCNN model in recognizing leaf features of plant live-crown images while using a relatively smaller number of training samples than complex CNN models such as AlexNet, GoogLeNet, and VGGNet.},
	urldate = {2022-12-26},
	journal = {Frontiers in Plant Science},
	author = {Liu, Keng-Hao and Yang, Meng-Hsien and Huang, Sheng-Ting and Lin, Chinsu},
	year = {2022},
}

@misc{european_environment_agency_eu_2017,
	title = {{EU} {Digital} {Elevation} model},
	url = {https://land.copernicus.eu/user-corner/publications/eu-dem-flyer},
	abstract = {The EU Digital Elevation Model (EU-DEM) combines data from different sources into a single consistent and homogeneous elevation dataset.},
	language = {en},
	urldate = {2022-12-14},
	author = {{European Environment Agency}},
	month = may,
	year = {2017},
	note = {Type: File},
}

@inproceedings{guerra_optimal_2019,
	title = {Optimal {UAV} movement control for farming area scanning using hyperspectral pushbroom sensors},
	doi = {10.1109/DCIS201949030.2019.8959829},
	abstract = {There is a growing interest in setting hyperspectral cameras on UAVs for periodically scanning farming areas with the goal of obtaining more information about the crops and soil state and optimize the productivity of the agricultural field. Hyperspectral pushbroom scanners offer a good trade off between spatial and spectral resolution for such purpose. However, this kind of hyperspectral cameras also introduces technical difficulties mainly related to the lack of intraframe spatial information, turning the registration of subsequent frames a hard task which is typically carried out off-board, once the flight mission has ended, preventing the possibility of obtaining real-time results. This work tackles this problem by controlling the aircraft flight movement in such a way that meaningful hyperspectral images can be directly obtained by just stitching the subsequent captured frames together. The main goal is to offer the possibility of analysing the acquired data in real time, avoiding the necessity of carrying out any registration process. The developed UAV movement control has been tested on real flight campaigns over a vineyard in the island of Gran Canaria. The obtained results verify the goodness of our proposal.},
	booktitle = {2019 {XXXIV} {Conference} on {Design} of {Circuits} and {Integrated} {Systems} ({DCIS})},
	author = {Guerra, Raúl and Horstrand, Pablo and Rodríguez, Aythami and Díaz, María and Morales, Alejandro and Jiménez, Adán and López, Sebastián and López, José F.},
	month = nov,
	year = {2019},
	keywords = {Aircraft, Cameras, Drones, Hyperspectral imaging, Sensors, Trajectory, UAV, drone, hyperspectral imaging, pushbroom sensors, smart farming},
	pages = {1--6},
}

@article{meerdink_multitarget_2022,
	title = {Multitarget {Multiple}-{Instance} {Learning} for {Hyperspectral} {Target} {Detection}},
	volume = {60},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2021.3060966},
	abstract = {In remote sensing, it is often challenging to acquire or collect a large data set that is accurately labeled. This difficulty is usually due to several issues, including but not limited to the study site’s spatial area and accessibility, errors in the global positioning system (GPS), and mixed pixels caused by an image’s spatial resolution. We propose an approach, with two variations, that estimates multiple-target signatures from training samples with imprecise labels: multitarget multiple-instance adaptive cosine estimator (MTMI-ACE) and multitarget multiple-instance spectral match filter (MTMI-SMF). The proposed methods address the abovementioned problems by directly considering the multiple-instance, imprecisely labeled data set. They learn a dictionary of target signatures that optimizes detection against a background using the adaptive cosine estimator (ACE) and spectral match filter (SMF). Experiments were conducted to test the proposed algorithms using a simulated hyperspectral data set, the MUUFL Gulfport hyperspectral data set collected over the University of Southern Mississippi–Gulfpark Campus, and the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) hyperspectral data set collected over Santa Barbara County, CA, USA. Both simulated and real hyperspectral target detection experiments show that the proposed algorithms are effective at learning target signatures and performing target detection.},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Meerdink, Susan and Bocinsky, James and Zare, Alina and Kroeger, Nicholas and McCurley, Connor and Shats, Daniel and Gader, Paul},
	year = {2022},
	keywords = {Adaptive cosine estimator (ACE), Dictionaries, Global Positioning System, Hyperspectral imaging, Libraries, Object detection, Training, Training data, hyperspectral, multiple instance, multiple target, spectral matched filter, target characterization, target detection},
	pages = {1--14},
}

@inproceedings{xin_convolution_2022,
	title = {Convolution {Enhanced} {Spatial}-{Spectral} {Unified} {Transformer} {Network} for {Hyperspectral} {Image} {Classification}},
	doi = {10.1109/IGARSS46834.2022.9884124},
	abstract = {Convolutional neural network has achieved great success in hyperspectral image classification for its excellent local context modeling capabilities. However, the convolution operation with fixed-size local receptive fields is difficult to establish long-distance dependence in hyperspectral im-age. To address this problem, we propose a spatial-spectral unified transformer network, which utilizes self-attention mechanisms to extract global spatial and spectral features. In addition, in order to introduce local spatial and spectral information, the convolution operation is integrated into the network. Specifically, spatial and spectral convolutional embedding layers are designed to generate embeddings of spatial patches and spectral bands. Besides, depthwise convolution is exploited in the locally-enhanced feedforward layer to bring locality into transformer. Experimental results on two datasets demonstrate that our proposed network has greatly improved compared with other state-of-the-art methods.},
	booktitle = {{IGARSS} 2022 - 2022 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Xin, Ziqi and Li, Zhongwei and Xu, Mingming and Wang, Leiquan and Zhu, Xue},
	month = jul,
	year = {2022},
	keywords = {Convolution, Convolutional neural networks, Feature extraction, Geoscience and remote sensing, Hyperspectral imaging, Image classification, Transformers, convolution, hyperspectral image classification, spatial-spectral unified network, transformer},
	pages = {2267--2270},
}

@article{yasar_benchmarking_2022,
	title = {Benchmarking analysis of {CNN} models for bread wheat varieties},
	issn = {1438-2385},
	url = {https://doi.org/10.1007/s00217-022-04172-y},
	doi = {10.1007/s00217-022-04172-y},
	abstract = {Most of the wheat produced and consumed worldwide is generally bread wheat and is used for bread making. Bread wheat varieties can affect the quality of bread. When comparing bread wheat to other varieties, there may be differences in taste, cost, and impact on human health. This study aims to classify bread wheat varieties using deep learning methods. Wheat cultivars used in this research (‘Ayten Abla’, ‘Bayraktar 2000’, ‘Hamitbey’, ‘Şanlı’, and ‘Tosunbey’) were obtained from the Central Field Crop Research Institute, Ministry of Agriculture and Forestry, Republic of Türkiye. First, a dataset of 8354 images of these wheat varieties was created. Then, the images in this dataset were trained with tree different Convolutional Neural Networks (CNNs) using the transfer learning method. The CNN models used are Inception-V3, Mobilenet-V2, and Resnet18, and the classification accuracies obtained are 97.37\%, 97.07\%, and 97.67\%, respectively. Finally, the images not used for training and validation of the CNN models were segmented using image processing techniques. The segmented images were classified as bread wheat and unidentified seeds in the Resnet18 CNN model.},
	language = {en},
	urldate = {2022-11-23},
	journal = {European Food Research and Technology},
	author = {Yasar, Ali},
	month = nov,
	year = {2022},
	keywords = {Bread wheat, CNN, Classification, Inception-V3, Mobilenet-V2, Resnet18},
}

@article{zhang_classification_2022,
	title = {Classification of {Fine}-{Grained} {Crop} {Disease} by {Dilated} {Convolution} and {Improved} {Channel} {Attention} {Module}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2077-0472},
	url = {https://www.mdpi.com/2077-0472/12/10/1727},
	doi = {10.3390/agriculture12101727},
	abstract = {Crop disease seriously affects food security and causes huge economic losses. In recent years, the technology of computer vision based on convolutional neural networks (CNNs) has been widely used to classify crop disease. However, the classification of fine-grained crop disease is still a challenging task due to the difficult identification of representative disease characteristics. We consider that the key to fine-grained crop disease identification lies in expanding the effective receptive field of the network and filtering key features. In this paper, a novel module (DC-DPCA) for fine-grained crop disease classification was proposed. DC-DPCA consists of two main components: (1) dilated convolution block, and (2) dual-pooling channel attention module. Specifically, the dilated convolution block is designed to expand the effective receptive field of the network, allowing the network to acquire information from a larger range of images, and to provide effective information input to the dual-pooling channel attention module. The dual-pooling channel attention module can filter out discriminative features more effectively by combining two pooling operations and constructing correlations between global and local information. The experimental results show that compared with the original networks (85.38\%, 83.22\%, 83.85\%, 84.60\%), ResNet50, VGG16, MobileNetV2, and InceptionV3 embedded with the DC-DPCA module obtained higher accuracy (87.14\%, 86.26\%, 86.24\%, and 86.77\%). We also provide three visualization methods to fully validate the rationality and effectiveness of the proposed method in this paper. These findings are crucial by effectively improving classification ability of fine-grained crop disease by CNNs. Moreover, the DC-DPCA module can be easily embedded into a variety of network structures with minimal time cost and memory cost, which contributes to the realization of smart agriculture.},
	language = {en},
	number = {10},
	urldate = {2022-11-23},
	journal = {Agriculture},
	author = {Zhang, Xiang and Gao, Huiyi and Wan, Li},
	month = oct,
	year = {2022},
	keywords = {attention mechanism, classification, convolutional neural networks, fine-grained crop disease},
	pages = {1727},
}

@article{kai_deep_2022,
	title = {Deep {Learning}-{Based} {Method} for {Classification} of {Sugarcane} {Varieties}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-4395},
	url = {https://www.mdpi.com/2073-4395/12/11/2722},
	doi = {10.3390/agronomy12112722},
	abstract = {The classification of sugarcane varieties using products derived from remote sensing allows for the monitoring of plants with different profiles without necessarily having physical contact with the study objects. However, differentiating between varieties can be challenging due to the similarity of the spectral characteristics of each crop. Thus, this study aimed to classify four sugarcane varieties through deep neural networks, subsequently comparing the results with traditional machine learning techniques. In order to provide more data as input for the classification models, along with the multi-band values of the pixels and vegetation indices, other information can be obtained from the sensor bands through RGB combinations by reconciling different bands so as to yield the characteristics of crop varieties. The methodology created to discriminate sugarcane varieties consisted of a dense neural network, with the number of hidden layers determined by the greedy layer-wise method and multiples of four neurons in each layer; additionally, a 5-fold evaluation in the training data was composed of Sentinel-2 band data, vegetation indices, and RGB combinations. Comparing the results acquired from each model with the hyperparameters selected by Bayesian optimisation, except for the neural network with manually defined parameters, it was possible to observe a greater precision of 99.55\% in the SVM model, followed by the neural network developed by the study, random forests, and kNN. However, the final neural network model prediction resulted in the 99.48\% accuracy of a six-hidden-layers network, demonstrating the potential of using neural networks in classification. Among the characteristics that contributed the most to the classification, the chlorophyll-sensitive bands, especially B6, B7, B11, and some RGB combinations, had the most impact on the correct classification of samples by the neural network model. Thus, the regions encompassing the near-infrared and shortwave infrared regions proved to be suitable for the discrimination of sugarcane varieties.},
	language = {en},
	number = {11},
	urldate = {2022-11-23},
	journal = {Agronomy},
	author = {Kai, Priscila Marques and de Oliveira, Bruna Mendes and da Costa, Ronaldo Martins},
	month = nov,
	year = {2022},
	keywords = {Sentinel-2, machine learning, precision agriculture, remote sensing, sugarcane},
	pages = {2722},
}

@article{meng_fine_2022,
	title = {Fine hyperspectral classification of rice varieties based on attention module {3D}-{2DCNN}},
	volume = {203},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169922007827},
	doi = {10.1016/j.compag.2022.107474},
	abstract = {Rice is an indispensable food crop for human beings. Rice varieties are closely related to disease resistance, insect resistance, lodging resistance, grain quality and yield. Different varieties of rice have similar appearance traits and change trends, which are difficult to distinguish. It is of great significance to classify and identify rice varieties with high precision in a wide range by objective and non-destructive detection methods. In this paper, the canopy hyperspectral images of rice varieties were obtained by using the S185 hyperspectral imaging device mounted on a UAV platform. And the spectral and spatial features of 14 rice varieties were automatically learned and deeply extracted by hybrid convolutional neural network structure. In addition, in order to improve the performance of the model, the article attempts to optimize the model with the end-to-end trainable attention module. Finally, extensive experiments are carried out to prove the validity of the model. Compared with the advanced methods, the 3D-CSAM-2DCNN proposed in this paper performed the best classification on fine classification of rice varieties. The overall accuracy of 98.93\% and the accuracy of more than 98.22\% for single variety has been achieved. The proposed model is conducive to automatic identification of fields and crop phenotypes research, and contributes new possibilities to promote the development of precision agriculture and smart agriculture.},
	language = {en},
	urldate = {2022-11-23},
	journal = {Computers and Electronics in Agriculture},
	author = {Meng, Ying and Ma, Zheng and Ji, Zeguang and Gao, Rui and Su, Zhongbin},
	month = dec,
	year = {2022},
	keywords = {Attention module, Convolutional neural network, Hyperspectral image classification, Rice variety},
	pages = {107474},
}

@inproceedings{virnodkar_application_2020,
	address = {Singapore},
	series = {Lecture {Notes} in {Networks} and {Systems}},
	title = {Application of {Machine} {Learning} on {Remote} {Sensing} {Data} for {Sugarcane} {Crop} {Classification}: {A} {Review}},
	isbn = {9789811506307},
	shorttitle = {Application of {Machine} {Learning} on {Remote} {Sensing} {Data} for {Sugarcane} {Crop} {Classification}},
	doi = {10.1007/978-981-15-0630-7_55},
	abstract = {Sugarcane is a major contributing component in the economy of tropical and subtropical countries like India, Brazil and China. Sugarcane agriculture is empowered with the advancements in the remote sensing technology because of its timely, non invasive, and labor and cost effective capability. Remote sensing data with machine learning algorithms like Support Vector Machine, Artificial Neural Network and Random Forest are proven to be suitable in sugarcane agriculture. The aim of this paper is to present a review of studies that implemented various machine learning algorithms based on remote sensing data in sugarcane crop mapping and classification.},
	language = {en},
	booktitle = {{ICT} {Analysis} and {Applications}},
	publisher = {Springer},
	author = {Virnodkar, Shyamal S. and Pachghare, Vinod K. and Patil, V. C. and Jha, Sunil Kumar},
	editor = {Fong, Simon and Dey, Nilanjan and Joshi, Amit},
	year = {2020},
	keywords = {Machine learning, Remote sensing, Sugarcane crop classification},
	pages = {539--555},
}

@article{hajjar_vine_2021,
	title = {Vine {Identification} and {Characterization} in {Goblet}-{Trained} {Vineyards} {Using} {Remotely} {Sensed} {Images}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/13/15/2992},
	doi = {10.3390/rs13152992},
	abstract = {This paper proposes a novel approach for living and missing vine identification and vine characterization in goblet-trained vine plots using aerial images. Given the periodic structure of goblet vineyards, the RGB color coded parcel image is analyzed using proper processing techniques in order to determine the locations of living and missing vines. Vine characterization is achieved by implementing the marker-controlled watershed transform where the centers of the living vines serve as object markers. As a result, a precise mortality rate is calculated for each parcel. Moreover, all vines, even the overlapping ones, are fully recognized providing information about their size, shape, and green color intensity. The presented approach is fully automated and yields accuracy values exceeding 95\% when the obtained results are assessed with ground-truth data. This unsupervised and automated approach can be applied to any type of plots presenting similar spatial patterns requiring only the image as input.},
	language = {en},
	number = {15},
	urldate = {2022-11-20},
	journal = {Remote Sensing},
	author = {Hajjar, Chantal and Ghattas, Ghassan and Sarkis, Maya Kharrat and Chamoun, Yolla Ghorra},
	month = jan,
	year = {2021},
	keywords = {Hough transform, goblet vineyards, missing and living vine identification, remote sensing, semantic segmentation, vine characterization, watershed transform},
	pages = {2992},
}

@article{friedman_regularization_2010,
	title = {Regularization {Paths} for {Generalized} {Linear} {Models} via {Coordinate} {Descent}},
	volume = {33},
	issn = {1548-7660},
	abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ(1) (the lasso), ℓ(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
	language = {eng},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
	year = {2010},
	pmid = {20808728},
	pmcid = {PMC2929880},
	pages = {1--22},
}

@article{mehmood_review_2012,
	title = {A review of variable selection methods in {Partial} {Least} {Squares} {Regression}},
	volume = {118},
	issn = {0169-7439},
	url = {https://www.sciencedirect.com/science/article/pii/S0169743912001542},
	doi = {10.1016/j.chemolab.2012.07.010},
	abstract = {With the increasing ease of measuring multiple variables per object the importance of variable selection for data reduction and for improved interpretability is gaining importance. There are numerous suggested methods for variable selection in the literature of data analysis and statistics, and it is a challenge to stay updated on all the possibilities. We therefore present a review of available methods for variable selection within one of the many modeling approaches for high-throughput data, Partial Least Squares Regression. The aim of this paper is mainly to collect and shortly present the methods in such a way that the reader easily can get an understanding of the characteristics of the methods and to get a basis for selecting an appropriate method for own use. For each method we also give references to its use in the literature for further reading, and also to software availability.},
	language = {en},
	urldate = {2022-11-20},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Mehmood, Tahir and Liland, Kristian Hovde and Snipen, Lars and Sæbø, Solve},
	month = aug,
	year = {2012},
	keywords = {PLS, Variable selection},
	pages = {62--69},
}

@article{liu_dimension_2019,
	title = {Dimension reduction and classification of hyperspectral images based on neural network sensitivity analysis and multi-instance learning},
	volume = {16},
	url = {http://www.doiserbia.nb.rs/Article.aspx?ID=1820-02141900003L},
	abstract = {Hyperspectral remote image sensing is a rapidly developing integrated technology used widely in numerous areas. The rich spectral information from hyperspectral images aids in recognition and classification of many types of objects, but the high dimensionality of these images leads to information redundancy. In this paper, we used sensitivity analysis for dimension reduction. However, another challenge is that hyperspectral images identify objects as either a "different body with the same spectrum" or "same body with a different spectrum." Therefore, it is difficult to maintain the correct correspondence between ground objects and samples, which hinders classification of the images. This issue can be addressed using multi-instance learning for classification. In our proposed method, we combined neural network sensitivity analysis with a multiinstance learning algorithm based on a support vector machine to achieve dimension reduction and accurate classification for hyperspectral images. Experimental results demonstrated that our method provided strong overall classification effectiveness when compared with prior methods.},
	number = {2},
	urldate = {2022-11-20},
	journal = {Computer Science and Information Systems},
	author = {Liu, Hui and Li, Chenming and Xu, Lizhong},
	year = {2019},
	pages = {443--468},
}

@article{zhang_spectral-spatial_2022,
	title = {Spectral-{Spatial} {Hyperspectral} {Unmixing} {Using} {Nonnegative} {Matrix} {Factorization}},
	volume = {60},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2021.3074364},
	abstract = {Remotely sensed hyperspectral images contain several bands (at about adjoining frequencies) for a similar zone on the surface of the Earth. Hyperspectral unmixing is a significant method for breaking down hyperspectral images into the components (endmembers) that conform each (potentially mixed) pixel and their abundance maps. Nonnegative matrix factorization (NMF) has attracted huge consideration because of the way that it can address mixed pixel scenarios. Most existing NMF unmixing techniques do not include spatial information in the analysis. An ongoing trend is to fuse the spatial and the spectral information contained in hyperspectral scenes to improve the solution. In this article, we build up another hyperspectral unmixing technique named spectral–spatial weighted sparse NMF (SSWNMF), in which two weighting factors are acquainted into the NMF model to upgrade the sparsity of the solution and capture the piecewise smooth structure of the data. We adopt a multiplicative iterative strategy to implement the proposed SSWNMF model. Our experimental results, conducted with both synthetic and real hyperspectral data, uncover that the proposed SSWNMF strategy can get accurate unmixing results over those gave by other unmixing strategies, with less parameter tuning.},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Zhang, Shaoquan and Zhang, Guorong and Li, Fan and Deng, Chengzhi and Wang, Shengqian and Plaza, Antonio and Li, Jun},
	year = {2022},
	keywords = {Blind source separation, Blind source separation (BSS), Force, Hyperspectral imaging, Matrix decomposition, Sparse matrices, Spatial databases, Standards, hyperspectral unmixing, nonnegative matrix factorization (NMF), spatially weighted unmixing, weighted sparsity},
	pages = {1--13},
}

@article{shi_incorporating_2014,
	title = {Incorporating spatial information in spectral unmixing: {A} review},
	volume = {149},
	issn = {0034-4257},
	shorttitle = {Incorporating spatial information in spectral unmixing},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425714001357},
	doi = {10.1016/j.rse.2014.03.034},
	abstract = {Spectral unmixing is the process of decomposing the spectral signature of a mixed pixel into a set of endmembers and their corresponding abundances. Endmembers are spectra of the pure materials present in the image and abundances at each pixel represent the percentage of each endmember that is present in the pixel. Many spectral unmixing techniques treat a pixel as independent of its neighbors, therefore, only spectral characteristics of the image are used to address the spectral unmixing problem. However, a number of recent studies have found that spatial autocorrelation provides useful information for spectral unmixing. Combining spatial information with its spectral counterpart can lead to improvements in the unmixing results. In this paper, the unmixing methods that incorporate spatial information are termed spatial spectral unmixing, whereas those exploiting only spectral information are referred to as spectral-only unmixing. We summarize the available spatial spectral unmixing methods according to the following three categories: 1) endmember extraction, 2) selection of endmember combinations, and 3) abundance estimation. An experiment-based comparison between representative spatial spectral and spectral-only unmixing methods is also presented in order to demonstrate the advantages of spatial spectral methods. Furthermore, considerations and suggestions of the incorporation of spatial information are provided. With this review, we hope to bring spatial spectral unmixing to the attention of the remote sensing community and stimulate new research initiatives to integrate both spatial and spectral information for unmixing purposes.},
	language = {en},
	urldate = {2022-11-19},
	journal = {Remote Sensing of Environment},
	author = {Shi, Chen and Wang, Le},
	month = jun,
	year = {2014},
	keywords = {Endmembers, Spatial dependence, Spectral Mixture Analysis, Spectral unmixing},
	pages = {70--87},
}

@inproceedings{bhatt_deep_2020,
	title = {Deep {Learning} in {Hyperspectral} {Unmixing}: {A} {Review}},
	shorttitle = {Deep {Learning} in {Hyperspectral} {Unmixing}},
	doi = {10.1109/IGARSS39084.2020.9324546},
	abstract = {In remote sensing, hyperspectral unmixing is very challenging inverse ill-posed problem which does not have closed-form solution. Since more than three decades, several researchers and practitioners have proposed remarkable algorithmic approaches for estimating endmembers and abundances from the data. Typically they construct a forward model (function) and then invert it for the unknowns. Meanwhile a few researchers have reported the use of artificial neural networks for unmixing. With the growth in computer technology, recently the researchers have started exploring deep learning to handle this critical issue. In deep neural networks, the underlying function is learned either by supervised training or by learning the hidden structures from the data. In this paper (within space constraint), we review the learning-based approaches and provide deep learning perspective to the spectral unmixing. We discuss spectral unmixing using three deep learning architectures, viz., autoencoders, convolutional neural network, and generative model. Further, we highlight unattended research problems in the unmixing and brief about possible use of deep learning. Finally, we attempt to discuss some of the practical challenges to be addressed and possible opportunities.},
	booktitle = {{IGARSS} 2020 - 2020 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Bhatt, Jignesh S. and Joshi, M. V.},
	month = sep,
	year = {2020},
	keywords = {Data mining, Deep learning, Estimation, Feature extraction, Hyperspectral imaging, Hyperspectral remote sensing, Spectral unmixing, Task analysis, Training},
	pages = {2189--2192},
}

@article{borsoi_spectral_2021,
	title = {Spectral {Variability} in {Hyperspectral} {Data} {Unmixing}: {A} comprehensive review},
	volume = {9},
	issn = {2168-6831},
	shorttitle = {Spectral {Variability} in {Hyperspectral} {Data} {Unmixing}},
	doi = {10.1109/MGRS.2021.3071158},
	abstract = {The spectral signatures of the materials contained in hyperspectral images, also called endmembers (EMs), can be significantly affected by variations in atmospheric, illumination, and environmental conditions that typically occur within an image. Traditional spectral unmixing (SU) algorithms neglect the spectral variability of the EMs, which propagates significant modeling errors throughout the whole unmixing process and compromises the quality of the results. Therefore, serious efforts have been dedicated to mitigating the effects of spectral variability in SU. This resulted in the development of algorithms that incorporate different strategies to enable the EMs to vary within a hyperspectral image, using, for instance, sets of spectral signatures known a priori as well as Bayesian, parametric, and local EM models.},
	number = {4},
	journal = {IEEE Geoscience and Remote Sensing Magazine},
	author = {Borsoi, Ricardo Augusto and Imbiriba, Tales and Bermudez, José Carlos Moreira and Richard, Cédric and Chanussot, Jocelyn and Drumetz, Lucas and Tourneret, Jean-Yves and Zare, Alina and Jutten, Christian},
	month = dec,
	year = {2021},
	keywords = {Atmospheric modeling, Bayes methods, Computational modeling, Environmental management, Hyperspectral imaging, Lighting},
	pages = {223--270},
}

@article{dutta_characterizing_2017,
	title = {Characterizing {Vegetation} {Canopy} {Structure} {Using} {Airborne} {Remote} {Sensing} {Data}},
	volume = {55},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2016.2620478},
	abstract = {Vegetation canopy structure plays an important role in the partitioning of incident solar radiation, photosynthesis, transpiration, and other scalar fluxes. The vertical foliage distribution of the plant canopy is represented by leaf area density (LAD), which is defined as the one-sided leaf area per unit volume. Airborne light detection and ranging (LiDAR) offers the possibility to characterize the 3-D variation of LAD over space, which still remains a challenge to estimate. Moreover, the low density of point cloud data generally offered by airborne LiDAR may be insufficient for accurate LAD estimation in dense overlapping forest canopies. We develop a method for the estimation of the LAD profile using a combination of airborne LiDAR and hyperspectral data using a feature-based data fusion approach. After identifying vegetation species using hyperspectral data, point cloud LiDAR data is used in a “tree-shaped” voxel approach to characterize the LAD of trees in a riparian forest setting. We also propose a set of relationships on simple geometry of overlap for the construction of tree shaped voxels. In a forest setting with overlapping canopies, the results indicate that the tree-shaped voxels are better able to attribute the LAD to the upper and middle parts of the overall canopy as well as individual tall and short trees compared with traditional cylindrical voxels.},
	number = {2},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Dutta, Debsunder and Wang, Kunxuan and Lee, Esther and Goodwell, Allison and Woo, Dong Kook and Wagner, Derek and Kumar, Praveen},
	month = feb,
	year = {2017},
	keywords = {Estimation, Hyperspectral, Hyperspectral imaging, Laser radar, Three-dimensional displays, Vegetation, Vegetation mapping, leaf area density (LAD), leaf area index (LAI), light detection and ranging (LiDAR), tree species, voxel},
	pages = {1160--1178},
}

@book{amigo_hyperspectral_2019,
	address = {Amsterdam, Netherlands},
	title = {Hyperspectral {Imaging}: {Volume} 32},
	isbn = {978-0-444-63977-6},
	shorttitle = {Hyperspectral {Imaging}},
	language = {Inglés},
	author = {Amigo, Jose Manuel and Benson, Vladlena},
	month = oct,
	year = {2019},
}

@article{matusik_data-driven_2003,
	title = {A data-driven reflectance model},
	volume = {22},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/882262.882343},
	doi = {10.1145/882262.882343},
	abstract = {We present a generative model for isotropic bidirectional reflectance distribution functions (BRDFs) based on acquired reflectance data. Instead of using analytical reflectance models, we represent each BRDF as a dense set of measurements. This allows us to interpolate and extrapolate in the space of acquired BRDFs to create new BRDFs. We treat each acquired BRDF as a single high-dimensional vector taken from a space of all possible BRDFs. We apply both linear (subspace) and non-linear (manifold) dimensionality reduction tools in an effort to discover a lower-dimensional representation that characterizes our measurements. We let users define perceptually meaningful parametrization directions to navigate in the reduced-dimension BRDF space. On the low-dimensional manifold, movement along these directions produces novel but valid BRDFs.},
	number = {3},
	urldate = {2022-11-19},
	journal = {ACM Transactions on Graphics},
	author = {Matusik, Wojciech and Pfister, Hanspeter and Brand, Matt and McMillan, Leonard},
	month = jul,
	year = {2003},
	keywords = {BRDF, image-based modeling, light reflection models, photometric measurements, reflectance},
	pages = {759--769},
}

@techreport{kokaly_usgs_2017,
	address = {Reston, VA},
	type = {{USGS} {Numbered} {Series}},
	title = {{USGS} {Spectral} {Library} {Version} 7},
	url = {http://pubs.er.usgs.gov/publication/ds1035},
	abstract = {We have assembled a library of spectra measured with laboratory, field, and airborne spectrometers. The instruments used cover wavelengths from the ultraviolet to the far infrared (0.2 to 200 microns [μm]). Laboratory samples of specific minerals, plants, chemical compounds, and manmade materials were measured. In many cases, samples were purified, so that unique spectral features of a material can be related to its chemical structure. These spectro-chemical links are important for interpreting remotely sensed data collected in the field or from an aircraft or spacecraft. This library also contains physically constructed as well as mathematically computed mixtures. Four different spectrometer types were used to measure spectra in the library: (1) Beckman™ 5270 covering the spectral range 0.2 to 3 µm, (2) standard, high resolution (hi-res), and high-resolution Next Generation (hi-resNG) models of Analytical Spectral Devices (ASD) field portable spectrometers covering the range from 0.35 to 2.5 µm, (3) Nicolet™ Fourier Transform Infra-Red (FTIR) interferometer spectrometers covering the range from about 1.12 to 216 µm, and (4) the NASA Airborne Visible/Infra-Red Imaging Spectrometer AVIRIS, covering the range 0.37 to 2.5 µm. Measurements of rocks, soils, and natural mixtures of minerals were made in laboratory and field settings. Spectra of plant components and vegetation plots, comprising many plant types and species with varying backgrounds, are also in this library. Measurements by airborne spectrometers are included for forested vegetation plots, in which the trees are too tall for measurement by a field spectrometer. This report describes the instruments used, the organization of materials into chapters, metadata descriptions of spectra and samples, and possible artifacts in the spectral measurements. To facilitate greater application of the spectra, the library has also been convolved to selected spectrometer and imaging spectrometers sampling and bandpasses, and resampled to selected broadband multispectral sensors. The native file format of the library is the SPECtrum Processing Routines (SPECPR) data format. This report describes how to access freely available software to read the SPECPR format. To facilitate broader access to the library, we produced generic formats of the spectra and metadata in text files. The library is provided on digital media and online at https://speclab.cr.usgs.gov/spectral-lib.html. A long-term archive of these data are stored on the USGS ScienceBase data server (https://dx.doi.org/10.5066/F7RR1WDJ).},
	number = {1035},
	urldate = {2022-11-19},
	institution = {U.S. Geological Survey},
	author = {Kokaly, Raymond F. and Clark, Roger N. and Swayze, Gregg A. and Livo, K. Eric and Hoefen, Todd M. and Pearson, Neil C. and Wise, Richard A. and Benzel, William M. and Lowers, Heather A. and Driscoll, Rhonda L. and Klein, Anna J.},
	year = {2017},
	doi = {10.3133/ds1035},
	pages = {68},
}

@misc{california_institute_of_technology_aviris_nodate,
	title = {{AVIRIS} {Data} {Portal}},
	url = {https://aviris.jpl.nasa.gov/dataportal/},
	urldate = {2022-11-17},
	author = {{California Institute of Technology}},
}

@article{jia_kernel-driven_2020,
	title = {A {Kernel}-{Driven} {BRDF} {Approach} to {Correct} {Airborne} {Hyperspectral} {Imagery} over {Forested} {Areas} with {Rugged} {Topography}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/3/432},
	doi = {10.3390/rs12030432},
	abstract = {Airborne hyper-spectral imaging has been proven to be an efficient means to provide new insights for the retrieval of biophysical variables. However, quantitative estimates of unbiased information derived from airborne hyperspectral measurements primarily require a correction of the anisotropic scattering properties of the land surface depicted by the bidirectional reflectance distribution function (BRDF). Hitherto, angular BRDF correction methods rarely combined viewing-illumination geometry and topographic information to achieve a comprehensive understanding and quantification of the BRDF effects. This is in particular the case for forested areas, frequently underlaid by rugged topography. This paper describes a method to correct the BRDF effects of airborne hyperspectral imagery over forested areas overlying rugged topography, referred in the reminder of the paper as rugged topography-BRDF (RT-BRDF) correction. The local viewing and illumination geometry are calculated for each pixel based on the characteristics of the airborne scanner and the local topography, and these two variables are used to adapt the Ross-Thick-Maignan and Li-Transit-Reciprocal kernels in the case of rugged topography. The new BRDF model is fitted to the anisotropy of multi-line airborne hyperspectral data. The number of pixels is set at 35,000 in this study, based on a stratified random sampling method to ensure a comprehensive coverage of the viewing and illumination angles and to minimize the fitting error of the BRDF model for all bands. Based on multi-line airborne hyperspectral data acquired with the Chinese Academy of Forestry’s LiDAR, CCD, and Hyperspectral system (CAF-LiCHy) in the Pu’er region (China), the results applying the RT-BRDF correction are compared with results from current empirical (C, and sun-canopy-sensor (SCS) adds C (SCS+C)) and semi-physical (SCS) topographic correction methods. Both quantitative assessment and visual inspection indicate that RT-BRDF, C, and SCS+C correction methods all reduce the topographic effects. However, the RT-BRDF method appears more efficient in reducing the variability in reflectance of overlapping areas in multiple flight-lines, with the advantage of reducing the BRDF effects caused by the combination of wide field of view (FOV) airborne scanner, rugged topography, and varying solar illumination angle over long flight time. Specifically, the average decrease in coefficient of variation (CV) is 3\% and 3.5\% for coniferous forest and broadleaved forest, respectively. This improvement is particularly marked in the near infrared (NIR) region (i.e., {\textbackslash}textgreater750 nm). This finding opens new possible applications of airborne hyperspectral surveys over large areas.},
	language = {en},
	number = {3},
	urldate = {2022-11-04},
	journal = {Remote Sensing},
	author = {Jia, Wen and Pang, Yong and Tortini, Riccardo and Schläpfer, Daniel and Li, Zengyuan and Roujean, Jean-Louis},
	month = jan,
	year = {2020},
	keywords = {BRDF correction, MODIS, airborne hyperspectral image, kernel-driven, remote sensing, rugged topography},
	pages = {432},
}

@article{queally_flexbrdf_2022,
	title = {{FlexBRDF}: {A} {Flexible} {BRDF} {Correction} for {Grouped} {Processing} of {Airborne} {Imaging} {Spectroscopy} {Flightlines}},
	volume = {127},
	issn = {2169-8961},
	shorttitle = {{FlexBRDF}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2021JG006622},
	doi = {10.1029/2021JG006622},
	abstract = {Bidirectional reflectance distribution function (BRDF) effects are a persistent issue for the analysis of vegetation in airborne imaging spectroscopy data, especially when mosaicking results from adjacent flightlines. With the advent of large airborne imaging efforts from NASA and the U.S. National Ecological Observatory Network (NEON), there is increasing need for methods that are flexible and automatable across images with diverse land cover. Flexible bidirectional reflectance distribution function (FlexBRDF) is built upon the widely used kernel method, with additional features including stratified random sampling across flightline groups, dynamic land cover stratification by normalized difference vegetation index (NDVI), interpolation of correction coefficients across NDVI bins, and the use of a reference solar zenith angle. We demonstrate FlexBRDF using nine long (150–400 km) airborne visible/infrared imaging spectrometer (AVIRIS)-Classic flightlines collected on 22 May 2013 over Southern California, where diverse land cover and a wide range of solar illumination yield significant BRDF effects. We further test the approach on additional AVIRIS-Classic data from California, AVIRIS-Next Generation data from the Arctic and India, and NEON imagery from Wisconsin. Comparison of overlapping areas of flightlines show that models built from multiple flightlines performed better than those built for single images (root mean square error improved up to 2.3\% and mean absolute deviation 2.5\%). Standardization to a common solar zenith angle among a flightline group improved performance, and interpolation across bins minimized between-bin boundaries. While BRDF corrections for individual sites suffice for local studies, FlexBRDF is an open source option that is compatible with bulk processing of large airborne data sets covering diverse land cover needed for calibration/validation of forthcoming spaceborne imaging spectroscopy missions.},
	language = {en},
	number = {1},
	urldate = {2022-11-04},
	journal = {Journal of Geophysical Research: Biogeosciences},
	author = {Queally, Natalie and Ye, Zhiwei and Zheng, Ting and Chlus, Adam and Schneider, Fabian and Pavlick, Ryan P. and Townsend, Philip A.},
	year = {2022},
	keywords = {AVIRIS, BRDF, NEON, Ross-Li kernels, bidirectional reflectance distribution function, imaging spectroscopy},
	pages = {e2021JG006622},
}

@article{aasen_quantitative_2018,
	title = {Quantitative {Remote} {Sensing} at {Ultra}-{High} {Resolution} with {UAV} {Spectroscopy}: {A} {Review} of {Sensor} {Technology}, {Measurement} {Procedures}, and {Data} {Correction} {Workflows}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {Quantitative {Remote} {Sensing} at {Ultra}-{High} {Resolution} with {UAV} {Spectroscopy}},
	url = {https://www.mdpi.com/2072-4292/10/7/1091},
	doi = {10.3390/rs10071091},
	abstract = {In the last 10 years, development in robotics, computer vision, and sensor technology has provided new spectral remote sensing tools to capture unprecedented ultra-high spatial and high spectral resolution with unmanned aerial vehicles (UAVs). This development has led to a revolution in geospatial data collection in which not only few specialist data providers collect and deliver remotely sensed data, but a whole diverse community is potentially able to gather geospatial data that fit their needs. However, the diversification of sensing systems and user applications challenges the common application of good practice procedures that ensure the quality of the data. This challenge can only be met by establishing and communicating common procedures that have had demonstrated success in scientific experiments and operational demonstrations. In this review, we evaluate the state-of-the-art methods in UAV spectral remote sensing and discuss sensor technology, measurement procedures, geometric processing, and radiometric calibration based on the literature and more than a decade of experimentation. We follow the ‘journey’ of the reflected energy from the particle in the environment to its representation as a pixel in a 2D or 2.5D map, or 3D spectral point cloud. Additionally, we reflect on the current revolution in remote sensing, and identify trends, potential opportunities, and limitations.},
	language = {en},
	number = {7},
	urldate = {2022-11-03},
	journal = {Remote Sensing},
	author = {Aasen, Helge and Honkavaara, Eija and Lucieer, Arko and Zarco-Tejada, Pablo J.},
	month = jul,
	year = {2018},
	keywords = {2D imager, Remotely Piloted Aircraft Systems (RPAS), calibration, drone, hyperspectral, imaging spectroscopy, low-altitude, multispectral, pushbroom, remote sensing, sensors, snapshot, spectral, spectroradiometers, unmanned aerial systems (UAS), unmanned aerial vehicles},
	pages = {1091},
}

@article{sousa_uav-based_2022,
	title = {{UAV}-{Based} {Hyperspectral} {Monitoring} {Using} {Push}-{Broom} and {Snapshot} {Sensors}: {A} {Multisite} {Assessment} for {Precision} {Viticulture} {Applications}},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {{UAV}-{Based} {Hyperspectral} {Monitoring} {Using} {Push}-{Broom} and {Snapshot} {Sensors}},
	url = {https://www.mdpi.com/1424-8220/22/17/6574},
	doi = {10.3390/s22176574},
	abstract = {Hyperspectral aerial imagery is becoming increasingly available due to both technology evolution and a somewhat affordable price tag. However, selecting a proper UAV + hyperspectral sensor combo to use in specific contexts is still challenging and lacks proper documental support. While selecting an UAV is more straightforward as it mostly relates with sensor compatibility, autonomy, reliability and cost, a hyperspectral sensor has much more to be considered. This note provides an assessment of two hyperspectral sensors (push-broom and snapshot) regarding practicality and suitability, within a precision viticulture context. The aim is to provide researchers, agronomists, winegrowers and UAV pilots with dependable data collection protocols and methods, enabling them to achieve faster processing techniques and helping to integrate multiple data sources. Furthermore, both the benefits and drawbacks of using each technology within a precision viticulture context are also highlighted. Hyperspectral sensors, UAVs, flight operations, and the processing methodology for each imaging type’ datasets are presented through a qualitative and quantitative analysis. For this purpose, four vineyards in two countries were selected as case studies. This supports the extrapolation of both advantages and issues related with the two types of hyperspectral sensors used, in different contexts. Sensors’ performance was compared through the evaluation of field operations complexity, processing time and qualitative accuracy of the results, namely the quality of the generated hyperspectral mosaics. The results shown an overall excellent geometrical quality, with no distortions or overlapping faults for both technologies, using the proposed mosaicking process and reconstruction. By resorting to the multi-site assessment, the qualitative and quantitative exchange of information throughout the UAV hyperspectral community is facilitated. In addition, all the major benefits and drawbacks of each hyperspectral sensor regarding its operation and data features are identified. Lastly, the operational complexity in the context of precision agriculture is also presented.},
	language = {en},
	number = {17},
	urldate = {2022-11-03},
	journal = {Sensors},
	author = {Sousa, Joaquim J. and Toscano, Piero and Matese, Alessandro and Di Gennaro, Salvatore Filippo and Berton, Andrea and Gatti, Matteo and Poni, Stefano and Pádua, Luís and Hruška, Jonáš and Morais, Raul and Peres, Emanuel},
	month = jan,
	year = {2022},
	keywords = {bands co-registration, hyperspectral data cube, imaging sensor, radiometric calibration, remote sensing, unmanned aerial vehicles},
	pages = {6574},
}

@article{xue_compact_2021,
	title = {Compact, {UAV}-mounted hyperspectral imaging system with automatic geometric distortion rectification},
	volume = {29},
	copyright = {\&\#169; 2021 Optical Society of America},
	issn = {1094-4087},
	url = {https://opg.optica.org/oe/abstract.cfm?uri=oe-29-4-6092},
	doi = {10.1364/OE.412853},
	abstract = {A highly compact hyperspectral imager with an automatic geometric rectification function is developed in this study, which can be mounted on a UAV for ultra-wide range hyperspectral imaging. For better application, the system can provide visible light image transmission and hyperspectral imaging in the real-time mode. A specific design is proposed to allow the visible light camera and hyperspectral camera to share the same telescope optical path, making the system have a high integration level with a total mass of 1.9 kilograms. Thanks to the sharing-optical-path design, the field of view (FOV), frame rate, and spatial resolution are modified the same between the visible light camera and hyperspectral camera. As a result, the geometric rectification is easily performed, and repeated rectifications are eliminated to improve the imaging efficiency. A FOV of 40 degrees in the frame direction and 26 degrees in the flight direction are realized with a focal length of 13mm, providing a large spectral range from 400 nm to 1000 nm and an excellent spectral resolution of 2.5 nm. An automatic geometric rectification workflow is presented and verified in experiments, which can improve the geometric rectification of hyperspectral images in the presence of low-quality UAV navigation data through the incorporation of frame images. Experimental results show that the relative accuracy of geometric rectification is less than 2 pixels when applying the algorithm to our system dataset.},
	language = {EN},
	number = {4},
	urldate = {2022-11-03},
	journal = {Optics Express},
	author = {Xue, Qingsheng and Yang, Bai and Wang, Fupeng and Tian, Zhongtian and Bai, Haoxuan and Li, Qian and Cao, Diansheng},
	month = feb,
	year = {2021},
	pages = {6092--6112},
}

@article{sagan_data-driven_2022,
	title = {Data-{Driven} {Artificial} {Intelligence} for {Calibration} of {Hyperspectral} {Big} {Data}},
	volume = {60},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2021.3091409},
	abstract = {Near-earth hyperspectral big data present both huge opportunities and challenges for spurring developments in agriculture and high-throughput plant phenotyping and breeding. In this article, we present data-driven approaches to address the calibration challenges for utilizing near-earth hyperspectral data for agriculture. A data-driven, fully automated calibration workflow that includes a suite of robust algorithms for radiometric calibration, bidirectional reflectance distribution function (BRDF) correction and reflectance normalization, soil and shadow masking, and image quality assessments was developed. An empirical method that utilizes predetermined models between camera photon counts (digital numbers) and downwelling irradiance measurements for each spectral band was established to perform radiometric calibration. A kernel-driven semiempirical BRDF correction method based on the Ross Thick-Li Sparse (RTLS) model was used to normalize the data for both changes in solar elevation and sensor view angle differences attributed to pixel location within the field of view. Following rigorous radiometric and BRDF corrections, novel rule-based methods were developed to conduct automatic soil removal; and a newly proposed approach was used for image quality assessment; additionally, shadow masking and plot-level feature extraction were carried out. Our results show that the automated calibration, processing, storage, and analysis pipeline developed in this work can effectively handle massive amounts of hyperspectral data and address the urgent challenges related to the production of sustainable bioenergy and food crops, targeting methods to accelerate plant breeding for improving yield and biomass traits.},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Sagan, Vasit and Maimaitijiang, Maitiniyazi and Paheding, Sidike and Bhadra, Sourav and Gosselin, Nichole and Burnette, Max and Demieville, Jeffrey and Hartling, Sean and LeBauer, David and Newcomb, Maria and Pauli, Duke and Peterson, Kyle T. and Shakoor, Nadia and Stylianou, Abby and Zender, Charles S. and Mockler, Todd C.},
	year = {2022},
	keywords = {Artificial intelligence, Atmospheric measurements, Atmospheric modeling, Bidirectional reflectance distribution function (BRDF) correction, Calibration, Cameras, Hyperspectral imaging, Radiometry, high-throughput phenotyping, image quality assessment, shadow compensation, soil removal},
	pages = {1--20},
}

@article{duan_land_2013,
	title = {Land {Surface} {Reflectance} {Retrieval} from {Hyperspectral} {Data} {Collected} by an {Unmanned} {Aerial} {Vehicle} over the {Baotou} {Test} {Site}},
	volume = {8},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0066972},
	doi = {10.1371/journal.pone.0066972},
	abstract = {To evaluate the in-flight performance of a new hyperspectral sensor onboard an unmanned aerial vehicle (UAV-HYPER), a comprehensive field campaign was conducted over the Baotou test site in China on 3 September 2011. Several portable reference reflectance targets were deployed across the test site. The radiometric performance of the UAV-HYPER sensor was assessed in terms of signal-to-noise ratio (SNR) and the calibration accuracy. The SNR of the different bands of the UAV-HYPER sensor was estimated to be between approximately 5 and 120 over the homogeneous targets, and the linear response of the apparent reflectance ranged from approximately 0.05 to 0.45. The uniform and non-uniform Lambertian land surface reflectance was retrieved and validated using in situ measurements, with root mean square error (RMSE) of approximately 0.01–0.07 and relative RMSE of approximately 5\%–12\%. There were small discrepancies between the retrieved uniform and non-uniform Lambertian land surface reflectance over the homogeneous targets and under low aerosol optical depth (AOD) conditions (AOD = 0.18). However, these discrepancies must be taken into account when adjacent pixels had large land surface reflectance contrast and under high AOD conditions (e.g. AOD = 1.0).},
	language = {en},
	number = {6},
	urldate = {2022-11-03},
	journal = {PLOS ONE},
	author = {Duan, Si-Bo and Li, Zhao-Liang and Tang, Bo-Hui and Wu, Hua and Ma, Lingling and Zhao, Enyu and Li, Chuanrong},
	month = jun,
	year = {2013},
	keywords = {Aerosols, Algorithms, Atmospheric layers, Data acquisition, Mathematical functions, Oxygen, Signal to noise ratio, Vapors},
	pages = {e66972},
}

@article{jakob_need_2017,
	title = {The {Need} for {Accurate} {Geometric} and {Radiometric} {Corrections} of {Drone}-{Borne} {Hyperspectral} {Data} for {Mineral} {Exploration}: {MEPHySTo}—{A} {Toolbox} for {Pre}-{Processing} {Drone}-{Borne} {Hyperspectral} {Data}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {The {Need} for {Accurate} {Geometric} and {Radiometric} {Corrections} of {Drone}-{Borne} {Hyperspectral} {Data} for {Mineral} {Exploration}},
	url = {https://www.mdpi.com/2072-4292/9/1/88},
	doi = {10.3390/rs9010088},
	abstract = {Drone-borne hyperspectral imaging is a new and promising technique for fast and precise acquisition, as well as delivery of high-resolution hyperspectral data to a large variety of end-users. Drones can overcome the scale gap between field and air-borne remote sensing, thus providing high-resolution and multi-temporal data. They are easy to use, flexible and deliver data within cm-scale resolution. So far, however, drone-borne imagery has prominently and successfully been almost solely used in precision agriculture and photogrammetry. Drone technology currently mainly relies on structure-from-motion photogrammetry, aerial photography and agricultural monitoring. Recently, a few hyperspectral sensors became available for drones, but complex geometric and radiometric effects complicate their use for geology-related studies. Using two examples, we first show that precise corrections are required for any geological mapping. We then present a processing toolbox for frame-based hyperspectral imaging systems adapted for the complex correction of drone-borne hyperspectral imagery. The toolbox performs sensor- and platform-specific geometric distortion corrections. Furthermore, a topographic correction step is implemented to correct for rough terrain surfaces. We recommend the c-factor-algorithm for geological applications. To our knowledge, we demonstrate for the first time the applicability of the corrected dataset for lithological mapping and mineral exploration.},
	language = {en},
	number = {1},
	urldate = {2022-11-03},
	journal = {Remote Sensing},
	author = {Jakob, Sandra and Zimmermann, Robert and Gloaguen, Richard},
	month = jan,
	year = {2017},
	keywords = {Minas de Riotinto, UAS, UAV, drone, exploration, hyperspectral, point matching, processing, structure-from-motion},
	pages = {88},
}

@misc{m_grana_hyperspectral_nodate,
	title = {Hyperspectral {Remote} {Sensing} {Scenes}},
	url = {https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes},
	urldate = {2022-11-02},
	author = {{M. Graña} and {M.A. Veganzons} and {B. Ayerdi}},
	note = {Type: Hyperspectral Remote Sensing Scenes},
}

@article{swayze_influence_2022,
	title = {Influence of {UAS} {Flight} {Altitude} and {Speed} on {Aboveground} {Biomass} {Prediction}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/9/1989},
	doi = {10.3390/rs14091989},
	abstract = {The management of low-density savannah and woodland forests for carbon storage presents a mechanism to offset the expense of ecologically informed forest management strategies. However, existing carbon monitoring systems draw on vast amounts of either field observations or aerial light detection and ranging (LiDAR) collections, making them financially prohibitive in low productivity systems where forest management focuses on promoting resilience to disturbance and multiple uses. This study evaluates how UAS altitude and flight speed influence area-based aboveground forest biomass model predictions. The imagery was acquired across a range of UAS altitudes and flight speeds that influence the efficiency of data collection. Data were processed using common structures from motion photogrammetry algorithms and then modeled using Random Forest. These results are compared to LiDAR observations collected from fixed-wing manned aircraft and modeled using the same routine. Results show a strong positive relationship between flight altitude and plot-based aboveground biomass modeling accuracy. UAS predictions increasingly outperformed (2–24\% increased variance explained) commercial airborne LiDAR strategies as acquisition altitude increased from 80–120 m. The reduced cost of UAS data collection and processing and improved biomass modeling accuracy over airborne LiDAR approaches could make carbon monitoring viable in low productivity forest systems.},
	language = {en},
	number = {9},
	urldate = {2022-10-18},
	journal = {Remote Sensing},
	author = {Swayze, Neal C. and Tinkham, Wade T. and Creasy, Matthew B. and Vogeler, Jody C. and Hoffman, Chad M. and Hudak, Andrew T.},
	month = jan,
	year = {2022},
	keywords = {area-based, carbon, forest, monitoring, random forest, structure from motion, uav, woodland},
	pages = {1989},
}

@article{lv_hyperspectral_2019,
	title = {Hyperspectral image classification based on multiple reduced kernel extreme learning machine},
	volume = {10},
	issn = {1868-808X},
	url = {https://doi.org/10.1007/s13042-019-00926-5},
	doi = {10.1007/s13042-019-00926-5},
	abstract = {This paper presents an efficient hyperspectral images classification method based on multiple reduced kernel extreme learning machine (MRKELM). The MRKELM model is developed on the basis of the multiple kernel leaning method and the reduced kernel extreme learning machine method. In the presented MRKELM, the kernel function are not fixed anymore, multiple kernels are adaptively trained as a hybrid kernel and the optimal kernel combination weights are jointly optimized. Finally, two simulation examples, classification of benchmark datasets and classification of hyperspectral images including Indian Pines, University of Pavia, and Salinas respectively, are used testify the performance of the proposed MRKELM method.},
	language = {en},
	number = {12},
	urldate = {2022-10-17},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Lv, Fei and Han, Min},
	month = dec,
	year = {2019},
	keywords = {Classification, Hyperspectral image, Multiple reduced kernel extreme learning machine, Reduced kernel extreme learning machine},
	pages = {3397--3405},
}

@article{ang_big_2021,
	title = {Big {Data} and {Machine} {Learning} {With} {Hyperspectral} {Information} in {Agriculture}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3051196},
	abstract = {Hyperspectral and multispectral information processing systems and technologies have demonstrated its usefulness for the improvement of agricultural productivity and practices by providing useful information to farmers and crop managers on the factors affecting crop status and growth. These technologies are widely used in a range of agriculture applications such as crop management, crop yield forecasting, crop disease detection, and the monitoring of agriculture land usage, water, and soil conditions. Hyperspectral information sensing can acquire several hundred spectral bands that cover the electromagnetic spectrum of an observational scene in a single acquisition. The resulting hyperspectral data cube contains a large volume of spatial and spectral information. The hyperspectral sequence of images or video further increases the data generation velocity and volume which lead to the Big data challenges particularly in agricultural remote sensing applications. This paper is structured to first give a comprehensive review of representative studies to provide insights into significant research efforts in agriculture using Big data, machine learning and deep learning with the focus on frameworks or architectures, information processing and analytics with hyperspectral and multispectral data. The potential for utilizing Big data, machine learning and deep learning for hyperspectral and multispectral data in agriculture is very promising. The paper then further explores the potential of using ensemble machine learning and scalable parallel discriminant analysis which takes into consideration the spatial and spectral components for Big data in agriculture. To the best of our knowledge, no similar review study on agriculture with Big data, machine learning and deep learning for hyperspectral and multispectral information processing has been reported. Furthermore, the potential of ensemble machine learning and scalable parallel discriminant analysis has not been explored in agriculture information processing. Experiments and data analytics have been performed on hyperspectral data from agriculture for validation. The results have shown the good performance of our approach.},
	journal = {IEEE Access},
	author = {Ang, Kenneth Li-Minn and Seng, Jasmine Kah Phooi},
	year = {2021},
	keywords = {Agriculture, Big Data, Deep learning, Hyperspectral imaging, Information processing, Satellites, Soil, big data, hyperspectral, machine learning, multispectral, parallel computing},
	pages = {36699--36718},
}

@inproceedings{hruska_machine_2018,
	address = {New York, NY, USA},
	series = {{ICGDA} '18},
	title = {Machine learning classification methods in hyperspectral data processing for agricultural applications},
	isbn = {978-1-4503-6445-4},
	url = {https://doi.org/10.1145/3220228.3220242},
	doi = {10.1145/3220228.3220242},
	abstract = {In agricultural applications hyperspectral imaging is used in cases where differences in spectral reflectance of the examined objects are small. However, the large amount of data generated by hyperspectral sensors requires advance processing methods. Machine learning approaches may play an important role in this task. They are known for decades, but they need high volume of data to compute accurate results. Until recently, the availability of hyperspectral data was a big drawback. It was first used in satellites, later in manned aircrafts and data availability from those platforms was limited because of logistics complexity and high price. Nowadays, hyperspectral sensors are available for unmanned aerial vehicles, which enabled to reach a high volume of data, thus overcoming these issues. This way, the aim of this paper is to present the status of the usage of machine learning approaches in the hyperspectral data processing, with a focus on agriculture applications. Nevertheless, there are not many studies available applying machine learning approach to hyperspectral data for agricultural applications. This apparent limitation was in fact the inspiration for making this survey. Preliminary results using UAV-based data are presented, showing the suitability of machine learning techniques in remote sensed data.},
	urldate = {2022-10-17},
	booktitle = {Proceedings of the {International} {Conference} on {Geoinformatics} and {Data} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Hruška, Jonáš and Adão, Telmo and Pádua, Luís and Marques, Pedro and Cunha, António and Peres, Emanuel and Sousa, António and Morais, Raul and Sousa, Joaquim J.},
	month = apr,
	year = {2018},
	keywords = {agriculture, deep learning, hyperspectral data, machine learning, remote sensing},
	pages = {137--141},
}

@inproceedings{kong_hyperspectral_2021,
	title = {Hyperspectral {Image} {Classification} {Method} {Based} on {Machine} {Learning}},
	doi = {10.1109/CEI52496.2021.9574610},
	abstract = {The research of hyperspectral image classification is one of the main contents of hyperspectral remote sensing application. In recent years, machine learning technology has made great progress in the field of image processing. In this paper, three common machine learning algorithms (KNN, SVM, 3D-CNN) are summarized, and experiments are carried out in Pavia University hyperspectral remote sensing data set, and the average accuracy is 85.34\%, 92.53\%, 96.86\%, respectively. The results show that 3D-CNN algorithm has the highest accuracy in hyperspectral image classification, followed by SVM and KNN.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Computer} {Science}, {Electronic} {Information} {Engineering} and {Intelligent} {Control} {Technology} ({CEI})},
	author = {Kong, Zhuo and Yang, HaiTao},
	month = sep,
	year = {2021},
	keywords = {CNN, Classification algorithms, Feature extraction, Hyperspectral, KNN, Kernel, Machine learning, Machine learning algorithms, SVM, Support vector machines, Training, machine learning},
	pages = {392--395},
}

@inproceedings{marques_grapevine_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Grapevine {Varieties} {Classification} {Using} {Machine} {Learning}},
	isbn = {978-3-030-30241-2},
	doi = {10.1007/978-3-030-30241-2_17},
	abstract = {Viticulture has a major impact in the European economy and over the years the intensive grapevine production led to the proliferation of many varieties. Traditionally these varieties are manually catalogued in the field, which is a costly and slow process and being, in many cases, very challenging to classify even for an experienced ampelographer. This article presents a cost-effective and automatic method for grapevine varieties classification based on the analysis of the leaf’s images, taken with an RGB sensor. The proposed method is divided into three steps: (1) color and shape features extraction; (2) training and; (3) classification using Linear Discriminant Analysis. This approach was applied in 240 leaf images of three different grapevine varieties acquired from the Douro Valley region in Portugal and it was able to correctly classify 87\% of the grapevine leaves. The proposed method showed very promising classification capabilities considering the challenges presented by the leaves which had many shape irregularities and, in many cases, high color similarities for the different varieties. The obtained results compared with manual procedure suggest that it can be used as an effective alternative to the manual procedure for grapevine classification based on leaf features. Since the proposed method requires a simple and low-cost setup it can be easily integrated on a portable system with real-time processing to assist technicians in the field or other staff without any special skills and used offline for batch classification.},
	language = {en},
	booktitle = {Progress in {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Marques, Pedro and Pádua, Luís and Adão, Telmo and Hruška, Jonáš and Sousa, José and Peres, Emanuel and Sousa, Joaquim J. and Morais, Raul and Sousa, António},
	editor = {Moura Oliveira, Paulo and Novais, Paulo and Reis, Luís Paulo},
	year = {2019},
	keywords = {Ampelography, Classification, Digital image processing, Grapevine leaves, Machine learning, Precision viticulture},
	pages = {186--199},
}

@article{agarla_analysis_2021,
	title = {An analysis of spectral similarity measures},
	volume = {29},
	url = {https://library.imaging.org/cic/articles/29/1/art00049},
	doi = {10.2352/issn.2169-2629.2021.29.300},
	abstract = {In this paper we analyze the most used measures for the assessment of spectral similarity of reflectance and radiance signals. First of all we divide them in five groups on the basis of the type of errors they measure. We proceed analyzing their mathematical definition to identify unintended behaviors and types of errors they are blind to. Then exploiting the Munsell atlas we analyze the correlation between metrics in terms of both Pearson's Linear Correlation Coefficient (PLCC) and Spearman's Rank Order Correlation Coefficient (SROCC). Finally we analyze the behaviour of the selected metrics with respect to two different color properties: the Chroma and the Lightness computed in the CIE L* a* b* color space. The source code of the spectral measures considered is available at the following link: \&lt;ext-link ext-link-type="url" xlink:href="https://celuigi.github.io/spectral-similarity-metrics-comparison/"\&gt;https://celuigi.github.io/spectral-similarity-metrics-comparison/\&lt;/ext-link\&gt;.},
	language = {en},
	urldate = {2022-10-17},
	journal = {Color and Imaging Conference},
	author = {Agarla, Mirko and Bianco, Simone and Celona, Luigi and Schettini, Raimondo and Tchobanou, Mikhail and Bianco, Simone and Celona, Luigi and Schettini, Raimondo and Tchobanou, Mikhail},
	month = nov,
	year = {2021},
	pages = {300--305},
}

@article{ren_novel_2022,
	title = {A {Novel} {Method} for {Hyperspectral} {Mineral} {Mapping} {Based} on {Clustering}-{Matching} and {Nonnegative} {Matrix} {Factorization}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/4/1042},
	doi = {10.3390/rs14041042},
	abstract = {The emergence of hyperspectral imagery paved a new way for rapid mineral mapping. As a classical hyperspectral classification method, spectral matching (SM) can automatically map the spatial distribution of minerals without the need for selecting training samples. However, due to the influence of noise, the mapping accuracy of SM is usually poor, and its per-pixel matching method is inefficient to some extent. To solve these problems, we propose an unsupervised clustering-matching mapping method, using a combination of k-means and SM (KSM). First, nonnegative matrix factorization (NMF) is used and combined with a simple and effective NMF initialization method (SMNMF) for feature extraction. Then, k-means is implemented to get the cluster centers of the extracted features and band depth, which are used for clustering and matching, respectively. Finally, dimensionless matching methods, including spectral angle mapper (SAM), spectral correlation angle (SCA), spectral gradient angle (SGA), and a combined matching method (SCGA) are used to match the cluster centers of band depth with a spectral library to obtain the mineral mapping results. A case study on the airborne hyperspectral image of Cuprite, Nevada, USA, demonstrated that the average overall accuracies of KSM based on SAM, SCA, SGA, and SCGA are approximately 22\%, 22\%, 35\%, and 33\% higher than those of SM, respectively, and KSM can save more than 95\% of the mapping time. Moreover, the mapping accuracy and efficiency of SMNMF are about 15\% and 38\% higher than those of the widely used NMF initialization method. In addition, the proposed SCGA could achieve promising mapping results at both high and low signal-to-noise ratios compared with other matching methods. The mapping method proposed in this study provides a new solution for the rapid and autonomous identification of minerals and other fine objects.},
	language = {en},
	number = {4},
	urldate = {2022-10-17},
	journal = {Remote Sensing},
	author = {Ren, Zhongliang and Zhai, Qiuping and Sun, Lin},
	month = jan,
	year = {2022},
	keywords = {clustering, hyperspectral mineral mapping, nonnegative matrix factorization, spectral matching},
	pages = {1042},
}

@article{noomen_continuum_2006,
	title = {Continuum removed band depth analysis for detecting the effects of natural gas, methane and ethane on maize reflectance},
	volume = {105},
	issn = {0034-4257},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425706002574},
	doi = {10.1016/j.rse.2006.07.009},
	abstract = {It is known that natural gas in the soil affects vegetation health, which may be detected through analysis of reflectance spectra. Since natural gas is invisible, changes in the vegetation could potentially indicate gas leakage. Although it is known that gas in soil affects plant reflectance, the relationship between natural gas and the development and reflectance properties of plants has not been studied. The objective of this study was to test whether natural gas and its two main components, methane and ethane, affect vegetation reflectance in the chlorophyll and water absorption regions. An experiment was carried out in which maize (Zea mays) plants were grown in pots that were flushed with 10 l of gas per day for 39±4 days. Leaf reflectance was measured once a week with a spectrophotometer. The reflectance was analysed using continuum removal of the blue (400–550 nm), red (550–750 nm) and two water absorption features (1370–1570 nm and 1870–2170 nm), after which the band depths and normalized band depths were analyzed for each treatment. The band depth analysis showed that ethane caused an initial increase of 10\% in reflectance between 560 and 590 nm, followed by a decrease during the course of the experiment. Normalized band depth analysis showed that ethane caused a reflectance shift of 1 to 5 nm towards longer wavelengths compared to the control reflectance in the visible region. All gases caused an increase in reflectance in the water absorption bands. The physiological reflectance index, PRI, which has previously linked water stress to photosynthetic activity, suggested that the hydrocarbon gases (particularly ethane) decreased the photosynthetic activity of the plants. The combination of reduced band depths in the chlorophyll and water absorption regions and the increased PRI suggests that ethane gas in the soil hampered a normal water uptake by maize plants in an early stage of their growth. Although further research is necessary to upscale the results from the laboratory to the field, the increased reflectance in the 560–590 nm region caused by ethane together with the increased PRI are promising indicators for gas leakage.},
	language = {en},
	number = {3},
	urldate = {2022-10-17},
	journal = {Remote Sensing of Environment},
	author = {Noomen, Marleen F. and Skidmore, Andrew K. and van der Meer, Freek D. and Prins, Herbert H. T.},
	month = dec,
	year = {2006},
	keywords = {Continuum removal, Ethane, Methane, Natural gas, PRI, Reflectance},
	pages = {262--270},
}

@article{nakamura_shape-based_2013,
	title = {A shape-based similarity measure for time series data with ensemble learning},
	volume = {16},
	issn = {1433-755X},
	url = {https://doi.org/10.1007/s10044-011-0262-6},
	doi = {10.1007/s10044-011-0262-6},
	abstract = {This paper introduces a shape-based similarity measure, called the angular metric for shape similarity (AMSS), for time series data. Unlike most similarity or dissimilarity measures, AMSS is based not on individual data points of a time series but on vectors equivalently representing it. AMSS treats a time series as a vector sequence to focus on the shape of the data and compares data shapes by employing a variant of cosine similarity. AMSS is, by design, expected to be robust to time and amplitude shifting and scaling, but sensitive to short-term oscillations. To deal with the potential drawback, ensemble learning is adopted, which integrates data smoothing when AMSS is used for classification. Evaluative experiments reveal distinct properties of AMSS and its effectiveness when applied in the ensemble framework as compared to existing measures.},
	language = {en},
	number = {4},
	urldate = {2022-10-17},
	journal = {Pattern Analysis and Applications},
	author = {Nakamura, Tetsuya and Taki, Keishi and Nomiya, Hiroki and Seki, Kazuhiro and Uehara, Kuniaki},
	month = nov,
	year = {2013},
	keywords = {Machine learning, Similarity measures, Time series analysis},
	pages = {535--548},
}

@article{ahmed_applied_2021,
	title = {Applied aerial spectroscopy: {A} case study on remote sensing of an ancient and semi-natural woodland},
	volume = {16},
	issn = {1932-6203},
	shorttitle = {Applied aerial spectroscopy},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0260056},
	doi = {10.1371/journal.pone.0260056},
	abstract = {An area of ancient and semi-natural woodland (ASNW) has been investigated by applied aerial spectroscopy using an unmanned aerial vehicle (UAV) with multispectral image (MSI) camera. A novel normalised difference spectral index (NDSI) algorithm was developed using principal component analysis (PCA). This novel NDSI was then combined with a simple segmentation method of thresholding and applied for the identification of native tree species as well as the overall health of the woodland. Using this new approach allowed the identification of trees at canopy level, across 7.4 hectares (73,934 m2) of ASNW, as oak (53\%), silver birch (37\%), empty space (9\%) and dead trees (1\%). This UAV derived data was corroborated, for its accuracy, by a statistically valid ground-level field study that identified oak (47\%), silver birch (46\%) and dead trees (7.4\%). This simple innovative approach, using a low-cost multirotor UAV with MSI camera, is both rapid to deploy, was flown around 100 m above ground level, provides useable high resolution (5.3 cm / pixel) data within 22 mins that can be interrogated using readily available PC-based software to identify tree species. In addition, it provides an overall oversight of woodland health and has the potential to inform a future woodland regeneration strategy.},
	language = {en},
	number = {11},
	urldate = {2022-10-17},
	journal = {PLOS ONE},
	author = {Ahmed, Shara and Nicholson, Catherine E. and Muto, Paul and Perry, Justin J. and Dean, John R.},
	month = nov,
	year = {2021},
	keywords = {Birches, Dendrology, Near-infrared spectroscopy, Oaks, Principal component analysis, Remote sensing, Silver, Trees},
	pages = {e0260056},
}

@inproceedings{singh_data_2022,
	title = {Data {Augmentation} {Through} {Spectrally} {Controlled} {Adversarial} {Networks} for {Classification} of {Multispectral} {Remote} {Sensing} {Images}},
	doi = {10.1109/IGARSS46834.2022.9884928},
	abstract = {Availability of limited training remote sensing datasets is one of the problems in deep learning, as deep architectures require a large number of training samples for proper training. In this paper, we present a technique for data augmentation based on a spectral indexed generative adversarial network to train deep convolutional neural networks. This technique uses the spectral characteristic of multispectral (MS) images to support data augmentation in order to generate realistic training samples with respect to each land-use and land-cover class. The impact of multispectral remote sensing data generated through the spectral indexed GAN are evaluated through classification experiments. Experimental results obtained on the classification of the Sentinel-2 Eurosatallband datasets show that data augmentation through spectral indexed GAN enhances the main accuracy metrics.},
	booktitle = {{IGARSS} 2022 - 2022 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Singh, Abhishek and Bruzzone, Lorenzo},
	month = jul,
	year = {2022},
	keywords = {Analytical models, Classification Accuracy, Convolutional neural networks, Data Augmentation, Deep learning, Generative Model, Generative adversarial networks, Measurement, Remote Sensing Image Analysis, Sensors, Training, Training Data},
	pages = {651--654},
}

@inproceedings{lin_hyperspectral_2022,
	title = {Hyperspectral and {Multispectral} {Image} {Fusion} {Via} {Nonnegative} {Matrix} {Factorization} and {Deep} {Prior} {Regularization}},
	doi = {10.1109/IGARSS46834.2022.9883179},
	abstract = {High-resolution hyperspectral image(HSI) is usually obtained by fusing a low-resolution HSI and a high-resolution RGB/multispectral image(MSI) due to physical imaging technique limitation. This paper proposes a novel fusion model, which factorizes the fusion image into two non-negative ma-trices based on linear spectral mixing model, and introduces deep prior, an implicit regularization function related to a deep denoising neural network, to model image texture laid in the factor matrix. Using the internal proximal altern ating linearized minimization algorithm, the fusion model is solved by a low-resolution and a high-resolution part: the low-resolution part updates one factor matrix based on the low-resolution HSI, whereas the high-resolution part updates the other factor matrix using a denoising neural network based on the high-resolution RGB/MSI. Experiments show that the proposed method delivers improved fusion performance on different datasets without a repeatedly training process, compared with other state-of-the-art methods.},
	booktitle = {{IGARSS} 2022 - 2022 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Lin, Baihong and Zhang, Yiqi and Lin, Zengrong and Wang, Xiaoqing and Huang, Haifeng},
	month = jul,
	year = {2022},
	keywords = {Geoscience and remote sensing, Hyperspectral image, Image texture, Imaging, Measurement, Neural networks, Noise reduction, Training, convolutional neural network(CNN), internal proximal alternating linearized minimization (iPALM), non-negative ma-trix factorization(NMF)},
	pages = {1440--1443},
}

@inproceedings{guan_spatial-spectral_2022,
	title = {Spatial-{Spectral} {Contrastive} {Learning} for {Hyperspectral} {Image} {Classification}},
	doi = {10.1109/IGARSS46834.2022.9883226},
	abstract = {In spite of being widely used in hyperspectral image (HSI) classification, most deep learning algorithms require plenty of labeled samples to achieve satisfactory performance. However, manual labeling is very time-consuming and laborious in practice. To solve such problem, we propose a method, called spatial-spectral contrastive learning (SSCL), to learn the representations of HSIs suitable for classification in an unsuper-vised manner. We demonstrate that the useful contents (e.g., semantics) are invariant under the spatial and spectral domains while the uninformative ones are usually not. Thus, we learn powerful representations that model domain-invariant information by defining a contrastive prediction task. Specifically, two signals are constructed for an HSI sample to include information of the two domains, and the representations of these two signals are then optimized to be similar, such that the domain-invariant contents are extracted. We conduct classification experiments on the learned representation with very few labels, the results of which verify the superiority of our method over the state-of-the-art techniques.},
	booktitle = {{IGARSS} 2022 - 2022 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Guan, Peiyan and Lam, Edmund Y.},
	month = jul,
	year = {2022},
	keywords = {Geoscience and remote sensing, Labeling, Manuals, Predictive models, Representation learning, Semantics, Task analysis, Unsupervised learning, contrastive learning, hyperspectral image classification},
	pages = {1372--1375},
}

@inproceedings{wang_reconstructing_2022,
	title = {Reconstructing {Hyperspectral} {Images} from {RGB} {Inputs} {Based} on {Intrinsic} {Image} {Decomposition}},
	doi = {10.1109/IGARSS46834.2022.9883751},
	abstract = {Spectral super-resolution (SR), which generally reconstructs hyperspectral images (HSIs) from RGB inputs, has attracted lots of attention recently. In this paper, a spectral SR algorithm based on intrinsic image decomposition (IID) is proposed, in which RGB images are decomposed into reflectance images and shading images to fully explore RGB features for HSI reconstruction. Considering that features of the reflectance image are only related to the material of objects, the sparsity of material reflectivity is used to reconstruct the reflectance image of HSI. Moreover, an convonlutional neural network (CNN) is constructed to reconstruct shading parts of HSI. Finally, these two reconstructed results are fused to generate the high spectral resolution HSI and an enhancement network is also designed to further improve the recontruction performance. Experimental results with two benchmark datasets, ICVL and CAVE, demonstrate that the performance of the proposed algorithm is superior to several state-of-the-art spectral SR algorithms.},
	booktitle = {{IGARSS} 2022 - 2022 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Wang, Nan and Mei, Shaohui and Zhang, Yifan and Zhang, Bowei and Ma, Mingyang and Zhang, Xiangqing},
	month = jul,
	year = {2022},
	keywords = {Convolutional neural network, Convolutional neural networks, Correlation, Geoscience and remote sensing, Hyperspectral, Image decomposition, Intrinsic image decomposition, Neural networks, Reflectivity, Super-resolution, Superresolution},
	pages = {2374--2377},
}

@inproceedings{cao_nonlocal_2022,
	title = {Nonlocal {Low}-{Rank} {Regularization} for {Hyperspectral} and {High}-{Resolution} {Remote} {Sensing} {Image} {Fusion}},
	doi = {10.1109/IGARSS46834.2022.9883100},
	abstract = {Fusion of high spatial resolution multispectral images (HR-MSI) and low spatial resolution hyperspectral images (LR-HSI) of the same scene can effectively combine spectral and spatial information to obtain high resolution hyperspectral images (HR-HSI), but it can also cause spectral distortion. To address this problem, we propose a new fusion algorithm (NLLR) based on a combination of low-rank prior and observation model in this paper. In the proposed NLLR method, we incorporate nonlocal spatial similarity and low-rank prior into the fusion problem to better simulate the spatial and spectral features of HR-HSI. By extracting tensor blocks from the hyperspectral and multispectral images, performing a chunking clustering operation on the hyperspectral and mul-tispectral data respectively, and constraining the fusion model using low-rank regularization to transform it into solving a convex optimization problem, followed by iterative optimization of the optimization problem using the alternating direction method of multiplier (ADMM), which can achieve an accurate reconstruction. Experimental results show that NLLR can provide better fusion performance compared to state-of-the-art fusion models.},
	booktitle = {{IGARSS} 2022 - 2022 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Cao, Meng and Bao, Wenxing and Qu, Kewen and Zhang, Xiaowu and Ma, Xuan},
	month = jul,
	year = {2022},
	keywords = {Convex functions, Feature extraction, Hyperspectral image, Iterative methods, Optimization, Spatial resolution, Tensors, Transforms, fusion, low-rank, nonlocal spatial similarity},
	pages = {1444--1447},
}

@inproceedings{peng_markov_2022,
	title = {Markov {Random} {Field} {Based} {Spectral}-{Spatial} {Fusion} {Network} for {Hyperspectral} {Image} {Classification}},
	doi = {10.1109/IGARSS46834.2022.9883452},
	abstract = {In hyperspectral image (HSI) classification task, effectively deriving and incorporating spatial information into spectral features is one of a key focus as it can largely influence the performance. Markov random fields (MRFs) are generative and flexible image texture models, and capable of effectively extracting spatial neighbourhood information along multiple spectral wavebands in an unsupervised way. Its parameter estimation process also shares strong compatibility with deep architecture, especially the convolutional neural networks. In this work, we propose an MRF based spectral-spatial fusion network (SSFNet) for HSI classification. Spatial features are extracted using MRF models and further fused with spectral information. Then the proposed SSFNet takes the fused features as input and produces reliable classification results. Comprehensive experiments conducted on the Indian pines and the Pavia university datasets are reported to verify the proposed method.},
	booktitle = {{IGARSS} 2022 - 2022 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Peng, Yao and Cui, Bin},
	month = jul,
	year = {2022},
	keywords = {Computational modeling, Correlation, Data mining, Deep architecture, Feature extraction, Hyperspectral image classification, Image texture, Markov random field, Training, spectral-spatial fusion network},
	pages = {3155--3158},
}

@article{hassanzadeh_broadacre_2021,
	title = {Broadacre {Crop} {Yield} {Estimation} {Using} {Imaging} {Spectroscopy} from {Unmanned} {Aerial} {Systems} ({UAS}): {A} {Field}-{Based} {Case} {Study} with {Snap} {Bean}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {Broadacre {Crop} {Yield} {Estimation} {Using} {Imaging} {Spectroscopy} from {Unmanned} {Aerial} {Systems} ({UAS})},
	url = {https://www.mdpi.com/2072-4292/13/16/3241},
	doi = {10.3390/rs13163241},
	abstract = {Accurate, precise, and timely estimation of crop yield is key to a grower’s ability to proactively manage crop growth and predict harvest logistics. Such yield predictions typically are based on multi-parametric models and in-situ sampling. Here we investigate the extension of a greenhouse study, to low-altitude unmanned aerial systems (UAS). Our principal objective was to investigate snap bean crop (Phaseolus vulgaris) yield using imaging spectroscopy (hyperspectral imaging) in the visible to near-infrared (VNIR; 400–1000 nm) region via UAS. We aimed to solve the problem of crop yield modelling by identifying spectral features explaining yield and evaluating the best time period for accurate yield prediction, early in time. We introduced a Python library, named Jostar, for spectral feature selection. Embedded in Jostar, we proposed a new ranking method for selected features that reaches an agreement between multiple optimization models. Moreover, we implemented a well-known denoising algorithm for the spectral data used in this study. This study benefited from two years of remotely sensed data, captured at multiple instances over the summers of 2019 and 2020, with 24 plots and 18 plots, respectively. Two harvest stage models, early and late harvest, were assessed at two different locations in upstate New York, USA. Six varieties of snap bean were quantified using two components of yield, pod weight and seed length. We used two different vegetation detection algorithms. the Red-Edge Normalized Difference Vegetation Index (RENDVI) and Spectral Angle Mapper (SAM), to subset the fields into vegetation vs. non-vegetation pixels. Partial least squares regression (PLSR) was used as the regression model. Among nine different optimization models embedded in Jostar, we selected the Genetic Algorithm (GA), Ant Colony Optimization (ACO), Simulated Annealing (SA), and Particle Swarm Optimization (PSO) and their resulting joint ranking. The findings show that pod weight can be explained with a high coefficient of determination (R2 = 0.78–0.93) and low root-mean-square error (RMSE = 940–1369 kg/ha) for two years of data. Seed length yield assessment resulted in higher accuracies (R2 = 0.83–0.98) and lower errors (RMSE = 4.245–6.018 mm). Among optimization models used, ACO and SA outperformed others and the SAM vegetation detection approach showed improved results when compared to the RENDVI approach when dense canopies were being examined. Wavelengths at 450, 500, 520, 650, 700, and 760 nm, were identified in almost all data sets and harvest stage models used. The period between 44–55 days after planting (DAP) the optimal time period for yield assessment. Future work should involve transferring the learned concepts to a multispectral system, for eventual operational use; further attention should also be paid to seed length as a ground truth data collection technique, since this yield indicator is far more rapid and straightforward.},
	language = {en},
	number = {16},
	urldate = {2022-10-17},
	journal = {Remote Sensing},
	author = {Hassanzadeh, Amirhossein and Zhang, Fei and van Aardt, Jan and Murphy, Sean P. and Pethybridge, Sarah J.},
	month = jan,
	year = {2021},
	keywords = {feature selection, hyperspectral imaging, machine learning, snap bean, unmanned aerial vehicle, yield modelling},
	pages = {3241},
}

@article{li_performance_2020,
	title = {Performance {Evaluation} of {Crop} {Segmentation} {Algorithms}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2969451},
	abstract = {This paper presents a thorough evaluation of twenty-one state-of-the-art widely-used crop segmentation algorithms, motived by their significance in vision tasks for further analysis. An ideal crop segmentation algorithm can effectively extract crop information, thus providing an important precondition for the application of intelligent agriculture analytics. In order to enable researchers in this field to fully understand various crop segmentation methods, this paper proposes a new classification strategy of object segmentation by dividing the algorithms into pixel-based and region-based approaches at first, and then systematically evaluating various crop segmentation methods with a unified data benchmark and four common metrics. A new dataset which incorporates crop variety, environment condition and observation distance into consideration is constructed for demonstrating the experiments and comparisons. The effectiveness and robustness of these algorithms were evaluated by three sets of comparative experiments. Based on the quantitative results, we summarize the advantages and disadvantages of the evaluated algorithms from the segmentation performances with four metric indicators. Furthermore, the discussion and evaluation results will provide great support for precision agriculture analysis.},
	journal = {IEEE Access},
	author = {Li, Yanan and Huang, Ziyun and Cao, Zhiguo and Lu, Hao and Wang, Haihui and Zhang, Shuiping},
	year = {2020},
	keywords = {Agriculture, Crop segmentation, Feature extraction, Green products, Image color analysis, Image segmentation, Indexes, Vegetation mapping, performance evaluation, pixel-based classification, region-based classification},
	pages = {36210--36225},
}

@inproceedings{padua_vineyard_2020,
	title = {Vineyard {Classification} {Using} {Machine} {Learning} {Techniques} {Applied} to {RGB}-{UAV} {Imagery}},
	doi = {10.1109/IGARSS39084.2020.9324380},
	abstract = {In this study machine learning methods were applied to RGB data obtained by an unmanned aerial vehicle (UAV) to assess this effectiveness in vineyard classification. The very high-resolution UAV-based imagery was subjected to a photogrammetric processing allowing the generation of different outcomes: orthophoto mosaic, crop surface model and five vegetation indices. The orthophoto mosaic was used in an object-based image analysis approach to group pixels with similar values into objects. Three machine learning techniques-support vector machine (SVM), random forest (RF) and artificial neural network (ANN)-were applied to classify the data into four classes: grapevine, shadow, soil and other vegetation. The data were divided with 22\% (n=240, 60 per class) for training purposes and 78\% (n = 850) for testing purposes. The mean value of the objects from each feature were used to create a dataset for prediction. The results demonstrated that both RF and ANN models showed a good performance, yet the RF classifier achieved better results.},
	booktitle = {{IGARSS} 2020 - 2020 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Pádua, Luís and Adão, Telmo and Hruška, Jonáš and Guimarães, Nathalie and Marques, Pedro and Peres, Emanuel and Sousa, Joaquim J.},
	month = sep,
	year = {2020},
	keywords = {Artificial neural networks, Pipelines, Precision viticulture, Radio frequency, Soil, Support vector machines, Vegetation, Vegetation mapping, artificial neural networks, object-based image analysis, random forests, support vector machines},
	pages = {6309--6312},
}

@article{aguiar_localization_2022,
	title = {Localization and {Mapping} on {Agriculture} {Based} on {Point}-{Feature} {Extraction} and {Semiplanes} {Segmentation} {From} {3D} {LiDAR} {Data}},
	volume = {9},
	issn = {2296-9144},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2022.832165},
	abstract = {Developing ground robots for agriculture is a demanding task. Robots should be capable of performing tasks like spraying, harvesting, or monitoring. However, the absence of structure in the agricultural scenes challenges the implementation of localization and mapping algorithms. Thus, the research and development of localization techniques are essential to boost agricultural robotics. To address this issue, we propose an algorithm called VineSLAM suitable for localization and mapping in agriculture. This approach uses both point- and semiplane-features extracted from 3D LiDAR data to map the environment and localize the robot using a novel Particle Filter that considers both feature modalities. The numeric stability of the algorithm was tested using simulated data. The proposed methodology proved to be suitable to localize a robot using only three orthogonal semiplanes. Moreover, the entire VineSLAM pipeline was compared against a state-of-the-art approach considering three real-world experiments in a woody-crop vineyard. Results show that our approach can localize the robot with precision even in long and symmetric vineyard corridors outperforming the state-of-the-art algorithm in this context.},
	urldate = {2022-10-17},
	journal = {Frontiers in Robotics and AI},
	author = {Aguiar, André Silva and Neves dos Santos, Filipe and Sobreira, Héber and Boaventura-Cunha, José and Sousa, Armando Jorge},
	year = {2022},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2022-10-17},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	doi = {10.48550/arXiv.2010.11929},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{fernandes_grapevine_2019,
	title = {Grapevine variety identification using “{Big} {Data}” collected with miniaturized spectrometer combined with support vector machines and convolutional neural networks},
	volume = {163},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169918310172},
	doi = {10.1016/j.compag.2019.104855},
	abstract = {Several experiments have been previously reported suggesting that the application of spectroscopy and machine learning allows the identification of grapevine varieties, however, up to now, the maximum number of varieties separated was twenty and the total number of sample spectra used does not go beyond the few hundreds. The present work aim is to answer the question: Is it possible to separate one variety from an enlarged group of other varieties when the number of samples is also significantly increased? With this in mind, a total of 35,833 spectra from leaves of 626 plants from 64 varieties were gathered for the study. This is a non-trivial evolution from previous works because it originates an increase in the variability of spectra which brings in a higher risk that a significant percentage of spectra of different varieties are equal and cannot be separated. Simultaneously, it was studied if a miniaturized and easy to use spectrometer could deliver data whose quality was enough to allow varieties separation even with data being collected in the field, non-destructively, and under uncontrolled solar lighting. This data was used to build support vector machines and convolutional neural networks for separating Touriga Nacional from 63 other varieties (including Touriga Franca) or Touriga Franca from 63 varieties (including Touriga Nacional), and the classification efficiencies are analysed.},
	language = {en},
	urldate = {2022-10-17},
	journal = {Computers and Electronics in Agriculture},
	author = {Fernandes, Armando M. and Utkin, Andrei B. and Eiras-Dias, José and Cunha, Jorge and Silvestre, José and Melo-Pinto, Pedro},
	month = aug,
	year = {2019},
	keywords = {Cultivar, Deep learning, Hyperspectral, Machine learning},
	pages = {104855},
}

@article{kiranyaz_1d_2021,
	title = {{1D} convolutional neural networks and applications: {A} survey},
	volume = {151},
	issn = {0888-3270},
	shorttitle = {{1D} convolutional neural networks and applications},
	url = {https://www.sciencedirect.com/science/article/pii/S0888327020307846},
	doi = {10.1016/j.ymssp.2020.107398},
	abstract = {During the last decade, Convolutional Neural Networks (CNNs) have become the de facto standard for various Computer Vision and Machine Learning operations. CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and millions of parameters have the ability to learn complex objects and patterns providing that they can be trained on a massive size visual database with ground-truth labels. With a proper training, this unique ability makes them the primary tool for various engineering applications for 2D signals such as images and video frames. Yet, this may not be a viable option in numerous applications over 1D signals especially when the training data is scarce or application specific. To address this issue, 1D CNNs have recently been proposed and immediately achieved the state-of-the-art performance levels in several applications such as personalized biomedical data classification and early diagnosis, structural health monitoring, anomaly detection and identification in power electronics and electrical motor fault detection. Another major advantage is that a real-time and low-cost hardware implementation is feasible due to the simple and compact configuration of 1D CNNs that perform only 1D convolutions (scalar multiplications and additions). This paper presents a comprehensive review of the general architecture and principals of 1D CNNs along with their major engineering applications, especially focused on the recent progress in this field. Their state-of-the-art performance is highlighted concluding with their unique properties. The benchmark datasets and the principal 1D CNN software used in those applications are also publicly shared in a dedicated website. While there has not been a paper on the review of 1D CNNs and its applications in the literature, this paper fulfills this gap.},
	language = {en},
	urldate = {2022-10-17},
	journal = {Mechanical Systems and Signal Processing},
	author = {Kiranyaz, Serkan and Avci, Onur and Abdeljaber, Osama and Ince, Turker and Gabbouj, Moncef and Inman, Daniel J.},
	month = apr,
	year = {2021},
	keywords = {Arrhythmia detection and identification, Artificial Neural Networks, Condition monitoring, Convolutional neural networks, Deep learning, Fault detection, Machine learning, Structural damage detection, Structural health monitoring},
	pages = {107398},
}

@inproceedings{kim_hyperspectral_2018,
	title = {Hyperspectral {Image} {Classification} {Based} on {Spectral} {Mixture} {Analysis} for {Crop} {Type} {Determination}},
	doi = {10.1109/IGARSS.2018.8519183},
	abstract = {For the application of agricultural area, remote sensing techniques were studied and applied for its advantages for continuous and quantitative monitoring. Especially, hyperspectral images have been studied for the precise agriculture since they provide chemical and physical information of vegetation. In this study, we analyzed crop types using hyperspectral image data collected by a ground scanner. Spectral mixture analysis, which is widely used for processing hyperspectral images, was adopted for the crop discrimination. Endmember extraction algorithms used in this study were N-FINDR, Vertex Component Analysis (VCA), and Simplex Identification via variable Splitting and Augmented Lagrangian (SISAL), and classification was processed using fully constrained linear spectral unmixing (FCLSU). This study presents the application of spectral mixture analysis for hyperspectral scanner data at canopy level and optimal endmember extraction algorithms for different crop types for precise agriculture.},
	booktitle = {{IGARSS} 2018 - 2018 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Kim, Yeji and Kim, Yongil},
	month = jul,
	year = {2018},
	keywords = {Agriculture, Classification algorithms, Hyperspectral images, Hyperspectral imaging, Signal processing algorithms, Vegetation mapping, classification, crop types, spectral mixture analysis},
	pages = {5304--5307},
}

@article{xuan_early_2022,
	title = {Early diagnosis and pathogenesis monitoring of wheat powdery mildew caused by blumeria graminis using hyperspectral imaging},
	volume = {197},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169922002381},
	doi = {10.1016/j.compag.2022.106921},
	abstract = {Powdery mildew caused by blumeria graminis is responsible for wheat yield losses in combination with a decline in quality. Hyperspectral imaging as a promising non-invasive sensor technique has potential for early diagnosis and pathogenesis monitoring of wheat powdery mildew, which is a practice that allows for precision crop protection. Hyperspectral images were first captured before inoculation as healthy samples and daily 2 to 5 days after inoculation (dai) as infected ones. Principal component analysis (PCA) was applied to observe the discrimination capability between samples at different infected stages, while a gray-level co-occurrence matrix (GLCM) was used to extract textural features from the first three principal component images. Then partial least squares discriminant analysis (PLS-DA) model was developed to evaluate the ability for early diagnosis of the disease using effective wavelengths, texture features and their fusion, respectively. Compared with the models using spectral or textural feature alone, PLS-DA model using the fused dataset obtained the best performances with classification accuracy of 91.4 \% in validation sets. Furthermore, spectral angle mapping (SAM) was performed to identify the infected tissue in wheat leaves 2 dai, and to monitor the pathogenesis of powdery mildew over time. The results from this study could be used to develop a portable field monitoring sensor for wheat powdery mildew.},
	language = {en},
	urldate = {2022-10-17},
	journal = {Computers and Electronics in Agriculture},
	author = {Xuan, Guantao and Li, Quankai and Shao, Yuanyuan and Shi, Yukang},
	month = jun,
	year = {2022},
	keywords = {Early diagnosis, Hyperspectral imaging, Pathogenesis monitoring, Powdery mildew, Wheat},
	pages = {106921},
}

@article{xu_spectralspatial_2018,
	title = {Spectral–{Spatial} {Unified} {Networks} for {Hyperspectral} {Image} {Classification}},
	volume = {56},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2018.2827407},
	abstract = {In this paper, we propose a spectral–spatial unified network (SSUN) with an end-to-end architecture for the hyperspectral image (HSI) classification. Different from traditional spectral–spatial classification frameworks where the spectral feature extraction (FE), spatial FE, and classifier training are separated, these processes are integrated into a unified network in our model. In this way, both FE and classifier training will share a uniform objective function and all the parameters in the network can be optimized at the same time. In the implementation of the SSUN, we propose a band grouping-based long short-term memory model and a multiscale convolutional neural network as the spectral and spatial feature extractors, respectively. In the experiments, three benchmark HSIs are utilized to evaluate the performance of the proposed method. The experimental results demonstrate that the SSUN can yield a competitive performance compared with existing methods.},
	number = {10},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Xu, Yonghao and Zhang, Liangpei and Du, Bo and Zhang, Fan},
	month = oct,
	year = {2018},
	keywords = {Computer architecture, Convolutional neural network (CNN), Feature extraction, Hyperspectral imaging, Iron, Logic gates, Training, deep learning, feature extraction (FE), hyperspectral image (HSI) classification, long short-term memory (LSTM)},
	pages = {5893--5909},
}

@misc{ahlswede_weakly_2022,
	title = {Weakly {Supervised} {Semantic} {Segmentation} of {Remote} {Sensing} {Images} for {Tree} {Species} {Classification} {Based} on {Explanation} {Methods}},
	url = {http://arxiv.org/abs/2201.07495},
	abstract = {The collection of a high number of pixel-based labeled training samples for tree species identification is time consuming and costly in operational forestry applications. To address this problem, in this paper we investigate the effectiveness of explanation methods for deep neural networks in performing weakly supervised semantic segmentation using only image-level labels. Specifically, we consider four methods:i) class activation maps (CAM); ii) gradient-based CAM; iii) pixel correlation module; and iv) self-enhancing maps (SEM). We compare these methods with each other using both quantitative and qualitative measures of their segmentation accuracy, as well as their computational requirements. Experimental results obtained on an aerial image archive show that:i) considered explanation techniques are highly relevant for the identification of tree species with weak supervision; and ii) the SEM outperforms the other considered methods. The code for this paper is publicly available at https://git.tu-berlin.de/rsim/rs\_wsss.},
	urldate = {2022-10-17},
	publisher = {arXiv},
	author = {Ahlswede, Steve and Thekke-Madam, Nimisha and Schulz, Christian and Kleinschmit, Birgit and Demir, Begüm},
	month = jan,
	year = {2022},
	doi = {10.48550/arXiv.2201.07495},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.6, I.5.3},
}

@article{santos-rufo_wavelength_2020,
	title = {Wavelength {Selection} {Method} {Based} on {Partial} {Least} {Square} from {Hyperspectral} {Unmanned} {Aerial} {Vehicle} {Orthomosaic} of {Irrigated} {Olive} {Orchards}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/20/3426},
	doi = {10.3390/rs12203426},
	abstract = {Identifying and mapping irrigated areas is essential for a variety of applications such as agricultural planning and water resource management. Irrigated plots are mainly identified using supervised classification of multispectral images from satellite or manned aerial platforms. Recently, hyperspectral sensors on-board Unmanned Aerial Vehicles (UAV) have proven to be useful analytical tools in agriculture due to their high spectral resolution. However, few efforts have been made to identify which wavelengths could be applied to provide relevant information in specific scenarios. In this study, hyperspectral reflectance data from UAV were used to compare the performance of several wavelength selection methods based on Partial Least Square (PLS) regression with the purpose of discriminating two systems of irrigation commonly used in olive orchards. The tested PLS methods include filter methods (Loading Weights, Regression Coefficient and Variable Importance in Projection); Wrapper methods (Genetic Algorithm-PLS, Uninformative Variable Elimination-PLS, Backward Variable Elimination-PLS, Sub-window Permutation Analysis-PLS, Iterative Predictive Weighting-PLS, Regularized Elimination Procedure-PLS, Backward Interval-PLS, Forward Interval-PLS and Competitive Adaptive Reweighted Sampling-PLS); and an Embedded method (Sparse-PLS). In addition, two non-PLS based methods, Lasso and Boruta, were also used. Linear Discriminant Analysis and nonlinear K-Nearest Neighbors techniques were established for identification and assessment. The results indicate that wavelength selection methods, commonly used in other disciplines, provide utility in remote sensing for agronomical purposes, the identification of irrigation techniques being one such example. In addition to the aforementioned, these PLS and non-PLS based methods can play an important role in multivariate analysis, which can be used for subsequent model analysis. Of all the methods evaluated, Genetic Algorithm-PLS and Boruta eliminated nearly 90\% of the original spectral wavelengths acquired from a hyperspectral sensor onboard a UAV while increasing the identification accuracy of the classification.},
	language = {en},
	number = {20},
	urldate = {2022-10-17},
	journal = {Remote Sensing},
	author = {Santos-Rufo, Antonio and Mesas-Carrascosa, Francisco-Javier and García-Ferrer, Alfonso and Meroño-Larriva, Jose Emilio},
	month = jan,
	year = {2020},
	keywords = {PLS, UAV, classification, hyperspectral, irrigation technique, olive tree, wavelength selection},
	pages = {3426},
}

@article{shi_learning_2022,
	title = {Learning {Multiscale} {Temporal}–{Spatial}–{Spectral} {Features} via a {Multipath} {Convolutional} {LSTM} {Neural} {Network} for {Change} {Detection} {With} {Hyperspectral} {Images}},
	volume = {60},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2022.3176642},
	abstract = {Change detection (CD) with hyperspectral images (HSIs) can be effectively performed using deep learning networks (DLNs) by taking advantage of HSIs for their abundant spectral and spatial information and the excellent performance of DLN in machine learning. By modeling the temporal dependence of multiscale representative features, more discriminative information reflecting land use and land cover (LULC) changes can be obtained by suppressing less correlated information while improving the robustness of pseudo-changes caused by imaging noises. However, preserving time-dependent multiscale representative features while extracting spatial–spectral features based on conventional DLN is difficult, mainly due to the structural limitation of conventional DLN. A multipath convolutional long short-term memory (LSTM) multipath convolutional long short-term memory neural network (MP-ConvLSTM) taking advantage of LSTM and convolutional neural network (CNN) through the designed parallel architecture to learn multilevel temporal dependencies of bitemporal HSIs, therefore, was proposed for extracting multiscale temporal–spatial–spectral features by combining hidden states from different paths of ConvLSTM in the present study. In the proposed MP-ConvLSTM, the efficient channel attention (ECA) module was introduced to refine features of different paths, and Siamese CNN was adopted to reduce HSIs’ dimensionality and extract preliminary features to build up an end-to-end trainable model for CD with HSIs. The validity of the MP-ConvLSTM was evaluated using the binary and multiclass CD datasets. The CD accuracy of the proposed MP-ConvLSTM was visually and statistically evaluated by different criteria and compared with those derived from several state-of-the-art (SOTA) CD algorithms. The experiments demonstrated that our proposed model not only outperformed those SOTA CD models but also exhibited better tradeoff between complexity and accuracy in general.},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Shi, Changjiang and Zhang, Zhijie and Zhang, Wanchang and Zhang, Chuanrong and Xu, Qiang},
	year = {2022},
	keywords = {Change detection (CD), Convolution, Convolutional neural networks, Feature extraction, Hyperspectral imaging, Redundancy, Siamese convolutional neural network (CNN), Task analysis, Three-dimensional displays, efficient channel attention (ECA), hyperspectral images (HSIs), long short-term memory (LSTM), multipath convolutional long short-term memory neural network (MP-ConvLSTM)},
	pages = {1--16},
}

@inproceedings{zhang_advances_2019,
	title = {Advances in crop fine classification based on {Hyperspectral} {Remote} {Sensing}},
	doi = {10.1109/Agro-Geoinformatics.2019.8820237},
	abstract = {Classification and recognition of crops is an important prerequisite for crop yield estimation and crop growth monitoring. Rapid and accurate acquisition of crop type, spatial distribution and area information can provide basic basis for crop planting structure optimization and structural reform of agricultural supply side. It is of great significance to the formulation of agricultural policy, the development of social economy and the guarantee of national food security. In recent years, hyperspectral remote sensing has been able to fine classify crop types and varieties and obtain spatial distribution maps and planting structure information of crops by virtue of its many bands, abundant spectral information and sensitivity to small spectral differences among ground objects. This paper summarizes the application of hyperspectral remote sensing in crop fine classification, summarizes the hyperspectral data sources commonly used in crop fine classification at home and abroad, such as Hyperion data, environmental satellite data, CASI data and OMIS data, and analyses the applicability of various data. Meanwhile, the methods of crop fine classification using hyperspectral remote sensing are summarized, including decision tree classification, support vector machine classification, multi-classifier integration, spatial-spectral feature classification, hyperspectral data and radar data fusion classification, and the characteristics of various classification methods are analyzed. It was found that the classification accuracy of crop fine classification based on hyperspectral data was higher (better than 90\%). But there are still some shortcomings: (1) At present, scholars at home and abroad focus on areas with simple planting structure. Most of the crop types in these areas are rice, wheat and other large-scale food crops, but less on cash crops such as sesame, rape, peanut and so on. (2) Hyperspectral remote sensing has high classification accuracy for regions with fewer crop types, but the classification accuracy needs to be improved in regions with many crop types. (3) Hyperspectral data has a high dimension and a large amount of data processing workload, which is not suitable for fine classification of crops in large-scale areas. Future research directions: (1) Expanding the scope of hyperspectral remote sensing monitoring objects, mainly cash crops. (2) Selecting areas with complex planting structure, fragmented plots, fluctuating topography and various crop types for fine classification of crops. (3) Attaching importance to the essential features of hyperspectral remote sensing fine classification and finding a stable classifier which is generally suitable for crop fine classification. (4) The mechanism of crop fine classification using hyperspectral remote sensing and the method of multi-source data fusion need to be further studied.},
	booktitle = {2019 8th {International} {Conference} on {Agro}-{Geoinformatics} ({Agro}-{Geoinformatics})},
	author = {Zhang, Ying and Wang, Di and Zhou, Qingbo},
	month = jul,
	year = {2019},
	keywords = {Agriculture, Crops, Dimensionality reduction, Feature extraction, Fine-classification, Hyperspectral, Hyperspectral imaging, Support vector machines},
	pages = {1--6},
}

@inproceedings{pande_bidirectional_2021,
	title = {Bidirectional {GRU} {Based} {Autoencoder} for {Dimensionality} {Reduction} in {Hyperspectral} {Images}},
	doi = {10.1109/IGARSS47720.2021.9555048},
	abstract = {Hyperspectral images (HSI) are being extensively used in land use/land cover classification because they possess high spectral resolution. Although, this leads to better reflectance distinguishability, the problem of high dimensionality also occurs, making the algorithms data greedy. To counter it, deep learning models, such as autoencoders, are being used. To exploit the contiguous nature of HSIs, sequential models like recurrent neural network (RNNs) are adopted. However, for longer sequences, RNNs exhibit vanishing gradients. Also, they fail to incorporate the future information, limiting their scope. Hence, we propose a Bidirectional Gated Recurrent Unit based autoencoder (BiGRUAE), to project the high dimensional features to a low dimensional space. The bidirectional nature captures the information, both from past and future states, while the gating mechanism of GRU prevents the vanishing gradient. We evaluate our method on two hyperspectral datasets, namely, Indian pines 2010 and Salinas, where our method surpasses the benchmark methods.},
	booktitle = {2021 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium} {IGARSS}},
	author = {Pande, Shivam and Banerjee, Biplab},
	month = jul,
	year = {2021},
	keywords = {Bidirectional GRU, Dimensionality reduction, Hyperspectral images, Image resolution, Limiting, Recurrent neural networks, Reflectivity, Training, Transforms, dimensionality reduction},
	pages = {2731--2734},
}

@article{hang_cascaded_2019,
	title = {Cascaded {Recurrent} {Neural} {Networks} for {Hyperspectral} {Image} {Classification}},
	volume = {57},
	issn = {0196-2892, 1558-0644},
	url = {http://arxiv.org/abs/1902.10858},
	doi = {10.1109/TGRS.2019.2899129},
	abstract = {By considering the spectral signature as a sequence, recurrent neural networks (RNNs) have been successfully used to learn discriminative features from hyperspectral images (HSIs) recently. However, most of these models only input the whole spectral bands into RNNs directly, which may not fully explore the specific properties of HSIs. In this paper, we propose a cascaded RNN model using gated recurrent units (GRUs) to explore the redundant and complementary information of HSIs. It mainly consists of two RNN layers. The first RNN layer is used to eliminate redundant information between adjacent spectral bands, while the second RNN layer aims to learn the complementary information from non-adjacent spectral bands. To improve the discriminative ability of the learned features, we design two strategies for the proposed model. Besides, considering the rich spatial information contained in HSIs, we further extend the proposed model to its spectral-spatial counterpart by incorporating some convolutional layers. To test the effectiveness of our proposed models, we conduct experiments on two widely used HSIs. The experimental results show that our proposed models can achieve better results than the compared models.},
	number = {8},
	urldate = {2022-10-17},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Hang, Renlong and Liu, Qingshan and Hong, Danfeng and Ghamisi, Pedram},
	month = aug,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {5384--5394},
}

@article{chen_spectral-spatial_2021,
	title = {Spectral-spatial feature fusion via dual-stream deep architecture for hyperspectral image classification},
	volume = {119},
	issn = {1350-4495},
	url = {https://www.sciencedirect.com/science/article/pii/S1350449521003078},
	doi = {10.1016/j.infrared.2021.103935},
	abstract = {Recently, deep learning has been widely used in hyperspectral images (HSIs) classification to extract spectral-spatial information. However, how to fuse and utilize spectral-spatial features more efficiently is a challenging task. To address this issue, we propose a spectral-spatial feature fusion via dual-stream deep architecture (SSDS) for HSIs classification in this letter that integrates two core modules of dual-stream and feature fusion. In detail, one stream applies a bi-direction gated recurrent unit (Bi-GRU) to extract spectral features at the pixel level, and another takes the corresponding image patch as an input for a shallow multi-scale parallel 2D convolutional neural network (CNN) to extract multi-scale spatial features. Subsequently, the spectral and spatial features of HSIs are fused by feature fusion module that consists of fusion-wise pooling and fully connected layers to learn discriminative and useful features adaptively. Moreover, a spectral attention module is designed to further extract correlation among spectral bands and reduce interference from irrelevant features. Experimental results on three HSIs datasets, i.e., Pavia University, Salinas, and Kennedy Space Center, demonstrate the effectiveness of the proposed SSDS method. The overall accuracy of SSDS on such three HSIs datasets is 99.22\%, 97.99\%, and 94.35\%, respectively, which increases at least 0.4\%, 0.33\%, and 0.36\% compared with other state-of-the-art methods. In addition, SSDS is less time-consuming than most baselines. For instance, the training time on Salinas data decreases from 1226.7 min (3DCNN) to 31.6 min.},
	language = {en},
	urldate = {2022-10-17},
	journal = {Infrared Physics \& Technology},
	author = {Chen, Rong and Li, Guanghui},
	month = dec,
	year = {2021},
	keywords = {Bi-direction gated recurrent unit (Bi-GRU), Convolutional neural network (CNN), Deep learning, Feature fusion, Hyperspectral images (HSIs) classification},
	pages = {103935},
}

@article{hao_geometry-aware_2021,
	title = {Geometry-{Aware} {Deep} {Recurrent} {Neural} {Networks} for {Hyperspectral} {Image} {Classification}},
	volume = {59},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2020.3005623},
	abstract = {Variants of deep networks have been widely used for hyperspectral image (HSI)-classification tasks. Among them, in recent years, recurrent neural networks (RNNs) have attracted considerable attention in the remote sensing community. However, complex geometries cannot be learned easily by the traditional recurrent units [e.g., long short-term memory (LSTM) and gated recurrent unit (GRU)]. In this article, we propose a geometry-aware deep recurrent neural network (Geo-DRNN) for HSI classification. We build this network upon two modules: a U-shaped network (U-Net) and RNNs. We first input the original HSI patches to the U-Net, which can be trained with very few images and obtain a preliminary classification result. We then add RNNs on the top of the U-Net so as to mimic the human brain to refine continuously the output-classification map. However, instead of using the traditional dot product in each gate of the RNNs, we introduce a Net-Gated GRU that increases the nonlinear representation power. Finally, we use a pretrained ResNet as a regularizer to improve further the ability of the proposed network to describe complex geometries. To this end, we construct a geometry-aware ResNet loss, which leverages the pretrained ResNet's knowledge about the different structures in the real world. Our experimental results on real HSIs and road topology images demonstrate that our approach outperforms the state-of-the-art classification methods and can learn complex geometries.},
	number = {3},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Hao, Siyuan and Wang, Wei and Salzmann, Mathieu},
	month = mar,
	year = {2021},
	keywords = {Deep learning, Feature extraction, Geometry, Hyperspectral imaging, Logic gates, Net-Gated recurrent neural networks (RNNs), Recurrent neural networks, U-shaped network (U-Net), gated recurrent unit (GRU), geometry-aware loss, hyperspectral image (HSI) classification, remote sensing},
	pages = {2448--2460},
}

@article{pan_spectral-spatial_2020,
	title = {Spectral-spatial classification for hyperspectral image based on a single {GRU}},
	volume = {387},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231220300692},
	doi = {10.1016/j.neucom.2020.01.029},
	abstract = {Deep learning methods have been successfully used to extract deep features of many hyperspectral tasks. Multiple neural networks have been introduced in the classification of hyperspectral images, such as convolutional neural network (CNN) and recurrent neural network (RNN). In this study, we offer a different perspective on addressing the hyperspectral pixel-level classification task. Most existing methods utilize complex models for this task, but the efficiency of these methods is often ignored. Based on this observation, we propose an effective tiny model for spectral-spatial classification on hyperspectral images based on a single gate recurrent unit (GRU). In our approach, the core GRU can learn spectral correlation within a whole spectrum input, and the spatial information can be fused as the initial hidden state of the GRU. By this way, spectral and spatial features are calculated and expanded together in a single GRU. By comparing the different utilization patterns of RNN with a variety of spatial information fusion methods, our approach demonstrates a competitive advantage in both accuracy and efficiency.},
	language = {en},
	urldate = {2022-10-17},
	journal = {Neurocomputing},
	author = {Pan, Erting and Mei, Xiaoguang and Wang, Quande and Ma, Yong and Ma, Jiayi},
	month = apr,
	year = {2020},
	keywords = {Deep learning, GRU, Hyperspectral image pixel-level classification, RNN},
	pages = {150--160},
}

@article{gutierrez_--go_2018,
	title = {On-{The}-{Go} {Hyperspectral} {Imaging} {Under} {Field} {Conditions} and {Machine} {Learning} for the {Classification} of {Grapevine} {Varieties}},
	volume = {9},
	issn = {1664-462X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6068396/},
	doi = {10.3389/fpls.2018.01102},
	abstract = {Grapevine varietal classification is an important plant phenotyping issue for grape growing and wine industry. This task has been achieved from destructive techniques like classic ampelography and DNA analysis under laboratory conditions. This work displays a new approach for the classification of a high number of grapevine (Vitis vinifera L.) varieties under field conditions using on-the-go hyperspectral imaging and different machine learning algorithms. On-the-go imaging was performed under natural illumination using a hyperspectral camera mounted on an all-terrain vehicle at 5 km/h. Spectra were acquired over two different leaf phenological stages on the canopy of 30 different varieties on a commercial vineyard located in La Rioja, Spain. A total of 1,200 spectral samples were generated. Support vector machines (SVM) and artificial neural networks (multilayer perceptrons, MLP) were used for the development of a large number of models, testing different algorithm parameters and spectral pre-processing techniques. Both classifiers yielded notable performance values and were able to train models with recall F1 scores and area under the receiver operating characteristic curve marks up to 0.99 for 5-fold cross validation. Statistical analyses supported that the best SVM kernel was linear and the best activation function for MLP was the hyperbolic tangent function. The prediction performance for individual varieties of MLP ranged from 0.94 to 0.99, displaying low levels of variability. In the case of SVM, slightly higher differences were obtained, ranging from 0.83 to 0.97 for individual varieties. These results support the possibility of deploying an on-the-go hyperspectral imaging system in the field capable of successfully classifying leaves from different grapevine varieties. This technology could thus be considered as a new useful non-destructive tool for plant phenotyping under field conditions.},
	urldate = {2022-10-17},
	journal = {Frontiers in Plant Science},
	author = {Gutiérrez, Salvador and Fernández-Novales, Juan and Diago, Maria P. and Tardaguila, Javier},
	month = jul,
	year = {2018},
	pmid = {30090110},
	pmcid = {PMC6068396},
	pages = {1102},
}

@article{bendel_detection_2020,
	title = {Detection of {Grapevine} {Leafroll}-{Associated} {Virus} 1 and 3 in {White} and {Red} {Grapevine} {Cultivars} {Using} {Hyperspectral} {Imaging}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/10/1693},
	doi = {10.3390/rs12101693},
	abstract = {Grapevine leafroll disease (GLD) is considered one of the most widespread grapevine virus diseases, causing severe economic losses worldwide. To date, six grapevine leafroll-associated viruses (GLRaVs) are known as causal agents of the disease, of which GLRaV-1 and -3 induce the strongest symptoms. Due to the lack of efficient curative treatments in the vineyard, identification of infected plants and subsequent uprooting is crucial to reduce the spread of this disease. Ground-based hyperspectral imaging (400–2500 nm) was used in this study in order to identify white and red grapevine plants infected with GLRaV-1 or -3. Disease detection models have been successfully developed for greenhouse plants discriminating symptomatic, asymptomatic, and healthy plants. Furthermore, field tests conducted over three consecutive years showed high detection rates for symptomatic white and red cultivars, respectively. The most important detection wavelengths were used to simulate a multispectral system that achieved classification accuracies comparable to the hyperspectral approach. Although differentiation of asymptomatic and healthy field-grown grapevines showed promising results further investigations are needed to improve classification accuracy. Symptoms caused by GLRaV-1 and -3 could be differentiated.},
	language = {en},
	number = {10},
	urldate = {2022-10-17},
	journal = {Remote Sensing},
	author = {Bendel, Nele and Kicherer, Anna and Backhaus, Andreas and Köckerling, Janine and Maixner, Michael and Bleser, Elvira and Klück, Hans-Christian and Seiffert, Udo and Voegele, Ralf T. and Töpfer, Reinhard},
	month = jan,
	year = {2020},
	keywords = {\textit{Vitis vinifera}, GLRaV, Phenoliner, disease detection, grapevine leafroll disease, plant phenotyping, spectral imaging},
	pages = {1693},
}

@article{mendes_vineinspector_2022,
	title = {{VineInspector}: {The} {Vineyard} {Assistant}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2077-0472},
	shorttitle = {{VineInspector}},
	url = {https://www.mdpi.com/2077-0472/12/5/730},
	doi = {10.3390/agriculture12050730},
	abstract = {Proximity sensing approaches with a wide array of sensors available for use in precision viticulture contexts can nowadays be considered both well-know and mature technologies. Still, several in-field practices performed throughout different crops rely on direct visual observation supported on gained experience to assess aspects of plants’ phenological development, as well as indicators relating to the onset of common plagues and diseases. Aiming to mimic in-field direct observation, this paper presents VineInspector: a low-cost, self-contained and easy-to-install system, which is able to measure microclimatic parameters, and also to acquire images using multiple cameras. It is built upon a stake structure, rendering it suitable for deployment across a vineyard. The approach through which distinguishable attributes are detected, classified and tallied in the periodically acquired images, makes use of artificial intelligence approaches. Furthermore, it is made available through an IoT cloud-based support system. VineInspector was field-tested under real operating conditions to assess not only the robustness and the operating functionality of the hardware solution, but also the AI approaches’ accuracy. Two applications were developed to evaluate VineInspector’s consistency while a viticulturist’ assistant in everyday practices. One was intended to determine the size of the very first grapevines’ shoots, one of the required parameters of the well known 3–10 rule to predict primary downy mildew infection. The other was developed to tally grapevine moth males captured in sex traps. Results show that VineInspector is a logical step in smart proximity monitoring by mimicking direct visual observation from experienced viticulturists. While the latter traditionally are responsible for a set of everyday practices in the field, these are time and resource consuming. VineInspector was proven to be effective in two of these practices, performing them automatically. Therefore, it enables both the continuous monitoring and assessment of a vineyard’s phenological development in a more efficient manner, making way to more assertive and timely practices against pests and diseases.},
	language = {en},
	number = {5},
	urldate = {2022-10-17},
	journal = {Agriculture},
	author = {Mendes, Jorge and Peres, Emanuel and Neves dos Santos, Filipe and Silva, Nuno and Silva, Renato and Sousa, Joaquim João and Cortez, Isabel and Morais, Raul},
	month = may,
	year = {2022},
	keywords = {Internet of Things, Scaled-YOLOv4, grapevine downy mildew, pest count, precision viticulture},
	pages = {730},
}

@article{kicherer_phenoliner_2017,
	title = {Phenoliner: {A} {New} {Field} {Phenotyping} {Platform} for {Grapevine} {Research}},
	volume = {17},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {Phenoliner},
	url = {https://www.mdpi.com/1424-8220/17/7/1625},
	doi = {10.3390/s17071625},
	abstract = {In grapevine research the acquisition of phenotypic data is largely restricted to the field due to its perennial nature and size. The methodologies used to assess morphological traits and phenology are mainly limited to visual scoring. Some measurements for biotic and abiotic stress, as well as for quality assessments, are done by invasive measures. The new evolving sensor technologies provide the opportunity to perform non-destructive evaluations of phenotypic traits using different field phenotyping platforms. One of the biggest technical challenges for field phenotyping of grapevines are the varying light conditions and the background. In the present study the Phenoliner is presented, which represents a novel type of a robust field phenotyping platform. The vehicle is based on a grape harvester following the concept of a moveable tunnel. The tunnel it is equipped with different sensor systems (RGB and NIR camera system, hyperspectral camera, RTK-GPS, orientation sensor) and an artificial broadband light source. It is independent from external light conditions and in combination with artificial background, the Phenoliner enables standardised acquisition of high-quality, geo-referenced sensor data.},
	language = {en},
	number = {7},
	urldate = {2022-10-17},
	journal = {Sensors},
	author = {Kicherer, Anna and Herzog, Katja and Bendel, Nele and Klück, Hans-Christian and Backhaus, Andreas and Wieland, Markus and Rose, Johann Christian and Klingbeil, Lasse and Läbe, Thomas and Hohl, Christian and Petry, Willi and Kuhlmann, Heiner and Seiffert, Udo and Töpfer, Reinhard},
	month = jul,
	year = {2017},
	keywords = {\textit{Vitis vinifera}, big data, geo-information, grapevine breeding, plant phenotyping},
	pages = {1625},
}

@article{nguyen_early_2021,
	title = {Early {Detection} of {Plant} {Viral} {Disease} {Using} {Hyperspectral} {Imaging} and {Deep} {Learning}},
	volume = {21},
	issn = {1424-8220},
	doi = {10.3390/s21030742},
	abstract = {Early detection of grapevine viral diseases is critical for early interventions in order to prevent the disease from spreading to the entire vineyard. Hyperspectral remote sensing can potentially detect and quantify viral diseases in a nondestructive manner. This study utilized hyperspectral imagery at the plant level to identify and classify grapevines inoculated with the newly discovered DNA virus grapevine vein-clearing virus (GVCV) at the early asymptomatic stages. An experiment was set up at a test site at South Farm Research Center, Columbia, MO, USA (38.92 N, -92.28 W), with two grapevine groups, namely healthy and GVCV-infected, while other conditions were controlled. Images of each vine were captured by a SPECIM IQ 400-1000 nm hyperspectral sensor (Oulu, Finland). Hyperspectral images were calibrated and preprocessed to retain only grapevine pixels. A statistical approach was employed to discriminate two reflectance spectra patterns between healthy and GVCV vines. Disease-centric vegetation indices (VIs) were established and explored in terms of their importance to the classification power. Pixel-wise (spectral features) classification was performed in parallel with image-wise (joint spatial-spectral features) classification within a framework involving deep learning architectures and traditional machine learning. The results showed that: (1) the discriminative wavelength regions included the 900-940 nm range in the near-infrared (NIR) region in vines 30 days after sowing (DAS) and the entire visual (VIS) region of 400-700 nm in vines 90 DAS; (2) the normalized pheophytization index (NPQI), fluorescence ratio index 1 (FRI1), plant senescence reflectance index (PSRI), anthocyanin index (AntGitelson), and water stress and canopy temperature (WSCT) measures were the most discriminative indices; (3) the support vector machine (SVM) was effective in VI-wise classification with smaller feature spaces, while the RF classifier performed better in pixel-wise and image-wise classification with larger feature spaces; and (4) the automated 3D convolutional neural network (3D-CNN) feature extractor provided promising results over the 2D convolutional neural network (2D-CNN) in learning features from hyperspectral data cubes with a limited number of samples.},
	language = {eng},
	number = {3},
	journal = {Sensors (Basel, Switzerland)},
	author = {Nguyen, Canh and Sagan, Vasit and Maimaitiyiming, Matthew and Maimaitijiang, Maitiniyazi and Bhadra, Sourav and Kwasniewski, Misha T.},
	month = jan,
	year = {2021},
	pmid = {33499335},
	pmcid = {PMC7866105},
	keywords = {2D-CNN, 3D-CNN, Badnavirus, Deep Learning, Finland, Hyperspectral Imaging, Plant Diseases, Plant Viruses, grapevine vein-clearing virus (GVCV), machine learning, plant disease, spectral statistics},
	pages = {742},
}

@inproceedings{belwalkar_accounting_2022,
	title = {Accounting for the {Spectral} {Resolution} on {Sif} {Retrieval} {From} a {Narrow}-{Band} {Airborne} {Imager} {Using} {Scope}},
	doi = {10.1109/IGARSS46834.2022.9884564},
	abstract = {Sub-nanometer hyperspectral imagers are increasingly being used to quantify solar-induced chlorophyll fluorescence (SIF) due to their ability to characterize narrow absorption features accurately. However, some limitations prevent their wide use in the operational context due to their high cost, weight, and complexity. On the other hand, more widely-used narrow-band hyperspectral imagers with 4–6 nm full width at half-maximum (FWHM) resolution could be a costeffective alternative for acquiring high-spatial-resolution hyperspectral imagery to derive SIF. Due to the large effects of the spectral resolution (SR) on the quantified fluorescence, the SIF levels derived from such airborne imagers with 4–6 nm FWHM are overestimated, requiring careful interpretation. In this study, we flew in tandem two airborne hyperspectral imagers with different spectral characteristics. These sensors' imagery was used to model the impact of SR on the SIF quantification using the Soil-Canopy Observation of Photosynthesis and Energy (SCOPE) model. A Support Vector Machine regression (SVR) model trained via SCOPE simulations was employed to quantify SIF at 1 nm SR from the original 5.8 nm FWHM resolution. The performance of the SIF quantification was evaluated theoretically with SCOPE and tested against airborne hyperspectral radiance and the derived SIF. Results showed that the estimated SIF at 1 nm SR agreed well with the reference SCOPE simulations (RMSE=0.097 mW/m2/nm/sr) and with airborne-quantified SIF (RMSE=0.094 mW/m2/nm/sr).},
	booktitle = {{IGARSS} 2022 - 2022 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Belwalkar, A. and Poblete, T. and Hornero, A. and Zarco-Tejada, P.J.},
	month = jul,
	year = {2022},
	keywords = {3FLD, Airborne, Atmospheric modeling, Biological system modeling, Fluorescence, Image resolution, Image sensors, Instruments, SCOPE, SIF, Support vector machines, hyperspectral, radiative transfer, solar-induced fluorescence},
	pages = {5440--5443},
}

@misc{li_faster_2022,
	title = {Faster hyperspectral image classification based on selective kernel mechanism using deep convolutional networks},
	url = {http://arxiv.org/abs/2202.06458},
	abstract = {Hyperspectral imagery is rich in spatial and spectral information. Using 3D-CNN can simultaneously acquire features of spatial and spectral dimensions to facilitate classification of features, but hyperspectral image information spectral dimensional information redundancy. The use of continuous 3D-CNN will result in a high amount of parameters, and the computational power requirements of the device are high, and the training takes too long. This letter designed the Faster selective kernel mechanism network (FSKNet), FSKNet can balance this problem. It designs 3D-CNN and 2D-CNN conversion modules, using 3D-CNN to complete feature extraction while reducing the dimensionality of spatial and spectrum. However, such a model is not lightweight enough. In the converted 2D-CNN, a selective kernel mechanism is proposed, which allows each neuron to adjust the receptive field size based on the two-way input information scale. Under the Selective kernel mechanism, it mainly includes two components, se module and variable convolution. Se acquires channel dimensional attention and variable convolution to obtain spatial dimension deformation information of ground objects. The model is more accurate, faster, and less computationally intensive. FSKNet achieves high accuracy on the IN, UP, Salinas, and Botswana data sets with very small parameters.},
	urldate = {2022-09-25},
	publisher = {arXiv},
	author = {Li, Guandong and Zhang, Chunju},
	month = feb,
	year = {2022},
	doi = {10.48550/arXiv.2202.06458},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{xue_attention-based_2021,
	title = {Attention-{Based} {Second}-{Order} {Pooling} {Network} for {Hyperspectral} {Image} {Classification}},
	volume = {59},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2020.3048128},
	abstract = {Deep learning (DL) has exhibited huge potentials for hyperspectral image (HSI) classification due to its powerful nonlinear modeling and end-to-end optimization characteristics. Although the superior performance of DL-based methods has been witnessed, some limitations can still be found. On the one hand, existing DL frameworks usually resorted to first-order statistical features, whereas they rarely considered second-order or higher order statistical features. On the other hand, the optimization of complex hyperparameters (e.g., the layer number and convolutional kernel size) is time-consuming and a very tough task, making the designed DL framework unexplainable. To overcome these challenges, we propose a novel attention-based second-order pooling network (A-SPN). First, a first-order feature operator is designed to model the spectral–spatial information of HSI. Second, an attention-based second-order pooling (A-SOP) operator is designed to model discriminative and representative features. Finally, a fully connected layer with softmax loss is used for classification. The proposed framework can obtain second-order statistical features in an end-to-end manner. In addition, A-SPN is free of complex hyperparameters tuning, making it more explainable and easily equipped for classification tasks. Experimental results based on three common hyperspectral data sets demonstrate that A-SPN outperforms other traditional and state-of-the-art DL-based HSI classification methods in terms of generalization performance with limited training samples, classification accuracy, convergence rate, and computational complexity.},
	number = {11},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Xue, Zhaohui and Zhang, Mengxue and Liu, Yifeng and Du, Peijun},
	month = nov,
	year = {2021},
	keywords = {Attention mechanism, Computer architecture, Correlation, Feature extraction, Hyperspectral imaging, Optimization, Structural engineering, Training, classification, deep learning (DL), hyperspectral image (HSI), second-order pooling},
	pages = {9600--9615},
}

@article{roy_attention-based_2021,
	title = {Attention-{Based} {Adaptive} {Spectral}–{Spatial} {Kernel} {ResNet} for {Hyperspectral} {Image} {Classification}},
	volume = {59},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2020.3043267},
	abstract = {Hyperspectral images (HSIs) provide rich spectral–spatial information with stacked hundreds of contiguous narrowbands. Due to the existence of noise and band correlation, the selection of informative spectral–spatial kernel features poses a challenge. This is often addressed by using convolutional neural networks (CNNs) with receptive field (RF) having fixed sizes. However, these solutions cannot enable neurons to effectively adjust RF sizes and cross-channel dependencies when forward and backward propagations are used to optimize the network. In this article, we present an attention-based adaptive spectral–spatial kernel improved residual network (A2S2K-ResNet) with spectral attention to capture discriminative spectral–spatial features for HSI classification in an end-to-end training fashion. In particular, the proposed network learns selective 3-D convolutional kernels to jointly extract spectral–spatial features using improved 3-D ResBlocks and adopts an efficient feature recalibration (EFR) mechanism to boost the classification performance. Extensive experiments are performed on three well-known hyperspectral data sets, i.e., IP, KSC, and UP, and the proposed A2S2K-ResNet can provide better classification results in terms of overall accuracy (OA), average accuracy (AA), and Kappa compared with the existing methods investigated. The source code will be made available at https://github.com/suvojit- \$0{\textbackslash}textbackslashtimes 55\$ aa/A2S2K-ResNet.},
	number = {9},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Roy, Swalpa Kumar and Manna, Suvojit and Song, Tiecheng and Bruzzone, Lorenzo},
	month = sep,
	year = {2021},
	keywords = {Channel attention, Data mining, Feature extraction, Hyperspectral imaging, Kernel, Neurons, Radio frequency, Training, convolutional neural networks (CNNs), hyperspectral images (HSIs), image classification, receptive field (RF), residual network (ResNet)},
	pages = {7831--7843},
}

@article{zhu_spectral-spatial-dependent_2021,
	title = {A {Spectral}-{Spatial}-{Dependent} {Global} {Learning} {Framework} for {Insufficient} and {Imbalanced} {Hyperspectral} {Image} {Classification}},
	issn = {2168-2267, 2168-2275},
	url = {http://arxiv.org/abs/2105.14327},
	doi = {10.1109/TCYB.2021.3070577},
	abstract = {Deep learning techniques have been widely applied to hyperspectral image (HSI) classification and have achieved great success. However, the deep neural network model has a large parameter space and requires a large number of labeled data. Deep learning methods for HSI classification usually follow a patchwise learning framework. Recently, a fast patch-free global learning (FPGA) architecture was proposed for HSI classification according to global spatial context information. However, FPGA has difficulty extracting the most discriminative features when the sample data is imbalanced. In this paper, a spectral-spatial dependent global learning (SSDGL) framework based on global convolutional long short-term memory (GCL) and global joint attention mechanism (GJAM) is proposed for insufficient and imbalanced HSI classification. In SSDGL, the hierarchically balanced (H-B) sampling strategy and the weighted softmax loss are proposed to address the imbalanced sample problem. To effectively distinguish similar spectral characteristics of land cover types, the GCL module is introduced to extract the long short-term dependency of spectral features. To learn the most discriminative feature representations, the GJAM module is proposed to extract attention areas. The experimental results obtained with three public HSI datasets show that the SSDGL has powerful performance in insufficient and imbalanced sample problems and is superior to other state-of-the-art methods. Code can be obtained at: https://github.com/dengweihuan/SSDGL.},
	urldate = {2022-09-25},
	journal = {IEEE Transactions on Cybernetics},
	author = {Zhu, Qiqi and Deng, Weihuan and Zheng, Zhuo and Zhong, Yanfei and Guan, Qingfeng and Lin, Weihua and Zhang, Liangpei and Li, Deren},
	year = {2021},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--15},
}

@article{roy_hybridsn_2020,
	title = {{HybridSN}: {Exploring} {3D}-{2D} {CNN} {Feature} {Hierarchy} for {Hyperspectral} {Image} {Classification}},
	volume = {17},
	issn = {1545-598X, 1558-0571},
	shorttitle = {{HybridSN}},
	url = {http://arxiv.org/abs/1902.06701},
	doi = {10.1109/LGRS.2019.2918719},
	abstract = {Hyperspectral image (HSI) classification is widely used for the analysis of remotely sensed images. Hyperspectral imagery includes varying bands of images. Convolutional Neural Network (CNN) is one of the most frequently used deep learning based methods for visual data processing. The use of CNN for HSI classification is also visible in recent works. These approaches are mostly based on 2D CNN. Whereas, the HSI classification performance is highly dependent on both spatial and spectral information. Very few methods have utilized the 3D CNN because of increased computational complexity. This letter proposes a Hybrid Spectral Convolutional Neural Network (HybridSN) for HSI classification. Basically, the HybridSN is a spectral-spatial 3D-CNN followed by spatial 2D-CNN. The 3D-CNN facilitates the joint spatial-spectral feature representation from a stack of spectral bands. The 2D-CNN on top of the 3D-CNN further learns more abstract level spatial representation. Moreover, the use of hybrid CNNs reduces the complexity of the model compared to 3D-CNN alone. To test the performance of this hybrid approach, very rigorous HSI classification experiments are performed over Indian Pines, Pavia University and Salinas Scene remote sensing datasets. The results are compared with the state-of-the-art hand-crafted as well as end-to-end deep learning based methods. A very satisfactory performance is obtained using the proposed HybridSN for HSI classification. The source code can be found at {\textbackslash}textbackslashurl\{https://github.com/gokriznastic/HybridSN\}.},
	number = {2},
	urldate = {2022-09-25},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Roy, Swalpa Kumar and Krishna, Gopal and Dubey, Shiv Ram and Chaudhuri, Bidyut B.},
	month = feb,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {277--281},
}

@misc{chakraborty_spectralnet_2021,
	title = {{SpectralNET}: {Exploring} {Spatial}-{Spectral} {WaveletCNN} for {Hyperspectral} {Image} {Classification}},
	shorttitle = {{SpectralNET}},
	url = {http://arxiv.org/abs/2104.00341},
	abstract = {Hyperspectral Image (HSI) classification using Convolutional Neural Networks (CNN) is widely found in the current literature. Approaches vary from using SVMs to 2D CNNs, 3D CNNs, 3D-2D CNNs. Besides 3D-2D CNNs and FuSENet, the other approaches do not consider both the spectral and spatial features together for HSI classification task, thereby resulting in poor performances. 3D CNNs are computationally heavy and are not widely used, while 2D CNNs do not consider multi-resolution processing of images, and only limits itself to the spatial features. Even though 3D-2D CNNs try to model the spectral and spatial features their performance seems limited when applied over multiple dataset. In this article, we propose SpectralNET, a wavelet CNN, which is a variation of 2D CNN for multi-resolution HSI classification. A wavelet CNN uses layers of wavelet transform to bring out spectral features. Computing a wavelet transform is lighter than computing 3D CNN. The spectral features extracted are then connected to the 2D CNN which bring out the spatial features, thereby creating a spatial-spectral feature vector for classification. Overall a better model is achieved that can classify multi-resolution HSI data with high accuracy. Experiments performed with SpectralNET on benchmark dataset, i.e. Indian Pines, University of Pavia, and Salinas Scenes confirm the superiority of proposed SpectralNET with respect to the state-of-the-art methods. The code is publicly available in https://github.com/tanmay-ty/SpectralNET.},
	urldate = {2022-09-25},
	publisher = {arXiv},
	author = {Chakraborty, Tanmay and Trehan, Utkarsh},
	month = apr,
	year = {2021},
	doi = {10.48550/arXiv.2104.00341},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{moraga_jigsawhsi_2022,
	title = {{JigsawHSI}: a network for {Hyperspectral} {Image} classification},
	shorttitle = {{JigsawHSI}},
	url = {http://arxiv.org/abs/2206.02327},
	abstract = {This article describes Jigsaw, a convolutional neural network (CNN) used in geosciences and based on Inception but tailored for geoscientific analyses. Introduces JigsawHSI (based on Jigsaw) and uses it on the land-use land-cover (LULC) classification problem with the Indian Pines, Pavia University and Salinas hyperspectral image data sets. The network is compared against HybridSN, a spectral-spatial 3D-CNN followed by 2D-CNN that achieves state-of-the-art results on the datasets. This short article proves that JigsawHSI is able to meet or exceed HybridSN's performance in all three cases. Additionally, the use of jigsaw in geosciences is highlighted, while the code and toolkit are made available.},
	urldate = {2022-09-25},
	publisher = {arXiv},
	author = {Moraga, Jaime and Duzgun, H. Sebnem},
	month = jun,
	year = {2022},
	doi = {10.48550/arXiv.2206.02327},
	keywords = {68T07, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.10, I.4.6, Statistics - Machine Learning},
}

@article{padua_monitoring_2020,
	title = {Monitoring of {Chestnut} {Trees} {Using} {Machine} {Learning} {Techniques} {Applied} to {UAV}-{Based} {Multispectral} {Data}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/18/3032},
	doi = {10.3390/rs12183032},
	abstract = {Phytosanitary conditions can hamper the normal development of trees and significantly impact their yield. The phytosanitary condition of chestnut stands is usually evaluated by sampling trees followed by a statistical extrapolation process, making it a challenging task, as it is labor-intensive and requires skill. In this study, a novel methodology that enables multi-temporal analysis of chestnut stands using multispectral imagery acquired from unmanned aerial vehicles is presented. Data were collected in different flight campaigns along with field surveys to identify the phytosanitary issues affecting each tree. A random forest classifier was trained with sections of each tree crown using vegetation indices and spectral bands. These were first categorized into two classes: (i) absence or (ii) presence of phytosanitary issues. Subsequently, the class with phytosanitary issues was used to identify and classify either biotic or abiotic factors. The comparison between the classification results, obtained by the presented methodology, with ground-truth data, allowed us to conclude that phytosanitary problems were detected with an accuracy rate between 86\% and 91\%. As for determining the specific phytosanitary issue, rates between 80\% and 85\% were achieved. Higher accuracy rates were attained in the last flight campaigns, the stage when symptoms are more prevalent. The proposed methodology proved to be effective in automatically detecting and classifying phytosanitary issues in chestnut trees throughout the growing season. Moreover, it is also able to identify decline or expansion situations. It may be of help as part of decision support systems that further improve on the efficient and sustainable management practices of chestnut stands.},
	language = {en},
	number = {18},
	urldate = {2022-08-20},
	journal = {Remote Sensing},
	author = {Pádua, Luís and Marques, Pedro and Martins, Luís and Sousa, António and Peres, Emanuel and Sousa, Joaquim J.},
	month = jan,
	year = {2020},
	keywords = {\textit{Castanea sativa}, chestnut ink disease, multi-temporal data analysis, nutritional deficiencies, phytosanitary status classification, precision agriculture, random forests, unmanned aerial vehicles},
	pages = {3032},
}

@article{soubry_monitoring_2017,
	title = {Monitoring vineyards with {UAV} and multi-sensors for the assessment of water stress and grape maturity},
	volume = {5},
	issn = {2291-3467},
	url = {https://cdnsciencepub.com/doi/full/10.1139/juvs-2016-0024},
	doi = {10.1139/juvs-2016-0024},
	number = {2},
	urldate = {2022-08-15},
	journal = {Journal of Unmanned Vehicle Systems},
	author = {Soubry, I. and Patias, P. and Tsioukas, V.},
	month = jun,
	year = {2017},
	pages = {37--50},
}

@article{peng_prediction_2022,
	title = {Prediction of the {Nitrogen}, {Phosphorus} and {Potassium} {Contents} in {Grape} {Leaves} at {Different} {Growth} {Stages} {Based} on {UAV} {Multispectral} {Remote} {Sensing}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/11/2659},
	doi = {10.3390/rs14112659},
	abstract = {The rapid and accurate acquisition of nitrogen, phosphorus and potassium nutrient contents in grape leaves is critical for improving grape yields and quality and for industrial development. In this study, crop growth was non-destructively monitored based on unmanned aerial vehicle (UAV) remote sensing technology. Three irrigation levels (W1, W2 and W3) and four fertilization levels (F3, F2, F1 and F0) were set in this study, and drip irrigation fertilization treatments adopted a complete block design. A correlation analysis was conducted using UAV multispectral image data obtained from 2019 to 2021 and the field-measured leaf nitrogen content (LNC), leaf potassium content (LKC) and leaf phosphorus content (LPC) values; from the results, the vegetation indices (VIs) that were sensitive to LNC, LKC and LPC were determined. By combining spectral indices with partial least squares (PLS), random forest (RF), support vector machine (SVM) and extreme learning machine (ELM) machine-learning algorithms, prediction models were established. Finally, the optimal combinations of spectral variables and machine learning models for predicting LNC, LPC and LKC in each grape growth period were determined. The results showed that: (1) there were high demands for nitrogen during the new shoot growth and flowering periods, potassium was the main nutrient absorbed in the fruit expansion period, and phosphorus was the main nutrient absorbed in the veraison and maturity periods; (2) combining multiple spectral variables with the RF, SVM and ELM models could result in improved LNC, LPC and LKC predictions. The optimal prediction model determination coefficient (R2) derived during the new shoot growth period was above 0.65, and that obtained during the other growth periods was above 0.75. The relative root mean square error (RRMSE) of the above models was below 0.20, and the Willmott consistency index (WIA) was above 0.88. In conclusion, UAV multispectral images have good application effects when predicting nutrient contents in grape leaves. This study can provide technical support for accurate vineyard nutrient management using UAV platforms.},
	language = {en},
	number = {11},
	urldate = {2022-08-15},
	journal = {Remote Sensing},
	author = {Peng, Xuelian and Chen, Dianyu and Zhou, Zhenjiang and Zhang, Zhitao and Xu, Can and Zha, Qing and Wang, Fang and Hu, Xiaotao},
	month = jan,
	year = {2022},
	keywords = {grape, leaf nutrient contents, machine learning, multispectral imagery, unmanned aerial vehicle},
	pages = {2659},
}

@article{gutierrez_assessing_2021,
	title = {Assessing and mapping vineyard water status using a ground mobile thermal imaging platform},
	volume = {39},
	issn = {1432-1319},
	url = {https://doi.org/10.1007/s00271-021-00735-1},
	doi = {10.1007/s00271-021-00735-1},
	abstract = {Water status directly affects yield and crop quality in grapevines. Precision viticulture demands the application of new available technologies and methodologies for accurate irrigation management in vineyards. The objective of this work was the development of an on-the-go thermal imaging application for the assessment and mapping of vineyard water status, building a dataset from a commercial Tempranillo (Vitis vinifera L.) vineyard over two consecutive seasons and validating it in another commercial vineyard from a different winegrowing region. Thermal imaging was performed with a thermal camera mounted in an all-terrain vehicle, moving at 5 km/h and operating at a distance from the canopy of 1.20 m. Stem water potential (Ψstem) was used for validation as the grapevine water status reference method, using a Scholander pressure chamber. Crop Water Stress Index (CWSI) and Stomatal Conductance Index (Ig) from a 4-day dataset were computed and correlated with Ψstem, delivering significant (p {\textbackslash}textless 0.0001) determination coefficients R2 up to 0.71. The prediction capability of this dataset was also validated in another commercial vineyard, achieving a prediction R2 up to 0.82 (RMSE of 0.123 MPa). The predicted values of both indices were thus employed for mapping vineyard water status in the second plot. These results evidence the potential applicability of on-the-go thermal imaging for assessing and mapping water status in commercial vineyards required for precision viticulture.},
	language = {en},
	number = {4},
	urldate = {2022-08-15},
	journal = {Irrigation Science},
	author = {Gutiérrez, Salvador and Fernández-Novales, Juan and Diago, María-Paz and Iñiguez, Rubén and Tardaguila, Javier},
	month = jul,
	year = {2021},
	pages = {457--468},
}

@article{santesteban_high-resolution_2017,
	series = {Special {Issue}: {Advances} on {ICTs} for {Water} {Management} in {Agriculture}},
	title = {High-resolution {UAV}-based thermal imaging to estimate the instantaneous and seasonal variability of plant water status within a vineyard},
	volume = {183},
	issn = {0378-3774},
	url = {https://www.sciencedirect.com/science/article/pii/S0378377416303201},
	doi = {10.1016/j.agwat.2016.08.026},
	abstract = {Thermal imaging can become a readily usable tool for crop agricultural water management, since it allows a quick determination of canopy surface temperature that, as linked to transpiration, can give an idea of crop water status. In the last years, the resolution of thermal imaging systems has increased and its weight decreased, fostering their implementation on Unmanned Aerial Vehicles (UAV) for civil and agricultural engineering purposes. This approach would overcome most of the limitations of on site thermal imaging, allowing mapping plant water status at either field or farm scale, taking thus into account the naturally existing or artificially induced variability at those scales. The aim of this work was to evaluate to which extent high-resolution thermal imaging allows evaluating the instantaneous and seasonal variability of water status within a vineyard. The novelty and significance of our approach is that the specifically designed and build unmanned aerial vehicle (UAV) provided very high-resolution imaging (pixel {\textbackslash}textless9cm), and that it was used at a commercially relevant acreage (7.5ha). This set-up was used to obtain Crop Water Stress Index (CWSI) from thermal images in a clear-sky day. CWSI values were and compared to stem water potential (Ψs) and stomatal conductance (gs) measured at 14 sampling sites across the vineyard at the moment when images where acquired. In order to evaluate the potential of CWSI acquired in a single day to estimate within-vineyard patterns of variation in water status, a spatial modeling approach was used. CWSI correlated well with Ψs and gs at the moment of image acquisition, showing to have a great potential to monitor instantaneous variations in water status within a vineyard. The information provided by thermal images proved to be relevant at a seasonal scale as well, although it did not match seasonal trends in water status but mimicked other physiological processes occurring during ripening. Therefore, if a picture of variations in water status is required, it would be necessary to acquire thermal images at several dates along the summer.},
	language = {en},
	urldate = {2022-08-15},
	journal = {Agricultural Water Management},
	author = {Santesteban, L. G. and Di Gennaro, S. F. and Herrero-Langreo, A. and Miranda, C. and Royo, J. B. and Matese, A.},
	month = mar,
	year = {2017},
	keywords = {Crop Water Stress Index, L., Stomatal conductance, UAV, Water potential},
	pages = {49--59},
}

@article{di_gennaro_evaluation_2020,
	title = {Evaluation of novel precision viticulture tool for canopy biomass estimation and missing plant detection based on 2.{5D} and {3D} approaches using {RGB} images acquired by {UAV} platform},
	volume = {16},
	issn = {1746-4811},
	url = {https://doi.org/10.1186/s13007-020-00632-2},
	doi = {10.1186/s13007-020-00632-2},
	abstract = {The knowledge of vine vegetative status within a vineyard plays a key role in canopy management in order to achieve a correct vine balance and reach the final desired yield/quality. Detailed information about canopy architecture and missing plants distribution provides useful support for farmers/winegrowers to optimize canopy management practices and the replanting process, respectively. In the last decade, there has been a progressive diffusion of UAV (Unmanned Aerial Vehicles) technologies for Precision Viticulture purposes, as fast and accurate methodologies for spatial variability of geometric plant parameters. The aim of this study was to implement an unsupervised and integrated procedure of biomass estimation and missing plants detection, using both the 2.5D-surface and 3D-alphashape methods.},
	number = {1},
	urldate = {2022-08-15},
	journal = {Plant Methods},
	author = {Di Gennaro, Salvatore Filippo and Matese, Alessandro},
	month = jul,
	year = {2020},
	keywords = {3D model, Canopy biomass, Missing plants, Precision viticulture, Unmanned aerial vehicle},
	pages = {91},
}

@article{pagliai_comparison_2022,
	title = {Comparison of {Aerial} and {Ground} {3D} {Point} {Clouds} for {Canopy} {Size} {Assessment} in {Precision} {Viticulture}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/5/1145},
	doi = {10.3390/rs14051145},
	abstract = {In precision viticulture, the intra-field spatial variability characterization is a crucial step to efficiently use natural resources by lowering the environmental impact. In recent years, technologies such as Unmanned Aerial Vehicles (UAVs), Mobile Laser Scanners (MLS), multispectral sensors, Mobile Apps (MA) and Structure from Motion (SfM) techniques enabled the possibility to characterize this variability with low efforts. The study aims to evaluate, compare and cross-validate the potentiality and the limits of several tools (UAV, MA, MLS) to assess the vine canopy size parameters (thickness, height, volume) by processing 3D point clouds. Three trials were carried out to test the different tools in a vineyard located in the Chianti Classico area (Tuscany, Italy). Each test was made of a UAV flight, an MLS scanning over the vineyard and a MA acquisition over 48 geo-referenced vines. The Leaf Area Index (LAI) were also assessed and taken as reference value. The results showed that the analyzed tools were able to correctly discriminate between zones with different canopy size characteristics. In particular, the R2 between the canopy volumes acquired with the different tools was higher than 0.7, being the highest value of R2 = 0.78 with a RMSE = 0.057 m3 for the UAV vs. MLS comparison. The highest correlations were found between the height data, being the highest value of R2 = 0.86 with a RMSE = 0.105 m for the MA vs. MLS comparison. For the thickness data, the correlations were weaker, being the lowest value of R2 = 0.48 with a RMSE = 0.052 m for the UAV vs. MLS comparison. The correlation between the LAI and the canopy volumes was moderately strong for all the tools with the highest value of R2 = 0.74 for the LAI vs. V\_MLS data and the lowest value of R2 = 0.69 for the LAI vs. V\_UAV data.},
	language = {en},
	number = {5},
	urldate = {2022-08-15},
	journal = {Remote Sensing},
	author = {Pagliai, Andrea and Ammoniaci, Marco and Sarri, Daniele and Lisci, Riccardo and Perria, Rita and Vieri, Marco and D’Arcangelo, Mauro Eugenio Maria and Storchi, Paolo and Kartsiotis, Simon-Paolo},
	month = jan,
	year = {2022},
	keywords = {LAI, LiDAR, UAV, mobile app, precision farming, remote sensing, sensor, spatial variability, vegetation index, vineyard},
	pages = {1145},
}

@incollection{bramley_12_2010,
	series = {Woodhead {Publishing} {Series} in {Food} {Science}, {Technology} and {Nutrition}},
	title = {12 - {Precision} {Viticulture}: managing vineyard variability for improved quality outcomes},
	isbn = {978-1-84569-484-5},
	shorttitle = {12 - {Precision} {Viticulture}},
	url = {https://www.sciencedirect.com/science/article/pii/B9781845694845500129},
	abstract = {Vineyards are variable. In spite of this, conventional approaches to winegrape production infer that the optimal practice is to apply uniform management strategies in vineyards on the assumption of homogeneity. The availability of a suite of technologies, which have collectively become known as Precision Viticulture, provides grapegrowers and winemakers with the means to move away from this ‘one-size-fits all’ approach. Instead, management may be targeted within vineyards according to variation in their inherent characteristics and particular goals in terms of grape yield and quality. Thus, better patches can be exploited whilst weaker areas may be improved. This chapter provides a summary and review of recent research into vineyard variability and the development and adoption of Precision Viticulture. A particular feature of this work has been its focus on improved fruit quality and wine flavour and aroma outcomes.},
	language = {en},
	urldate = {2022-08-15},
	booktitle = {Managing {Wine} {Quality}},
	publisher = {Woodhead Publishing},
	author = {Bramley, R. G. V.},
	editor = {Reynolds, Andrew G.},
	month = jan,
	year = {2010},
	doi = {10.1533/9781845699284.3.445},
	keywords = {remote sensing, spatial variability, terroir, vineyard survey, yield mapping},
	pages = {445--480},
}

@article{campos_development_2019,
	title = {Development of canopy vigour maps using {UAV} for site-specific management during vineyard spraying process},
	volume = {20},
	issn = {1573-1618},
	url = {https://doi.org/10.1007/s11119-019-09643-z},
	doi = {10.1007/s11119-019-09643-z},
	abstract = {Site-specific management of crops represents an important improvement in terms of efficiency and efficacy of the different labours, and its implementation has experienced a large development in the last decades, especially for field crops. The particular case of the spray application process for what are called “specialty crops” (vineyard, orchard fruits, citrus, olive trees, etc.) represents one of the most controversial and influential actions directly related with economical, technical, and environmental aspects. This study was conducted with the main objective to find possible correlations between data obtained from remote sensing technology and the actual canopy characteristics. The potential correlation will be the starting point to develop a variable rate application technology based on prescription maps previously developed. An unmanned aerial vehicle (UAV) equipped with a multispectral camera was used to obtain data to build a canopy vigour map of an entire parcel. By applying the specific software DOSAVIÑA®, the canopy map was then transformed into a practical prescription map, which was uploaded into the dedicated software embedded in the sprayer. Adding to this information precise georeferenced placement of the sprayer, the system was able to modify the working parameters (pressure) in real time in order to follow the prescription map. The results indicate that site-specific management for spray application in vineyards result in a 45\% reduction of application rate when compared with conventional spray application. This fact leads to a equivalent reduction of the amount of pesticide when concentration is maintained constant, showing once more that new technologies can help to achieve the goal of the European legislative network of safe use of pesticides.},
	language = {en},
	number = {6},
	urldate = {2022-08-15},
	journal = {Precision Agriculture},
	author = {Campos, Javier and Llop, Jordi and Gallart, Montserrat and García-Ruiz, Francisco and Gras, Anna and Salcedo, Ramón and Gil, Emilio},
	month = dec,
	year = {2019},
	keywords = {Canopy map, DOSAVIÑA, Spray map, Unmanned aerial vehicle (UAV), Variable rate application},
	pages = {1136--1156},
}

@article{di_gennaro_spectral_2022,
	title = {Spectral {Comparison} of {UAV}-{Based} {Hyper} and {Multispectral} {Cameras} for {Precision} {Viticulture}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/3/449},
	doi = {10.3390/rs14030449},
	abstract = {Analysis of the spectral response of vegetation using optical sensors for non-destructive remote monitoring represents a key element for crop monitoring. Considering the wide presence on the market of unmanned aerial vehicle (UAVs) based commercial solutions, the need emerges for clear information on the performance of these products to guide the end-user in their choice and utilization for precision agriculture applications. This work aims to compare two UAV based commercial products, represented by DJI P4M and SENOP HSC-2 for the acquisition of multispectral and hyperspectral images, respectively, in vineyards. The accuracy of both cameras was evaluated on 6 different targets commonly found in vineyards, represented by bare soil, bare-stony soil, stony soil, soil with dry grass, partially grass covered soil and canopy. Given the importance of the radiometric calibration, four methods for multispectral images correction were evaluated, taking in account the irradiance sensor equipped on the camera (M1–M2) and the use of an empirical line model (ELM) based on reference reflectance panels (M3–M4). In addition, different DJI P4M exposure setups were evaluated. The performance of the cameras was evaluated by means of the calculation of three widely used vegetation indices (VIs), as percentage error (PE) with respect to ground truth spectroradiometer measurements. The results highlighted the importance of reference panels for the radiometric calibration of multispectral images (M1–M2 average PE = 21.8–100.0\%; M3–M4 average PE = 11.9–29.5\%). Generally, the hyperspectral camera provided the best accuracy with a PE ranging between 1.0\% and 13.6\%. Both cameras showed higher performance on the pure canopy pixel target, compared to mixed targets. However, this issue can be easily solved by applying widespread segmentation techniques for the row extraction. This work provides insights to assist end-users in the UAV spectral monitoring to obtain reliable information for the analysis of spatio-temporal variability within vineyards.},
	language = {en},
	number = {3},
	urldate = {2022-08-13},
	journal = {Remote Sensing},
	author = {Di Gennaro, Salvatore Filippo and Toscano, Piero and Gatti, Matteo and Poni, Stefano and Berton, Andrea and Matese, Alessandro},
	month = jan,
	year = {2022},
	keywords = {imaging sensor, precision agriculture, radiometric calibration, remote sensing, spectral signature, vegetation indices},
	pages = {449},
}

@article{matese_beyond_2021,
	title = {Beyond the traditional {NDVI} index as a key factor to mainstream the use of {UAV} in precision viticulture},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-81652-3},
	doi = {10.1038/s41598-021-81652-3},
	abstract = {In the last decade there has been an exponential growth of research activity on the identification of correlations between vegetational indices elaborated by UAV imagery and productive and vegetative parameters of the vine. However, the acquisition and analysis of spectral data require costs and skills that are often not sufficiently available. In this context, the identification of geometric indices that allow the monitoring of spatial variability with low-cost instruments, without spectral analysis know-how but based on photogrammetry techniques with high-resolution RGB cameras, becomes extremely interesting. The aim of this work was to evaluate the potential of new canopy geometry-based indices for the characterization of vegetative and productive agronomic parameters compared to traditional NDVI based on spectral response of the canopy top. Furthermore, considering grape production as a key parameter directly linked to the economic profit of farmers, this study provides a deeper analysis focused on the development of a rapid yield forecast methodology based on UAV data, evaluating both traditional linear and machine learning regressions. Among the yield assessment models, one of the best results was obtained with the canopy thickness which showed high performance with the Gaussian process regression models (R2 = 0.80), while the yield prediction average accuracy of the best ML models reached 85.95\%. The final results obtained confirm the feasibility of this research as a global yield model, which provided good performance through an accurate validation step realized in different years and different vineyards.},
	language = {en},
	number = {1},
	urldate = {2022-08-13},
	journal = {Scientific Reports},
	author = {Matese, Alessandro and Di Gennaro, Salvatore Filippo},
	month = feb,
	year = {2021},
	keywords = {Image processing, Imaging and sensing, Machine learning, Plant sciences},
	pages = {2721},
}

@article{sassu_advances_2021,
	title = {Advances in {Unmanned} {Aerial} {System} {Remote} {Sensing} for {Precision} {Viticulture}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/3/956},
	doi = {10.3390/s21030956},
	abstract = {New technologies for management, monitoring, and control of spatio-temporal crop variability in precision viticulture scenarios are numerous. Remote sensing relies on sensors able to provide useful data for the improvement of management efficiency and the optimization of inputs. unmanned aerial systems (UASs) are the newest and most versatile tools, characterized by high precision and accuracy, flexibility, and low operating costs. The work aims at providing a complete overview of the application of UASs in precision viticulture, focusing on the different application purposes, the applied equipment, the potential of technologies combined with UASs for identifying vineyards’ variability. The review discusses the potential of UASs in viticulture by distinguishing five areas of application: rows segmentation and crop features detection techniques; vineyard variability monitoring; estimation of row area and volume; disease detection; vigor and prescription maps creation. Technological innovation and low purchase costs make UASs the core tools for decision support in the customary use by winegrowers. The ability of the systems to respond to the current demands for the acquisition of digital technologies in agricultural fields makes UASs a candidate to play an increasingly important role in future scenarios of viticulture application.},
	language = {en},
	number = {3},
	urldate = {2022-08-13},
	journal = {Sensors},
	author = {Sassu, Alberto and Gambella, Filippo and Ghiani, Luca and Mercenaro, Luca and Caria, Maria and Pazzona, Antonio Luigi},
	month = jan,
	year = {2021},
	keywords = {3D vineyard characterization, UAS, canopy height model, precision farming, precision viticulture, remote sensing, sustainability of resources, vegetation index, vineyard detection and segmentation},
	pages = {956},
}

@article{ammoniaci_state_2021,
	title = {State of the {Art} of {Monitoring} {Technologies} and {Data} {Processing} for {Precision} {Viticulture}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2077-0472},
	url = {https://www.mdpi.com/2077-0472/11/3/201},
	doi = {10.3390/agriculture11030201},
	abstract = {Precision viticulture (PV) aims to optimize vineyard management, reducing the use of resources, the environmental impact and maximizing the yield and quality of the production. New technologies as UAVs, satellites, proximal sensors and variable rate machines (VRT) are being developed and used more and more frequently in recent years thanks also to informatics systems able to read, analyze and process a huge number of data in order to give the winegrowers a decision support system (DSS) for making better decisions at the right place and time. This review presents a brief state of the art of precision viticulture technologies, focusing on monitoring tools, i.e., remote/proximal sensing, variable rate machines, robotics, DSS and the wireless sensor network.},
	language = {en},
	number = {3},
	urldate = {2022-08-13},
	journal = {Agriculture},
	author = {Ammoniaci, Marco and Kartsiotis, Simon-Paolo and Perria, Rita and Storchi, Paolo},
	month = mar,
	year = {2021},
	keywords = {DSS, UAV, WSN, grapevine, proximal sensing, remote sensing, variable-rate technology, vineyard},
	pages = {201},
}

@article{fan_towards_2021,
	title = {Towards {Measuring} {Shape} {Similarity} of {Polygons} {Based} on {Multiscale} {Features} and {Grid} {Context} {Descriptors}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2220-9964},
	url = {https://www.mdpi.com/2220-9964/10/5/279},
	doi = {10.3390/ijgi10050279},
	abstract = {In spatial analysis applications, measuring the shape similarity of polygons is crucial for polygonal object retrieval and shape clustering. As a complex cognition process, measuring shape similarity should involve finding the difference between polygons, as objects in observation, in terms of visual perception and the differences of the regions, boundaries, and structures formed by the polygons from a mathematical point of view. In existing approaches, the shape similarity of polygons is calculated by only comparing their mathematical characteristics while not taking human perception into consideration. Aiming to solve this problem, we use the features of context and texture of polygons, since they are basic visual perception elements, to fit the cognition purpose. In this paper, we propose a contour diffusion method for the similarity measurement of polygons. By converting a polygon into a grid representation, the contour feature is represented as a multiscale statistic feature, and the region feature is transformed into condensed grid of context features. Instead of treating shape similarity as a distance between two representations of polygons, the proposed method observes similarity as a correlation between textures extracted by shape features. The experiments show that the accuracy of the proposed method is superior to that of the turning function and Fourier descriptor.},
	language = {en},
	number = {5},
	urldate = {2022-07-19},
	journal = {ISPRS International Journal of Geo-Information},
	author = {Fan, Hongchao and Zhao, Zhiyao and Li, Wenwen},
	month = may,
	year = {2021},
	keywords = {grid context, shape similarity, texture},
	pages = {279},
}

@inproceedings{veltkamp_shape_2001,
	title = {Shape matching: similarity measures and algorithms},
	shorttitle = {Shape matching},
	doi = {10.1109/SMA.2001.923389},
	abstract = {Shape matching is an important ingredient in shape retrieval, recognition and classification, alignment and registration, and approximation and simplification. This paper treats various aspects that are needed to solve shape matching problems: choosing the precise problem, selecting the properties of the similarity measure that are needed for the problem, choosing the specific similarity measure, and constructing the algorithm to compute the similarity. The focus is on methods that lie close to the field of computational geometry.},
	booktitle = {Proceedings {International} {Conference} on {Shape} {Modeling} and {Applications}},
	author = {Veltkamp, R.C.},
	month = may,
	year = {2001},
	keywords = {Algorithm design and analysis, Computational geometry, Fourier transforms, Labeling, Particle measurements, Shape measurement, Solids, Standards development, Statistics, Wavelet transforms},
	pages = {188--197},
}

@article{qiaoping_shape_2002,
	title = {Shape similarity measures of linear entities},
	volume = {5},
	issn = {1009-5020},
	url = {https://doi.org/10.1007/BF02833888},
	doi = {10.1007/BF02833888},
	abstract = {The essential of feature matching technology lies in how to measure the similarity of spatial entities. Among all the possible similarity measures, the shape similarity measure is one of the most important measures because it is easy to collect the necessary parameters and it is also well matched with the human intuition. In this paper a new shape similarity measure of linear entities based on the differences of direction change along each line is presented and its effectiveness is illustrated.},
	number = {2},
	urldate = {2022-07-19},
	journal = {Geo-spatial Information Science},
	author = {Qiaoping, Zhang and Deren, Li and Jianya, Gong},
	month = jan,
	year = {2002},
	keywords = {differences of direction change, feature matching, shape analysis, similarity measures of spatial entities},
	pages = {62--67},
}

@article{puletti_unsupervised_2014,
	title = {Unsupervised classification of very high remotely sensed images for grapevine rows detection},
	volume = {47},
	issn = {null},
	url = {https://doi.org/10.5721/EuJRS20144704},
	doi = {10.5721/EuJRS20144704},
	abstract = {In viticulture, knowledge of vineyard vigour represents a useful tool for management. Over large areas, the grapevine vigour is mapped by remote sensing usually with vegetation indices like NDVI. To achieve good correlations between NDVI and other vine parameters the rows of a vineyard must be previously identified. This paper presents an unsupervised classification method for the identification of grapevine rows. Only the red channel of an RGB aerial image is considered as input data. The image is first masked preserving only the considered vineyard and then pre-processed with a high pass filter. The pixel populations are split in “row” and “inter-row” subset through a Ward's modified technique. The proposed methodology is compared with standard object oriented procedure tested on six vineyards located in Tuscany using as reference manually digitalized vine rows.},
	number = {1},
	urldate = {2022-05-03},
	journal = {European Journal of Remote Sensing},
	author = {Puletti, Nicola and Perria, Rita and Storchi, Paolo},
	month = jan,
	year = {2014},
	keywords = {Airborne Remote Sensing, NDVI, precision viticulture, unsupervised classification},
	pages = {45--54},
}

@article{comba_vineyard_2015,
	title = {Vineyard detection from unmanned aerial systems images},
	volume = {114},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169915000861},
	doi = {10.1016/j.compag.2015.03.011},
	abstract = {In viticulture, the adoption of precision agriculture techniques is nowadays increasingly essential to reach required high product quality standards. New reliable tools for mapping crop variability indexes in a vineyard or a single parcel are necessary to deploy site-specific management practices. In this paper, a new method to automatically detect vine rows in gray-scale aerial images is presented. The developed image processing algorithm is constituted by three main steps based on dynamic segmentation, Hough Space Clustering and Total Least Squares techniques. The procedure’s reliability has also been proven in the presence of disturbance elements, like dense inter-row grassing, bushes and trees shadows, by properly detecting vine rows in the vineyard images. Moreover, its adaptive features allow it to obtain optimal results in the presence of uneven image illumination due, for example, to the presence of clouds or steep terrain slopes. The extracted row and inter-row information, besides being the basis for vineyard characterization maps computation, like vine plants vigor maps, could also be used as a reference for other precision viticulture tasks such as, for example, path planning of unmanned ground vehicles.},
	language = {en},
	urldate = {2022-05-03},
	journal = {Computers and Electronics in Agriculture},
	author = {Comba, Lorenzo and Gay, Paolo and Primicerio, Jacopo and Ricauda Aimonino, Davide},
	month = jun,
	year = {2015},
	keywords = {Precision viticulture, Remote sensing, Unmanned aerial vehicles, Vineyard detection},
	pages = {78--87},
}

@article{poblete-echeverria_detection_2017,
	title = {Detection and {Segmentation} of {Vine} {Canopy} in {Ultra}-{High} {Spatial} {Resolution} {RGB} {Imagery} {Obtained} from {Unmanned} {Aerial} {Vehicle} ({UAV}): {A} {Case} {Study} in a {Commercial} {Vineyard}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {Detection and {Segmentation} of {Vine} {Canopy} in {Ultra}-{High} {Spatial} {Resolution} {RGB} {Imagery} {Obtained} from {Unmanned} {Aerial} {Vehicle} ({UAV})},
	url = {https://www.mdpi.com/2072-4292/9/3/268},
	doi = {10.3390/rs9030268},
	abstract = {The use of Unmanned Aerial Vehicles (UAVs) in viticulture permits the capture of aerial Red-Green-Blue (RGB) images with an ultra-high spatial resolution. Recent studies have demonstrated that RGB images can be used to monitor spatial variability of vine biophysical parameters. However, for estimating these parameters, accurate and automated segmentation methods are required to extract relevant information from RGB images. Manual segmentation of aerial images is a laborious and time-consuming process. Traditional classification methods have shown satisfactory results in the segmentation of RGB images for diverse applications and surfaces, however, in the case of commercial vineyards, it is necessary to consider some particularities inherent to canopy size in the vertical trellis systems (VSP) such as shadow effect and different soil conditions in inter-rows (mixed information of soil and weeds). Therefore, the objective of this study was to compare the performance of four classification methods (K-means, Artificial Neural Networks (ANN), Random Forest (RForest) and Spectral Indices (SI)) to detect canopy in a vineyard trained on VSP. Six flights were carried out from post-flowering to harvest in a commercial vineyard cv. Carménère using a low-cost UAV equipped with a conventional RGB camera. The results show that the ANN and the simple SI method complemented with the Otsu method for thresholding presented the best performance for the detection of the vine canopy with high overall accuracy values for all study days. Spectral indices presented the best performance in the detection of Plant class (Vine canopy) with an overall accuracy of around 0.99. However, considering the performance pixel by pixel, the Spectral indices are not able to discriminate between Soil and Shadow class. The best performance in the classification of three classes (Plant, Soil, and Shadow) of vineyard RGB images, was obtained when the SI values were used as input data in trained methods (ANN and RForest), reaching overall accuracy values around 0.98 with high sensitivity values for the three classes.},
	language = {en},
	number = {3},
	urldate = {2022-05-03},
	journal = {Remote Sensing},
	author = {Poblete-Echeverría, Carlos and Olmedo, Guillermo Federico and Ingram, Ben and Bardeen, Matthew},
	month = mar,
	year = {2017},
	keywords = {Otsu method, artificial neural network, classification, image analysis, precision viticulture, random forest, remote sensing, spatial variability},
	pages = {268},
}

@inproceedings{karatzinis_towards_2020,
	title = {Towards an {Integrated} {Low}-{Cost} {Agricultural} {Monitoring} {System} with {Unmanned} {Aircraft} {System}},
	doi = {10.1109/ICUAS48674.2020.9213900},
	abstract = {Over the last years, an intensified interest has been shown in many studies for precision agriculture. Unmanned Aircraft Systems (UASs) are capable of solving a plethora of surveying tasks due to their flexibility, independence and customization. The incorporation of UASs remote sensing in precision agriculture enhances the abilities of crop mapping, management and identification through vegetation indices. In addition to this, different image analysis and computer vision processes were adopted trying to facilitate field operations in cooperation with human intervention to enhance the overall performance. In this paper, we present a practically oriented application on vineyards towards an integrated low-cost system which utilizes Spiral-STC (Spanning Tree Coverage) algorithm as a Coverage Path Planning (CPP) method. Based on the resulted flight campaign, UAV images were collected, and the incorporated image analysis processes finally extract vegetation knowledge. Also, geo referenced orthophotos and computer vision applications complete the generated oversight of the field. These supportive tools provide farmers with useful information (crop health indicators, weather predictions) letting them extrapolate knowledge and identify crop irregularities.},
	booktitle = {2020 {International} {Conference} on {Unmanned} {Aircraft} {Systems} ({ICUAS})},
	author = {Karatzinis, Georgios D. and Apostolidis, Savvas D. and Kapoutsis, Athanasios Ch. and Panagiotopoulou, Liza and Boutalis, Yiannis S. and Kosmatopoulos, Elias B.},
	month = sep,
	year = {2020},
	keywords = {Aircraft},
	pages = {1131--1138},
}

@misc{zhang_rethinking_2020,
	title = {Rethinking {Localization} {Map}: {Towards} {Accurate} {Object} {Perception} with {Self}-{Enhancement} {Maps}},
	shorttitle = {Rethinking {Localization} {Map}},
	url = {http://arxiv.org/abs/2006.05220},
	abstract = {Recently, remarkable progress has been made in weakly supervised object localization (WSOL) to promote object localization maps. The common practice of evaluating these maps applies an indirect and coarse way, i.e., obtaining tight bounding boxes which can cover high-activation regions and calculating intersection-over-union (IoU) scores between the predicted and ground-truth boxes. This measurement can evaluate the ability of localization maps to some extent, but we argue that the maps should be measured directly and delicately, i.e., comparing the maps with the ground-truth object masks pixel-wisely. To fulfill the direct evaluation, we annotate pixel-level object masks on the ILSVRC validation set. We propose to use IoU-Threshold curves for evaluating the real quality of localization maps. Beyond the amended evaluation metric and annotated object masks, this work also introduces a novel self-enhancement method to harvest accurate object localization maps and object boundaries with only category labels as supervision. We propose a two-stage approach to generate the localization maps by simply comparing the similarity of point-wise features between the high-activation and the rest pixels. Based on the predicted localization maps, we explore to estimate object boundaries on a very large dataset. A hard-negative suppression loss is proposed for obtaining fine boundaries. We conduct extensive experiments on the ILSVRC and CUB benchmarks. In particular, the proposed Self-Enhancement Maps achieve the state-of-the-art localization accuracy of 54.88\% on ILSVRC. The code and the annotated masks are released at https://github.com/xiaomengyc/SEM.},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Zhang, Xiaolin and Wei, Yunchao and Yang, Yi and Wu, Fei},
	month = jun,
	year = {2020},
	doi = {10.48550/arXiv.2006.05220},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{nolan_automated_2015,
	title = {Automated detection and segmentation of vine rows using high resolution {UAS} imagery in a commercial vineyard},
	url = {https://research.monash.edu/en/publications/automated-detection-and-segmentation-of-vine-rows-using-high-reso},
	language = {English},
	urldate = {2022-05-03},
	booktitle = {Proceedings - 21st {International} {Congress} on {Modelling} and {Simulation}, {MODSIM} 2015},
	publisher = {Modelling and Simulation Society of Australia and New Zealand Inc. (MSSANZ)},
	author = {Nolan, A. P. and Park, S. and O'Connell, M. and Fuentes, S. and Ryu, D. and Chung, H.},
	year = {2015},
	pages = {1406--1412},
}

@inproceedings{fuentes-penailillo_using_2018,
	title = {Using clustering algorithms to segment {UAV}-based {RGB} images},
	doi = {10.1109/ICA-ACCA.2018.8609822},
	abstract = {This article describes the implementation of two segmentation algorithms in combination with the Triangular Greenness Index (TGI) derived from images obtained from an unmanned aerial vehicle (UAV), with the objective of segmenting shadow, soil and vegetation data obtained from a commercial vineyard cv. Cabernet Sauvignon. The importance of this segmentation lies in the recent development in tools that allow remote monitoring of crops but that nevertheless still have unresolved methodological aspects. The precise differentiation of these classes would allow the development of more complex monitoring techniques based on multispectral and thermal sensors. The results of this investigation showed that both k means and Clustering Large Applications (CLARA) allowed to differentiate three classes in the images corresponding to soil, shade and vegetation. However, CLARA showed a better performance when differentiating the layer corresponding to vegetation.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Automation}/{XXIII} {Congress} of the {Chilean} {Association} of {Automatic} {Control} ({ICA}-{ACCA})},
	author = {Fuentes-Peñailillo, F. and Ortega-Farías, S. and Rivera, M. and Bardeen, M. and Moreno, M.},
	month = oct,
	year = {2018},
	keywords = {Agricultural engineering, Agriculture, Clustering algorithms, Image processing, Image segmentation, Indexes, Open source software, Partitioning algorithms, Soil, Unmanned aerial vehicles, Vegetation mapping},
	pages = {1--5},
}

@article{de_castro_3-d_2018,
	title = {3-{D} {Characterization} of {Vineyards} {Using} a {Novel} {UAV} {Imagery}-{Based} {OBIA} {Procedure} for {Precision} {Viticulture} {Applications}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/10/4/584},
	doi = {10.3390/rs10040584},
	abstract = {Precision viticulture has arisen in recent years as a new approach in grape production. It is based on assessing field spatial variability and implementing site-specific management strategies, which can require georeferenced information of the three dimensional (3D) grapevine canopy structure as one of the input data. The 3D structure of vineyard fields can be generated applying photogrammetric techniques to aerial images collected with Unmanned Aerial Vehicles (UAVs), although processing the large amount of crop data embedded in 3D models is currently a bottleneck of this technology. To solve this limitation, a novel and robust object-based image analysis (OBIA) procedure based on Digital Surface Model (DSM) was developed for 3D grapevine characterization. The significance of this work relies on the developed OBIA algorithm which is fully automatic and self-adaptive to different crop-field conditions, classifying grapevines, and row gap (missing vine plants), and computing vine dimensions without any user intervention. The results obtained in three testing fields on two different dates showed high accuracy in the classification of grapevine area and row gaps, as well as minor errors in the estimates of grapevine height. In addition, this algorithm computed the position, projected area, and volume of every grapevine in the field, which increases the potential of this UAV- and OBIA-based technology as a tool for site-specific crop management applications.},
	language = {en},
	number = {4},
	urldate = {2022-05-04},
	journal = {Remote Sensing},
	author = {De Castro, Ana I. and Jiménez-Brenes, Francisco M. and Torres-Sánchez, Jorge and Peña, José M. and Borra-Serrano, Irene and López-Granados, Francisca},
	month = apr,
	year = {2018},
	keywords = {digital surface model, grapevine canopy mapping, image classification, low cost RGB camera, precision agriculture, remote sensing, site-specific treatments},
	pages = {584},
}

@inproceedings{kerkech_vine_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Vine {Disease} {Detection} by {Deep} {Learning} {Method} {Combined} with {3D} {Depth} {Information}},
	isbn = {978-3-030-51935-3},
	doi = {10.1007/978-3-030-51935-3_9},
	abstract = {Vine disease detection (VDD) is an important asset to predict a probable contagion of virus or fungi. Diseases that spreads through the vineyard has a huge economic impact, therefore it is considered as a challenge for viticulture. Automatic detection and mapping of vine disease in earlier stage can help to limit its impact and reduces the use of chemicals. This study deals with the problem of locating symptomatic areas in images from an unmanned aerial vehicle (UAV) using the visible and infrared domains. This paper, proposes a new method, based on segmentation by a convolutional neuron network SegNet and a depth map (DM), to delineate the asymptomatic regions in the vine canopy. The results obtained showed that SegNet combined with the depth information give better accuracy than a SegNet segmentation alone.},
	language = {en},
	booktitle = {Image and {Signal} {Processing}},
	publisher = {Springer International Publishing},
	author = {Kerkech, Mohamed and Hafiane, Adel and Canals, Raphael and Ros, Frederic},
	editor = {El Moataz, Abderrahim and Mammass, Driss and Mansouri, Alamin and Nouboud, Fathallah},
	year = {2020},
	keywords = {3D, Deep learning, Depth map, Unmanned aerial vehicle, Vine disease detection},
	pages = {82--90},
}

@article{kerkech_vine_2020-1,
	title = {Vine disease detection in {UAV} multispectral images using optimized image registration and deep learning segmentation approach},
	volume = {174},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S016816991932558X},
	doi = {10.1016/j.compag.2020.105446},
	abstract = {One of the major goals of tomorrow’s agriculture is to increase agricultural productivity but above all the quality of production while significantly reducing the use of inputs. Meeting this goal is a real scientific and technological challenge. Smart farming is among the promising approaches that can lead to interesting solutions for vineyard management and reduce the environmental impact. Automatic vine disease detection can increase efficiency and flexibility in managing vineyard crops, while reducing the chemical inputs. This is needed today more than ever, as the use of pesticides is coming under increasing scrutiny and control. The goal is to map diseased areas in the vineyard for fast and precise treatment, thus guaranteeing the maintenance of a healthy state of the vine which is very important for yield management. To tackle this problem, a method is proposed here for Mildew disease detection in vine field using a deep learning segmentation approach on Unmanned Aerial Vehicle (UAV) images. The method is based on the combination of the visible and infrared images obtained from two different sensors. A new image registration method was developed to align visible and infrared images, enabling fusion of the information from the two sensors. A fully convolutional neural network approach uses this information to classify each pixel according to different instances, namely, shadow, ground, healthy and symptom. The proposed method achieved more than 92\% of detection at grapevine-level and 87\%at leaf level, showing promising perspectives for computer aided disease detection in vineyards.},
	language = {en},
	urldate = {2022-05-04},
	journal = {Computers and Electronics in Agriculture},
	author = {Kerkech, Mohamed and Hafiane, Adel and Canals, Raphael},
	month = jul,
	year = {2020},
	keywords = {Convolutional neural network, Disease mapping, Image registration, Precision agriculture, Unmanned aerial vehicle (UAV)},
	pages = {105446},
}

@article{barros_multispectral_2022,
	title = {Multispectral vineyard segmentation: {A} deep learning comparison study},
	volume = {195},
	issn = {0168-1699},
	shorttitle = {Multispectral vineyard segmentation},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169922000990},
	doi = {10.1016/j.compag.2022.106782},
	abstract = {Digital agriculture has evolved significantly over the last few years due to the technological developments in automation and computational intelligence applied to the agricultural sector, including vineyards which are a relevant crop in the Mediterranean region. In this work, a study is presented of semantic segmentation for vine detection in real-world vineyards by exploring state-of-the-art deep segmentation networks and conventional unsupervised methods. Camera data have been collected on vineyards using an Unmanned Aerial System (UAS) equipped with a dual imaging sensor payload, namely a high-definition RGB camera and a five-band multispectral and thermal camera. Extensive experiments using deep-segmentation networks and unsupervised methods have been performed on multimodal datasets representing four distinct vineyards located in the central region of Portugal. The reported results indicate that SegNet, U-Net, and ModSegNet have equivalent overall performance in vine segmentation. The results also show that multimodality slightly improves the performance of vine segmentation, but the NIR spectrum alone generally is sufficient on most of the datasets. Furthermore, results suggest that high-definition RGB images produce equivalent or higher performance than any lower resolution multispectral band combination. Lastly, Deep Learning (DL) networks have higher overall performance than classical methods. The code and dataset are publicly available on https://github.com/Cybonic/DL\_vineyard\_segmentation\_study.git.},
	language = {en},
	urldate = {2022-05-04},
	journal = {Computers and Electronics in Agriculture},
	author = {Barros, T. and Conde, P. and Gonçalves, G. and Premebida, C. and Monteiro, M. and Ferreira, C. S. S. and Nunes, U. J.},
	month = apr,
	year = {2022},
	keywords = {Deep learning, Multispectral, Precision agriculture, Vineyard segmentation},
	pages = {106782},
}

@article{fawakherji_multi-spectral_2021,
	title = {Multi-{Spectral} {Image} {Synthesis} for {Crop}/{Weed} {Segmentation} in {Precision} {Farming}},
	volume = {146},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889021001469},
	doi = {10.1016/j.robot.2021.103861},
	abstract = {An effective perception system is a fundamental component for farming robots, as it enables them to properly perceive the surrounding environment and to carry out targeted operations. The most recent methods make use of state-of-the-art machine learning techniques to learn a valid model for the target task. However, those techniques need a large amount of labeled data for training. A recent approach to deal with this issue is data augmentation through Generative Adversarial Networks (GANs), where entire synthetic scenes are added to the training data, thus enlarging and diversifying their informative content. In this work, we propose an alternative solution with respect to the common data augmentation methods, applying it to the fundamental problem of crop/weed segmentation in precision farming. Starting from real images, we create semi-artificial samples by replacing the most relevant object classes (i.e., crop and weeds) with their synthesized counterparts. To do that, we employ a conditional GAN (cGAN), where the generative model is trained by conditioning the shape of the generated object. Moreover, in addition to RGB data, we take into account also near-infrared (NIR) information, generating four channel multi-spectral synthetic images. Quantitative experiments, carried out on three publicly available datasets, show that (i) our model is capable of generating realistic multi-spectral images of plants and (ii) the usage of such synthetic images in the training process improves the segmentation performance of state-of-the-art semantic segmentation convolutional networks.},
	language = {en},
	urldate = {2022-05-05},
	journal = {Robotics and Autonomous Systems},
	author = {Fawakherji, Mulham and Potena, Ciro and Pretto, Alberto and Bloisi, Domenico D. and Nardi, Daniele},
	month = dec,
	year = {2021},
	keywords = {Agricultural robotics, Crop/weed detection, Semantic segmentation, cGANs},
	pages = {103861},
}

@article{huang_type-2_2021,
	title = {A {Type}-2 {Fuzzy} {Clustering} and {Quantum} {Optimization} {Approach} for {Crops} {Image} {Segmentation}},
	volume = {23},
	issn = {1562-2479},
	doi = {10.1007/s40815-020-01009-2},
	abstract = {Automatic detection of crop yield ripeness is a tedious task because of the presence of various intensities of color in crops. One of the solutions to this problem is the monitoring of those crops by performing segmentation operations. This operation can help to distinguish the ripe and non-ripe regions among the crop images. For this purpose, this study presents a new hybrid crop image segmentation method utilizing type-2 fuzzy set (T2FS), K-means clustering algorithm, and modified quantum optimization algorithm (MQOA). The proposed method fully utilizes the indispensable qualities of these three techniques by (a) using T2FS to represent each color component of crop images in terms of secondary memberships, (b) applying K-means clustering algorithm to extract the similar features from the set of type-2 entropy values obtained from the secondary memberships, and (c) exploiting MQOA to optimize the distance function used in K-means clustering algorithm to obtain the optimal clusters. The performance of the proposed method is assessed based on the experiments carried out on color images of cherry tomatoes. Evidence of experimental results suggests that the proposed method produces extremely effective segmented images relative to those well-known color image segmentation methods available in the literature of pattern recognition and computer vision domains. © 2021, Taiwan Fuzzy Systems Association.},
	language = {English},
	number = {3},
	journal = {International Journal of Fuzzy Systems},
	author = {Huang, Y.-P. and Singh, P. and Kuo, W.-L. and Chu, H.-C.},
	year = {2021},
	keywords = {Fuzzy set, Image segmentation, K-means clustering, Modified quantum optimization algorithm (MQOA)},
	pages = {615--629},
}

@article{gao_recognition_2022,
	title = {A recognition method of multispectral images of soybean canopies based on neural network},
	volume = {68},
	issn = {1574-9541},
	doi = {10.1016/j.ecoinf.2021.101538},
	abstract = {Multispectral images of soybean canopies can reflect plant physiological information and growth status effectively, which is of great significance for soybean high-quality breeding, scientific cultivation, and fine management. At present, it is uneven of the gray level difference of the soybean multispectral images occurred in the leaf edge, and is also small of the gray level difference between the target and the background, resulting in inaccurate recognition of the soybean canopies from the multispectral images. Thus, a multispectral images' recognition method of soybean canopies was proposed based on the neural network. First, the method of Gaussian smoothing filter was used to preprocess the raw soybean multispectral images (green light, near-infrared, red light, red edge, and visible light), which maintained the leaf edge details of the soybean multispectral image. Second, the feedforward neural network model was established to extract the canopy region in the soybean multispectral images. In addition, image morphology operation was used to improve the recognition effects of the soybean canopy. Finally, four quantitative indexes were used to evaluate the experimental results. The results showed that the average effective segmentation rate of the proposed method was 91.69\%, the under-segmentation rate was reduced by 33.34\%, and the over-segmentation rate was reduced by 48.43\%. The difference between the pixel average entropy of the proposed method and the standard canopy image was only 0.2295. The research results can provide not only reliable data for further analysis of physiological and ecological indexes of the soybean canopy, but also technical support for multispectral image recognition of other crop canopies. © 2021 Elsevier B.V.},
	language = {English},
	journal = {Ecological Informatics},
	author = {Gao, S. and Guan, H. and Ma, X.},
	year = {2022},
	keywords = {Algorithm evaluation, Image recognition, Multispectral images, Neural network, Soybean canopy},
}

@article{nagaraju_convolution_2022,
	title = {Convolution network model based leaf disease detection using augmentation techniques},
	volume = {39},
	issn = {0266-4720},
	doi = {10.1111/exsy.12885},
	abstract = {Agriculture plays a vital role in a country's economy. Thus, better yields of crops are required for growth of agriculture-based industries. Deep Learning (DL) techniques have led to remarkable achievements in image classification and recognition. However, DL networks rely heavily on large data sets to prevent overfitting. Image augmentation is one of the DL techniques gaining attention in avoiding the risk of overfitting. The most common Image augmentation techniques like rotation, zoom, and shift used in the existing research allow to generate new images from the original set and increases the images quantity but cannot minimize the misclassification error. The present research can provide a better solution to provide sufficient quantity of training images to a convolutional neural network model to handle the overfitting and classification problems. Therefore, two learning algorithms image preprocessing and transformation algorithm (IPTA) and image masking and REC-based hybrid segmentation algorithm (IMHSA) are proposed to address the problem of limited dataset and convolutional neural network model overfitting during classification. IPTA is an adaptive supervised learning approach to transform the original images into augmented ones and IMHSA is an unsupervised approach for Red, Green, Blue (RGB) image segmentation. Later, the Histogram threshold technique is applied to form all the possible regions used to split the diseased leaf into comparable regions. A novel convolutional neural network model is also proposed to evaluate the performance of the IPTA approach. The model is trained on two independent datasets, one generated before and one generated after IPTA was applied. Plots of precision and loss functions are used to assess the acquired results. The experimental results demonstrated that before using IPTA, the training accuracy was 83\%, while the validation accuracy was 65\%. After using IPTA, the proposed model attained a training accuracy of 74\% and a validation accuracy of 73\%, thereby solving the overfitting problem. The experimental results proved that the proposed model outperforms while classifying the RGB images with the support of image augmentation. © 2021 John Wiley \& Sons Ltd.},
	language = {English},
	number = {4},
	journal = {Expert Systems},
	author = {Nagaraju, M. and Chawla, P. and Upadhyay, S. and Tiwari, R.},
	year = {2022},
	keywords = {computer vision, deep learning, image preprocessing techniques, neural network models, pattern recognition},
}

@article{sainte_fare_garnot_multi-modal_2022,
	title = {Multi-modal temporal attention models for crop mapping from satellite time series},
	volume = {187},
	issn = {0924-2716},
	doi = {10.1016/j.isprsjprs.2022.03.012},
	abstract = {Optical and radar satellite time series are synergetic: optical images contain rich spectral information, while C-band radar captures useful geometrical information and is immune to cloud cover. Motivated by the recent success of temporal attention-based methods across multiple crop mapping tasks, we propose to investigate how these models can be adapted to operate on several modalities. We implement and evaluate multiple fusion schemes, including a novel approach and simple adjustments to the training procedure, significantly improving performance and efficiency with little added complexity. We show that most fusion schemes have advantages and drawbacks, making them relevant for specific settings. We then evaluate the benefit of multimodality across several tasks: parcel classification, pixel-based segmentation, and panoptic parcel segmentation. We show that by leveraging both optical and radar time series, multimodal temporal attention-based models can outmatch single-modality models in terms of performance and resilience to cloud cover. To conduct these experiments, we augment the PASTIS dataset (Garnot and Landrieu, 2021a) with spatially aligned radar image time series. The resulting dataset, PASTIS-R, constitutes the first large-scale, multimodal, and open-access satellite time series dataset with semantic and instance annotations. (Dataset available at: https://zenodo.org/record/5735646) © 2022 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
	language = {English},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Sainte Fare Garnot, V. and Landrieu, L. and Chehata, N.},
	year = {2022},
	keywords = {Data fusion, Deep learning, Multi-temporal fusion, SAR, Sentinel satellite, Temporal attention},
	pages = {294--305},
}

@inproceedings{shen_crop_2020,
	title = {Crop identification using {UAV} image segmentation},
	volume = {11427},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11427/1142720/Crop-identification-using-UAV-image-segmentation/10.1117/12.2552195.full},
	doi = {10.1117/12.2552195},
	abstract = {Crop identification is important to precise agriculture, traditional survey is laboring intensive and classic remote sensing image processing methods are highly dependent on expertise, aiming at this problem, a U-Net based deep segmentation network is proposed for crop identification in UAV remote sensing image segmentation. First, we preprocess the UAV remote images of dataset for training. Then, we replace the transposed convolutional layer in U-Net upsampling block with sub pixel convolutional layer, to solve the problem that edge structure information lost and the low segmentation accuracy. The experiment result shows that, compared with original U-Net and other state of art segmentation model, the proposed method can improve the edge smoothness and pixel wise accuracy. The method uses sub pixel convolution layer for upsampling can be applied to other segmentation model, and the way uses segmentation model to identify the crop species can be extended to other remote sensing image processing filed.},
	urldate = {2022-05-05},
	booktitle = {Second {Target} {Recognition} and {Artificial} {Intelligence} {Summit} {Forum}},
	publisher = {SPIE},
	author = {Shen, Xiaohai and Teng, Yan and Fu, Han and Wan, Zhida and Zhang, Xuewu},
	month = jan,
	year = {2020},
	pages = {480--484},
}

@article{shenming_new_2022,
	title = {A new hyperspectral image classification method based on spatial-spectral features},
	volume = {12},
	issn = {2045-2322},
	doi = {10.1038/s41598-022-05422-5},
	abstract = {In recent years, more and more deep learning frameworks are being applied to hyperspectral image classification tasks and have achieved great results. However, the existing network models have higher model complexity and require more time consumption. Traditional hyperspectral image classification methods tend to ignore the correlation between local spatial features. In this paper, a new hyperspectral image classification method is proposed, which combines two-dimensional Gabor filter with random patch convolution (GRPC) feature extraction to obtain spatial-spectral feature information. The method firstly performs dimensionality reduction through principal component analysis and linear discriminant analysis and extracts the edge texture and spatial information of the image using a Gabor filter for the reduced-dimensional image. Next, the extracted information is convolved with random patches to extract spectral features. Finally, the spatial features and multi-level spectral features are fused to classify the images using the Support Vector Machine classifier. In order to verify the performance of this method, experiments were conducted on three widely used datasets of Indian Pines, Pavia University and Kennedy Space Center. The overall classification accuracy reached 98.09\%, 99.64\% and 96.53\%, which are all higher than other comparison methods. The experimental results reveal the superiority of the proposed method in classification accuracy. © 2022, The Author(s).},
	language = {English},
	number = {1},
	journal = {Scientific Reports},
	author = {Shenming, Q. and Xiang, L. and Zhihua, G.},
	year = {2022},
}

@article{jiang_rapid_2022,
	title = {Rapid and non-destructive detection of natural mildew degree of postharvest {Camellia} oleifera fruit based on hyperspectral imaging},
	volume = {123},
	issn = {1350-4495},
	doi = {10.1016/j.infrared.2022.104169},
	abstract = {Camellia oleifera fruit is a kind of popular woody oil crops in China, which is susceptible to natural mildew damage during storage. This problem significantly depreciates its value and poses a potential threat to human health. A hyperspectral imaging (HSI) system covered visible and near-infrared (Vis-NIR, 400–1000 nm) range was employed to identify Camellia oleifera fruit with three different natural mildew degrees (slight, moderate, and severe). Spectra were primarily extracted from representative regions of interest (ROIs), and principal component analysis (PCA) of extracted spectra revealed that PC1 and PC2 were effective for the identification. Hyperspectral images were then processed using PC transformation for exploratory visual detection by displaying PC1 and PC2 score images. Three modeling methods including partial least squares-discriminant analysis (PLS-DA), k-nearest neighbor (KNN), and classification and regression tree (CART) were subsequently evaluated in terms of their capacity to establish the natural mildew degree estimation model developed by full spectra. Raw spectra without any preprocessing constructed the optimal PLS-DA model with the highest 90.8\% correct classification rate (CCR) of success in external prediction set. After that, key wavelengths selected by competitive adaptive reweighted sampling (CARS) algorithm built an optimal simplified model, achieving the best performance with the highest CCR of 83.3\%, AUC value of 0.89, and Kappa coefficient of 0.75 in prediction set. Therefore, CARS-PLS-DA model was finally employed to visually classify the three different natural mildew degrees. As a result, the general natural mildew degrees of Camellia oleifera samples were readily discernible in pixel-wise manner by generating classification maps. The overall results illustrated that HSI offered an alternative way in detecting and visualizing natural mildew degrees of Camellia oleifera fruit. © 2022 Elsevier B.V.},
	language = {English},
	journal = {Infrared Physics and Technology},
	author = {Jiang, H. and Jiang, X. and Ru, Y. and Chen, Q. and Li, X. and Xu, L. and Zhou, H. and Shi, M.},
	year = {2022},
	keywords = {Camellia oleifera fruit, Chemometrics, Hyperspectral imaging, Mildew degree, Visualization},
}

@article{wei_crops_2021,
	title = {Crops {Fine} {Classification} in {Airborne} {Hyperspectral} {Imagery} {Based} on {Multi}-{Feature} {Fusion} and {Deep} {Learning}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/13/15/2917},
	doi = {10.3390/rs13152917},
	abstract = {Hyperspectral imagery has been widely used in precision agriculture due to its rich spectral characteristics. With the rapid development of remote sensing technology, the airborne hyperspectral imagery shows detailed spatial information and temporal flexibility, which open a new way to accurate agricultural monitoring. To extract crop types from the airborne hyperspectral images, we propose a fine classification method based on multi-feature fusion and deep learning. In this research, the morphological profiles, GLCM texture and endmember abundance features are leveraged to exploit the spatial information of the hyperspectral imagery. Then, the multiple spatial information is fused with the original spectral information to generate classification result by using the deep neural network with conditional random field (DNN+CRF) model. Specifically, the deep neural network (DNN) is a deep recognition model which can extract depth features and mine the potential information of data. As a discriminant model, conditional random field (CRF) considers both spatial and contextual information to reduce the misclassification noises while keeping the object boundaries. Moreover, three multiple feature fusion approaches, namely feature stacking, decision fusion and probability fusion, are taken into account. In the experiments, two airborne hyperspectral remote sensing datasets (Honghu dataset and Xiong’an dataset) are used. The experimental results show that the classification performance of the proposed method is satisfactory, where the salt and pepper noise is decreased, and the boundary of the ground object is preserved.},
	language = {en},
	number = {15},
	urldate = {2022-05-06},
	journal = {Remote Sensing},
	author = {Wei, Lifei and Wang, Kun and Lu, Qikai and Liang, Yajing and Li, Haibo and Wang, Zhengxiang and Wang, Run and Cao, Liqin},
	month = jan,
	year = {2021},
	keywords = {conditional random field, crops fine classification, deep neural network, hyperspectral imagery, multi-feature fusion},
	pages = {2917},
}

@article{wei_precise_2019,
	title = {Precise {Crop} {Classification} {Using} {Spectral}-{Spatial}-{Location} {Fusion} {Based} on {Conditional} {Random} {Fields} for {UAV}-{Borne} {Hyperspectral} {Remote} {Sensing} {Imagery}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/11/17/2011},
	doi = {10.3390/rs11172011},
	abstract = {The precise classification of crop types is an important basis of agricultural monitoring and crop protection. With the rapid development of unmanned aerial vehicle (UAV) technology, UAV-borne hyperspectral remote sensing imagery with high spatial resolution has become the ideal data source for the precise classification of crops. For precise classification of crops with a wide variety of classes and varied spectra, the traditional spectral-based classification method has difficulty in mining large-scale spatial information and maintaining the detailed features of the classes. Therefore, a precise crop classification method using spectral-spatial-location fusion based on conditional random fields (SSLF-CRF) for UAV-borne hyperspectral remote sensing imagery is proposed in this paper. The proposed method integrates the spectral information, the spatial context, the spatial features, and the spatial location information in the conditional random field model by the probabilistic potentials, providing complementary information for the crop discrimination from different perspectives. The experimental results obtained with two UAV-borne high spatial resolution hyperspectral images confirm that the proposed method can solve the problems of large-scale spatial information modeling and spectral variability, improving the classification accuracy for each crop type. This method has important significance for the precise classification of crops in hyperspectral remote sensing imagery.},
	language = {en},
	number = {17},
	urldate = {2022-05-06},
	journal = {Remote Sensing},
	author = {Wei, Lifei and Yu, Ming and Liang, Yajing and Yuan, Ziran and Huang, Can and Li, Rong and Yu, Yiwei},
	month = jan,
	year = {2019},
	keywords = {conditional random fields, hyperspectral remote sensing imagery, precise crop classification, spatial features, spatial location, unmanned aerial vehicle},
	pages = {2011},
}

@article{mei_cascade_2022,
	title = {Cascade {Residual} {Capsule} {Network} for {Hyperspectral} {Image} {Classification}},
	volume = {15},
	issn = {2151-1535},
	doi = {10.1109/JSTARS.2022.3166972},
	abstract = {Theconvolution neural network (CNN) has recently shown the good performance in hyperspectral image (HSI) classification tasks. Many CNN-based methods crop image patches from original HSI as inputs. However, the input HSI cubes usually contain background and many hyperspectral pixels with different land-cover labels. Therefore, the spatial context information on objects of the same category is diverse in HSI cubes, which will weaken the discrimination of spectral–spatial features. In addition, CNN-based methods still face challenges in dealing with the spectral similarity between HSI cubes of spatially adjacent categories, which will limit the classification accuracy. To address the aforementioned issues, we propose a cascade residual capsule network (CRCN) for HSI classification. First, a residual module is designed to learn high-level spectral features of input HSI cubes in the spectral dimension. The residual module is employed to solve the problem of the spectral similarity between HSI cubes of spatially adjacent categories. And then two 3-D convolution layers are exploited to extract high-level spatial–spectral features. Finally, a capsule structure is developed to characterize spatial context orientation representations of objects, which can effectively deal with the diverse spatial context information on objects of the same category in HSI cubes. The capsule module is composed of two 3-D convolution layers and the capsule structure, which is connected to the residual module in series to construct the proposed CRCN. Experimental results on four public HSI datasets demonstrate the superiority of the proposed CRCN over six state-of-the-art models.},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Mei, Zhiming and Yin, Zengshan and Kong, Xinwei and Wang, Long and Ren, Han},
	year = {2022},
	keywords = {Cascade residual capsule network (CRCN), Convolution, Convolutional neural networks, Feature extraction, Hyperspectral imaging, Kernel, Solid modeling, Technological innovation, convolution neural network (CNN), high-level spatial–spectral feature extraction, hyperspectral image (HSI) classification, spatial context orientation representations},
	pages = {3089--3106},
}

@article{wang_review_2022,
	title = {A {Review} of {Deep} {Learning} in {Multiscale} {Agricultural} {Sensing}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/3/559},
	doi = {10.3390/rs14030559},
	abstract = {Population growth, climate change, and the worldwide COVID-19 pandemic are imposing increasing pressure on global agricultural production. The challenge of increasing crop yield while ensuring sustainable development of environmentally friendly agriculture is a common issue throughout the world. Autonomous systems, sensing technologies, and artificial intelligence offer great opportunities to tackle this issue. In precision agriculture (PA), non-destructive and non-invasive remote and proximal sensing methods have been widely used to observe crops in visible and invisible spectra. Nowadays, the integration of high-performance imagery sensors (e.g., RGB, multispectral, hyperspectral, thermal, and SAR) and unmanned mobile platforms (e.g., satellites, UAVs, and terrestrial agricultural robots) are yielding a huge number of high-resolution farmland images, in which rich crop information is compressed. However, this has been accompanied by challenges, i.e., ways to swiftly and efficiently making full use of these images, and then, to perform fine crop management based on information-supported decision making. In the past few years, deep learning (DL) has shown great potential to reshape many industries because of its powerful capabilities of feature learning from massive datasets, and the agriculture industry is no exception. More and more agricultural scientists are paying attention to applications of deep learning in image-based farmland observations, such as land mapping, crop classification, biotic/abiotic stress monitoring, and yield prediction. To provide an update on these studies, we conducted a comprehensive investigation with a special emphasis on deep learning in multiscale agricultural remote and proximal sensing. Specifically, the applications of convolutional neural network-based supervised learning (CNN-SL), transfer learning (TL), and few-shot learning (FSL) in crop sensing at land, field, canopy, and leaf scales are the focus of this review. We hope that this work can act as a reference for the global agricultural community regarding DL in PA and can inspire deeper and broader research to promote the evolution of modern agriculture.},
	language = {en},
	number = {3},
	urldate = {2022-05-06},
	journal = {Remote Sensing},
	author = {Wang, Dashuai and Cao, Wujing and Zhang, Fan and Li, Zhuolin and Xu, Sheng and Wu, Xinyu},
	month = jan,
	year = {2022},
	keywords = {convolutional neural networks, deep learning, few-shot learning, precision agriculture, proximal sensing, remote sensing, transfer learning},
	pages = {559},
}

@inproceedings{reshma_comparative_2017,
	title = {Comparative analysis of classification techniques for crop classification using airborne hyperspectral data},
	doi = {10.1109/WiSPNET.2017.8300164},
	abstract = {Crop classification using high-dimensional and high-resolution data is a challenging task. Though a large number of classes can be obtained from the hyperspectral data, the "curse of dimensionality" causes the classification accuracy to be less than the expected value. A minimum noise transform has been applied to the data in this work, to reduce dimensionality and improve classification accuracy. This paper compares the different methods of supervised and unsupervised classification for the identification of different crops in a field. The results showed that it is better to use supervised methods over unsupervised as they yield better classification accuracy and kappa coefficient.},
	booktitle = {2017 {International} {Conference} on {Wireless} {Communications}, {Signal} {Processing} and {Networking} ({WiSPNET})},
	author = {Reshma, S. and Veni, S.},
	month = mar,
	year = {2017},
	keywords = {Agriculture, Conferences, Hyperspectral imaging, Neural networks, Principal component analysis, Support vector machines},
	pages = {2272--2276},
}

@article{babu_inherent_2021,
	title = {Inherent {Feature} {Extraction} and {Soft} {Margin} {Decision} {Boundary} {Optimization} {Technique} for {Hyperspectral} {Crop} {Classification}},
	volume = {12},
	issn = {2156-5570},
	url = {https://thesai.org/Publications/ViewPaper?Volume=12&Issue=12&Code=IJACSA&SerialNo=85},
	doi = {10.14569/IJACSA.2021.0121285},
	abstract = {Crop productivity and disaster management can be enhanced by employing hyperspectral images. Hyperspectral imaging is widely utilized in identifying and classifying objects on the ground surface for various agriculture application uses such as crop mapping, flood management, identifying crops damaged due to flood/drought, etc. Hyperspectral imaging-based crop classification is a very challenging task because of spectral dimensions and poor spatial feature representation. Designing efficient feature extraction and dimension reduction techniques can address high dimensionality problems. Nonetheless, achieving good classification accuracies with minimal computation overhead is a challenging task in Hyperspectral imaging-based crop classification. In meeting research challenges, this work presents Hyperspectral imaging-based crop classification using soft-margin decision boundary optimization (SMDBO) based Support Vector Machine (SVM) along with Image Fusion-Recursive Filter (IFRF) and Inherent Feature Extraction (IFE). In this work, IFRF is used for reducing spectral features with meaningful representation. Then, IFE is used for differentiating physical properties and shading elements of different objects spatially. Both spectral and spatial features extracted are trained using SMDBO-SVM for performing hyperspectral object classification. Using SMDBO-SVM for Hyperspectral object classification aid in addressing class imbalance issues; thus, the proposed IFE-SMDBO-SVM model achieves better accuracies and with minimal misclassification in comparison with state-of-art statistical and Deep Learning (DL) based Hyperspectral object classification model using standard dataset Indian Pines and Pavia University.},
	language = {en},
	number = {12},
	urldate = {2022-05-06},
	journal = {International Journal of Advanced Computer Science and Applications (IJACSA)},
	author = {Babu, M. C. Girish and C, Padma M.},
	year = {2021},
}

@inproceedings{huang_classification_2022,
	title = {Classification {Method} for {Crop} by fusion {Hyper} {Spectral} and {LiDAR} {Data}},
	doi = {10.1109/ICMTMA54903.2022.00205},
	abstract = {Hyperspectral remote sensing and Lidar data are two important data source, Hyperspectral image has abundant spectral information and texture information. Airborne LiDAR can acquire the vertical structure characteristics of objects. Based on the above two points,It carried out a study to fuse hyperspectral imagery and LiDAR data. Morphological attribute profile was used to extract features, and sparse multinomial logistic regression was used to do classification. The fusion and classification effect in different combinations of characteristics were also investigated. The aerial hyperspectral image and Lidar data were used to validate this method. The results show that the classification accuracy obtained by this method is significantly higher than obtained by using hyperspectral images and point cloud data, the fused method can obtain better classification results with higher accuracy and stability, and the best classification accuracy is 92.89\% by fusion method.},
	booktitle = {2022 14th {International} {Conference} on {Measuring} {Technology} and {Mechatronics} {Automation} ({ICMTMA})},
	author = {Huang, Zuowei and Xie, Shixiong},
	month = jan,
	year = {2022},
	keywords = {Feature extraction, Fuses, Hyperspectral imagery, Laser radar, Lidar data, Mechatronics, Point cloud compression, Soft sensors, Support vector machines, data fusion, extented morphological attribute profile},
	pages = {1011--1014},
}

@article{agilandeeswari_crop_2022,
	title = {Crop {Classification} for {Agricultural} {Applications} in {Hyperspectral} {Remote} {Sensing} {Images}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/3/1670},
	doi = {10.3390/app12031670},
	abstract = {Hyperspectral imaging (HSI), measuring the reflectance over visible (VIS), near-infrared (NIR), and shortwave infrared wavelengths (SWIR), has empowered the task of classification and can be useful in a variety of application areas like agriculture, even at a minor level. Band selection (BS) refers to the process of selecting the most relevant bands from a hyperspectral image, which is a necessary and important step for classification in HSI. Though numerous successful methods are available for selecting informative bands, reflectance properties are not taken into account, which is crucial for application-specific BS. The present paper aims at crop mapping for agriculture, where physical properties of light and biological conditions of plants are considered for BS. Initially, bands were partitioned according to their wavelength boundaries in visible, near-infrared, and shortwave infrared regions. Then, bands were quantized and selected via metrics like entropy, Normalized Difference Vegetation Index (NDVI), and Modified Normalized Difference Water Index (MNDWI) from each region, respectively. A Convolutional Neural Network was designed with the finer generated sub-cube to map the selective crops. Experiments were conducted on two standard HSI datasets, Indian Pines and Salinas, to classify different types of crops from Corn, Soya, Fallow, and Romaine Lettuce classes. Quantitatively, overall accuracy between 95.97\% and 99.35\% was achieved for Corn and Soya classes from Indian Pines; between 94.53\% and 100\% was achieved for Fallow and Romaine Lettuce classes from Salinas. The effectiveness of the proposed band selection with Convolutional Neural Network (CNN) can be seen from the resulted classification maps and ablation study.},
	language = {en},
	number = {3},
	urldate = {2022-05-06},
	journal = {Applied Sciences},
	author = {Agilandeeswari, Loganathan and Prabukumar, Manoharan and Radhesyam, Vaddi and Phaneendra, Kumar L. N. Boggavarapu and Farhan, Alenizi},
	month = jan,
	year = {2022},
	keywords = {CNN, NDVI, agriculture, band selection, crops, hyperspectral imaging},
	pages = {1670},
}

@article{lu_hyperspectral_2022,
	title = {Hyperspectral {Imaging} {With} {Machine} {Learning} to {Differentiate} {Cultivars}, {Growth} {Stages}, {Flowers}, and {Leaves} of {Industrial} {Hemp} ({Cannabis} sativa {L}.)},
	volume = {12},
	issn = {1664-462X},
	url = {https://www.frontiersin.org/article/10.3389/fpls.2021.810113},
	abstract = {As an emerging cash crop, industrial hemp (Cannabis sativa L.) grown for cannabidiol (CBD) has spurred a surge of interest in the United States. Cultivar selection and harvest timing are important to produce CBD hemp profitably and avoid economic loss resulting from the tetrahydrocannabinol (THC) concentration in the crop exceeding regulatory limits. Hence there is a need for differentiating CBD hemp cultivars and growth stages to aid in cultivar and genotype selection and optimization of harvest timing. Current methods that rely on visual assessment of plant phenotypes and chemical procedures are limited because of its subjective and destructive nature. In this study, hyperspectral imaging was proposed as a novel, objective, and non-destructive method for differentiating hemp cultivars, growth stages as well as plant organs (leaves and flowers). Five cultivars of CBD hemp were grown greenhouse conditions and leaves and flowers were sampled at five growth stages 2–10 weeks in 2-week intervals after flower initiation and scanned by a benchtop hyperspectral imaging system in the spectral range of 400–1000 nm. The acquired images were subjected to image processing procedures to extract the spectra of hemp samples. The spectral profiles and scatter plots of principal component analysis of the spectral data revealed a certain degree of separation between hemp cultivars, growth stages, and plant organs. Machine learning based on regularized linear discriminant analysis achieved the accuracy of up to 99.6\% in differentiating the five hemp cultivars. Plant organ and growth stage need to be factored into model development for hemp cultivar classification. The classification models achieved 100\% accuracy in differentiating the five growth stages and two plant organs. This study demonstrates the effectiveness of hyperspectral imaging for differentiating cultivars, growth stages and plant organs of CBD hemp, which is a potentially useful tool for growers and breeders of CBD hemp.},
	urldate = {2022-05-06},
	journal = {Frontiers in Plant Science},
	author = {Lu, Yuzhen and Young, Sierra and Linder, Eric and Whipker, Brian and Suchoff, David},
	year = {2022},
}

@article{lauwers_hyperspectral_2022,
	title = {Hyperspectral classification of poisonous solanaceous weeds in processing {Phaseolus} vulgaris {L}. and {Spinacia} oleracea {L}.},
	volume = {196},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S0168169922002253},
	doi = {10.1016/j.compag.2022.106908},
	abstract = {Poisonous weeds can occasionally unintentionally be co-harvested and pose a threat to human health as separation techniques during processing are not sufficient. Hence, elimination prior to harvest is required. For this reason, an exploratory study is performed to investigate the possibilities of an automatic detection system. The objective of this article is, firstly, to know if Phaseolus vulgaris and Spinacia oleracea are hyperspectrally separable from Solanum nigrum, Solanum tuberosum and Datura stramonium using spectrometer measurements. Secondly, the influence of different varieties/populations and of different pedohydrological and climatic conditions on this classification is investigated. Finally, it is examined whether it is possible to appoint discriminative wavelengths. To this means, the following analyses were performed: I and II) crop and weed species, and different populations or varieties of these species in varying conditions, were classified using hyperspectral spectrometer measurements and regularized logistic regression (RLR), III) data of consecutive years were investigated for similarities in order to indicate robust important regions in the electromagnetic spectrum with the use of RLR and IV) a subset of commercial of-the-shelf (COTS) filters was created for further research in the field. Results showed that the poisonous weed species D. stramonium, S. nigrum and S. tuberosum are hyperspectrally separable from the investigated crops. The accuracy of the two-class classification of poisonous weeds with S. oleracea and of these weeds with P. vulgaris was 0.982 and 0.977, respectively (I). Inclusion of different crop varieties, weed populations or different growing conditions in the model with S. oleracea and poisonous weeds resulted in a small decrease in weed recall (0.95 vs. 0.99) and crop precision (0.93 vs. 0.97). In future research, care must be taken to proper sample fields to cover the genetic variation present within weed populations and crop varieties, and diverse growing conditions (II). The bands selected using RLR did not show any consistency when using data of consecutive years and, therefore, RLR is not a suitable method to select robust wavelength regions for detection of poisonous weeds in vegetable crops to guide future research (III). With the use of COTS filters it was possible to select ten filters that worked sufficiently for both crops and are recommended for further research in the field. In addition, the authors recommend the use of a high resolution RGB camera to benefit from object-based image analysis to increase classification accuracy (IV).},
	language = {en},
	urldate = {2022-05-06},
	journal = {Computers and Electronics in Agriculture},
	author = {Lauwers, Marlies and Nuyttens, David and De Cauwer, Benny and Pieters, Jan},
	month = may,
	year = {2022},
	keywords = {Precision agriculture, Processing industry, Regularized logistic regression, Toxic weeds, Weed detection},
	pages = {106908},
}

@article{barreto_radiometric_2019,
	title = {Radiometric {Assessment} of a {UAV}-{Based} {Push}-{Broom} {Hyperspectral} {Camera}},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/21/4699},
	doi = {10.3390/s19214699},
	abstract = {The use of unmanned aerial vehicles (UAVs) for Earth and environmental sensing has increased significantly in recent years. This is particularly true for multi- and hyperspectral sensing, with a variety of both push-broom and snap-shot systems becoming available. However, information on their radiometric performance and stability over time is often lacking. The authors propose the use of a general protocol for sensor evaluation to characterize the data retrieval and radiometric performance of push-broom hyperspectral cameras, and illustrate the workflow with the Nano-Hyperspec (Headwall Photonics, Boston USA) sensor. The objectives of this analysis were to: (1) assess dark current and white reference consistency, both temporally and spatially; (2) evaluate spectral fidelity; and (3) determine the relationship between sensor-recorded radiance and spectroradiometer-derived reflectance. Both the laboratory-based dark current and white reference evaluations showed an insignificant increase over time ({\textbackslash}textless2\%) across spatial pixels and spectral bands for {\textbackslash}textgreater99.5\% of pixel–waveband combinations. Using a mercury/argon (Hg/Ar) lamp, the hyperspectral wavelength bands exhibited a slight shift of 1-3 nm against 29 Hg/Ar wavelength emission lines. The relationship between the Nano-Hyperspec radiance values and spectroradiometer-derived reflectance was found to be highly linear for all spectral bands. The developed protocol for assessing UAV-based radiometric performance of hyperspectral push-broom sensors showed that the Nano-Hyperspec data were both time-stable and spectrally sound.},
	language = {en},
	number = {21},
	urldate = {2022-05-11},
	journal = {Sensors},
	author = {Barreto, M. Alejandra P. and Johansen, Kasper and Angel, Yoseline and McCabe, Matthew F.},
	month = jan,
	year = {2019},
	keywords = {Headwall Nano-Hyperspec, UAV, hyperspectral, radiometric, remote sensing, spectroradiometer},
	pages = {4699},
}

@article{noauthor_hyperspectral_2021,
	title = {Hyperspectral image-based vegetation index ({HSVI}): {A} new vegetation index for urban ecological research},
	volume = {103},
	issn = {1569-8432},
	shorttitle = {Hyperspectral image-based vegetation index ({HSVI})},
	url = {https://www.sciencedirect.com/science/article/pii/S0303243421002361},
	doi = {10.1016/j.jag.2021.102529},
	abstract = {Accurately monitoring the quantity and quality of urban vegetation contributes to regional greening efforts and improves the understanding of vegetati…},
	language = {en},
	urldate = {2022-05-23},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	month = dec,
	year = {2021},
	pages = {102529},
}

@article{bendel_evaluating_2020,
	title = {Evaluating the suitability of hyper- and multispectral imaging to detect foliar symptoms of the grapevine trunk disease {Esca} in vineyards},
	volume = {16},
	issn = {1746-4811},
	doi = {10.1186/s13007-020-00685-3},
	abstract = {Background: Grapevine trunk diseases (GTDs) such as Esca are among the most devastating threats to viticulture. Due to the lack of efficient preventive and curative treatments, Esca causes severe economic losses worldwide. Since symptoms do not develop consecutively, the true incidence of the disease in a vineyard is difficult to assess. Therefore, an annual monitoring is required. In this context, automatic detection of symptoms could be a great relief for winegrowers. Spectral sensors have proven to be successful in disease detection, allowing a non-destructive, objective, and fast data acquisition. The aim of this study is to evaluate the feasibility of the in-field detection of foliar Esca symptoms over three consecutive years using ground-based hyperspectral and airborne multispectral imaging. Results: Hyperspectral disease detection models have been successfully developed using either original field data or manually annotated data. In a next step, these models were applied on plant scale. While the model using annotated data performed better during development, the model using original data showed higher classification accuracies when applied in practical work. Moreover, the transferability of disease detection models to unknown data was tested. Although the visible and near-infrared (VNIR) range showed promising results, the transfer of such models is challenging. Initial results indicate that external symptoms could be detected pre-symptomatically, but this needs further evaluation. Furthermore, an application specific multispectral approach was simulated by identifying the most important wavelengths for the differentiation tasks, which was then compared to real multispectral data. Even though the ground-based multispectral disease detection was successful, airborne detection remains difficult. Conclusions: In this study, ground-based hyperspectral and airborne multispectral approaches for the detection of foliar Esca symptoms are presented. Both sensor systems seem to be suitable for the in-field detection of the disease, even though airborne data acquisition has to be further optimized. Our disease detection approaches could facilitate monitoring plant phenotypes in a vineyard. © 2020, The Author(s).},
	language = {English},
	number = {1},
	journal = {Plant Methods},
	author = {Bendel, N. and Kicherer, A. and Backhaus, A. and Klück, H.-C. and Seiffert, U. and Fischer, M. and Voegele, R.T. and Töpfer, R.},
	year = {2020},
	keywords = {Disease detection, Grapevine trunk disease, Phenoliner, Phenotyping platform, Plant phenotyping, Precision viticulture, Spectral imaging},
}

@article{bendig_combining_2015,
	title = {Combining {UAV}-based plant height from crop surface models, visible, and near infrared vegetation indices for biomass monitoring in barley},
	volume = {39},
	issn = {1569-8432},
	url = {https://www.sciencedirect.com/science/article/pii/S0303243415000446},
	doi = {10.1016/j.jag.2015.02.012},
	abstract = {In this study we combined selected vegetation indices (VIs) and plant height information to estimate biomass in a summer barley experiment. The VIs were calculated from ground-based hyperspectral data and unmanned aerial vehicle (UAV)-based red green blue (RGB) imaging. In addition, the plant height information was obtained from UAV-based multi-temporal crop surface models (CSMs). The test site is a summer barley experiment comprising 18 cultivars and two nitrogen treatments located in Western Germany. We calculated five VIs from hyperspectral data. The normalised ratio index (NRI)-based index GnyLi (Gnyp et al., 2014) showed the highest correlation (R2=0.83) with dry biomass. In addition, we calculated three visible band VIs: the green red vegetation index (GRVI), the modified GRVI (MGRVI) and the red green blue VI (RGBVI), where the MGRVI and the RGBVI are newly developed VI. We found that the visible band VIs have potential for biomass prediction prior to heading stage. A robust estimate for biomass was obtained from the plant height models (R2=0.80–0.82). In a cross validation test, we compared plant height, selected VIs and their combination with plant height information. Combining VIs and plant height information by using multiple linear regression or multiple non-linear regression models performed better than the VIs alone. The visible band GRVI and the newly developed RGBVI are promising but need further investigation. However, the relationship between plant height and biomass produced the most robust results. In summary, the results indicate that plant height is competitive with VIs for biomass estimation in summer barley. Moreover, visible band VIs might be a useful addition to biomass estimation. The main limitation is that the visible band VIs work for early growing stages only.},
	language = {en},
	urldate = {2022-05-24},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Bendig, Juliane and Yu, Kang and Aasen, Helge and Bolten, Andreas and Bennertz, Simon and Broscheit, Janis and Gnyp, Martin L. and Bareth, Georg},
	month = jul,
	year = {2015},
	keywords = {GnyLi, Green red vegetation index, NDVI, Point cloud, SAVI, Structure from motion},
	pages = {79--87},
}

@article{agam_insight_2013,
	title = {An insight to the performance of crop water stress index for olive trees},
	volume = {118},
	issn = {0378-3774},
	url = {https://www.sciencedirect.com/science/article/pii/S0378377412003186},
	doi = {10.1016/j.agwat.2012.12.004},
	abstract = {Optimization of olive oil quantity and quality requires finely tuned water management, as increased irrigation, up to a certain level, results in increasing yield, but a certain degree of stress improves oil quality. Monitoring tools that provide accurate information regarding orchard water status would therefore be beneficial. Amongst the various existing methods, those having high resolution, either temporally (i.e., continuous) or spatially, have the maximum adoption potential. One of the commonly used spatial methods is the Crop Water Stress Index (CWSI). The objective of this research was to test the ability of the CWSI to characterize water status dynamics of olive trees as they enter into and recover from stress, and on a diurnal scale. CWSI was tested in an empirical form and in two analytical configurations. In an experiment conducted in a lysimeter facility in the northwestern Negev, Israel, irrigation was withheld for 6 days for 5 of 15 trees, while daily irrigation continued for the rest of the trees. After resuming irrigation, the trees were monitored for 5 additional days. Water status measurements and thermal imaging were conducted daily between 12:00 and 14:00. Diurnal monitoring (predawn to after dusk) of the same indicators was conducted on the day of maximum stress. Continuous meteorological data were acquired throughout the experimental period. Empirical and analytical CWSI were calculated based on canopy temperature extracted from thermal images. The empirical CWSI differentiated between well watered and stressed trees, and depicted the water status dynamics during the drought and recovery periods as well as on a diurnal scale. Analytical approaches did not perform as well at either time scale. In conclusion, the empirical CWSI seems to be promising even given its limitations, while analytical forms of CWSI still require improvement before they can be used as a water status monitoring tool for olive orchards. Practically, it is proposed to compute the wet temperature analytically and set the dry temperature to 5°C higher than air temperature.},
	language = {en},
	urldate = {2022-05-24},
	journal = {Agricultural Water Management},
	author = {Agam, N. and Cohen, Y. and Berni, J. A. J. and Alchanatis, V. and Kool, D. and Dag, A. and Yermiyahu, U. and Ben-Gal, A.},
	month = feb,
	year = {2013},
	keywords = {Canopy temperature, Crop water stress index, Fully transpiring leaf, Non-transpiring leaf, Olive tree, Water status},
	pages = {79--86},
}

@article{sangjan_optimization_2022,
	title = {Optimization of {UAV}-{Based} {Imaging} and {Image} {Processing} {Orthomosaic} and {Point} {Cloud} {Approaches} for {Estimating} {Biomass} in a {Forage} {Crop}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/14/10/2396},
	doi = {10.3390/rs14102396},
	abstract = {Forage and field peas provide essential nutrients for livestock diets, and high-quality field peas can influence livestock health and reduce greenhouse gas emissions. Above-ground biomass (AGBM) is one of the vital traits and the primary component of yield in forage pea breeding programs. However, a standard method of AGBM measurement is a destructive and labor-intensive process. This study utilized an unmanned aerial vehicle (UAV) equipped with a true-color RGB and a five-band multispectral camera to estimate the AGBM of winter pea in three breeding trials (two seed yields and one cover crop). Three processing techniques—vegetation index (VI), digital surface model (DSM), and 3D reconstruction model from point clouds—were used to extract the digital traits (height and volume) associated with AGBM. The digital traits were compared with the ground reference data (measured plant height and harvested AGBM). The results showed that the canopy volume estimated from the 3D model (alpha shape, α = 1.5) developed from UAV-based RGB imagery’s point clouds provided consistent and high correlation with fresh AGBM (r = 0.78–0.81, p {\textbackslash}textless 0.001) and dry AGBM (r = 0.70–0.81, p {\textbackslash}textless 0.001), compared with other techniques across the three trials. The DSM-based approach (height at 95th percentile) had consistent and high correlation (r = 0.71–0.95, p {\textbackslash}textless 0.001) with canopy height estimation. Using the UAV imagery, the proposed approaches demonstrated the potential for estimating the crop AGBM across winter pea breeding trials.},
	language = {en},
	number = {10},
	urldate = {2022-05-24},
	journal = {Remote Sensing},
	author = {Sangjan, Worasit and McGee, Rebecca J. and Sankaran, Sindhuja},
	month = jan,
	year = {2022},
	keywords = {3D reconstruction model, above-ground biomass, digital surface model, unmanned aerial vehicle, vegetation indices},
	pages = {2396},
}
