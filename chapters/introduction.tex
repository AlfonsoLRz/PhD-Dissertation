\setchapterpreamble[u]{\margintoc}
\chapter{Introduction}
\labch{intro}

\lettrine[findent=0pt, lines=3]{\textbf{S}}{ }ensing real-world processes help to monitor, predict and optimize the activities that depend on them and therefore act according to the analyzed data. Numerous devices are aimed at this task, which can be at least classified according to the sensing distance, the wavelength interval captured by the device detectors and the data acquisition mechanism, either passive or active. The latter two classifications are based on how a device works as provided by the manufacturer. However, choosing a sensing distance is up to researchers and consumers, depending on their objectives, area of expertise and budget, as well as on the dimensions and environmental conditions of the study area, among other factors. Despite contact-based sensors being implemented nowadays to measure variables such as humidity, temperature or rainfall amount, these require a high degree of maintenance and are more prone to deteriorate under adverse environmental conditions \cite{silva_low-cost_2019, morais_versatile_2021}. Instead of covering large areas, each one of these sensors communicates measurements of very specific locations within and outside a network. The arranged layout, logically and physically, must be carefully designed for intra- and extra-communication involving nodes in different topological levels. Therefore, these technologies have a worse spatial awareness and are more appropriate to cope with real-time, rapid and constant monitoring. 

\marginnote[.1cm]{\textbf{Remote sensing} techniques overcome the spatial limitations of devices that measure data in contact with the target surface and phenomena.} 
Given the cited drawbacks, \textbf{Remote sensing} (\acrshort{rs}) techniques help to mitigate them by acquiring data from remote platforms. \acrshort{rs} refers to the science and art of acquiring information about objects and phenomena without being in contact with them \cite{lillesand_remote_2015}. Similarly to the human eye, data is acquired from the environment in the form of impulses from light stimuli corresponding to some wavelength intervals. At least two components are involved in the reading process: the sensors and the platforms where they are mounted. The first will be the matter of discussion throughout this dissertation, whereas platforms were initially narrowed to satellites. 

\begin{marginfigure}[0cm]
	\includegraphics{figs/introduction/landsat8_lapalma.jpg}
	\caption{Cumbre Vieja volcano eruption observed from Landsat-8 \cite{nasa_earth_observatory_lava_2021}.}
	\label{fig:la_palma_landsat8}
\end{marginfigure}
\textbf{Satellite imaging} is better suited for the monitoring of changes over large areas using time series spanned over months and years. These data help to understand the dynamics of human and nature interaction as well as the impact of natural phenomena (Figure \ref{fig:la_palma_landsat8}). Some applications of the large collections of available satellite data are the analysis of land use, deforestation, land changes and urban settlements \cite{asokan_change_2019}. Nevertheless, the spatial resolution and revisit period of non-commercial satellite programmes harden their applicability to fine-grained monitoring tasks. The level of detail (\acrshort{lod}) of these tasks can refer to \textbf{spatial resolution}, \textbf{temporal resolution} or both, which are the main \textbf{limitations} of satellite imaging besides their cost. In spite of the described drawbacks, the use of satellite imagery is on the rise due to the steady reduction of Ground Sampling Distance (\acrshort{gsd}) and low periods for revisiting the same points. 

Figure \ref{fig:scopus_search_platforms} depicts the amount of research devoted to each remote sensing platform. The Scopus searches were the following: $(p_1 \lor p_2 ... \lor p_n) \land (\textit{remote} \hspace{1mm} \land \hspace{1mm} \textit{sensing})$, with $p_i$ being one of the platforms depicted in the legend. 

\begin{figure}[!ht]
	\includegraphics[width=\linewidth]{figs/introduction/platform_timeline.png}
	\caption{Number of manuscripts related to different \acrshort{rs} platforms. }
    \label{fig:scopus_search_platforms}
\end{figure}

However, it was until recently that satellite resolution was restricted by governments. These restrictions as well as the described limitations led to the use of alternative platforms. Besides satellites, \textbf{airborne} (fixed-wing and helicopters), \textbf{Unmanned Aerial Systems} (\acrshort{uas}) (Figure \ref{fig:dji300}) and other \textbf{land platforms} (mobile and static proximal-sensing) are the most frequent \cite{lillesand_remote_2015}. The choice of any of them always involves trade-offs concerning manoeuvrability, land coverage, repeat coverage, spatial resolution, spatial accuracy, cost or field of view (\acrshort{fov}) \cite{toth_remote_2016}. However, \textbf{\acrshort{uas}} have been gaining interest in the last decade as they transitioned from military tools in the early 2000s to easy-to-deploy, small and low-cost systems, thus widening their applicability to civilian activities and research. These platforms range from hand-sized to large aircraft that can be either controlled by human intervention or be partially, even fully, autonomous. Figure \ref{fig:dji300} shows a standard \acrshort{uas} designed by DJI which can carry up to 2.7 \si{\kilo\gram}, with a size of $810 \times 660 \times 430$ \si{\milli\meter}.
\begin{marginfigure}[-3cm]
	\includegraphics{figs/introduction/dji300.png}
	\caption{Quadcopter Matrice 300 RTK coupled with a dual RGB-thermal sensor (Zenmuse H20T). }
	\label{fig:dji300}
\end{marginfigure}

\begin{figure}[!ht]
	\includegraphics{figs/introduction/introduction_scheme.png}
	\caption{Overall procedure to acquire data with remote sensing platforms and sensors. Firstly, sensors are coupled on spaceborne, airborne and terrestrial vehicles or operated by humans, either by carrying them as a backpack or in hand. Then, sensor products can be grouped into graphical and numerical results, though most of them produce both kinds of data. Finally, the sensing products are processed and interpreted to provide users with valuable analyses. }
    \label{fig:introduction_scheme}
\end{figure}

Therefore, \acrshort{uas}s represent a cost-efficient tool for acquiring high-resolution data. The most common sensors mounted on \acrshort{uas} are cameras and \acrshort{lidar} (Light Detection and Ranging) sensors, with the first comprising a large number of imaging devices. Similarly, these groups also lead to the distinction between passive and active sensors. While the second has both transmitter and receiver components, the first is only aimed at capturing incoming radiance from the scene surfaces. Also, the observations of imaging devices generate 2D images, whereas the immediate results of active sensing are 3D point clouds. Accordingly, the technology involved in \textbf{active sensing} is much more \textbf{prohibitive} than imagers, which have been widespread for the last decades with consumer-grade devices. Still, both kinds of sensors can be applied to the collection of complementary resources. However, the acquisition of multiple high-resolution datasets poses several advantages and drawbacks. 

\marginnote[-1.5cm]{The digitization of assets, processes and systems is greatly helped by reconstructions from sensor data. These allow connecting virtual and physical replicas through a data stream, thus shaping the fundamentals of \textbf{Digital twins}.}
Firstly, observations from multiple sensors can be interpreted as different features that complete a \textbf{knowledge-based system}, with each one providing information within a wavelength range. Furthermore, most of the algorithms aimed at analyzing and drawing conclusions from sensor data benefit from the use of complementary features. The contribution of each one in the extracted conclusions can be adjusted through weights that determine how important are these for the analysis. Unless a large number of features are used, which can lead to the curse of dimensionality problem, it is safe to say that the greater number of features, the better. On the other hand, \textbf{high-resolution data} help to generate \textbf{precise models that are easier to analyze and visualize by human operators}. Denser data also involves denser geometry and reconstructions, thus omitting fewer details of target surfaces and easing the construction of digital models that emulate the Earth's scenarios and processes. 

Different layers of information are ideally noise-free and represented under the same coordinate system. In reality, navigation sensors present small positioning errors that harden the fusion of several layers of information. Despite being small, these are reported as shifts, rotation and scale variations among different sensor data. Furthermore, environmental conditions such as atmospheric composition, wind, temperature or solar radiation can vary from one flight to another and so do the acquired data, even under similar flight plans. These changes not only affect observations performed in a short period of time with different sensors but also time series acquired with previously used tools. Although tedious, variations concerning positioning and orientation errors can be diminished by including reference points measured with sensors of higher precision in easily recognizable image features. Other shortcomings are the noise induced by faulty device detectors, especially for active sensors, unwanted atmospheric particles and surfaces whose bouncing behaviour leads to unreal geometry. Therefore, a system capable of accurately fusing the outcome of every sensor coupled on a \acrshort{uas} is necessary to facilitate the later processing stages and provide reliable conclusions.

Besides the information that can be directly extracted from remotely sensed data, other kinds of information may be required for \textbf{computer vision} tasks. For instance, it would be of great help to distinguish materials and instances in pixels and 3D points. However, these data cannot be inferred from nature without previous knowledge or trained models. Accordingly, one of the main challenges is the \textbf{labelling of large datasets}, which must be performed by human operators \cite{li_image_2021, basu_deepsat_2015} or unsupervised algorithms aimed at transforming and extracting relevant features. None of these methods is perfect and thus can result in spurious labelling data that can mislead the learning algorithms. Hence, the task of building large datasets for training computer vision algorithms is very time-consuming. It comprises acquiring, processing and extracting features with \textbf{considerable computational and human resources}. All these time-consuming stages can be avoided with the existence of publicly available datasets for \acrshort{rs}, motivated by Artificial Intelligence (\acrshort{ai}) algorithms. These cover a wide variety of sensors and spectral wavelengths, but the attached data present the same drawbacks as previously explained. Moreover, these datasets may not comprise areas of interest for case studies that fall out of state-of-the-art trends. 

% Given the cited drawbacks, from which cost and time stand out, datasets could be replaced, at least partially, by synthetically generated information from virtual sensors. The main challenge arises therefore from emulating the sensor mechanism. On the other hand, one advantage is that data acquired from a virtual sensor are linked to digital models without uncertainty. Likewise, digital models are augmented with features that could be directly transferred to the sensing results. These features include the aforementioned semantic annotation or the instances' Id. The most frequently simulated sensors are Radar and \acrshort{lidar} systems, though synthetic images have also been recently generated with the rise of \acrshort{dl} approaches capable of generating new data.  

Given the drawbacks of using real datasets, from which cost and time consumption stand out, \textbf{synthetic information} generated by \textbf{virtual sensors} could be used as a partial or complete replacement. The main challenge lies in emulating the sensor mechanism. However, the process is much less time-consuming and enables adding any kind of knowledge to the sensing results. These are obtained from digital models with no uncertainty, in contrast to what occurs in nature, and they can be enriched with features such as semantic annotations which are later transferred to synthetic datasets. It neither requires acquiring prohibitive sensors such as \acrshort{lidar}, and therefore is also much more cost-efficient. The most frequently simulated sensors are Radar and \acrshort{lidar} systems, but recent advances in Deep Learning (\acrshort{dl}) have made it possible to generate synthetic images as well. 

Sensing products are typically \textbf{transformed} into other \textbf{data representations} that are not provided by the sensor. Accordingly, individual images are not sufficient to interpret the scenario in some contexts. A complete vision of it can be provided by joining images, thus resulting in 2D maps and 3D points calculated by estimating the camera pose and finding features in common among several images. During these transformations, estimated radiometric and geometric properties may be affected by precision loss in the interpolation process. Therefore, it is important to compare the results with other reliable data sources. For instance, \acrshort{lidar} is known to produce highly accurate point clouds, which can be used as a reference to evaluate the quality of point clouds reconstructed from imagery. Similarly, images can serve as reference data for measuring radiometric changes.  

On the other hand, \textbf{large volumes of data are also harder to operate in terms of computing, storage and visualization}. Thousands of images or millions of points can be hardly operated in commodity hardware due to the required storage and computing capabilities. However, current trends in informatics have favoured the proliferation of personal and professional computers with large storage capacity, more efficient access to data and great multi-threading capabilities. The latter can be performed over the Central Processing Unit (\acrshort{cpu}), whose trend is to increase the number of cores, and the Graphic Processing Unit (\acrshort{gpu}), composed of millions of units that can massively solve small tasks and cooperate among small thread groups. Chapter \ref{sec:fundamentals_rs} delves into the most common data formats in \acrshort{rs}; at this moment, it is enough to think of data as large volumes of digital numbers (\acrshort{dn}). Despite large storage being inexpensive nowadays, challenges such as the partial retrieval of information remain, especially for interactive applications with high data throughput \cite{bejar-martos_strategies_2022, ogayar-anguita_nested_2023}. 

\begin{marginfigure}[-.5cm]
	\includegraphics{figs/introduction/hintze.png}
	\caption{Point cloud with 2.4M points, reconstructed using 900 images from the Hintze Hall (Model uploaded by \textit{Thomas Flynn} in \textit{Sketchfab}).  }
	\label{fig:hintze_hall}
\end{marginfigure}
Another drawback in real-time applications is the \textbf{visualization of these large volumes of data}. Traditional rendering pipelines are built as a set of static stages, where the model geometry and topology are iteratively transformed into pixels. The colours of these come from the shading of one or multiple light sources over the textures of a surface, using a discretization of the formulae that describe light interaction. Unlike synthetic models designed by human operators, the shading of sensor data depends on the recorded wavelengths and is not calculated. Thus, alternative pipelines are needed to visualize these data more efficiently. According to the \acrshort{gpu} architecture, the data can be sorted and organized in data structures in pursuit of workload balancing and geometry simplifications that help to trick user perception. These requirements are even stricter for Virtual Reality (\acrshort{vr}) devices that render every scenario at least once for each eye, even more whether other rendering techniques are included, e.g., shadow mapping.  

Previous methods are simply a procedure that allows extracting text-based or visual clues that help in the recognition of weaknesses and failures in processes. Accordingly, the \textbf{last step} is to \textbf{classify, segment and recognize features} from data to optimize future operations. The main contributions of \acrshort{rs} techniques in Precision Agriculture (\acrshort{pa}) are yield estimation, crop-type inventory, measuring of water content and Leaf Area Index (\acrshort{lai}), control of diseases and insects as well as moisture, changes, growth, stress and drought monitoring, among other risk factors such as snow or fire \cite{huang_agricultural_2018}. These monitoring tasks are intended to maximize profitability and minimize waste and pollution. 
\marginnote[-3cm]{This dissertation has Precision Agriculture (\acrshort{pa}) as its main research field and therefore, the data analysis has been narrowed to 1) segment crops into ground and vegetation, and 2) phenotyping of a large number of plant varieties in a non-destructive way.}

\section{Historical background and technology details}

The term Remote sensing was first coined in the 1960s by Evelyn Pruitt during her work at the US Office of Naval Research to refer to satellite and aircraft instrumentation that measure reflected and emitted radiation. However, topographic mapping was first suggested in 1849 and attempted in 1858 by F. Tournachon from a captive balloon over France. From here, the camera size has been reduced without the need of carrying a darkroom. Other airborne platforms were explored, either as a result of war needs or simply innovation, ranging from kite-camera systems (1906; see Figure \ref{fig:san_francisco_kite}) to pigeons (1909) and aircraft (1908) \cite{emery_introduction_2017}.

\begin{figure}[!ht]
	\includegraphics{figs/introduction/san_francisco_kitecamera.jpg}
	\caption{Photograph of San Francisco after the 1906 earthquake, taken from a camera held by seven kites. }
    \label{fig:san_francisco_kite}
\end{figure}

Satellite imaging, as known today, was first possible by the ideation of Konstantín Tsiolkovsky about rocketry to explore space, published under the title \textit{Exploring Space using jet propulsion devices}. This idea led to the first successful launch of the Sputnik satellite in 1957 and the development of Earth-orbiting satellites aimed at atmospherical monitoring. The Television and Infrared Observation Satellite (\acrshort{tiros}) was made operational in 1960 and integrated a small infrared system and a narrow-angle camera capturing data in the visible wavelengths. In 1978, a breakthrough change in satellites and coupled sensors came with \acrshort{tiros}-N (N for new). This spacecraft integrated a very high-resolution radiometer with 1 \si{\kilo\meter} footprint and four channels (visible, near-infrared, midrange-infrared and thermal infrared). Nowadays, an advanced version of these satellites is still operative under the name of \acrshort{avhrr} (Advanced Very High-Resolution Radiometer) with a similar footprint, though it captures six channels instead of four \cite{national_oceanic_and_atmospheric_administration_avhrr3_nodate}.   

\marginnote[.1cm]{An extensive repository of recent satellite missions can be found in the Earth Observation Portal, maintained by the European Space Agency \cite{earth_observation_portal_earth_nodate}.} 
From currently operative satellite programs, Landsat provides the largest collection of continuously acquired \acrshort{rs} data. It was first tested as part of NASA's NIMBUS program and there are currently two active satellites, Landsat-8 and Landsat-9, while the other seven have been terminated or planned to be decommissioned (Landsat-7). The payload of Landsat-9 consists of two instruments: Operational Land Imager (\acrshort{oli2}) and Thermal Infrared Sensor (\acrshort{tirs2}) \cite{masek_landsat_2020, nasa_earth_observatory_landsat_2021-1, us_geological_survey_landsat_2019}. A total of 11 bands and 740 scenes are collected every day, including red, blue, green, near-infrared, shortwave infrared, thermal, panchromatic, coastal and cirrus bands, ranging from a resolution of 15 \si{\meter} to 100 \si{\meter}. Other notable satellite programs are the China–Brazil Earth Resources Satellite (\acrshort{cbers}) and the Copernicus programme financed by the European Commission. The \acrshort{cbers}-4A satellite is equipped with three imaging tools capturing five different spectrum intervals with a spatial resolution of 2-55 \si{\meter} \cite{instituto_nacional_de_pesquisas_espaciais_inpecbers_2019}. On the other hand, the Sentinel-2 mission acquires 13 bands in the visible, shortwave and near-infrared spectrum (see Figure \ref{fig:sentinel2}) with a spatial resolution ranging from 10 \si{\meter} to 60 \si{\meter} \cite{european_environment_agency_eu_2017}. Its cycle to revisit the same Earth's location is ten days, in comparison with the 16 days needed by Landsat-9 or 31 days from \acrshort{cbers}-4A.

\begin{figure}[!ht]
	\includegraphics{figs/introduction/sentinel2_bands.png}
	\caption{Three shortwave infrared bands acquired by Sentinel-2 (band 9, 935-955 \si{\micro\meter}, band 11, 1567-1658 \si{\micro\meter}, and band 12, 2114-2889 \si{\micro\meter}). }
    \label{fig:sentinel2}
\end{figure}

As observed, the main limitations of satellite datasets are their temporal and spatial resolution. However, these drawbacks are being mitigated nowadays, especially by commercial missions. For instance, the commercial satellite Pléiades Neo (VHR-2020) from Airbus Defense \& Space \cite{airbus_pleiades_2021} is able to daily acquire seven bands with a Ground Sampling Distance (\acrshort{gsd}) of up to 30 \si{\centi\meter} for panchromatic images. Even for governmental missions, the repeat cycle is typically reduced as several twin missions could be active at the same moment with similar instruments and orbits. Accordingly, the offset in Landsat-8 and Landsat-9 trajectories allows acquiring data from the same point every 8 days \cite{masek_landsat_2020}. On the other hand, spatial resolution is steadily improving as most governmental restrictions have been lifted. Yet, spaceborne platforms are the most expensive Remote sensing technology by a huge lead. 

In addition to satellite imaging, the use of \acrshort{uas} has considerably increased over the last decade and has attracted the interest of both researchers and commercial applications. The most frequent components of remote sensing \acrshort{uas} are imaging sensors as well as navigation and communication systems. The latter allows the operator to manoeuvre the platform within a communication range and transfer data in a bidirectional stream. Regardless of the navigating mission, either manual or waypoint-based, Global Positioning System (\acrshort{gps}) and Inertial Measurement Unit (\acrshort{imu}) sensors are used to calculate and record the craft's position, orientation and movement. These components are especially relevant to perform accurate flight missions, and their precision is generally described by means of vertical and horizontal error in meters (\si{\meter}). In this regard, the most recent \acrshort{uas} series from DJI achieve vertical and horizontal errors of up to 10 \si{\centi\meter} using Real-time Kinematic (\acrshort{rtk}) positioning rather than \acrshort{gps}. As one would expect, \acrshort{uas} with lightweight and high-performance navigation sensors are more prohibitive.    

In terms of safety regulations, the term \acrshort{uas} does not only refer to the vehicle but also to the coupled sensors, which are following introduced. Sensors typically coupled on aircraft vary according to the platform's flight altitude and cruising speed as well as application requirements. Fixed and rotating wing \acrshort{uas} solutions are considerably more limited in flying height and speed than other airborne vehicles, including helicopters and gyroplanes. Apart from the platform limitations, the flight altitude may be limited by \acrshort{uas} regulations to avoid entering the domain of other airspace vehicles. Therefore, sensors and missions that require lower speed, lower flight altitude, and thus higher precision, are especially convenient for \acrshort{uas}. A widespread example of this is the monitoring of transmission lines, which happens to have a very thin structure and therefore require slower mapping. In comparison, other technologies such as Interferometric synthetic aperture radar (\acrshort{insar}) have been mainly applied to spaceborne missions to track changes on the Earth's surface.
\marginnote[-2.9cm]{The operability of unmanned aircraft over the Single European Sky airspace is controlled in the European Union by regulation 2019/947. Among other rules, the maximum flight altitude is established as 120 \si{\meter} over the Earth's surface unless an obstacle is overflown.} 

\section{Aims and objectives}

According to previously presented challenges, the overall aim of this dissertation is to contribute to some of the stages presented in Figure \ref{fig:introduction_scheme}. A significant number of challenges have already been briefly reviewed, including the collection of datasets from multiple sensors, difficulties in the correction and fusion of these, the generation of products with larger dimensionality (e.g., 2D $\rightarrow$ 3D) or the analysis of final products. Accordingly, the objectives of this work are the following:
\begin{itemize}
    \item The correction and processing of data as collected by airborne surveys. Rather than a single problem, this objective involves studying how can be every data source corrected, including geometrical and radiometric distortions. The first must be solved in most cases, since it enables the later fusion of data, whereas the latter depends on whether the radiance is further analyzed or not.
    \item The matching of images collected by different sensors, thus overcoming differences concerning 1) triggering timestamps, 2) optical aberrations, 3) optical systems and 4) wavelength intervals. The image matching ought to work over images with notable intensity dissimilarity to obtain reliable transformations that allow projecting one data source into another. 
    \item The efficient generation of large and dense 3D point clouds from multiple data sources without geometric inaccuracies. The proposed pipeline ought to tackle most of the drawbacks of traditional photogrammetry. In addition, this is a time-consuming task that is further stressed by including multiple data sources; hence, it must be addressed using accelerated computing.
    \marginnote[-0.5cm]{Despite visible, multispectral, thermal infrared and hyperspectral datasets covering a wide part of the spectrum, these will be projected to a 3D baseline point cloud generated with photogrammetry. Image-based reconstructions are less reliable than other optical sensing products, such as \acrshort{lidar}. However, it is also more cost prohibitive than most of the revised optical imaging sensors.}
    \item To emulate \acrshort{lidar} sensors over 3D synthetic scenarios. Nonetheless, this is also a time-consuming task that is especially tough to model in terms of returned intensity. The efficient generation of large labelled \acrshort{lidar} datasets must be addressed, again, using accelerating computing, by modelling the surfaces' properties.
    \item To demonstrate the applicability of previous results. Though not exhaustive, this objective involves detecting anomalies, segmenting some items apart from others or phenotyping a large number of vegetation varieties. It can be either performed with traditional geometry-based techniques as well as using \acrshort{ai} algorithms.
\end{itemize}

\section{Organization}

This dissertation comprises six different parts which are following detailed:

\newcommand{\partTabSize}{1mm}

\small \noindent \textbf{\textls[30]{PART I}} \normalsize\hspace{\partTabSize} This part includes the current chapter. A shallow introduction to \acrshort{rs} is presented together with the main challenges concerning data fusion. Then, the fundamentals of this dissertation are presented in Chapter \ref{sec:fundamentals_rs}, and later, a review of the state-of-the-art is given in Chapter \ref{sec:context_rs} to introduce previous work on the fusion and simulation of \acrshort{uas}-based data.

\small \noindent \textbf{\textls[30]{PART II}} \normalsize\hspace{\partTabSize} Here, the fundamentals of our methods are presented, including which kind of sensors and data are available. The correction and the image fusion algorithms are isolated in Chapter \ref{sec:image_fusion}. In summary, this part comprises the materials and methods that are shared among most of this dissertation's chapters.

\small \noindent \textbf{\textls[30]{PART III}} \normalsize\hspace{\partTabSize} This part explains the generation of 3D point clouds consisting of multiple layers: visible, infrared, multispectral and hyperspectral data. The visible colour is presented as the baseline 3D point cloud that is enhanced with the rest of the datasets. First, thermographic imagery is fused with \acrshort{rgb} data to propose an efficient projection in the Graphics Processing Unit, the so-called \acrshort{gpu}. This procedure is following applied to multispectral data, despite this being significantly more time-consuming and intricate since the latter comprises four spectral bands. Then, a hyperspectral point cloud is generated following a different pipeline, yet accelerated, that fits the conditions on which these data were acquired.  

\small \noindent \textbf{\textls[30]{PART IV}} \normalsize\hspace{\partTabSize} Instead of correcting and fusing real-world datasets, others are simulated using detailed synthetic scenarios that are rapidly labelled with semantic tags. Simulations are aimed at modelling products similar to those collected by real sensors, including their geometrical properties. The fundamentals of \acrshort{lidar} simulations are reviewed in Chapter \ref{sec:lidar_simulation}, whereas radiance properties require further considerations which are detailed in Chapter \ref{sec:lidar_intensity}. Then, these simulations are used to assist in the scanning of indoor scenarios such as buildings in Chapter \ref{sec:lidar_optimization}.

\small \noindent \textbf{\textls[30]{PART V}} \normalsize\hspace{\partTabSize} This part shows multiple applications of the collected data and the derived results, including the simulated \acrshort{lidar} point clouds, reconstructed thermal point clouds and hyperspectral swaths. The first are Thermal infrared data are applied to the detection of thermal anomalies that may be linked to buried remains at an archaeological site. Hyperspectral swaths are, on the other hand, corrected and processed for the classification of red and white grapevine varieties using Deep Learning.

\small \noindent \textbf{\textls[30]{PART VI}} \normalsize\hspace{\partTabSize} Chapter \ref{sec:conclusions} concludes this dissertation by highlighting the main contributions and pointing out future work.