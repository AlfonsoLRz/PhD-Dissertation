\setchapterpreamble[u]{\margintoc}
\chapter{Introduction}
\labch{intro}

Sensing real-world processes help to monitor, predict and optimize the activities that depend on them and therefore act according to the analyzed data. Numerous devices are aimed at this task, which can be at least classified according to the sensing distance, the wavelength range captured by the device's detectors and the data acquisition mechanism, either passive or active. The latter two classifications are based on the device's functioning, as provided by the manufacturer. However, the former is up to researchers and consumers depending on their objectives and area of expertise, as well as on the budget, size and environmental conditions of the study area, among others. Despite contact-based sensors being implemented nowadays to measure variables such as humidity, temperature or rainfall amount, these require a high degree of maintenance and are more prone to deteriorate under adverse environmental conditions \cite{silva_low-cost_2019, morais_versatile_2021}. Instead of covering large areas, each one of the previous sensors communicates measurements of discrete and specific locations within and outside a network. The arranged layout, logically and physically, must be carefully designed for intra- and extra-communication involving nodes in different topological levels. Therefore, these technologies lack spatial awareness in large areas while being more appropriate to implement a real-time and constant monitoring system. 

\marginnote[.1cm]{\textbf{\gls{Remote sensing}} techniques overcome the spatial limitations of devices that measure data in contact with the target surface and phenomena.} 
Given the cited drawbacks, \textbf{\gls{Remote sensing}} (\acrfull{rsLabel}) techniques help to mitigate them by acquiring data from remote platforms. RS refers to the science and art of acquiring information about objects and phenomena without being in contact with them \cite{lillesand_remote_2015}. Similarly to the human eye, data is acquired from the environment in the form of impulses from light stimuli corresponding to some wavelength intervals. At least two components are involved in the reading process: sensors and the platform where they are mounted. The first will be the matter of discussion throughout this dissertation, whereas the latter was initially narrowed to satellite platforms. The term \gls{Remote sensing} was first coined in the 1960s by Evelyn Pruitt during her work at the US Office of Naval Research to refer to satellite and aircraft instrumentation to measure reflected and emitted radiation. However, topographic mapping was first suggested in 1849 and attempted in 1858 by F. Tournachon from a captive balloon over France. From here, the camera size was reduced without requiring to carry a darkroom. Other airborne platforms were explored, either suggested by war needs or simply innovation, ranging from kite-camera systems (1906; see Figure \ref{fig:san_francisco_kite}), pigeons (1909) and aircrafts (1908).

\begin{figure}[!ht]
	\includegraphics{figs/introduction/san_francisco_kitecamera.jpg}
	\caption{Photograph of San Francisco after the 1906 earthquake, taken from a camera held by seven kites. }
    \label{fig:san_francisco_kite}
\end{figure}

Satellite imaging, as known today, was first possible by the ideation of Konstantin Tsiolkovsky about rocketry to explore space, published under the title \textit{Exploring Space using jet propulsion devices}. This idea resulted in the first successful launch of the Sputnik satellite in 1957 and led to the development of Earth-orbiting satellites aimed at atmospherical monitoring. The Television and Infrared Observation Satellite (TIROS) was made operational in 1960 and integrated a small infrared system and a narrow-angle camera capturing data in the visible wavelengths. In 1978, a breakthrough change in satellites and coupled sensors came with TIROS-N (N for new). This spacecraft integrated a very high-resolution radiometer with 1 \si{\kilo\meter} footprint and four channels (visible, near-infrared, midrange-infrared and thermal infrared). Nowadays, an advanced version of these satellites is still operative under the name of AVHRR-3 (Advanced Very High-Resolution Radiometer) with a similar footprint, though capturing six channels instead of four \cite{national_oceanic_and_atmospheric_administration_avhrr3_nodate}.   

\marginnote[.1cm]{An extensive repository of recent satellite missions can be found in the Earth Observation Portal, maintained by the European Space Agency \cite{earth_observation_portal_earth_nodate}.} 
From currently operative satellite programs, Landsat provides the largest collection of continuously acquired \gls{Remote sensing} data. It was first tested as part of NASA's NIMBUS program and there are currently two active satellites, Landsat 8 and Landsat 9, while the other seven have been terminated or planned to be decommissioned (Landsat 7). Landsat 9 payload consists of two instruments: Operational Land Imager (OLI-2) and the Thermal Infrared Sensor (TIRS-2). A total of 11 bands and 740 scenes are collected every day, including red, blue, green, near-infrared, shortwave infrared, thermal, panchromatic, coastal and cirrus bands, ranging from a resolution of 15 \si{\meter} to 100 \si{\meter}. Other notable satellite programs are the China–Brazil Earth Resources Satellite (CBERS) and Copernicus programme financed by the European Commission. The CBERS-04A satellite is equipped with three imaging tools capturing five different spectrum intervals with a spatial resolution of 2\si{\meter}-55 \si{\meter} \cite{instituto_nacional_de_pesquisas_espaciais_inpecbers_2019}. On the other hand, the Sentinel-2 mission acquires 13 bands in the visible, shortwave and near-infrared spectrum (see Figure \ref{fig:sentinel2}) with a spatial resolution ranging from 10 \si{\meter} to 60 \si{\meter} \cite{european_environment_agency_eu_2017}. Its cycle to revisit the same Earth's point is ten days, in comparison with the 16 days needed by Landsat 9 or 31 days from CBERS-04A.

\begin{marginfigure}[-1cm]
	\includegraphics{figs/introduction/landsat8_lapalma.jpg}
	\caption{Cumbre Vieja volcano eruption observed from Landsat-8 \cite{nasa_earth_observatory_lava_2021}.}
	\label{fig:la_palma_landsat8}
\end{marginfigure}

\begin{figure}[!ht]
	\includegraphics{figs/introduction/sentinel2_bands.png}
	\caption{Three shortwave infrared bands acquired by Sentinel-2 (band 9, 935-955 \si{\micro\meter}, band 11, 1567-1658 \si{\micro\meter}, and band 12, 2114-2889 \si{\micro\meter}). }
    \label{fig:sentinel2}
\end{figure}

Therefore, satellite imaging is better suited for the monitoring of changes over large areas using time series spanned over months and years. These data help to understand the dynamics of human and nature interaction as well as the impact of natural phenomena (Figure \ref{fig:la_palma_landsat8}). Some applications of the large collections of available satellite data are the analysis of land use, deforestation, land changes and urban settlements \cite{asokan_change_2019}. Nevertheless, the spatial resolution and revisit period of non-commercial satellite programmes harden their applicability to fine-grained monitoring tasks. The level of detail (LOD) of these tasks can refer to spatial resolution, temporal resolution or both, which are the main limitations of satellite imaging besides their cost. In spite of the described drawbacks, the use of satellite imagery is on the rise due to the steady reduction of Ground Sampling Distance (GSD) and low periods for revisiting the same points (see Figure \ref{fig:scopus_search_platforms}). For instance, the commercial satellite Pléiades Neo (VHR-2020) from Airbus Defense \& Space \cite{airbus_pleiades_2021} is able to daily acquire seven bands with a GSD of 30 \si{\centi\meter}. Even for governmental missions, the repeat cycle is typically reduced as several twin missions could be active at the same moment with similar instruments and orbits. Accordingly, the offset in Landsat-8 and Landsat-9 trajectories allows acquiring data from the same point every 8 days \cite{masek_landsat_2020}. Still, spaceborne platforms are the most expensive by a huge lead.

\begin{figure}[!ht]
	\includegraphics[width=\linewidth]{figs/introduction/platform_timeline.png}
	\caption{Number of manuscripts related to different \gls{Remote sensing} platforms. The Scopus searches were the following: $(p_1 \lor p_2 ... \lor p_n) \land (\textit{remote} \hspace{1mm} \land \hspace{1mm} \textit{sensing})$, with $p_i$ being the platforms corresponding to the vehicles depicted in each legend group. }
    \label{fig:scopus_search_platforms}
\end{figure}

\begin{marginfigure}[.7cm]
	\includegraphics{figs/introduction/dji300.png}
	\caption{Quadcopter Matrice 300 RTK coupled with a dual RGB-thermal sensor (Zenmuse H20T). }
	\label{fig:dji300}
\end{marginfigure}
However, it was not until recently that satellite resolution was not restricted by governments. These restrictions as well as the described limitations led to the use of alternative platforms. Besides satellites, airborne (fixed-wing and helicopters), Unmanned Aerial Systems (UAS) (Figure \ref{fig:dji300}) and other land platforms (mobile and static proximal-sensing) are the most frequent \cite{lillesand_remote_2015}. The choice of any of them always involves trade-offs concerning manoeuvrability, land coverage, repeat coverage, spatial resolution, spatial accuracy, cost or field of view (FOV). However, UAS have been gaining interest in the last decade as they transitioned from military tools in the early 2000s to easy-to-deploy, small and low-cost systems, thus widening their applicability to civilian activities and research. These platforms range from hand-sized to large aircraft that can be either controlled by human intervention or be partially, even fully, autonomous. Figure \ref{fig:dji300} shows a standard UAS designed by DJI which can carry up to 2.7\si{\kilo\gram}, with dimensions of 501 × 403 × 252 \si{\milli\meter}.

\begin{figure}[!ht]
	\includegraphics{figs/introduction/introduction_scheme.png}
	\caption{Overall procedure to acquire data with remote sensing platforms and sensors. Firstly, sensors are coupled on spaceborne, airborne and terrestrial vehicles or operated by humans, either by carrying them as a backpack or in hand. Then, sensor products can be grouped into graphical and numerical results, though most of them produce both kinds of data. Finally, the sensing products are processed and interpreted to provide users with valuable analyses. }
    \label{fig:introduction_scheme}
\end{figure}

The most frequent components of remote sensing UAS are imaging sensors as well as navigation and communication systems. The latter allows the operator to manoeuvre the platform within a communication range and transfer data in a bidirectional stream. Regardless of the navigating mission, either manual or waypoint-based, Global Positioning System (GPS) and Inertial Measurement Unit (IMU) sensors are used to calculate and record the craft's position, orientation and movement. These components are especially relevant to perform accurate flight missions, and their precision is generally described by means of vertical and horizontal error in meters (\si{\meter}). In this regard, the most recent UAS series from DJI achieve vertical and horizontal errors of up to 1\si{\meter} using Real-time Kinematic positioning rather than GPS. As would be expected, UAS with lightweight and high-performance navigation sensors are more prohibitive than those with coarse navigation.    

\marginnote[.1cm]{The operability of unmanned aircraft over the Single European Sky airspace is regulated in the European Union by regulation 2019/947. Among other rules, the maximum flight altitude is established as 120\si{\meter} over the Earth's surface unless an obstacle is overflown.} 
In terms of safety regulations, the term UAS does not only refer to the vehicle but also to the coupled sensors, which are following introduced. Sensors typically coupled on aircraft vary according to the platform's flight altitude and cruising speed as well as application requirements. Fixed and rotating wing UAS solutions are considerably more limited in flying height and speed than other airborne vehicles, including helicopters and gyroplanes. Apart from platform limitations, the flight altitude may be limited by UAS regulations to avoid entering the domain of other airspace vehicles. Therefore, sensors and missions that require lower speed, lower flight altitude, and thus higher precision, are especially convenient for UAS. A widespread example of this in \gls{Remote sensing} is the monitoring of transmission lines, which happens to have a very thin structure and thus require slower mapping. In comparison, other technologies such as InSAR (Interferometric synthetic aperture radar) have been mainly applied to space-borne missions to track changes on Earth's surface.

Therefore, UAS represent a cost-efficient tool for acquiring high-resolution data. The most common sensors mounted on UAS are cameras and LiDAR (Light Detection and Ranging) sensors, with the first comprising a large number of imaging devices. Similarly, these groups also lead to the distinction of passive and active sensors. While the second have both transmitter and receiver components, the first are only aimed at capturing incoming radiance from the scene surfaces. However, the acquisition of multiple high-precision data poses several advantages and drawbacks. 

\marginnote[.1cm]{The digitization of assets, processes and systems is greatly helped by reconstructions from sensor data. These allow connecting virtual and physical replicas through a data stream, thus shaping the fundamentals of \textbf{Digital twins}.}
Firstly, observations from multiple sensors can be interpreted as different features that complete a knowledge-based system, with each one providing information within a wavelength range. Furthermore, most of the algorithms aimed at analyzing and drawing conclusions from sensor data benefit from the use of complementary features. The contribution of each one in the extracted conclusions can be adjusted through weights that determine how important are these for the analysis. Unless a large number of features are used, which can lead to the curse of dimensionality problem, it is safe to say that the greater number of features, the better. On the other hand, high-resolution data help to generate precise models that are easier to analyze and visualize by human operators. Denser data also involves denser geometry and reconstructions, thus omitting fewer details of target surfaces and easing the construction of digital models that emulate Earth's scenarios and processes. 

Different layers of information are ideally noise-free and represented under the same coordinate system. In the reality, navigation sensors present small positioning errors that harden the fusion of several layers of information. Despite being small, these are noted as shifts, rotation and scale variations among different sensor data. Furthermore, environmental conditions such as atmospheric composition, wind, temperature or solar radiation can vary from one flight to another and so do the acquired data, even under similar flight plans. These changes not only affect observations performed in a short period of time with different sensors but also time series acquired with previously used tools. Despite being tedious, variations concerning positioning and orientation errors can be diminished by including reference data measured with precise sensors in easily recognizable image features. Other shortcomings are the noise induced by faulty device detectors, especially for active sensors, unwanted atmospheric particles and surfaces whose bouncing behaviour leads to unreal geometry. Therefore, a system capable of accurately fusing the outcome of every sensor integrated with a UAS is necessary to facilitate the later processing stages and provide reliable conclusions. 

Sensing products are typically transformed into other data representations that are not provided by the sensor. Accordingly, individual images are not sufficient to interpret the scenario in some contexts. A complete vision of it can be provided by joining images, thus resulting in 2D maps, and 3D points calculated by estimating the camera poses and finding features in common among several images. During this transformation process, estimated radiometric and geometric properties can suffer of precision loss on the interpolation, and thus, a comparison can be established with other reliable data sources. For instance, LiDAR is known to generate accurate point clouds that can be used as the source for measuring the quality of point clouds reconstructed from imagery. Similarly, images can serve as the source data for measuring radiometric distances.   

On the other hand, large volumes of data are also harder to operate in terms of computing, storage and visualization. Thousands of images or millions of points can be hardly operated in commodity hardware due to the required storage and computing capabilities. However, current trends in informatics have favoured the proliferation of personal and professional computers with large storage capacity, more efficient access to data and great multi-threading capabilities. The latter can be performed over the Central Processing Unit (CPU), whose trend is to increase the number of cores, and the Graphic Processing Unit (GPU), composed of millions of units that can massively solve small tasks and cooperate among small thread groups. Section x delves into the most common data formats in \gls{Remote sensing}; at this moment, it is enough to think of data as large volumes of digital numbers (DN). Despite large storage being inexpensive nowadays, challenges such as the partial retrieval of information remain, especially for interactive applications with high data throughput \cite{bejar-martos_strategies_2022, ogayar-anguita_nested_2023}. 

\begin{marginfigure}[.5cm]
	\includegraphics{figs/introduction/hintze.png}
	\caption{Point cloud with 2.4M points reconstructed using 900 photos at the Hintze Hall (Model uploaded by \textit{Thomas Flynn} in \textit{Sketchfab}).  }
	\label{fig:hintze_hall}
\end{marginfigure}
Another drawback in real-time applications is the visualization of these large volumes of data. Traditional rendering pipelines are built as a set of static stages, where the model's geometry and topology are iteratively transformed into pixels. The colours of these come from the shading of one or multiple light sources over the textures of a surface, using a discretization of the formulae that describe light interaction. Unlike synthetic models designed by human operators, most of the sensor data are shaded according to the recorded wavelengths. Thus, alternative pipelines are needed to visualize these data. According to the architecture of GPUs, the data can be sorted and organized in data structures in pursuit of workload balancing and geometry simplifications that help to trick user perception. These requirements are even tougher for Virtual Reality (VR) devices that render every scenario at least once for each eye, though it can be increased if other rendering techniques are included, e.g., shadow mapping.  

\marginnote[2cm]{This dissertation has Precision Agriculture as its main research field and therefore, the data analysis has been narrowed to 1) segment crops into ground and vegetation, and 2) phenotyping of a large number of plant varieties in a non destructive way.}
Previous methods are simply the procedure that allows extracting text-based or visual clues that help in the recognition of weaknesses and failures in processes. Accordingly, the last step is to classify, segment and recognize features from input data to optimize future operations. The main contributions of \gls{Remote sensing} techniques in Precision Agriculture are  yield estimation, crop-type inventory, measuring of water content, leaf area index (LAI) and control of diseases and insects as well as moisture, changes, growth, stress and drought monitoring, among other risk factors such as snow or fire \cite{huang_agricultural_2018}. These monitoring tasks are intended to maximize profitability and minimize waste and pollution. 

A significant number of datasets have been published with the steadily increasing importance of imagery classification using Machine Learning (ML) and Deep Learning (DL) methods from Artificial Intelligence (AI). Thus, the analysis step is nowadays easier than ever to implement using large datasets from where AI methods learn. However, most of large \gls{Remote sensing} image collections are based on spaceborne sensors capturing any kind of scenario, rather than being focused on Precision Agriculture. Another challenge arises from the labelling of these large collections, as it must be performed by unsupervised algorithms, which transform and extract relevant features, or human operators \cite{li_image_2021, basu_deepsat_2015}. Neither method is perfect and thus can result in spurious labelling data that can mislead the learning algorithms. Furthermore, manual tagging is typically done over visible bands, which are easier to interpret for humans, and therefore, only a few datasets include bands further than R, G, B. In this regard, hyperspectral sensors mitigate the latter problem as they co-acquire a large number of bands from which we can extract a false color image that imitates an RGB product.

Although there exist some notable datasets for \gls{Remote sensing} and a wide varieties of sensors and spectral bands, these may not be valid for case studies that fall out of state-of-the-art trends. It is possible to conduct a large number of surveys and create datasets on our own. However, it is incredibly time-consuming to acquire, process and extract features, including labels, as these manual tasks require computational resources and human operators. At least partially, data from sensors could be synthetically generated by emulating the sensor mechanism. The main advantage is that data acquired from a virtual sensor are linked to digital models without uncertainty. Likewise, digital models are augmented with features that could be directly transferred to the sensing results. The most frequently simulated sensors are Radar and LiDAR systems, though synthetic images have also been recently generated with the rise of DL approaches capable of generating new data.  

\section{Aims and objectives}

\section{Organization}

This dissertation is organized into the following four chapters:

\small \textls[30]{PART I} \normalsize\hspace{3mm} This part is the current chapter. A shallow introduction to \gls{Remote sensing} is presented together with the main challenges concerning data fusion. Then, a review of the state-of-the-art is given in \nameref{sec:fundamentals_rs} to introduce previous work on the fusion and simulation of UAV-based data.

\small \textls[30]{PART I} \normalsize\hspace{3mm} Here, the work of this dissertation is presented in multiple chapters. In \nameref{sec:thermography}, a method for the efficient generation of dense thermographic point clouds is proposed. First, the fusion of visible and thermographic images is explained, and then, the point cloud is generated from the previously processed images.   